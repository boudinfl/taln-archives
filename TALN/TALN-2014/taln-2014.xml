<?xml version="1.0" encoding="UTF-8"?>
<!--
	pas d'auteurs/affiliations : taln-2014-long-017, taln-2014-long-021, taln-2014-long-032, taln-2014-court-018, taln-2014-court-032, taln-2014-demo-002
	pas de numéros de pages : taln-2014-court-003, taln-2014-court-015, taln-2014-court-022, taln-2014-court-034, taln-2014-demo-002, taln-2014-demo-005, taln-2014-demo-007
	problèmes avec les articles des sessions posters étiquetage 1 et 2
	problèmes encodage : taln-2014-court-021
-->
<conference>
	<edition>
		<acronyme>TALN'2014</acronyme>
		<titre>21e conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Marseille</ville>
		<pays>France</pays>
		<dateDebut>2014-07-01</dateDebut>
		<dateFin>2014-07-04</dateFin>
		<presidents>
			<president>
				<prenom>Philippe</prenom>
				<nom>Blache</nom>
			</president>
			<president>
				<prenom>Frédéric</prenom>
				<nom>Béchet</nom>
			</president>
		</presidents>
		<editeurs>
			<editeur>
				<prenom>Brigitte</prenom>
				<nom>Bigi</nom>
			</editeur>
		</editeurs>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<siteWeb>http://www.taln2014.org</siteWeb>
	</edition>
	<articles>
		<article id="taln-2014-long-001" session="Fouille de données et TAL">
			<auteurs>
				<auteur>
					<prenom>Morgane</prenom>
					<nom>Marchand</nom>
					<email>morgane.marchand@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Mesnard</nom>
					<email>olivier.mesnard@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>anne.vilnat@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus Centre Nano-Innov Saclay, 91191 Gif-sur-Yvette Cedex</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, Université Paris-Sud, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Influence des marqueurs multi-polaires dépendant du domaine pour la fouille d’opinion au niveau du texte</titre>
			<type>long</type>
			<pages>1-12</pages>
			<resume>Les méthodes de détection automatique de l’opinion dans des textes s’appuient sur l’association d’une polarité d’opinion aux mots des textes, par lexique ou par apprentissage. Or, certains mots ont des polarités qui peuvent varier selon le domaine thématique du texte. Nous proposons dans cet article une étude des mots ou groupes de mots marqueurs d’opinion au niveau du texte et qui ont une polarité changeante en fonction du domaine. Les expériences, effectuées à la fois sur des corpus français et anglais, montrent que la prise en compte de ces marqueurs permet d’améliorer de manière significative la classification de l’opinion au niveau du texte lors de l’adaptation d’un domaine source à un domaine cible. Nous montrons également que ces marqueurs peuvent être utiles, de manière limitée, lorsque l’on est en présence d’un mélange de domaines. Si les domaines ne sont pas explicites, utiliser une séparation automatique des documents permet d’obtenir les mêmes améliorations.</resume>
			<mots_cles>Fouille d’opinion, adaptation au domaine, marqueurs multi-polaires</mots_cles>
			<title></title>
			<abstract>In this article, we propose a study on the words or multi-words which are good indicators of the opinion polarity of a text but have different polarity depending on the domain. We have performed experiments on French and English corpora, which show that taking these multi-polarity words into account improve the opinion classification at text level in a domain adaptation framework. We also show that these words are useful when the corpus contains several domains. If these domains are not explicit, using a automatic domain characterization (e.g. wich Topic Modeling approaches) allows to achieve the same results.</abstract>
			<keywords>Opinion mining, domain adaptation, multi-polarity markers</keywords>
		</article>
		<article id="taln-2014-long-002" session="Fouille de données et TAL">
			<auteurs>
				<auteur>
					<prenom>Adrien</prenom>
					<nom>Bougouin</nom>
					<email>adrien.bougouin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Florian</prenom>
					<nom>Boudin</nom>
					<email>florian.boudin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Daille</nom>
					<email>beatrice.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA – UMR CNRS 6241, 2 rue de la Houssinière 44322 Nantes Cedex 3, France</affiliation>
			</affiliations>
			<titre>Influence des domaines de spécialité dans l’extraction de termes-clés</titre>
			<type>long</type>
			<pages>13-24</pages>
			<resume>Les termes-clés sont les mots ou les expressions polylexicales qui représentent le contenu principal d’un document. Ils sont utiles pour diverses applications, telles que l’indexation automatique ou le résumé automatique, mais ne sont pas toujours disponibles. De ce fait, nous nous intéressons à l’extraction automatique de termes-clés et, plus particulièrement, à la difficulté de cette tâche lors du traitement de documents appartenant à certaines disciplines scientifiques. Au moyen de cinq corpus représentant cinq disciplines différentes (archéologie, linguistique, sciences de l’information, psychologie et chimie), nous déduisons une échelle de difficulté disciplinaire et analysons les facteurs qui influent sur cette difficulté.</resume>
			<mots_cles>Extraction de termes-clés, articles scientifiques, domaines de spécialité, méthodes non-supervisées</mots_cles>
			<title></title>
			<abstract>Keyphrases are single or multi-word expressions that represent the main content of a document. Keyphrases are useful in many applications such as document indexing or text summarization. However, most documents are not provided with keyphrases. To tackle this problem, researchers propose methods to automatically extract keyphrases from documents of various nature. In this paper, we focus on the difficulty of automatic keyphrase extraction in scientific papers from various areas. Using five corpora representing five areas (archaeology, linguistics, information sciences, psychology and chemistry), we observe the difficulty scale and analyze factors inducing a higher or a lower difficulty</abstract>
			<keywords>Keyphrase extraction, scientific papers, specific domain, unsupervised methods</keywords>
		</article>
		<article id="taln-2014-long-003" session="Fouille de données et TAL">
			<auteurs>
				<auteur>
					<prenom>Emanuela</prenom>
					<nom>Boros</nom>
					<email>emanuela.boros@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>brigitte.grau@limsi.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, F-91191, Gif-sur-Yvette</affiliation>
				<affiliation affiliationId="2">LIMSI, Rue John von Neumann, Campus Universitaire d’Orsay, F-91405 Orsay cedex</affiliation>
				<affiliation affiliationId="3">ENSIIE, 1 square de la résistance F-91025 Évry cedex</affiliation>
			</affiliations>
			<titre>Étiquetage en rôles événementiels fondé sur l’utilisation d’un modèle neuronal</titre>
			<type>long</type>
			<pages>25-35</pages>
			<resume>Les systèmes d’extraction d’information doivent faire face depuis toujours à une double difficulté : d’une part, ils souffrent d’une dépendance forte vis-à-vis du domaine pour lesquels ils ont été développés ; d’autre part, leur coût de développement pour un domaine donné est important. Le travail que nous présentons dans cet article se focalise sur la seconde problématique en proposant néanmoins une solution en relation avec la première. Plus précisément, il aborde la tâche d’étiquetage en rôles événementiels dans le cadre du remplissage de formulaire (template filling) en proposant pour ce faire de s’appuyer sur un modèle de représentation distribuée de type neuronal. Ce modèle est appris à partir d’un corpus représentatif du domaine considéré sans nécessiter en amont l’utilisation de prétraitements linguistiques élaborés. Il fournit un espace de représentation permettant à un classifieur supervisé traditionnel de se dispenser de l’utilisation de traits complexes et variés (traits morphosyntaxiques, syntaxiques ou sémantiques). Par une série d’expérimentations menées sur le corpus de la campagne d’évaluation MUC-4, nous montrons en particulier que cette approche permet de dépasser les performances de l’état de l’art et que cette différence est d’autant plus importante que la taille du corpus d’entraînement est faible. Nous montrons également l’intérêt de l’adaptation de ce type de modèle au domaine traité par rapport à l’utilisation de représentations distribuées à usage générique.</resume>
			<mots_cles>Extraction d’information, extraction de rôles événementiels, modèles de langage neuronaux</mots_cles>
			<title></title>
			<abstract>Information Extraction systems must cope with two problems : they heavily depend on the considered domain but the cost of development for a domain-specific system is important. We propose a new solution for role labeling in the event-extraction task that relies on using unsupervised word representations (word embeddings) as word features. We automatically learn domain-relevant distributed representations from a domain-specific unlabeled corpus without complex linguistic processing and use these features in a supervised classifier. Our experimental results on the MUC-4 corpus show that this system outperforms state-of-the-art systems on this event extraction task, especially when the amount of annotated data is small.We also show that using word representations induced on a domain-relevant dataset achieves better results than using more general word embeddings.</abstract>
			<keywords>Information extraction, event role filler detection, neural language models</keywords>
		</article>
		<article id="taln-2014-long-004" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>William</prenom>
					<nom>Léchelle</nom>
					<email>lechellw@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">DIRO, Université de Montréal</affiliation>
			</affiliations>
			<titre>Utilisation de représentations de mots pour l’étiquetage de rôles sémantiques suivant FrameNet</titre>
			<type>long</type>
			<pages>36-45</pages>
			<resume>D’après la sémantique des cadres de Fillmore, les mots prennent leur sens par rapport au contexte événementiel ou situationnel dans lequel ils s’inscrivent. FrameNet, une ressource lexicale pour l’anglais, définit environ 1000 cadres conceptuels couvrant l’essentiel des contextes possibles. Dans un cadre conceptuel, un prédicat appelle des arguments pour remplir les différents rôles sémantiques associés au cadre. Nous cherchons à annoter automatiquement ces rôles sémantiques, étant donné le cadre sémantique et le prédicat, à l’aide de modèles à maximum d’entropie. Nous montrons que l’utilisation de représentations distribuées de mots pour situer sémantiquement les arguments apporte une information complémentaire au modèle, et améliore notamment l’étiquetage de cadres avec peu d’exemples d’entrainement</resume>
			<mots_cles>rôles sémantiques, représentations distribuées, maximum d’entropie</mots_cles>
			<title></title>
			<abstract>According to Frame Semantics (Fillmore 1976), words’ meaning are best understood considering the semantic frame they play a role in, for the frame is what gives them context. FrameNet defines about 1000 such semantic frames, along with the roles arguments can fill in this frame. Our task is to automatically label arguments’ roles, given their span, the frame, and the predicate, using maximum entropy models. We make use of distributed word representations to improve generalisation over the few training exemples available for each frame.</abstract>
			<keywords>semantic role labelling, distributed word representations</keywords>
		</article>
		<article id="taln-2014-long-005" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Lonneke</prenom>
					<nom>van der Plas</nom>
					<email>Lonneke.vanderPlas@ims.uni-stuttgart.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marianna</prenom>
					<nom>Apidianaki</nom>
					<email>Marianna.Apidianaki@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IMS, Pfaffenwaldring 5B, 70569 Stuttgart, Germany</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, Rue John von Neumann, Campus Universitaire d’Orsay Bât 508, 91405 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>46-55</pages>
			<resume>Nous abordons la question du transfert d’annotations sémantiques, et plus spécifiquement d’étiquettes sur les prédicats, d’une langue à l’autre sur la base de corpus parallèles. Des travaux antérieurs ont transféré ces annotations directement au niveau des tokens, conduisant à un faible rappel. Nous présentons une approche globale de transfert qui agrège des informations repérées dans l’ensemble du corpus parallèle. Nous montrons que la performance de la méthode globale est supérieure aux résultats antérieurs en termes de rappel sans trop affecter la précision.</resume>
			<mots_cles>transfert inter-langue, annotation sémantique automatique, prédicats, désambiguïsation lexicale, corpus parallèles</mots_cles>
			<title>Cross-lingualWord Sense Disambiguation for Predicate Labelling of French</title>
			<abstract>We address the problem of transferring semantic annotations, more specifically predicate labellings, from one language to another using parallel corpora. Previous work has transferred these annotations directly at the token level, leading to low recall. We present a global approach to annotation transfer that aggregates information across the whole parallel corpus.We show that this global method outperforms previous results in terms of recall without sacrificing precision too much.</abstract>
			<keywords>cross-lingual transfer, automatic semantic annotation, predicates, Word Sense Disambiguation, parallel corpora</keywords>
		</article>
		<article id="taln-2014-long-006" session="Parsing 1">
			<auteurs>
				<auteur>
					<prenom>Assaf</prenom>
					<nom>Urieli</nom>
					<email>assaf.urieli@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS: CNRS &amp; Université de Toulouse, Toulouse, France</affiliation>
				<affiliation affiliationId="2">Joliciel Informatique SARL, 2 avenue du Cardié, 09000 Foix, France</affiliation>
			</affiliations>
			<titre>Améliorer l’étiquetage de “que” par les descripteurs ciblés et les règles</titre>
			<type>long</type>
			<pages>56-66</pages>
			<resume>Les outils TAL statistiques robustes, et en particulier les étiqueteurs morphosyntaxiques, utilisent souvent des descripteurs “pauvres”, qui peuvent être appliqués facilement à n’importe quelle langue, mais qui ne regarde pas plus loin que 1 ou 2 tokens à droite et à gauche et ne prennent pas en compte des classes d’équivalence syntaxiques. Bien que l’étiquetage morphosyntaxique atteint des niveaux élevés d’exactitude (autour de 97 %), les 3 % d’erreurs qui subsistent induisent systématiquement une baisse de 3 % dans l’exactitude du parseur. Parmi les phénomènes les plus faciles à cibler à l’aide de l’injection de connaissances linguistiques plus riches sont les mots fonctionnels ambigus, tels que le mot “que” en français. Dans cette étude, nous cherchons à améliorer l’étiquetage morphosyntaxique de “que” par l’utilisation de descripteurs ciblés et riches lors de l’entraînement, et par l’utilisation de règles symboliques qui contournent le modèle statistique lors de l’analyse. Nous atteignons une réduction du taux d’erreur de 45 % par les descripteurs riches, et de 55 % si on ajoute des règles.</resume>
			<mots_cles>étiquetage morphosyntaxique, apprentissage automatique supervisé, descripteurs riches, systèmes statistiques robustes</mots_cles>
			<title></title>
			<abstract>Robust statistical NLP tools, and in particular pos-taggers, often use knowledge-poor features, which are easily applicable to any language but do not look beyond 1 or 2 tokens to the right and left and do not make use of syntactic equivalence classes. Although pos-tagging tends to get high accuracy scores (around 97%), the remaining 3% errors systematically result in a 3% loss in parsing accuracy. Some of the easiest phenomena to target via the injection of richer linguistic knowledge are ambiguous function words, such as “que” in French. In this study, we attempt to improve the pos-tagging of “que” through the use of targeted knowledge-rich features during training, and symbolic rules which override the statistical model during analysis. We reduce the error rate by 45% using targeted knowledge-rich features, and 55% if we add rules.</abstract>
			<keywords>pos-tagging, supervised machine learning, knowledge-rich features, robust statistical systems</keywords>
		</article>
		<article id="taln-2014-long-007" session="Parsing 1">
			<auteurs>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Villemonte de la Clergerie</nom>
					<email>Eric.De_La_Clergerie@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA - Rocquencourt - B.P. 105 78153 Le Chesnay Cedex, FRANCE</affiliation>
			</affiliations>
			<titre>Jouer avec des analyseurs syntaxiques</titre>
			<type>long</type>
			<pages>67-78</pages>
			<resume>Nous présentons DYALOG-SR, un analyseur syntaxique statistique par dépendances développé dans le cadre de la tâche SPRML 2013 portant sur un jeu de 9 langues très différentes. L’analyseur DYALOG-SR implémente un algorithme d’analyse par transition (à la MALT), étendu par utilisation de faisceaux et de techniques de programmation dynamique. Une des particularité de DYALOG-SR provient de sa capacité à prendre en entrée des treillis de mots, particularité utilisée lors de SPMRL13 pour traiter des treillis en Hébreu et reprise plus récemment sur des treillis produits par SXPIPE pour le français. Disposant par ailleurs avec FRMG d’un analyseur alternatif pour le français, nous avons expérimenté un couplage avec DYALOG-SR, nous permettant ainsi d’obtenir les meilleurs résultats obtenus à ce jour sur le French TreeBank.</resume>
			<mots_cles>Analyse syntaxique, Analyse syntaxique par dépendances, faisceaux, Programmation Dynamique, Treillis de mots, Couplage d’analyseurs</mots_cles>
			<title></title>
			<abstract>We present DYALOG-SR, a statistical dependency parser developed for the SPRML 2013 shared task over 9 very different languages. DYALOG-SR implements a shift-reduce parsing algorithm (a la MALT), extended with beams and dynamic programming techniques. One of the specificities of DYALOG-SR is its ability to handle word lattices as input, which was used for handling Hebrew lattices and more recently French ones produced by SXPIPE. Having access to FRMG, an alternative parser for French, we also tried a coupling with DYALOG-SR, providing us the best results so far on the French TreeBank</abstract>
			<keywords>Parsing, Dependency Parsing, Beams, Dynamic Programming, Word Lattice, Parser coupling</keywords>
		</article>
		<article id="taln-2014-long-008" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Alain</prenom>
					<nom>Polguère</nom>
					<email>alain.polguere@univ-lorraine.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF, CNRS &amp; Université de Lorraine, 44 av. de la Libération, BP 30687 , 54063 Nancy Cedex</affiliation>
			</affiliations>
			<titre>Principes de modélisation systémique des réseaux lexicaux</titre>
			<type>long</type>
			<pages>79-90</pages>
			<resume>Nous présentons une approche de la construction manuelle des ressources lexicales à large couverture fondée sur le recours à un type particulier de réseau lexical appelé système lexical. En nous appuyant sur l’expérience acquise dans le cadre de la construction du Réseau Lexical du Français (RL-fr), nous offrons tout d’abord une caractérisation formelle des systèmes lexicaux en tant que graphes d’unités lexicales de type « petits mondes » principalement organisés à partir du système des fonctions lexicales Sens-Texte. Nous apportons ensuite des arguments pour justifier la pertinence du modèle proposé, tant du point de vue théorique qu’applicatif.</resume>
			<mots_cles>système lexical, base de données lexicale, structure du lexique, réseau lexical, graphe petit monde, proxémie, Lexicologie Explicative et Combinatoire, fonction lexicale, Réseau Lexical du Français (RL-fr)</mots_cles>
			<title></title>
			<abstract>We introduce a new approach for manually constructing broad-coverage lexical ressources based on a specific type of lexical network called lexical system. Drawing on experience gained from the construction of the French Lexical Network (fr-LN), we begin by formally characterizing lexical systems as “small-world” graphs of lexical units that are primarily organized around the system of Meaning-Text lexical functions. We then give arguments in favor of the proposed model that are both theory- and application-oriented.</abstract>
			<keywords>lexical system, lexical database, structure of the lexicon, lexical network, small-world graph, proxemy, Explanatory Combinatorial Lexicology, lexical function, French Lexical Network (fr-LN)</keywords>
		</article>
		<article id="taln-2014-long-009" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Núria</prenom>
					<nom>Gala</nom>
					<email>nuria.gala@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thomas</prenom>
					<nom>François</nom>
					<email>tfrancois@uclouvain.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>dbernhard@unistra.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cédrick</prenom>
					<nom>Fairon</nom>
					<email>cfairon@uclouvain.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF-CNRS UMR 7279, Aix Marseille Université</affiliation>
				<affiliation affiliationId="2">CENTAL, Université Catholique de Louvain</affiliation>
				<affiliation affiliationId="3">LILPA, Université de Strasbourg</affiliation>
			</affiliations>
			<titre>Un modèle pour prédire la complexité lexicale et graduer les mots</titre>
			<type>long</type>
			<pages>91-102</pages>
			<resume>Analyser la complexité lexicale est une tâche qui, depuis toujours, a principalement retenu l’attention de psycholinguistes et d’enseignants de langues. Plus récemment, cette problématique a fait l’objet d’un intérêt grandissant dans le domaine du traitement automatique des langues (TAL) et, en particulier, en simplification automatique de textes. L’objectif de cette tâche est d’identifier des termes et des structures difficiles à comprendre par un public cible et de proposer des outils de simplification automatisée de ces contenus. Cet article aborde la question lexicale en identifiant un ensemble de prédicteurs de la complexité lexicale et en évaluant leur efficacité via une analyse corrélationnelle. Les meilleures de ces variables ont été intégrées dans un modèle capable de prédire la difficulté lexicale dans un contexte d’apprentissage du français.</resume>
			<mots_cles>complexité lexicale, analyse morphologique, mots gradués, ressources lexicales</mots_cles>
			<title></title>
			<abstract>Analysing lexical complexity is a task that has mainly attracted the attention of psycholinguists and language teachers. More recently, this issue has seen a growing interest in the field of Natural Language Processing (NLP) and, in particular, that of automatic text simplification. The aim of this task is to identify words and structures which may be difficult to understand by a target audience and provide automated tools to simplify these contents. This article focuses on the lexical issue by identifying a set of predictors of the lexical complexity whose efficiency are assessed with a correlational analysis. The best of those variables are integrated into a model able to predict the difficulty of words for learners of French.</abstract>
			<keywords>lexical complexity, morphological analysis, graded words, lexical resources</keywords>
		</article>
		<article id="taln-2014-long-010" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Lionel</prenom>
					<nom>Ramadier</nom>
					<email>lionel.ramadier@lirmm.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Manel</prenom>
					<nom>Zarrouk</nom>
					<email>manel.zarrouk@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>mathieu.lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Micheau</nom>
					<email>antoine.micheau@imaios.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, 161, rue ADA 34392 Montpellier Cedex 5</affiliation>
				<affiliation affiliationId="2">IMAIOS, 34090 Montpellier</affiliation>
			</affiliations>
			<titre>Annotations et inférences de relations dans un réseau lexico-sémantique: application à la radiologie</titre>
			<type>long</type>
			<pages>103-112</pages>
			<resume>Les ontologies spécifiques à un domaine ont une valeur inestimable malgré les nombreux défis liés à leur développement. Dans la plupart des cas, les bases de connaissances spécifiques à un domaine sont construites avec une portée limitée. En effet, elles ne prennent pas en compte les avantages qu’il pourrait y avoir à combiner une ontologie de spécialité à une ontologie générale. En outre, la plupart des ressources existantes manque de méta-informations sur les annotations (informations fréquentielles : de fréquent à rare ; ou des informations de pertinence : pertinent, non pertinent et inférable). Nous présentons dans cet article un réseau lexical dédié à la radiologie construit sur un réseau lexical généraliste (JeuxDeMots). Ce réseau combine poids et annotations sur des relations typées entre des termes et des concepts, un mécanisme d’inférence et de réconciliation dans le but d’améliorer la qualité et la couverture du réseau. Nous étendons ce mécanisme afin de prendre en compte non seulement les relations mais aussi les annotations. Nous décrivons la manière de laquelle les annotations améliorent le réseau en imposant de nouvelles contraintes spécialement celles basées sur la connaissance médicale. Nous présentons par la suite des résultats préliminaires.</resume>
			<mots_cles>réseau lexical, inférence, annotation, radiologie</mots_cles>
			<title></title>
			<abstract>Relations annotation and inference in a lexical-semantic network : application to radiology Domain specific ontologies are invaluable despite many challenges associated with their development. In most cases, domain knowledge bases are built with very limited scope without considering the benefits of plunging domain knowledge to a general ontology. Furthermore, most existing resources lack meta-information about association strength (weights) and annotations (frequency information like frequent, rare ... or relevance information (pertinent or irrelevant)). In this paper, we are presenting a semantic resource for radiology built over an existing general semantic lexical network (JeuxDeMots). This network combines weight and annotations on typed relations between terms and concepts. Some inference mechanisms are applied to the network to improve its quality and coverage. We extend this mechanism to relation annotation. We describe how annotations are handled and how they improve the network by imposing new constraints especially those founded on medical knowledge. We present then some results</abstract>
			<keywords>relation inference, lexical semantic network, relation annotation, radiology</keywords>
		</article>
		<article id="taln-2014-long-011" session="Gestion des erreurs en TAL">
			<auteurs>
				<auteur>
					<prenom>Maud</prenom>
					<nom>Pironneau</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Brunelle</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Simon</prenom>
					<nom>Charest</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Druide informatique, 1435 rue Saint-Alexandre, bureau 1040, Montréal (Québec)</affiliation>
			</affiliations>
			<titre>Correction automatique par résolution d’anaphores pronominales</titre>
			<type>long</type>
			<pages>113-124</pages>
			<resume>Cet article décrit des travaux réalisés dans le cadre du développement du correcteur automatique d’un logiciel commercial d’aide à la rédaction du français. Nous voulons corriger des erreurs uniquement détectables lorsque l’antécédent de certains pronoms est connu. Nous décrivons un algorithme de résolution des anaphores pronominales intra- et interphrastiques s’appuyant peu sur la correspondance de la morphologie, puisque celle-ci est possiblement erronée, mais plutôt sur des informations robustes comme l’analyse syntaxique fine et des cooccurrences fiables. Nous donnons un aperçu de nos résultats sur un vaste corpus de textes réels et, tout en tentant de préciser les critères décisifs, nous montrons que certains types de corrections anaphoriques sont d’une précision respectable.</resume>
			<mots_cles>Correcteur, Anaphores, Pronom, Saillance, Approche multistratégique, Cooccurrences</mots_cles>
			<title></title>
			<abstract>This article relates work done in order to expand the performance of a commercial French grammar checker. We try to achieve the correction of errors only detectable when an anaphora pronoun is linked with its antecedent. The algorithm searches for the antecedent within the same sentence as the pronoun as well as in previous sentences. It relies only slightly on morphology agreement, since it is what we are trying to correct, and uses instead information from a robust syntactic parsing and reliable cooccurrences. We give examples of our results on a vast corpus, and try to identify the key criteria for successful detection. We show that some types of corrections are precise enough to be integrated in a large scale commercial software.</abstract>
			<keywords>Grammar checker, Anaphora, Pronoun, Salience, Multi-Strategy Approach, Cooccurrences</keywords>
		</article>
		<article id="taln-2014-long-012" session="Gestion des erreurs en TAL">
			<auteurs>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Iris</prenom>
					<nom>Eshkol-Taravella</nom>
					<email>iris.eshkol@univ-orleans.fr</email>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yoann</prenom>
					<nom>Dupont</nom>
					<email>yoann.dupont@etu.univ-paris3.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ilaine</prenom>
					<nom>Wang</nom>
					<email>i.wang@u-paris10.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">université Paris 3 – Sorbonne Nouvelle</affiliation>
				<affiliation affiliationId="2">Lattice, UMR 8094</affiliation>
				<affiliation affiliationId="3">université d’Orléans</affiliation>
				<affiliation affiliationId="4">LLL, UMR 7270</affiliation>
			</affiliations>
			<titre>Peut-on bien chunker avec de mauvaises étiquettes POS ?</titre>
			<type>long</type>
			<pages>125-136</pages>
			<resume>Dans cet article, nous testons deux approches distinctes pour chunker un corpus oral transcrit, en cherchant à minimiser les étapes de correction manuelle. Nous ré-utilisons tout d’abord un chunker appris sur des données écrites, puis nous tentons de ré-apprendre un chunker spécifique de l’oral à partir de données annotées et corrigées manuellement, mais en faible quantité. L'objectif est d'atteindre les meilleurs résultats possibles pour le chunker en se passant autant que possible de la correction manuelle des étiquettes POS. Nos expériences montrent qu’il est possible d’apprendre un nouveau chunker performant pour l’oral à partir d’un corpus de référence annoté de petite taille, sans intervention sur les étiquettes POS.</resume>
			<mots_cles>chunker, étiquetage POS, apprentissage automatique, corpus oral, disfluences</mots_cles>
			<title></title>
			<abstract>In this paper, we test two distinct approaches to chunk transcribed oral data, trying to minimize the phases of manual correction. First, we use an existing chunker, learned from written texts, then we try to learn a new specific chunker from a small amount of manually corrected labeled oral data. The purpose is to reach the best possible results for the chunker with as few manual corrections of the POS labels as possible. Our experiments show that it is possible to learn a new effective chunker for oral data from a labeled reference corpus of small size, without any manual correction of POS labels.</abstract>
			<keywords>chunker, POS labeling, machine learning, oral corpus, disfluencies</keywords>
		</article>
		<article id="taln-2014-long-013" session="Gestion des erreurs en TAL">
			<auteurs>
				<auteur>
					<prenom>Marion</prenom>
					<nom>Baranes</nom>
					<email>Marion.Baranes@inria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>Benoit.Sagot@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">viavoo, 92100 Boulogne Billancourt</affiliation>
				<affiliation affiliationId="2">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
			</affiliations>
			<titre>Normalisation de textes par analogie: le cas des mots inconnus</titre>
			<type>long</type>
			<pages>137-148</pages>
			<resume>Dans cet article, nous proposons et évaluons un système permettant d’améliorer la qualité d’un texte bruité notamment par des erreurs orthographiques. Ce système a vocation à être intégré à une architecture complète d’extraction d’information, et a pour objectif d’améliorer les résultats d’une telle tâche. Pour chaque mot qui est inconnu d’un lexique de référence et qui n’est ni une entité nommée ni une création lexicale, notre système cherche à proposer une ou plusieurs normalisations possibles (une normalisation valide étant un mot connu dont le lemme est le même que celui de la forme orthographiquement correcte). Pour ce faire, ce système utilise des techniques de correction automatique lexicale par règle qui reposent sur un système d’induction de règles par analogie.</resume>
			<mots_cles>normalisation textuelle, correction orthographique, analogie</mots_cles>
			<title></title>
			<abstract>Analogy-based Text Normalization : the case of unknowns words. In this paper, we describe and evaluate a system for improving the quality of noisy texts containing non-word errors. It is meant to be integrated into a full information extraction architecture, and aims at improving its results. For each word unknown to a reference lexicon which is neither a named entity nor a neologism, our system suggests one or several normalization candidates (any known word which has the same lemma as the spell-corrected form is a valid candidate). For this purpose, we use an analogybased approach for acquiring normalisation rules and use them in the same way as lexical spelling correction rules.</abstract>
			<keywords>Text normalization, Spell checking, Analogy</keywords>
		</article>
		<article id="taln-2014-long-014" session="Modèles linguistiques">
			<auteurs>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Bride</nom>
					<email>Bride@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Tim</prenom>
					<nom>Van de Cruys</nom>
					<email>Cruys@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Asher</nom>
					<email>Asher@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université Paul Sabatier, 118 route de Narbonne, F-31062 TOULOUSE CEDEX 9</affiliation>
			</affiliations>
			<titre>Une évaluation approfondie de différentes méthodes de compositionalité sémantique</titre>
			<type>long</type>
			<pages>149-160</pages>
			<resume>Au cours des deux dernières décennies, de nombreux algorithmes ont été développés pour capturer la sémantique des mots simples en regardant leur répartition dans un grand corpus, et en comparant ces distributions dans un modèle d’espace vectoriel. En revanche, il n’est pas trivial de combiner les objets algébriques de la sémantique distributionnelle pour arriver à une dérivation d’un contenu pour des expressions complexes, composées de plusieurs mots. Notre contribution a deux buts. Le premier est d’établir une large base de comparaison pour les méthodes de composition pour le cas adjectif_nom. Cette base nous permet d’évaluer en profondeur la performance des différentes méthodes de composition. Notre second but est la proposition d’une nouvelle méthode de composition, qui est une généralisation de la méthode de Baroni &amp; Zamparelli (2010). La performance de notre nouvelle méthode est également évaluée sur notre nouveau ensemble de test.</resume>
			<mots_cles>sémantique lexicale, sémantique distributionnelle, compositionalité</mots_cles>
			<title></title>
			<abstract>In the course of the last two decades, numerous algorithms have sprouted up that successfully capture the semantics of single words by looking at their distribution in text, and comparing these distributions in a vector space model. However, it is not straightforward to construct meaning representations beyond the level of individual words – i.e. the combination of words into larger units – using distributional methods. Our contribution is twofold. First of all, we carry out a large scale evaluation, comparing different composition methods within the distributional framework for the case of adjective-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for adjective-noun composition, which is a generalization of the approach by Baroni &amp; Zamparelli (2010). The performance of our novel method is equally evaluated on our new dataset.</abstract>
			<keywords>lexical semantics, distributional semantics, compositionality</keywords>
		</article>
		<article id="taln-2014-long-015" session="Modèles linguistiques">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>laurence.danlos@inria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aleksandre</prenom>
					<nom>Maskharashvili</nom>
					<email>aleksandre.maskharashvili@inria.fr</email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
					<affiliationId>6</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sylvain</prenom>
					<nom>Pogodalla</nom>
					<email>sylvain.pogodalla@inria.fr</email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
					<affiliationId>6</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Diderot (Paris 7), Paris, F-75013, France</affiliation>
				<affiliation affiliationId="2">ALPAGE, INRIA Paris–Rocquencourt, Paris, F-75013, France</affiliation>
				<affiliation affiliationId="3">Institut Universitaire de France, Paris, F-75005, France</affiliation>
				<affiliation affiliationId="4">INRIA, Villers-lès-Nancy, F-54600, France</affiliation>
				<affiliation affiliationId="5">Université de Lorraine, LORIA, UMR 7503, Vandoeuvre-lès-Nancy, F-54500, France</affiliation>
				<affiliation affiliationId="6">CNRS, LORIA, UMR 7503, Vandoeuvre-lès-Nancy, F-54500, France</affiliation>
			</affiliations>
			<titre>Génération de textes : G-TAG revisité avec les Grammaires Catégorielles Abstraites</titre>
			<type>long</type>
			<pages>161-172</pages>
			<resume>G-TAG est un formalisme dédié à la génération de textes. Il s’appuie sur les Grammaires d’Arbres Adjoints (TAG) qu’il étend avec des notions propres permettant de construire une forme de surface à partir d’une représentation conceptuelle. Cette représentation conceptuelle est indépendante de la langue, et le formalisme G-TAG a été conçu pour la mise en oeuvre de la synthèse dans une langue cible à partir de cette représentation. L’objectif de cet article est d’étudier G-TAG et les notions propres que ce formalisme introduit par le biais des Grammaires Catégorielles Abstraites (ACG) en exploitant leurs propriétés de réversibilité intrinsèque et leur propriété d’encodage des TAG. Nous montrons que les notions clefs d’arbre de g-dérivation et de lexicalisation en G-TAG s’expriment naturellement en ACG. La construction des formes de surface peut alors utiliser les algorithmes généraux associés aux ACG et certaines constructions absentes de G-TAG peuvent être prises en compte sans modification supplémentaire.</resume>
			<mots_cles>TAG, G-TAG, génération, réalisation syntaxique, grammaires catégorielles abstraites</mots_cles>
			<title></title>
			<abstract>G-TAG is a formalism dedicated to text generation. It relies on the Tree Adjoining Grammar (TAG) formalism and extends it with several specific notions allowing for the construction of a surface form from a conceptual representation. This conceptual representation is independent from the target language. The goal of this paper is to study G-TAG and its specific notions from the perspective given by Abstract Categorial Grammars (ACG). We use the reversibility property of ACG and the encoding of TAG they offer. We show that the key G-TAG notions of g-derivation tree and lexicalization are naturally expressed in ACG. The construction of surface forms can then rely on the general ACG algorithms and some operations that G-TAG is lacking can be freely accounted for.</abstract>
			<keywords>TAG, G-TAG, generation, syntactic realization, abstract categorial grammars</keywords>
		</article>
		<article id="taln-2014-long-016" session="Méthodes numériques pour le TAL">
			<auteurs>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Wisniewski</nom>
					<email>Wisniewski.Guillaume@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Pécheux</nom>
					<email>Pecheux.Nicolas@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Elena</prenom>
					<nom>Knyazeva</nom>
					<email>Knyazeva.Elena@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Allauzen</nom>
					<email>Allauzen.Alexandre@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>Yvon.François@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Sud, 91 403 Orsay CEDEX</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, 91 403 Orsay CEDEX</affiliation>
			</affiliations>
			<titre>Apprentissage partiellement supervisé d’un étiqueteur morpho-syntaxique par transfert cross-lingue</titre>
			<type>long</type>
			<pages>173-183</pages>
			<resume>Les méthodes de transfert cross-lingue permettent partiellement de pallier l’absence de corpus annotés, en particulier dans le cas de langues peu dotées en ressources linguistiques. Le transfert d’étiquettes morpho-syntaxiques depuis une langue riche en ressources, complété et corrigé par un dictionnaire associant à chaque mot un ensemble d’étiquettes autorisées, ne fournit cependant qu’une information de supervision incomplète. Dans ce travail, nous reformulons ce problème dans le cadre de l’apprentissage ambigu et proposons une nouvelle méthode pour apprendre un analyseur de manière faiblement supervisée à partir d’un modèle à base d’historique. L’évaluation de cette approche montre une amélioration sensible des performances par rapport aux méthodes de l’état de l’art pour trois langues sur quatre considérées, avec des gains jusqu’à 3,9% absolus ou 35,8% relatifs.</resume>
			<mots_cles>apprentissage partiellement supervisé, analyse morpho-syntaxique, transfert cross-lingue</mots_cles>
			<title></title>
			<abstract>When Part-of-Speech annotated data is scarce, e.g. for under resourced languages, one can turn to crosslingual transfer and crawled dictionaries to collect partially supervised data. We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model. This method is evaluated on four languages and yields improvements over state-of-the-art for three of them, with gains up to 3.9% absolute or 35.8% relative.</abstract>
			<keywords>Weakly Supervised Learning, Part-of-Speech Tagging, Cross-Lingual Transfer</keywords>
		</article>
		<article id="taln-2014-long-017" session="Méthodes numériques pour le TAL">
			<auteurs>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Hernandez</nom>
					<email>nicolas.hernandez@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes</affiliation>
			</affiliations>
			<titre>Construire un corpus monolingue annoté comparable Expérience à partir d’un corpus annoté morpho-syntaxiquement</titre>
			<type>long</type>
			<pages>184-195</pages>
			<resume>Motivé par la problématique de construction automatique d’un corpus annoté morpho-syntaxiquement distinct d’un corpus source, nous proposons une définition générale et opérationnelle de la relation de la comparabilité entre des corpus monolingues annotés. Cette définition se veut indépendante du domaine applicatif. Nous proposons une mesure de la relation de comparabilité et une procédure de construction d’un corpus comparable. Enfin nous étudions la possibilité d’utiliser la mesure de la perplexité définie dans la théorie de l’information comme moyen de prioriser les phrases à sélectionner pour construire un corpus comparable. Nous montrons que cette mesure joue un rôle mais qu’elle n’est pas suffisante.</resume>
			<mots_cles>Corpus comparable, Corpus monolingue, Corpus annoté, Mesure de la comparabilité, Construction de corpus comparable, Analyse morpho-syntaxique</mots_cles>
			<title></title>
			<abstract>This work is motivated by the will of creating a new part-of-speech annotated corpus in French from an existing one. In this context, we proprose a general and operational definition of the comparability relation between annotated monolingual corpora.We propose a comparability measure and a procedure to build semi-automatically a comparable corpus from a source one. We study the use of the perplexity (information theory motivated measure) as a way to rank the sentences to select for building a comparable corpus. We show that the measure can play a role but that it is not sufficient.</abstract>
			<keywords>Comparable corpus, Monolingual corpus, Annotated corpus,Measuring comparability, Building comparable corpus, Part-of-Speech tagging</keywords>
		</article>
		<article id="taln-2014-long-018" session="Méthodes numériques pour le TAL">
			<auteurs>
				<auteur>
					<prenom>Hatim</prenom>
					<nom>Khouzaimi</nom>
					<email>hatim.khouzaimi@orange.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Laroche</nom>
					<email>romain.laroche@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 38-40 rue du Général Leclerc 92794 Issy-les-Moulineaux, France</affiliation>
				<affiliation affiliationId="2">Laboratoire Informatique d’Avignon, 339 chemin des Meinajaries 84911 Avignon, France</affiliation>
			</affiliations>
			<titre>Vers une approche simplifiée pour introduire le caractère incrémental dans les systèmes de dialogue</titre>
			<type>long</type>
			<pages>196-207</pages>
			<resume>Le dialogue incrémental est au coeur de la recherche actuelle dans le domaine des systèmes de dialogue. Plusieurs architectures et modèles ont été publiés comme (Allen et al., 2001; Schlangen &amp; Skantze, 2011). Ces approches ont permis de comprendre différentes facettes du dialogue incrémental, cependant, les implémenter nécessite de repartir de zéro car elles sont fondamentalement différentes des architectures qui existent dans les systèmes de dialogue actuels. Notre approche se démarque par sa réutilisation de l’existant pour tendre vers une nouvelle génération de systèmes de dialogue qui ont un comportement incrémental mais dont le fonctionnement interne est basé sur les principes du dialogue traditionnel. Ce papier propose d’intercaler un module, appelé Scheduler, entre le service et le client. Ce Scheduler se charge de la gestion des événements asynchrones, de manière à reproduire le comportement des systèmes incrémentaux vu du client. Le service, de son côté, ne se comporte pas de manière incrémentale.</resume>
			<mots_cles>Systèmes de Dialogue, Traitement Incrémental, Architecture des Systèmes de Dialogue</mots_cles>
			<title></title>
			<abstract>Incremental dialogue is at the heart of current research in the field of dialogue systems. Several architectures and models have been published such as (Allen et al., 2001; Schlangen &amp; Skantze, 2011). This work has made it possible to understand many aspects of incremental dialogue, however, in order to implement these solutions, one needs to start from scratch as the existing architectures are inherently different. Our approach is different as it tends towards a new generation of incremental systems that behave incrementally but work internally in a traditional way. This paper suggests inserting a new module, called the Scheduler, between the service and the client. This Scheduler manages the asynchronous events, hence reproducing the behaviour of incremental systems from the client’s point of view. On the other end, the service does not work incrementally.</abstract>
			<keywords>Dialogue Systems, Incremental Processing, Dialogue Systems Architecture</keywords>
		</article>
		<article id="taln-2014-long-019" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Nabil</prenom>
					<nom>Hathout</nom>
					<email>Nabil.Hathout@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fiammetta</prenom>
					<nom>Namer</nom>
					<email>Fiammetta.Namer@univ-lorraine.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR 5263 CLLE/ERSS, CNRS &amp; Université Toulouse Le Mirail, Toulouse</affiliation>
				<affiliation affiliationId="2">UMR 7118 ATILF, CNRS &amp; Université de Lorraine, Nancy</affiliation>
			</affiliations>
			<titre>La base lexicale Démonette : entre sémantique constructionnelle et morphologie dérivationnelle</titre>
			<type>long</type>
			<pages>208-219</pages>
			<resume>Démonette est une base de données lexicale pour le français dont les sommets (entrées lexicales) et les arcs (relations morphologiques entre les sommets) sont annotés au moyen d’informations morpho-sémantiques. Elle résulte d’une conception originale intégrant deux approches radicalement opposées : Morphonette, une ressource basée sur les analogies dérivationnelles, et DériF, un analyseur à base de règles linguistiques. Pour autant, Démonette n’est pas la simple fusion de deux ressources pré-existantes : cette base possède une architecture compatible avec l’approche lexématique de la morphologie ; son contenu peut être étendu au moyen de données issues de sources diverses. L'article présente le modèle Démonette et le contenu de sa version actuelle : 31 204 verbes, noms d'action, noms d’agent, et adjectifs de propriété dont les liens morphologiques donnent à voir des définitions bi-orientées entre ascendants et entre lexèmes en relation indirecte. Nous proposons enfin une évaluation de Démonette qui comparée à Verbaction obtient un score de 84% en rappel et de 90% en précision.</resume>
			<mots_cles>Réseau lexical, Morphologie dérivationnelle, Famille morphologique, Sémantique lexicale, Français</mots_cles>
			<title></title>
			<abstract>Démonette is a lexical database whose vertices (lexical entries) and edges (morphological relations between the vertices) are annotated with morpho-semantic information. It results from an original design incorporating two radically different approaches: Morphonette, a resource based on derivational analogies and DériF, an analyzer based on linguistic rules. However, Daemonette is not a simple merger of two pre-existing ressources: its architecture is fully compatible with the lexematic approach to morphology; its contents can be extended using data from various other sources. The article presents the Démonette model and the content of its current version, including 31,204 verbs, action nouns, agent nouns and property adjectives, where morphological links between both direct ascendants and indirectly related words have bi-oriented definitions. Finally, Démonette is assessed with respect to Verbaction with a recall of 84% and a precision of 90%.</abstract>
			<keywords>Lexical Network, Derivational morphology, Morphological family, Lexical semantics, French</keywords>
		</article>
		<article id="taln-2014-long-020" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Claveau</nom>
					<email>vincent.claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ewa</prenom>
					<nom>Kijak</nom>
					<email>ewa.kijak@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA - CNRS - Univ Rennes 1, Campus de Beaulieu, F-35042 Rennes</affiliation>
				<affiliation affiliationId="2">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, F-91191 Gif-sur-Yvette</affiliation>
			</affiliations>
			<titre>Explorer le graphe de voisinage pour améliorer les thésaurus distributionnels</titre>
			<type>long</type>
			<pages>220-231</pages>
			<resume>Dans cet article, nous abordons le problème de construction et d’amélioration de thésaurus distributionnels. Nous montrons d’une part que les outils de recherche d’information peuvent être directement utilisés pour la construction de ces thésaurus, en offrant des performances comparables à l’état de l’art. Nous nous intéressons d’autre part plus spécifiquement à l’amélioration des thésaurus obtenus, vus comme des graphes de plus proches voisins. En tirant parti de certaines des informations de voisinage contenues dans ces graphes nous proposons plusieurs contributions. 1) Nous montrons comment améliorer globalement les listes de voisins en prenant en compte la réciprocité de la relation de voisinage, c’est-à-dire le fait qu’un mot soit un voisin proche d’un autre et vice-versa. 2) Nous proposons également une méthode permettant d’associer à chaque liste de voisins (i.e. à chaque entrées du thésaurus construit) un score de confiance. 3) Enfin, nous montrons comment utiliser ce score de confiance pour réordonner les listes de voisins les plus proches. Ces différentes contributions sont validées expérimentalement et offrent des améliorations significatives sur l’état de l’art.</resume>
			<mots_cles>thésaurus distributionnel, graphe de k proches voisins, fenêtre de Parzen, algorithme hongrois, Tnormes, recherche d’information</mots_cles>
			<title></title>
			<abstract>In this paper, we address the issue of building and improving a distributional thesaurus.We first show that existing tools from the information retrieval domain can be directly used in order to build a thesaurus with state-of-the-art performance. Secondly, we focus more specifically on improving the obtained thesaurus, seen as a graph of k-nearest neighbors. By exploiting information about the neighborhood contained in this graph, we propose several contributions. 1)We show how the lists of neighbors can be globally improved by examining the reciprocity of the neighboring relation, that is, the fact that a word can be close of another and vice-versa. 2) We also propose a method to associate a confidence score to any lists of nearest neighbors (i.e. any entry of the thesaurus). 3) Last, we demonstrate how these confidence scores can be used to reorder the closest neighbors of a word. These different contributions are validated through experiments and offer significant improvement over the state-of-theart.</abstract>
			<keywords>distributional thesaurus, k nearest neighbor graph, Parzen window, Hungarian algorithm, T-norms, information retrieval</keywords>
		</article>
		<article id="taln-2014-long-021" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Amandine</prenom>
					<nom>Périnet</nom>
					<email>amandine.perinet@edu.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Hamon</nom>
					<email>hamon@limsi.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INSERM, U1142, LIMICS, F-75006, Paris, France; Sorbonne Universités, UPMC Univ Paris 06, UMR_S 1142, LIMICS, F-75006, Paris, France; Université Paris 13, Sorbonne Paris Cité, LIMICS, (UMR_S 1142), F-93430, Villetaneuse, France</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, 91403 Orsay, France</affiliation>
				<affiliation affiliationId="3">Université Paris 13, Sorbonne Paris Cité, 93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Réduction de la dispersion des données par généralisation des contextes distributionnels : application aux textes de spécialité</titre>
			<type>long</type>
			<pages>232-243</pages>
			<resume>Les modèles d’espace vectoriels mettant en oeuvre l’analyse distributionnelle s’appuient sur la redondance d’informations se trouvant dans le contexte des mots à associer. Cependant, ces modèles souffrent du nombre de dimensions considérable et de la dispersion des données dans la matrice des vecteurs de contexte. Il s’agit d’un enjeu majeur sur les corpus de spécialité pour lesquels la taille est beaucoup plus petite et les informations contextuelles moins redondantes. Nous nous intéressons au problème de la limitation de la dispersion des données sur des corpus de spécialité et proposons une méthode permettant de densifier la matrice en généralisant les contextes distributionnels. L’évaluation de la méthode sur un corpus médical en français montre qu’avec une petite fenêtre graphique et l’indice de Jaccard, la généralisation des contextes avec des relations fournies par des patrons lexico-syntaxiques permet d’améliorer les résultats, alors qu’avec une large fenêtre et le cosinus, il est préférable de généraliser avec des relations obtenues par inclusion lexicale.</resume>
			<mots_cles>Analyse distributionnelle, textes de spécialité, hyperonymie, dispersion des données, modèle d’espace vectoriel, méthode hybride</mots_cles>
			<title></title>
			<abstract>Vector space models implement the distributional hypothesis relying on the repetition of information occurring in the contexts of words to associate. However, these models suffer from a high number of dimensions and data sparseness in the matrix of contextual vectors. This is a major issue with specialized corpora that are of much smaller size and with much lower context frequencies.We tackle the problem of data sparseness on specialized texts and we propose a method that allows to make the matrix denser, by generalizing of distributional contexts. The evaluation of the method is performed on a French medical corpus, and shows that with a small graphical window and the Jaccard Index, the context generalization with lexico-syntactic patterns improves the results, while with a large window and the cosine measure, it is better to generalize with lexical inclusion.</abstract>
			<keywords>Distributional analysis, specialized texts, hypernymy, data sparseness, Vector Space Model, hybrid method</keywords>
		</article>
		<article id="taln-2014-long-022" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Juliette</prenom>
					<nom>Conrath</nom>
					<email>Conrath.Juliette@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stergos</prenom>
					<nom>Afantenos</nom>
					<email>Afantenos.Stergos@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicholas</prenom>
					<nom>Asher</nom>
					<email>Asher.Nicholas@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Muller</nom>
					<email>Muller.Philippe@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université Toulouse &amp; CNRS, Univ. Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse</affiliation>
			</affiliations>
			<titre>Extraction non supervisée de relations sémantiques lexicales</titre>
			<type>long</type>
			<pages>244-255</pages>
			<resume>Nous présentons une base de connaissances comportant des triplets de paires de verbes associés avec une relation sémantique/discursive, extraits du corpus français frWaC par une méthode s’appuyant sur la présence d’un connecteur discursif reliant deux verbes. Nous détaillons plusieurs mesures visant à évaluer la pertinence des triplets et la force d’association entre la relation sémantique/discursive et la paire de verbes. L’évaluation intrinsèque est réalisée par rapport à des annotations manuelles. Une évaluation de la couverture de la ressource est également réalisée par rapport au corpus Annodis annoté discursivement. Cette étude produit des résultats prometteurs démontrant l’utilité potentielle de notre ressource pour les tâches d’analyse discursive mais aussi des tâches de nature sémantique.</resume>
			<mots_cles>discours, sémantique, sémantique lexicale</mots_cles>
			<title></title>
			<abstract>This paper presents a knowledge base containing triples involving pairs of verbs associated with semantic or discourse relations. The relations in these triples are marked by discourse connectors between two adjacent instances of the verbs in the triple in the large French corpus, frWaC. We detail several measures that evaluate the relevance of the triples and the strength of their association. We use manual annotations to evaluate our method, and also study the coverage of our ressource with respect to the discourse annotated corpus Annodis. Our positive results show the potential impact of our ressource for discourse analysis tasks as well as semantically oriented tasks.</abstract>
			<keywords>discourse, semantics, lexical semantics</keywords>
		</article>
		<article id="taln-2014-long-023" session="Traduction Automatique">
			<auteurs>
				<auteur>
					<prenom>Quoc-Khanh</prenom>
					<nom>Do</nom>
					<email>Quoc-Khanh.Do@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Allauzen</nom>
					<email>Alexandre.Allauzen@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>François.Yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI/CNRS, rue John von Neumann, Campus Universitaire Orsay 91 403 Orsay</affiliation>
				<affiliation affiliationId="2">Université Paris Sud, 91 403 Orsay</affiliation>
			</affiliations>
			<titre>Modèles de langue neuronaux: une comparaison de plusieurs stratégies d’apprentissage</titre>
			<type>long</type>
			<pages>256-267</pages>
			<resume>Alors que l’importance des modèles neuronaux dans le domaine du traitement automatique des langues ne cesse de croître, les difficultés de leur apprentissage continue de freiner leur diffusion au sein de la communauté. Cet article étudie plusieurs stratégies, dont deux sont originales, pour estimer des modèles de langue neuronaux, en se focalisant sur l’ajustement du pas d’apprentissage. Les résultats expérimentaux montrent, d’une part, l’importance que revêt la conception de cette stratégie. D’autre part, le choix d’une stratégie appropriée permet d’apprendre efficacement des modèles de langue donnant lieu à des résultats à l’état de l’art en traduction automatique, avec un temps de calcul réduit et une faible influence des hyper-paramètres.</resume>
			<mots_cles>Réseaux de neurones, modèles de langue n-gramme, traduction automatique statistique</mots_cles>
			<title></title>
			<abstract>If neural networks play an increasingly important role in natural language processing, training issues still hinder their dissemination in the community. This paper studies different learning strategies for neural language models (including two new strategies), focusing on the adaptation of the learning rate. Experimental results show the impact of the design of such strategy. Moreover, provided the choice of an appropriate training regime, it is possible to efficiently learn language models that achieves state of the art results in machine translation with a lower training time and a reduced impact of hyper-parameters.</abstract>
			<keywords>Neural networks, n-gram language models, statistical machine translation</keywords>
		</article>
		<article id="taln-2014-long-024" session="Traduction Automatique">
			<auteurs>
				<auteur>
					<prenom>Nasredine</prenom>
					<nom>Semmar</nom>
					<email>nasredine.semmar@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Saadane</nom>
					<email>houda.saadane@e.u-grenoble3.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut CEA LIST, DIASI, Laboratoire Vision et Ingénierie des Contenus, CEA Saclay – Nano-INNOV, 91191 Gif-sur-Yvette Cedex</affiliation>
				<affiliation affiliationId="2">LIDILEM, Université Stendhal-Grenoble III, Domaine Universitaire, 1180, avenue centrale, 38400 Saint Martin d'Hères</affiliation>
			</affiliations>
			<titre>Etude de l’impact de la translittération de noms propres sur la qualité de l’alignement de mots à partir de corpus parallèles français-arabe</titre>
			<type>long</type>
			<pages>268-279</pages>
			<resume>Les lexiques bilingues jouent un rôle important en recherche d'information interlingue et en traduction automatique. La construction manuelle de ces lexiques est lente et coûteuse. Les techniques d’alignement de mots sont généralement utilisées pour automatiser le processus de construction de ces lexiques à partir de corpus de textes parallèles. L’alignement de formes simples et de syntagmes nominaux à partir de corpus parallèles est une tâche relativement bien maîtrisée pour les langues à écriture latine, mais demeure une opération complexe pour l’appariement de textes n’utilisant pas la même écriture. Dans la perspective d’utiliser la translittération de noms propres de l’arabe vers l’écriture latine en alignement de mots et d’étudier son impact sur la qualité d’un lexique bilingue français-arabe construit automatiquement, cet article présente, d’une part, un système de translittération de noms propres de l’arabe vers l’écriture latine, et d’autre part, un outil d’alignement de mots simples et composés à partir de corpus de textes parallèles français-arabe. Le lexique bilingue produit par l’outil d'alignement de mots intégrant la translittération a été évalué en utilisant deux approches : une évaluation de la qualité d’alignement à l’aide d’un alignement de référence construit manuellement et une évaluation de l’impact de ce lexique bilingue sur la qualité de traduction du système de traduction automatique statistique Moses. Les résultats obtenus montrent que la translittération améliore aussi bien la qualité de l’alignement de mots que celle de la traduction.</resume>
			<mots_cles>Lexique bilingue, translittération, alignement de mots, traduction automatique statistique, évaluation</mots_cles>
			<title></title>
			<abstract>Bilingual lexicons play a vital role in cross-language information retrieval and machine translation. The manual construction of these lexicons is often costly and time consuming. Word alignment techniques are generally used to construct bilingual lexicons from parallel texts. Aligning single words and nominal syntagms from parallel texts is relatively a well controlled task for languages using Latin script but it is complex when the source and target languages do not share the same written script. A solution to this issue consists in writing the proper names present in the parallel corpus in the same written script. This paper presents, on the one hand, a system for automatic transliteration of proper names from Arabic to Latin script, and on the other hand, a tool to align single and compound words from French- Arabic parallel text corpora. We have evaluated the word alignment tool integrating transliteration using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the statistical machine translation system Moses. The obtained results show that transliteration of proper names from Arabic to Latin improves the quality of both alignment and translation.</abstract>
			<keywords>Bilingual lexicon, transliteration, word alignment, statistical machine translation, evaluation</keywords>
		</article>
		<article id="taln-2014-long-025" session="Traduction Automatique">
			<auteurs>
				<auteur>
					<prenom>Souhir</prenom>
					<nom>Gahbiche-Braham</nom>
					<email>souhir@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hélène</prenom>
					<nom>Bonneau-Maynard</nom>
					<email>hbm@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, B.P. 133, F-91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="2">Université Paris Sud</affiliation>
			</affiliations>
			<titre>Adaptation thématique pour la traduction automatique de dépêches de presse</titre>
			<type>long</type>
			<pages>280-291</pages>
			<resume>L’utilisation de méthodes statistiques en traduction automatique (TA) implique l’exploitation de gros corpus parallèles représentatifs de la tâche de traduction visée. La relative rareté de ces ressources fait que la question de l’adaptation au domaine est une problématique centrale en TA. Dans cet article, une étude portant sur l’adaptation thématique des données journalistiques issues d’une même source est proposée. Dans notre approche, chaque phrase d’un document est traduite avec le système de traduction approprié (c.-à-d. spécifique au thème dominant dans la phrase). Deux scénarios de traduction sont étudiés : (a) une classification manuelle, reposant sur la codification IPTC ; (b) une classification automatique. Nos expériences montrent que le scénario (b) conduit à des meilleures performances (à l’aune des métriques automatiques), que le scénario (a). L’approche la meilleure pour la métrique BLEU semble toutefois consister à ne pas réaliser d’adaptation ; on observe toutefois qu’adapter permet de lever certaines ambiguïtés sémantiques.</resume>
			<mots_cles>adaptation thématique, classification automatique, traduction automatique</mots_cles>
			<title></title>
			<abstract>Statistical approaches used in machine translation (MT) require the availability of large parallel corpora for the task at hand. The relative scarcity of thes resources makes domain adaptation a central issue in MT. In this paper, a study of thematic adaptation for News texts is presented. All data are produced by the same source : News articles. In our approach, each sentence is translated with the appropriate translation system (specific to the dominant theme for the sentence). Two machine translation scenarios are considered : (a) a manual classification, based on IPTC codification ; (b) an automatic classification. Our experiments show that scenario (b) leads to better performance (in terms of automatic metrics) than scenario (a) . The best approach for the BLEU metric however seems to dispense with adaptation alltogether. Nonetheless, we observe that domain adaptation sometimes resolves some semantic ambiguities.</abstract>
			<keywords>domain adaptation, automatic classification, machine translation</keywords>
		</article>
		<article id="taln-2014-long-026" session="Traitement de corpus">
			<auteurs>
				<auteur>
					<prenom>Maxime</prenom>
					<nom>Amblard</nom>
					<email>maxime.amblard@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email>karen.fort@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lorraine, LORIA, INRIA, CNRS UMR 7503, Vandoeuvre-lès-Nancy 54500 France</affiliation>
			</affiliations>
			<titre>Étude quantitative des disfluences dans le discours de schizophrènes : automatiser pour limiter les biais</titre>
			<type>long</type>
			<pages>292-303</pages>
			<resume>Nous présentons dans cet article les résultats d’expériences que nous avons menées concernant les disfluences dans le discours de patients schizophrènes (en remédiation). Ces expériences ont eu lieu dans le cadre d’une étude plus large recouvrant d’autres niveaux d’analyse linguistique, qui devraient aider à l’identification d’indices linguistiques conduisant au diagnostic de schizophrénie. Cette étude fait la part belle aux outils de traitement automatique des langues qui permettent le traitement rapide de grandes masses de données textuelles (ici, plus de 375 000 mots). La première phase de l’étude, que nous présentons ici, a confirmé la corrélation entre l’état schizophrène et le nombre de disfluences présentes dans le discours.</resume>
			<mots_cles>discours pathologique, schizophrénie, disfluences</mots_cles>
			<title></title>
			<abstract>We present in this article the results of experiments we led concerning disfluencies in the discourse of schizophrenic patients (in remediation). These experiments are part of a larger study dealing with other levels of linguistic analysis, that could eventually help identifying clues leading to the diagnostic of the disease. This study largely relies on natural language processing tools, which allow for the rapid processing of massive textual data (here, more than 375,000 words). The first phase of the study, which we present here, confirmed the correlation between schizophrenia and the number of disfluences appearing in the discourse.</abstract>
			<keywords>pathological discourse, schizophrenia, disfluencies</keywords>
		</article>
		<article id="taln-2014-long-027" session="Traitement de corpus">
			<auteurs>
				<auteur>
					<prenom>Iris</prenom>
					<nom>Eshkol-Taravella</nom>
					<email>iris.eshkol@univ-orleans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email>natalia.grabar@univ-lille3.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS UMR 7270 LLL, Université d’Orléans, 45100 Orléans, France</affiliation>
				<affiliation affiliationId="2">CNRS UMR 8163 STL, Université Lille 3, 59653 Villeneuve d’Ascq, France</affiliation>
			</affiliations>
			<titre>Repérage et analyse de la reformulation paraphrastique dans les corpus oraux</titre>
			<type>long</type>
			<pages>304-315</pages>
			<resume>Notre travail porte sur la détection automatique de la reformulation paraphrastique dans les corpus oraux. L’approche proposée est une approche syntagmatique qui tient compte des marqueurs de reformulation paraphrastique et des spécificités de l’oral. L’annotation manuelle effectuée par deux annotateurs permet d’obtenir une description fine et multidimensionnelle des données de référence. Une méthode automatique est proposée afin de décider si les tours de parole comportent ou ne comportent pas des reformulations paraphrastiques. Les résultats obtenus montrent jusqu’à 66,4 % de précision. L’analyse de l’annotation manuelle indique qu’il existe peu de segments paraphrastiques avec des modifications morphologiques (flexion, dérivation ou composition) ou de segments qui montrent l’équivalence syntaxique.</resume>
			<mots_cles>Paraphrase, reformulation, corpus oral, marqueurs de reformulation paraphrastique</mots_cles>
			<title></title>
			<abstract>Our work addresses the automatic detection of paraphrastic rephrasing in spoken corpus. The proposed approach is syntagmatic. It is based on paraphrastic rephrasing markers and the specificities of the spoken language. Manual annotation performed by two annotators provides fine-grained and multi-dimensional description of the reference data. Automatic method is proposed in order to decide whether sentences contain or not the paraphrases. The obtained results show up to 66.4% precision. The analysis of the manual annotations indicates that there are few cases in which paraphrastic segments show morphological modifications (inflection, derivation or compounding) or syntactic equivalence.</abstract>
			<keywords>Paraphrase, reformulation, spoken corpus, markers of paraphrastic rephrasing</keywords>
		</article>
		<article id="taln-2014-long-028" session="Traitement de corpus">
			<auteurs>
				<auteur>
					<prenom>Raja</prenom>
					<nom>Ayed</nom>
					<email>ayed.raja@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ibrahim</prenom>
					<nom>Bounhas</nom>
					<email>bounhas.ibrahim@yahoo.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bilel</prenom>
					<nom>Elayeb</nom>
					<email>bilel.elayeb@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Narjès</prenom>
					<nom>Bellamine Ben Saoud</nom>
					<email>narjes.bellamine@ensi.rnu.tn</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Evrard</nom>
					<email>fabrice.evrard@enseeiht.fr</email>
					<affiliationId>5</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire de recherche RIADI, ENSI, Université de la Manouba, 2010, Tunisie</affiliation>
				<affiliation affiliationId="2">Laboratoire de l'informatique pour les systèmes industriels, Institut Supérieur de Documentation, Université de la Manouba, 2010,Tunisie</affiliation>
				<affiliation affiliationId="3">Institut de technologies des Émirats, P.O. Box: 41009, Abu Dhabi, Émirats arabes unis</affiliation>
				<affiliation affiliationId="4">Institut supérieur de l’informatique, ISI, Université de Tunis El Manar, 1002, Tunisie</affiliation>
				<affiliation affiliationId="5">Institut de recherche en informatique de Toulouse (IRIT), 02 rue Camichel, 31071 Toulouse, France</affiliation>
			</affiliations>
			<titre>Evaluation d’une approche de classification possibiliste pour la désambiguïsation des textes arabes</titre>
			<type>long</type>
			<pages>316-327</pages>
			<resume>La désambiguïsation morphologique d’un mot arabe consiste à identifier l’analyse morphologique appropriée correspondante à ce mot. Dans cet article, nous présentons trois modèles de désambiguïsation morphologique de textes arabes non voyellés basés sur la classification possibiliste. Cette approche traite les données imprécises dans les phases d’apprentissage et de test, étant donné que notre modèle apprend à partir de données non étiquetés. Nous testons notre approche sur deux corpus, à savoir le corpus du Hadith et le Treebank Arabe. Ces corpus contiennent des données de types différents classiques et modernes. Nous comparons nos modèles avec des classifieurs probabilistes et statistiques. Pour ce faire, nous transformons la structure des ensembles d’apprentissage et de test pour remédier au problème d’imperfection des données.</resume>
			<mots_cles>Traitement Automatique des Langues Naturelles, Désambiguïsation Morphologique de l’Arabe, Théorie des Possibilités, Classification Possibiliste</mots_cles>
			<title></title>
			<abstract>Morphological disambiguation of Arabic words consists in identifying their appropriate morphological analysis. In this paper, we present three models of morphological disambiguation of non-vocalized Arabic texts based on possibilistic classification. This approach deals with imprecise training and testing datasets, as we learn from untagged texts. We experiment our approach on two corpora i.e. the Hadith corpus and the Arabic Treebank. These corpora contain data of different types: traditional and modern. We compare our models to probabilistic and statistical classifiers. To do this, we transform the structure of the training and the test sets to deal with imprecise data.</abstract>
			<keywords>Natural Language Processing, Arabic Morphological Disambiguation, Possibility Theory, Possibilistic Classification</keywords>
		</article>
		<article id="taln-2014-long-029" session="Parsing 2">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Crabbé</nom>
					<email>bcrabbe@linguist.univ-paris-diderot.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, INRIA, Université Paris Diderot Place Paul Ricoeur , 75013 Paris</affiliation>
			</affiliations>
			<titre>Un analyseur discriminant de la famille LR pour l’analyse en constituants</titre>
			<type>long</type>
			<pages>328-339</pages>
			<resume>On propose un algorithme original d’analyse syntaxique déterministe en constituants pour le langage naturel inspiré de LR (Knuth, 1965). L’algorithme s’appuie sur un modèle d’apprentissage discriminant pour réaliser la désambiguisation (Collins, 2002). On montre que le modèle discriminant permet de capturer plus finement de l’information morphologique présente dans les données, ce qui lui permet d’obtenir des résultats état de l’art en temps comme en exactitude pour l’analyse syntaxique du français.</resume>
			<mots_cles>Analyse guidée par les têtes, analyse LR, temps linéaire, modèle discriminant, inférence approximative</mots_cles>
			<title></title>
			<abstract>We provide a new weighted parsing algorithm for deterministic context free grammar parsing inspired by LR (Knuth, 1965). The parser is weighted by a discriminative model that allows determinism (Collins, 2002).We show that the discriminative model allows to take advantage of morphological information available in the data, hence allowing to achieve state of the art results both in time and in accurracy for parsing French.</abstract>
			<keywords>Head driven parsing, LR parsing, linear time, discriminative modelling, approximate inference</keywords>
		</article>
		<article id="taln-2014-long-030" session="Parsing 2">
			<auteurs>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Fauconnier</nom>
					<email>Jean-Philippe.Fauconnier@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Sorin</nom>
					<email>Laurent.Sorin@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mouna</prenom>
					<nom>Kamel</nom>
					<email>Mouna.Kamel@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mustapha</prenom>
					<nom>Mojahid</nom>
					<email>Mustapha.Mojahid@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Aussenac-Gilles</nom>
					<email>Nathalie.Aussenac-Gilles@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Détection automatique de la structure organisationnelle de documents à partir de marqueurs visuels et lexicaux</titre>
			<type>long</type>
			<pages>340-351</pages>
			<resume>La compréhension d’un texte s’opère à travers les niveaux d’information visuelle, logique et discursive, et leurs relations d’interdépendance. La majorité des travaux ayant étudié ces relations a été menée dans le cadre de la génération de textes, où les propriétés visuelles sont inférées à partir des éléments logiques et discursifs. Les travaux présentés ici adoptent une démarche inverse en proposant de générer automatiquement la structure organisationnelle du texte (structure logique) à partir de sa forme visuelle. Le principe consiste à (i) labelliser des blocs visuels par apprentissage afin d’obtenir des unités logiques et (ii) relier ces unités par des relations de coordination ou de subordination pour construire un arbre. Pour ces deux tâches, des Champs Aléatoires Conditionnels et un Maximum d’Entropie sont respectivement utilisés. Après apprentissage, les résultats aboutissent à une exactitude de 80,46% pour la labellisation et 97,23% pour la construction de l’arbre.</resume>
			<mots_cles>discours, structure organisationnelle, mise en forme matérielle, marqueurs métadiscursifs, champs aléatoires conditionnels, maximum d’entropie</mots_cles>
			<title></title>
			<abstract>The process of understanding a document uses both visual, logic and discursive information along with the mutual relationships between those levels. Most approaches studying those relationships were conducted in the frame of text generation, where the text visual properties are infered from logical and discursive elements. We chose in our work to take the opposite path by trying to infer the logical structure of texts using their visual forms. To do so, we (i) assign a logical label to each visual block and (ii) we try to connect those logical units with coordination or subordination relationships, in order to build a logical tree. We used respectively a Conditional Random Fields and a Maximum Entropy algorithms for those two tasks. After a learning phase, the obtained models give us a 80,46% accuracy for task (i) and a 97,23% accuracy for task (ii).</abstract>
			<keywords>discourse, organizational structure, text formating, metadiscursive markers, conditional random fields, maximum entropy</keywords>
		</article>
		<article id="taln-2014-long-031" session="Parsing 2">
			<auteurs>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Prost</nom>
					<email>Prost@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, CNRS – Université Montpellier 2, 161 rue Ada, 34090 Montpellier, France</affiliation>
			</affiliations>
			<titre>Jugement exact de grammaticalité d’arbre syntaxique probable</titre>
			<type>long</type>
			<pages>352-362</pages>
			<resume>La robustesse de l’analyse probabiliste s’obtient généralement au détriment du jugement de grammaticalité sur la phrase analysée. Les analyseurs comme le Stanford Parser, ou les Reranking Parsers ne sont, en effet, pas capables de dissocier une analyse probable grammaticale d’une analyse probable erronée, et ce qu’elle porte sur une phrase elle-même grammaticale ou non. Dans cet article nous montrons que l’adoption d’une représentation syntaxique basée sur la théorie logique des modèles, accompagnée d’une structure syntaxique classique (par exemple de type syntagmatique), est de nature à permettre la résolution exacte de différents problèmes tels que celui du jugement de grammaticalité. Afin de démontrer la praticité et l’utilité d’une alliance entre symbolique et stochastique, nous nous appuyons sur une représentation de la syntaxe par modèles, ainsi que sur une grammaire de corpus, pour présenter une méthode de résolution exacte pour le jugement de grammaticalité d’un arbre syntagmatique probable. Nous présentons des résultats expérimentaux sur des arbres issus d’un analyseur probabiliste, qui corroborent l’intérêt d’une telle alliance.</resume>
			<mots_cles>Jugement de grammaticalité, syntaxe par modèles, Grammaires de Propriétés, analyse syntaxique probabiliste</mots_cles>
			<title></title>
			<abstract>The robustness of probabilistic parsing generally comes at the expense of grammaticality judgment – the grammaticality of the most probable output parse remaining unknown. Parsers, such as the Stanford or the Reranking ones, can not discriminate between grammatical and ungrammatical probable parses, whether their surface realisations are themselves grammatical or not. In this paper we show that a Model-Theoretic representation of Syntax alleviates the grammaticality judgment on a parse tree. In order to demonstrate the practicality and usefulness of an alliance between stochastic parsing and knowledge-based representation, we introduce an exact method for putting a binary grammatical judgment on a probable phrase structure. We experiment with parse trees generated by a probabilistic parser. We show experimental evidence on parse trees generated by a probabilistic parser to confirm our hypothesis.</abstract>
			<keywords>Grammaticality judgement, Model-Theoretic Syntax, Property Grammar, probabilistic syntactic parsing</keywords>
		</article>
		<article id="taln-2014-long-032" session="Lexique 3">
			<auteurs>
				<auteur>
					<prenom>Mokhtar-Boumeyden</prenom>
					<nom>Billami</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>José</prenom>
					<nom>Camacho-Collados</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Evelyne</prenom>
					<nom>Jacquey</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Kister</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
			</affiliations>
			<titre>Annotation sémantique et validation terminologique en texte intégral en SHS</titre>
			<type>long</type>
			<pages>363-376</pages>
			<resume>Nos travaux se focalisent sur la validation d'occurrences de candidats termes en contexte. Les contextes d'occurrences proviennent d'articles scientifiques des sciences du langage issus du corpus SCIENTEXT1. Les candidats termes sont identifiés par l'extracteur automatique de termes de la plate-forme TTC-TermSuite et sont ensuite projetés dans les textes. La problématique générale de cet article est d'étudier dans quelle mesure les contextes sont à même de fournir des critères linguistiques pertinents pour valider ou rejeter chaque occurrence de candidat terme selon qu'elle relève d'un usage terminologique en sciences du langage ou non (langue générale, transdisciplinaire, autre domaine scientifique). Pour répondre à cette question, nous comparons deux méthodes d'exploitation (l'une inspirée de la textométrie et l'autre de Lesk) avec des contextes d'occurrences du même corpus annotés manuellement et mesurons si une annotation sémantique des contextes améliore l'exactitude des choix réalisés automatiquement.</resume>
			<mots_cles>Annotation sémantique, extraction et désambiguïsation terminologique, textométrie, texte intégral</mots_cles>
			<title></title>
			<abstract>Our work is in the field of the validation of term candidates occurrences in context. The textual data used in this article comes from the freely available corpus SCIENTEXT. The term candidates are computed by the platform TTC-TermSuite and their occurrences are projected in the texts. The main issue of this article is to examine how contexts are able to provide relevant linguistic criteria to validate or reject each occurrence of term candidates according to the distinction between a terminological and a non terminological use (general language, transdisciplinary use, use coming from another science). To answer this question, we compare two methods (a textometric one and another inspired from Lesk) with the manual annotation of the same corpus and we evaluate if a semantic annotation of contexts improves the accuracy of the choices made automatically.</abstract>
			<keywords>Semantic Annotation, Terminological Extraction and Disambiguation, Textual Metrics (Specificity), Full Text</keywords>
		</article>
		<article id="taln-2014-long-033" session="Lexique 3">
			<auteurs>
				<auteur>
					<prenom>Charlotte</prenom>
					<nom>Roze</nom>
					<email>charlotte.roze@unicaen.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Charnois</nom>
					<email>thierry.charnois@lipn.univ-paris13.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dominique</prenom>
					<nom>Legallois</nom>
					<email>dominique.legallois@unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Ferrari</nom>
					<email>stephane.ferrari@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathilde</prenom>
					<nom>Salles</nom>
					<email>mathilde.salles@unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, Université de Caen Basse-Normandie, Campus 2, 14000 Caen, France</affiliation>
				<affiliation affiliationId="2">CRISCO, Université de Caen Basse-Normandie, Campus 1, 14000 Caen, France</affiliation>
				<affiliation affiliationId="3">LIPN, Université Paris 13 Sorbonne Paris Cité, 93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Identification des noms sous-spécifiés, signaux de l’organisation discursive</titre>
			<type>long</type>
			<pages>377-388</pages>
			<resume>Dans cet article, nous nous intéressons aux noms sous-spécifiés, qui forment une classe d’indices de l’organisation discursive. Ces indices ont été peu étudiés dans le cadre de l’analyse du discours et en traitement automatique des langues. L’objectif est d’effectuer une étude linguistique de leur participation à la structuration discursive, notamment lorsqu’ils interviennent dans des séquences organisationnelles fréquentes (e.g. le patron Problème-Solution). Dans cet article, nous présentons les différentes étapes mises en oeuvre pour identifier automatiquement ces noms en corpus. En premier lieu, nous détaillons la construction d’un lexique de noms sous-spécifiés pour le français à partir d’un corpus constitué de 7 années du journal Le Monde. Puis nous montrons comment utiliser des techniques fondées sur la fouille de données séquentielles pour acquérir de nouvelles constructions syntaxiques caractéristiques des emplois de noms sousspécifiés. Enfin, nous présentons une méthode d’identification automatique des occurrences de noms sous-spécifiés et son évaluation.</resume>
			<mots_cles>noms sous-spécifiés, motifs séquentiels, structure discursive</mots_cles>
			<title></title>
			<abstract>In this paper, we focus on shell nouns, a class of items involved in the signaling of discourse organisation. These signals have been little studied in Natural Language Processing and within discourse analysis theories. The main goal is to study their participation to discourse organisation, especially when they occur in Problem-Solution patterns. In this paper, we present the different steps involved in shell nouns identification of these nouns. First, we present the lexical acquisition of shell nouns from a large corpus. Second, we show how a method based on the extraction of sequential patterns (sequential data mining techniques) allows to discover new syntactic patterns specific to the use of shell nouns. Finally, we present a shell nouns identification system that we evaluate.</abstract>
			<keywords>shell nouns, sequential patterns, discourse structure</keywords>
		</article>
		<article id="taln-2014-court-001" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email>laurent.besacier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG, Université de Grenoble, UJF - BP 53, 38041 Grenoble Cedex 9</affiliation>
			</affiliations>
			<titre>Traduction automatisée d’une oeuvre littéraire: une étude pilote</titre>
			<type>court</type>
			<pages>389-394</pages>
			<resume>Les techniques actuelles de traduction automatique (TA) permettent de produire des traductions dont la qualité ne cesse de croitre. Dans des domaines spécifiques, la post-édition (PE) de traductions automatiques permet, par ailleurs, d’obtenir des traductions de qualité relativement rapidement. Mais un tel pipeline (TA+PE) est il envisageable pour traduire une oeuvre littéraire ? Cet article propose une ébauche de réponse à cette question. Un essai de l’auteur américain Richard Powers, encore non disponible en français, est traduit automatiquement puis post-édité et révisé par des traducteurs non-professionnels. La plateforme de post-édition du LIG utilisée permet de lire et éditer l’oeuvre traduite en français continuellement, suggérant (pour le futur) une communauté de lecteurs-réviseurs qui améliorent en continu les traductions de leur auteur favori. En plus de la présentation des résultats d’évaluation expérimentale du pipeline TA+PE (système de TA utilisé, scores automatiques), nous discutons également la qualité de la traduction produite du point de vue d’un panel de lecteurs (ayant lu la traduction en français, puis répondu à une enquête). Enfin, quelques remarques du traducteur français de R. Powers, sollicité à cette occasion, sont présentées à la fin de cet article.</resume>
			<mots_cles>traduction automatique, TA, oeuvre littéraire, post-édition</mots_cles>
			<title></title>
			<abstract>Current machine translation (MT ) techniques are continuously improving. In specific areas, post-editing (PE) allows to obtain high-quality translations relatively quickly. But is such a pipeline (MT+PE) usable to translate a literary work (fiction, short story) ? This paper tries to bring a preliminary answer to this question. A short story by American writer Richard Powers, still not available in French, is automatically translated and post-edited and then revised by nonprofessional translators. The LIG post-editing platform allows to read and edit the short story suggesting (for the future) a community of readers-editors that continuously improve the translations of their favorite author. In addition to presenting experimental evaluation results of the pipeline MT+PE (MT system used, auomatic evaluation), we also discuss the quality of the translation output from the perspective of a panel of readers (who read the translated short story in French, and answered to a survey afterwards). Finally, some remarks of the official french translator of R. Powers, requested on this occasion, are given at the end of this article.</abstract>
			<keywords>machine translation, MT, litterature, fiction, post-edition</keywords>
		</article>
		<article id="taln-2014-court-002" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Li</prenom>
					<nom>Gong</nom>
					<email>li.gong@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>francois.yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Orsay, France</affiliation>
				<affiliation affiliationId="2">Univ. Paris-Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Vers un développement plus efficace des systèmes de traduction statistique : un peu de vert dans un monde de BLEU</titre>
			<type>court</type>
			<pages>395-400</pages>
			<resume>Dans cet article, nous montrons comment l’utilisation conjointe d’une technique d’alignement de phrases parallèles à la demande et d’estimation de modèles de traduction à la volée permet une réduction en temps très notable (jusqu’à 93% dans nos expériences) par rapport à un système à l’état de l’art, tout en offrant un compromis en termes de qualité très intéressant dans certaines configurations. En particulier, l’exploitation immédiate de documents traduits permet de compenser très rapidement l’absence d’un corpus de développement.</resume>
			<mots_cles>traduction automatique statistique, développement efficace, temps de calcul</mots_cles>
			<title></title>
			<abstract>In this article, we show how using both on-demand alignment of parallel sentences and on-the-fly estimation of translation models can yield massive reduction (up to 93% in our experiments) in development time as compared to a state-of-the-art system, while offering an interesting tradeoff as regards translation quality under some configurations. We show in particular that the absence of a development set can be quickly compensated by immediately using translated documents.</abstract>
			<keywords>statistical machine translation, efficient development, computation time</keywords>
		</article>
		<article id="taln-2014-court-003" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Yidong</prenom>
					<nom>Chen</nom>
					<email>ydchen@xmu.edu.cn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lingxiao</prenom>
					<nom>Wang</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Boitet</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xiaodong</prenom>
					<nom>Shi</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">School of Information Science and Technology, Xiamen University, Xiamen, Fujian, China</affiliation>
				<affiliation affiliationId="2">GETALP, Laboratoire d’Informatique Grenoble (LIG), Université Joseph Fourier, Grenoble, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>401-406</pages>
			<resume>Nous présentons un projet collaboratif en cours mené par l'université de Grenoble et l'université de Xiamen, et visant à créer des instances d'un nouveau type de système de traduction automatique statistique utilisant des ressources lexico-sémantiques et discursives. Le but concret est de développer des systèmes de TAS chinois-français pour des sites boursiers et économiques. Comme très peu de corpus et de dictionnaires bilingues chinois-français sont disponibles sur Internet, l'anglais est utilisé comme "pivot" pour construire les équivalents chinois-français par transitivité. Outre la description générale de ce projet, nous décrivons les progrès sur deux axes de recherche liés à ce projet. Pour cela, nous utilisons une méthode, proposée par XMU, d'induction de probabilité fondée sur la similarité thématique, qui produit des tables de traduction C-F à partir de tables de traduction C-E et E-F. Pour disposer de bons corpus parallèles C-F, nous utilisons un système Web de post-édition collaborative qui peut déclencher l'amélioration incrémentale du système de TA en utilisant des métriques d'évaluation de TA et en extrayant la "meilleure partie" de la mémoire de traductions courante.</resume>
			<mots_cles>traduction automatique statistique (SMT), chinois-français, domaine économique</mots_cles>
			<title>On-going Cooperative Research towards Developing Economy-Oriented Chinese-French SMT Systems with a New SMT Framework</title>
			<abstract>We present an on-going collaborative project pursued by Grenoble University and Xiamen University and aiming at creating instances of a new kind of SMT system using semantics and discourse-related resources. The concrete goal is to develop Chinese-French systems specialized to stock option and economic websites. Since very few Chinese-French bilingual corpora and dictionaries are freely available on Internet, English is used as a “pivot” for constructing the Chinese-French translation equivalents by transitivity. For this, we use a method, proposed by XMU, of probability induction based on topic similarity, which produces C-F translation tables from C-E and E-F translation tables. For getting good C-F parallel corpora, we use a web-based collaborative post-editing system that can trigger the incremental improvement of the MT system by using MT evaluation metrics and extracting the "best part" of the current translation memory.</abstract>
			<keywords>SMT, Chinese-French, Economic Domain</keywords>
		</article>
		<article id="taln-2014-court-004" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Juan Antonio</prenom>
					<nom>Lossio-Ventura</nom>
					<email>juan.lossio@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Clement</prenom>
					<nom>Jonquet</nom>
					<email>clement.jonquet@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Roche</nom>
					<email>mathieu.roche@cirad.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maguelonne</prenom>
					<nom>Teisseire</nom>
					<email>teisseire@teledetection.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, Université de Montpellier 2, CNRS, Montpellier - France</affiliation>
				<affiliation affiliationId="2">Irstea, Cirad, TETIS, Montpellier - France</affiliation>
			</affiliations>
			<titre>Extraction automatique de termes combinant différentes informations</titre>
			<type>court</type>
			<pages>407-412</pages>
			<resume>Pour une communauté, la terminologie est essentielle car elle permet de décrire, échanger et récupérer les données. Dans de nombreux domaines, l’explosion du volume des données textuelles nécessite de recourir à une automatisation du processus d’extraction de la terminologie, voire son enrichissement. L’extraction automatique de termes peut s’appuyer sur des approches de traitement du langage naturel. Des méthodes prenant en compte les aspects linguistiques et statistiques proposées dans la littérature, résolvent quelques problèmes liés à l’extraction de termes tels que la faible fréquence, la complexité d’extraction de termes de plusieurs mots, ou l’effort humain pour valider les termes candidats. Dans ce contexte, nous proposons deux nouvelles mesures pour l’extraction et le “ranking” des termes formés de plusieurs mots à partir des corpus spécifiques d’un domaine. En outre, nous montrons comment l’utilisation du Web pour évaluer l’importance d’un terme candidat permet d’améliorer les résultats en terme de précision. Ces expérimentations sont réalisées sur le corpus biomédical GENIA en utilisant des mesures de la littérature telles que C-value.</resume>
			<mots_cles>Extraction Automatique de Termes, Mesure basée sur le Web, Mesure Linguistique, Mesure Statistique, Traitement Automatique du Langage Biomédical</mots_cles>
			<title></title>
			<abstract>Comprehensive terminology is essential for a community to describe, exchange, and retrieve data. In multiple domain, the explosion of text data produced has reached a level for which automatic terminology extraction and enrichment is mandatory. Automatic Term Extraction (or Recognition) methods use natural language processing to do so. Methods featuring linguistic and statistical aspects as often proposed in the literature, rely some problems related to term extraction as low frequency, complexity of the multi-word term extraction, human effort to validate candidate terms. In contrast, we present two new measures for extracting and ranking muli-word terms from domain-specific corpora, covering the all mentioned problems. In addition we demonstrate how the use of the Web to evaluate the significance of a multi-word term candidate, helps us to outperform precision results obtain on the biomedical GENIA corpus with previous reported measures such as C-value.</abstract>
			<keywords>Automatic Term Extraction, Web-based measure, Linguistic-based measure, Statistic-based measure, Biomedical Natural Language Processing</keywords>
		</article>
		<article id="taln-2014-court-005" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Gilles</prenom>
					<nom>Boyé</nom>
					<email>gboye@u-bordeaux-montaigne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anna</prenom>
					<nom>Kupsc</nom>
					<email>akupsc@u-bordeaux-montaigne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Bordeaux-Montaigne, Domaine universitaire, 33607 Pessac Cedex CLLE-ERSS, UMR5263, CNRS, 5 allées Antonio Machado, 31058 Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Analyse automatique d’espaces thématiques</titre>
			<type>court</type>
			<pages>413-418</pages>
			<resume>Basé sur les calculs d’entropie conditionnelle de (Bonami &amp; Boyé, à paraître), nous proposons un analyseur automatique de la flexion dans le cadre de la morphologie thématique qui produit le graphe de régularités du paradigme. Le traitement se base sur un lexique de 6440 verbes extraits du BDLex (de Calmès &amp; Pérennou, 1998) associés à leurs fréquences dans Lexique3 (New et al., 2001). L’algorithme se compose de trois éléments : calcul de l’entropie conditionnelle entre paires de formes fléchies, distillation des paradigmes, construction du graphe de régularités. Pour l’entropie, nous utilisons deux modes de calcul différents, l’un se base sur la distribution de l’effectif des verbes entre leurs différentes options, l’autre sur la distribution des lexèmes verbaux en fonction de leurs fréquences pour contrebalancer l’influence des verbes ultra-fréquents sur les calculs.</resume>
			<mots_cles>morphologie flexionnelle, espaces thématiques, graphe des régularité, français, verbes</mots_cles>
			<title></title>
			<abstract>Based on the entropy calculations of (Bonami &amp; Boyé, à paraître), we propose an automatic analysis of inflection couched in the stem spaces framework. Our treatment is based on a lexicon of 6440 verbs present in BDLex (de Calmès &amp; Pérennou, 1998) and associated with their frequencies from Lexique3 (New et al., 2001). The algorithm we propose consists in three steps : computing conditional entropy between all pairs of inflected forms, distilling the paradigms and constructing a regularity graph. For computing entropy, we use two methods : the first one is based on count of verbs in a given distribution whereas the second one takes into account the frequency of each verbal lemma in the distribution to compensate for the bias introduced by the ultra-frequent verbs in the calculation.</abstract>
			<keywords>Inflectional morphology, stem spaces, regularity graph, French, verbs</keywords>
		</article>
		<article id="taln-2014-court-006" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Sandra</prenom>
					<nom>Milena Castellanos Páez</nom>
					<email>sandra.castellanos@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG - GETALP. Grenoble, France</affiliation>
			</affiliations>
			<titre>Extraction et représentation des constructions à verbe support en espagnol</titre>
			<type>court</type>
			<pages>419-424</pages>
			<resume>Le traitement informatique de constructions à verbe support (prendre une photo, faire une présentation) est une tâche difficile en TAL. Cela est également vrai en espagnol, où ces constructions sont fréquentes dans les textes, mais ne font pas souvent partie des lexiques exploitables par une machine. Notre objectif est d'extraire des constructions à verbe support à partir d’un très grand corpus de l'espagnol. Nous peaufinons un ensemble de motifs morphosyntaxiques fondés sur un grand nombre de verbe support possibles. Ensuite, nous filtrons cette liste en utilisant des seuils et des mesures d'association. Bien que tout à fait classique, cette méthode permet l'extraction de nombreuses expressions de bonne qualité. À l’avenir, nous souhaitons étudier les représentations sémantiques de ces constructions dans des lexiques multilingues.</resume>
			<mots_cles>Expressions à verbe support, extraction, corpus, expressions polylexicales</mots_cles>
			<title></title>
			<abstract>The computational treatment of support verb constructions (take a picture, make a presentation) is a challenging task in NLP. This is also true in Spanish, where these constructions are frequent in texts, but not frequently included in machine-readable lexicons. Our goal is to extract support verb constructions from a very large corpus of Spanish. We fine-tune a set of morpho-syntactic patterns based on a large set of possible support verbs. Then, we filter this list using thresholds and association measures. While quite standard, this methodology allows the extraction of many good-quality expressions. As future work, we would like to investigate semantic representations for these constructions in multilingual lexicons.</abstract>
			<keywords>Support verb expressions, extraction, corpus, multiword expressions</keywords>
		</article>
		<article id="taln-2014-court-007" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>Benoit.Sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>Laurence.Danlos@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Margot</prenom>
					<nom>Colinet</nom>
					<email>Margot.Colinet@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
				<affiliation affiliationId="2">LLF, CNRS &amp; Université Paris-Diderot, 75013 Paris</affiliation>
			</affiliations>
			<titre>Sous-catégorisation en pour et syntaxe lexicale</titre>
			<type>court</type>
			<pages>425-430</pages>
			<resume>La sous-catégorisation d’arguments introduits par la préposition pour a été sous-étudiée par le passé, comme en témoigne l’incomplétude des ressources lexico-syntaxiques existantes sur ce point. Dans cet article, nous présentons rapidement les différents types de sous-catégorisation en pour, qui contrastent avec les emplois de pour comme connecteur de discours. Nous décrivons l’intégration des arguments en pour au lexique syntaxique Lefff , enrichissant ainsi les informations de sous-catégorisation de nombreuses entrées verbales, nominales, adjectivales et adverbiales.</resume>
			<mots_cles>Sous-catégorisation, Arguments en pour, Lexiques syntaxiques</mots_cles>
			<title></title>
			<abstract>Sub-categorized arguments introduced by the French preposition pour has been under-studied in previous work, as can be seen from the incompleteness of existing lexical-syntactic resources in that regard. In this paper, we briefly introduce the various types of sub-categorization in pour, which are to be distinguished from occurrences of pour as a discourse connective. We describe how we added arguments in pour within the syntactic lexicon Lefff , thus refining sub-categorization information for many verbal, nominal, adjectival and adverbial entries.</abstract>
			<keywords>Sub-categorization, Arguments in pour, Syntactic lexicons</keywords>
		</article>
		<article id="taln-2014-court-008" session="Étiquetage 1">
			<auteurs>
				<auteur>
					<prenom>Ingrid</prenom>
					<nom>Falk</nom>
					<email>ifalk@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>dbernhard@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Gérard</nom>
					<email>christophegerard@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Potier-Ferry</nom>
					<email>romainpotierferry@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LiLPa, Université de Strasbourg</affiliation>
			</affiliations>
			<titre>Étiquetage morpho-syntaxique pour des mots nouveaux</titre>
			<type>court</type>
			<pages>431-436</pages>
			<resume>Les outils d’étiquetage automatique sont plus ou moins robustes en ce qui concerne l’étiquetage de mots inconnus, non rencontrés dans le corpus d’apprentissage. Il est important de connaître de manière précise la performance de ces outils lorsqu’on cible plus particulièrement l’étiquetage de néologismes formels. En effet, la catégorie grammaticale constitue un critère important à la fois pour leur identification et leur documentation. Nous présentons une évaluation et une comparaison de 7 étiqueteurs morphosyntaxiques du français, à partir d’un corpus issu du Wiktionnaire. Les résultats montrent que l’utilisation de traits de forme ou morphologiques est favorable à l’étiquetage correct des mots nouveaux.</resume>
			<mots_cles>étiquetage morphosyntaxique, évaluation, néologie formelle</mots_cles>
			<title></title>
			<abstract>Part-of-speech (POS) taggers are more or less robust with respect to the labeling of unknown words not found in the training corpus. It is important to know precisely how these tools perfom when we target part-of-speech tagging for formal neologisms. Indeed, grammatical category is an important criterion for both their identification and documentation. We present an evaluation and comparison of 7 POS taggers for French, based on a corpus built from Wiktionary. The results show that the use of form-related or morphological features supports the accurate tagging of new words.</abstract>
			<keywords>part-of-speech tagging, evaluation, formal neologisms</keywords>
		</article>
		<article id="taln-2014-court-009" session="Étiquetage 1">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>Benoit.Sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kata</prenom>
					<nom>Gábor</nom>
					<email>Kata.Gabor@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
			</affiliations>
			<titre>Détection et correction automatique d’entités nommées dans des corpus OCRisés</titre>
			<type>court</type>
			<pages>437-442</pages>
			<resume>La correction de données textuelles obtenues par reconnaissance optique de caractères (OCR) pour atteindre une qualité éditoriale reste aujourd’hui une tâche coûteuse, car elle implique toujours une intervention humaine. La détection et la correction automatiques d’erreurs à l’aide de modèles statistiques ne permettent de traiter de façon utile que les erreurs relevant de la langue générale. C’est pourtant dans certaines entités nommées que résident les erreurs les plus nombreuses, surtout dans des données telles que des corpus de brevets ou des textes juridiques. Dans cet article, nous proposons une architecture d’identification et de correction par règles d’un large éventail d’entités nommées (non compris les noms propres). Nous montrons que notre architecture permet d’atteindre un bon rappel et une excellente précision en correction, ce qui permet de traiter des fautes difficiles à traiter par les approches statistiques usuelles.</resume>
			<mots_cles>OCR, Entités nommées, Détection d’erreurs, Correction d’erreurs</mots_cles>
			<title></title>
			<abstract>Correction of textual data obtained by optical character recognition (OCR) for reaching editorial quality is an expensive task, as it still involves human intervention. The coverage of statistical models for automated error detection and correction is inherently limited to errors that resort to general language. However, a large amount of errors reside in domain-specific named entities, especially when dealing with data such as patent corpora or legal texts. In this paper, we propose a rule-based architecture for the identification and correction of a wide range of named entities (proper names not included). We show that our architecture achieves a good recall and an excellent correction accuracy on error types that are difficult to adress with statistical approaches.</abstract>
			<keywords>OCR, Named entities, Error Detection, Error Correction</keywords>
		</article>
		<article id="taln-2014-court-010" session="Étiquetage 1">
			<auteurs>
				<auteur>
					<prenom>Amine</prenom>
					<nom>Chennoufi</nom>
					<email>chennoufi.amin@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Azzeddine</prenom>
					<nom>Mazroui</nom>
					<email>azze.mazroui@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Mohamed I / Faculté des Sciences / Département de Mathématiques et Informatiques Oujda, Maroc</affiliation>
			</affiliations>
			<titre>Méthodes de lissage d’une approche morpho-statistique pour la voyellation automatique des textes arabes</titre>
			<type>court</type>
			<pages>443-448</pages>
			<resume>Nous présentons dans ce travail un nouveau système de voyellation automatique des textes arabes en utilisant trois étapes. Durant la première phase, nous avons intégré une base de données lexicale contenant les mots les plus fréquents de la langue arabe avec l’analyseur morphologique AlKhalil Morpho Sys pour fournir les voyellations possibles pour chaque mot. Le second module dont l’objectif est d’éliminer l'ambiguïté repose sur une approche statistique dont l’apprentissage a été effectué sur un corpus constitué de textes de livres arabes et utilisant les modèles de Markov cachés (HMM) où les mots non voyellés représentent les états observés et les mots voyellés sont ses états cachés. Le système utilise les techniques de lissage pour contourner le problème des transitions des mots absentes et l'algorithme de Viterbi pour sélectionner la solution optimale. La troisième étape utilise un modèle HMM basé sur les caractères pour traiter le cas des mots non analysés.</resume>
			<mots_cles>Langue arabe, voyellation automatique, analyse morphologique, modèle de Markov caché, corpus, lissage, algorithme de Viterbi</mots_cles>
			<title></title>
			<abstract>We present in this work a new approach for the Automatic diacritization for Arabic texts using three stages. During the first phase, we integrated a lexical database containing the most frequent words of Arabic with morphological analysis by Alkhalil Morpho Sys which provided possible diacritization for each word. The objective of the second module is to eliminate the ambiguity using a statistical approach in which the learning phase was performed on a corpus composed by several Arabic books. This approach uses the hidden Markov models (HMM) with Arabic unvowelized words taken as observed states and vowelized words are considered as hidden states. The system uses smoothing techniques to circumvent the problem of unseen word transitions in the corpus and the Viterbi algorithm to select the optimal solution. The third step uses a HMM model based on the characters to deal with the case of unanalyzed words.</abstract>
			<keywords>Arabic language, Automatic diacritization, morphological analysis, hidden Markov model, corpus, smoothing, Viterbi Algorithm</keywords>
		</article>
		<article id="taln-2014-court-011" session="Traitement de corpus 1">
			<auteurs>
				<auteur>
					<prenom>Mathieu-Henri</prenom>
					<nom>Falco</nom>
					<email>Mathieu-Henri.Falco@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Moriceau</nom>
					<email>Veronique.Moriceau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Université Paris-Sud, 91405 Orsay, France</affiliation>
			</affiliations>
			<titre>Évaluation d’un système d’extraction de réponses multiples sur le Web par comparaison à des humains</titre>
			<type>court</type>
			<pages>449-454</pages>
			<resume>Dans cet article, nous proposons une évaluation dans un cadre utilisateur de Citron, un système de question-réponse en français capable d’extraire des réponses à des questions à réponses multiples (questions possédant plusieurs réponses correctes différentes) en domaine ouvert à partir de documents provenant du Web. Nous présentons ici le protocole expérimental et les résultats pour nos deux expériences utilisateurs qui visent à (1) comparer les performances de Citron par rapport à celles d’un être humain pour la tâche d’extraction de réponses multiples et (2) connaître la satisfaction d’un utilisateur devant différents formats de présentation de réponses.</resume>
			<mots_cles>système de question-réponse, réponses multiples, évaluation utilisateur</mots_cles>
			<title></title>
			<abstract>In this paper, we propose a user evaluation of Citron, a question-answering system in French which extracts answers for multiple answer questions (expecting different correct answers) in open domain from Web documents. We present here our experimental protocol and results for user evaluations which aim at (1) comparing multiple answer extraction performances of Citron and users, and (2) knowing user preferences about multiple answer presentation.</abstract>
			<keywords>question-answering system, multiple answers, user evaluation</keywords>
		</article>
		<article id="taln-2014-court-012" session="Traitement de corpus 1">
			<auteurs>
				<auteur>
					<prenom>Natalie</prenom>
					<nom>Schluter</nom>
					<email>natalie.schluter@mah.se</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Department of Computer Science, School of Technology, Malmö University, Malmö, Sweden</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>455-460</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Centrality Measures for Non-Contextual Graph-Based Unsupervised Single Document Keyword Extraction</title>
			<abstract>The manner in which keywords fulfill the role of being central to a document is frustratingly still an open question. In this paper, we hope to shed some light on the essence of keywords in scientific articles and thereby motivate the graph-based approach to keyword extraction. We identify the document model captured by the text graph generated as input to a number of centrality metrics, and overview what these metrics say about keywords. In doing so, we achieve state-of-the-art results in unsupervised non-contextual single document keyword extraction.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2014-court-013" session="Traitement de corpus 1">
			<auteurs>
				<auteur>
					<prenom>Rémy</prenom>
					<nom>Kessler</nom>
					<email>remy.kessler@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Béchet</nom>
					<email>nicolas.bechet@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Audrey</prenom>
					<nom>Laplante</nom>
					<email>audrey.laplante@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dominic</prenom>
					<nom>Forest</nom>
					<email>dominic.forest@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">C.P. 6128, succursale Centre-ville, Montréal H3C 3J7, Canada</affiliation>
				<affiliation affiliationId="2">IRISA, UMR 6074, INSA Rennes</affiliation>
			</affiliations>
			<titre>Détection de périodes musicales d’une collection de musique par apprentissage</titre>
			<type>court</type>
			<pages>461-466</pages>
			<resume>Dans ces travaux, nous présentons une approche afin d’étiqueter une large collection de chansons francophones. Nous avons développé une interface utilisant les paroles comme point d’entrée afin d’explorer cette collection de musique avec des filtres en fonction de chaque période musicale. Dans un premier temps, nous avons collecté paroles et métadonnées de différentes sources sur leWeb. Nous présentons dans cet article une méthode originale permettant d’attribuer de manière automatique la décennie de sortie des chansons de notre collection. Basée sur un système évalué au cours d’une des campagnes DEFT, l’approche combine fouille de textes et apprentissage supervisé et aborde la problématique comme une tâche de classification multi classes. Nous avons par la suite enrichi le modèle d’un certain nombre de traits supplémentaires tels que les tags sociaux afin d’observer leur influence sur les résultats.</resume>
			<mots_cles>Fouille de textes, apprentissage supervisé, paroles de chansons, tags sociaux</mots_cles>
			<title></title>
			<abstract>In this paper, we present an approach to label a large collection of songs in French by decade. We have developed an information visualization interface that allows users to browse the collection searching for lyrics with musical periods dependent filters. We first harvested lyrics and metadata from various sources on the web. We present in this article an original method to automatically assign the decade of songs for our collection. The original system was developed for a DEFT challenge and combined text mining and machine learning with a multi class approach. We subsequently enriched the model with additional features such as social tags to determine their impact on the results.</abstract>
			<keywords>text mining, machine learning, lyrics, social tagging</keywords>
		</article>
		<article id="taln-2014-court-014" session="Traitement de corpus 1">
			<auteurs>
				<auteur>
					<prenom>Thomas</prenom>
					<nom>François</nom>
					<email>thomas.francois@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laetitia</prenom>
					<nom>Brouwers</nom>
					<email>laetitia.brouwers@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hubert</prenom>
					<nom>Naets</nom>
					<email>hubert.naets@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cédrick</prenom>
					<nom>Fairon</nom>
					<email>cedrick.fairon@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL, IL&amp;C, UCLouvain 1, Place Blaise Pascal, 1348 Louvain-la-Neuve</affiliation>
			</affiliations>
			<titre>AMESURE: une plateforme de lisibilité pour les textes administratifs</titre>
			<type>court</type>
			<pages>467-472</pages>
			<resume>Cet article présente une plateforme dédiée à l’évaluation de la difficulté des textes administratifs, dans un but d’aide à la rédaction. La plateforme propose d’une part une formule de lisibilité spécialisée pour les textes administratifs, dont la conception repose sur une nouvelle méthode d’annotation. Le modèle classe correctement 58% des textes sur une échelle à 5 niveaux et ne commet d’erreurs graves que dans 9% des cas. La plateforme propose d’autre part un diagnostic plus précis des difficultés spécifiques d’un texte, sous la forme d’indicateurs numériques, mais aussi d’une localisation de ces difficultés directement dans le document.</resume>
			<mots_cles>formule de lisibilité, textes administratifs, aide à la rédaction</mots_cles>
			<title></title>
			<abstract>This paper presents a platform aiming to assess the difficulty of administrative texts, mostly for editorial assistance purposes. The platform first offers a readability formula specialized for administrative texts, the development of which required the design of a dedicated annotation procedure. The resulting model correctly classifies 58% of the texts on a 5-levels scale and commits serious misclassifications in only 9% of the cases. Moreover, the platform offers a more accurate diagnosis of the difficulty of a text in the form of numerical indicators corresponding to various textual characteristics. It also locates specific local difficulties directly in the text.</abstract>
			<keywords>readability formula, administrative texts, editorial assistance</keywords>
		</article>
		<article id="taln-2014-court-015" session="Sentiments">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Brun</nom>
					<email>caroline.brun@xerox.xrce.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>Roux</nom>
					<email>claude.roux@xerox.xrce.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">XRCE, 6, chemin de Maupertuis, 38240 Meylan</affiliation>
			</affiliations>
			<titre>Décomposition des « hash tags » pour l’amélioration de la classification en polarité des « tweets »</titre>
			<type>court</type>
			<pages>473-478</pages>
			<resume>Les « mots dièses» ou « hash tags » sont le moyen naturel de lier entre eux différents tweets. Certains « hash tags » sont en fait de petites phrases dont la décomposition peut se révéler particulièrement utile lors d’une analyse d’opinion des tweets. Nous allons montrer dans cet article comment l’on peut automatiser cette décomposition et cette analyse de façon à améliorer la détection de la polarité des tweets.</resume>
			<mots_cles>hash tag, tweet, analyse d’opinion, TAL</mots_cles>
			<title></title>
			<abstract>Hash tags are the natural way through which tweets are linked to each other. Some of these hash tags are actually little sentences, whose decompositions can prove quite useful when mining opinions from tweets. We will show in this article how we can automatically detect the inner polarity of these hash tags, through their decomposition and analysis.</abstract>
			<keywords>hash tag, tweet, opinion mining, NLP</keywords>
		</article>
		<article id="taln-2014-court-016" session="Sentiments">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Langlet</nom>
					<email>caroline.langlet@telecom-paristech.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Chloé</prenom>
					<nom>Clavel</nom>
					<email>chloe.clavel@telecom-paristech.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut Mines-Télécom ; Télécom ParisTech ; CNRS LTCI, Paris</affiliation>
			</affiliations>
			<titre>Modélisation des questions de l’agent pour l’analyse des affects, jugements et appréciations de l’utilisateur dans les interactions humain-agent</titre>
			<type>court</type>
			<pages>479-484</pages>
			<resume>Cet article aborde la question des expressions d’attitudes (affects, jugements, appréciations) chez l’utilisateur dans le cadre d’échanges avec un agent virtuel. Il propose une méthode pour l’analyse des réponses à des questions fermées destinée à interroger les attitudes de l’utilisateur. Cette méthode s’appuie sur une formalisation des questions de l’agent – sous la forme d’une fiche linguistique – et sur une analyse de la réponse de l’utilisateur, pour créer un modèle utilisateur. La fiche linguistique de l’agent est structurée par un ensemble d’attributs relatifs, d’une part, à l’attitude à laquelle réfère la question, d’autre part, à sa forme morphosyntaxique. L’analyse de la réponse, quant à elle, repose sur un ensemble de règles sémantiques et syntaxiques définies par une grammaire formelle. A partir des résultats fournis par cette analyse et des informations contenues dans la fiche linguistique de l’agent, des calculs sémantiques sont effectués pour définir la valeur de la réponse et construire le modèle utilisateur.</resume>
			<mots_cles>affect, jugement, appréciation, interaction humain-agent, questions-réponses</mots_cles>
			<title></title>
			<abstract>This paper tackles the issue of user’s attitudinal expressions in an human-agent interaction. It introduces a method for analysing the user’s anwers to the agent’s yes/no questions about attitude. In order to build a user model, this method relies on a formalization of the agent’s questions and a linguistic analysis of the user’s answer. This formalization comprises a set of attributes regarding the attitude expressed and the morphosyntaxic form of the question. The answer is then analyzed by using a set of semantic and syntactic rules included in a formal grammar. Finally, the semantic value of the answer can be calculated by using the results provided by this analysis and the information given by the question formalization. This computation is next integrated in the user model.</abstract>
			<keywords>affect, jugement, appreciation, human-agent interaction, questions-answers</keywords>
		</article>
		<article id="taln-2014-court-017" session="Outils">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Barthélemy</nom>
					<email>francois.barthelemy@inria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA – Alpage, domaine de Voluceau 78153 Rocquencourt</affiliation>
				<affiliation affiliationId="2">CNAM – Cedric, 292 rue Saint-Martin, 75003 Paris</affiliation>
			</affiliations>
			<titre>KNG: un outil pour l’écriture facile de cascades de transducteurs</titre>
			<type>court</type>
			<pages>485-490</pages>
			<resume>Cet article présente une bibliothèque python appelée KNG permettant d’écrire facilement des automates et transducteurs finis. Grâce à une gestion soigneuse des codages et des entrées-sorties, cette bibliothèque permet de réaliser une cascade de transducteurs au moyen de tubes unix reliant des scripts python.</resume>
			<mots_cles>Machines finies à états, cascade de transducteur, expression régulière</mots_cles>
			<title></title>
			<abstract>This paper presents a Python library called KNG which provides facilities to write easily finite state automata and transducers. Through careful management of encodings and input/output, a transducer cascade may be implented using unix pipes connecting python scripts.</abstract>
			<keywords>Finite State Machines, Transducer Cascade, Regular Expression</keywords>
		</article>
		<article id="taln-2014-court-018" session="Outils">
			<auteurs>
				<auteur>
					<prenom>Raoul</prenom>
					<nom>Blin</nom>
					<email>blin@ehess.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de Recherches Linguistiques sur l’Asie Orientale, EHESS 131 bd St-Michel, 75005 Paris, France.</affiliation>
			</affiliations>
			<titre>Comparaison de deux outils d'analyse de corpus japonais pour l'aide au linguiste, Sagace et Mecab</titre>
			<type>court</type>
			<pages>491-498</pages>
			<resume>L'objectif est de comparer deux outils d'analyse de corpus de textes bruts pour l'aide à la recherche en linguistique japonaise. Les deux outils représentent chacun une approche spécifique. Le premier, Sagace, recherche un patron sans prise en compte de son environnement. Le second, un dispositif à base de Mecab, recherche les patrons après analyse morphologique complète des phrases. Nous comparons les performances en temps et en précision. Il ressort de cette analyse que les performances de Sagace sont globalement un peu inférieures à celles des dispositifs à base de Mecab, mais qu'elles restent tout à fait honorables voire meilleures pour certaines tâches.</resume>
			<mots_cles>Japonais, Corpus, Analyseurs, Mecab, Sagace</mots_cles>
			<title></title>
			<abstract>The purpose is to compare two tools used for helping linguist to analyze large corpora of raw japanese text. Each tool is representative of a specific approach. The first one, Sagace, search a pattern without taking into account its distribution. The second one is based on the morphological analyzer Mecab. It first analyzes the whole sentence before counting the searched pattern. We compare the processing time, needed ressources, and the quality of the results. It appears that performances of Sagace are globaly slightly lower than the Mecab system, but it doesn't defer so much. It may even be punctually better.</abstract>
			<keywords>Japanese, Corpus, Comparison, Mecab, Sagace</keywords>
		</article>
		<article id="taln-2014-court-019" session="Outils">
			<auteurs>
				<auteur>
					<prenom>Giulia</prenom>
					<nom>Barreca</nom>
					<email>giulia.barreca@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>George</prenom>
					<nom>Christodoulides</nom>
					<email>george@mycontent.gr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire MoDyCo, CNRS, Université Paris Ouest Nanterre La Défense 200, avenue de la République, FR-92001 Nanterre, France</affiliation>
				<affiliation affiliationId="2">Centre Valibel, Institut Langue &amp; Communication, Université de Louvain, Place Blaise Pascal 1, 1348 Louvain-la-Neuve, Belgique</affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Un concordancier multi-niveaux et multimédia pour des corpus oraux</titre>
			<type>court</type>
			<pages>499-504</pages>
			<resume>Les concordanciers jouent depuis longtemps un rôle important dans l’analyse des corpus linguistiques, tout comme dans les domaines de la philologie, de la littérature, de la traduction et de l’enseignement des langues. Toutefois, il existe peu de concordanciers qui soient capables d’associer des annotations à plusieurs niveaux et synchronisées avec le signal sonore. L’essor des grands corpus de français parlé introduit une augmentation des exigences au niveau de la performance. Dans ce travail à caractère préliminaire, nous avons développé un prototype de concordancier multi-niveaux et multimédia, que nous avons testé sur le corpus de français parlé du projet Phonologie du Français Contemporain (PFC, 1,5 million de tokens de transcription alignée au niveau de l’énoncé). L’outil permet non seulement d’enrichir les résultats des concordances grâce aux données relevant de plusieurs couches d’annotation du corpus (annotation morphosyntaxique, lemme, codage de la liaison, codage du schwa etc.), mais aussi d’élargir les modalités d’accès au corpus.</resume>
			<mots_cles>concordancier, annotation multi-niveaux, linguistique de corpus, didactique du FLE</mots_cles>
			<title></title>
			<abstract>Concordances have always played an important role in the analysis of language corpora, for studies in humanities, literature, linguistics, translation and language teaching. However, very few of the available systems support multi-level queries against a richly-annotated, sound-aligned spoken corpus. The rapid growth in the development of spoken corpora, particularly for French, increases the need for scalable, high-performance solutions. We present the preliminary results of our project to develop a multi-level multimedia concordancer for spoken language corpora. We test our prototype on the PFC corpus of spoken French (1.5 million tokens, transcriptions aligned to the utterance level). Our tool allows researchers to query the corpus and produce concordances correlating several annotation levels (part-of-speech tags, lemmas, annotation of phonological phenomena such as the liaison and schwa, etc.) while allowing for multi-modal access to the associated sound recordings and other data.</abstract>
			<keywords>concordance tool, multi-level annotation, corpus linguistics, French language teaching</keywords>
		</article>
		<article id="taln-2014-court-020" session="Étiquetage 2">
			<auteurs>
				<auteur>
					<prenom>Perrine</prenom>
					<nom>Brusini</nom>
					<email>pbrusini@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Amsili</nom>
					<email>amsili@linguist.univ-paris-diderot.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Chemla</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Christophe</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Language, Cognition and Development Lab, Int. School for Advanced Studies (SISSA), Trieste</affiliation>
				<affiliation affiliationId="2">Laboratoire de Linguistique Formelle, CNRS &amp; Université Paris Diderot</affiliation>
				<affiliation affiliationId="3">Laboratoire de Sciences Cognitives et Psycholinguistique (CNRS &amp; ENS, EHESS)</affiliation>
			</affiliations>
			<titre>Simulation de l’apprentissage des contextes nominaux/verbaux par n-grammes</titre>
			<type>court</type>
			<pages>505-510</pages>
			<resume>On présente une étude d’apprentissage visant à montrer que les contextes locaux dans un corpus de parole adressée aux enfants peuvent être exploités, avec des méthodes statistiques simples, pour prédire la catégorie (nominale vs. verbale) d’un mot inconnu. Le modèle présenté, basé sur la mémorisation de n-grammes et sur une « graine sémantique » (un petit nombre de noms et verbes supposés connus et catégorisés) montre une excellente précision à toutes les tailles de graine sémantique, et un rappel plus faible, qui croît avec la taille de la graine sémantique. Les contextes les plus utilisés sont ceux qui contiennent des mots fonctionnels. Cette étude de faisabilité démontre que les très jeunes enfants pourraient exploiter les contextes de mots inconnus pour prédire leur catégorie syntaxique.</resume>
			<mots_cles>apprentissage, modélisation de l’acquisition du langage, n-grammes</mots_cles>
			<title></title>
			<abstract>A learning study is presented whose aim is to show that local contexts, in a child-directed speech corpus, can be exploited, with simple statistical methods, to predict the category (noun vs. verb) of unknown words. The model we present here is based on the memorisation of n-grams and on a “semantic seed” (a small number of nouns and verbs supposedly known and well categorised). It shows an excellent precision for every size of the semantic seed, and its recall grows along with the size of the semantic seed. The most useful contexts are the ones that include function words. This feasibility study shows that very young children could exploit the contexts of unknown words to predict their syntactic category.</abstract>
			<keywords>learning, language acquisition modeling, n-gram</keywords>
		</article>
		<article id="taln-2014-court-021" session="Étiquetage 2">
			<auteurs>
				<auteur>
					<prenom>Anaïs</prenom>
					<nom>Ollagnier</nom>
					<email>anais.ollagnier@openedition.org</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sébastien</prenom>
					<nom>Fournier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Bellot</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Béchet</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix-Marseille Université, CNRS, ENSAM, Université de Toulon LSIS UMR 7296, 13397, Marseille, France</affiliation>
				<affiliation affiliationId="2">Aix-Marseille Université, CNRS, CLEO OpenEdition UMS 3287, 13451, Marseille, France</affiliation>
				<affiliation affiliationId="3">Aix-Marseille Université, CNRS, LIF UMR 7279, 13288, Marseille, France</affiliation>
			</affiliations>
			<titre>Impact de la nature et de la taille des corpus d'apprentissage sur les performances dans la détection automatique des entités nommées</titre>
			<type>court</type>
			<pages>511-516</pages>
			<resume>Nous présentons une étude comparative sur l’impact de la nature et de la taille des corpus d’apprentissage sur les performances dans la détection automatique des entités nommées. Cette évaluation se présente sous la forme de multiples modulations de trois corpus français. Deux des corpus sont issus du catalogue des ressources linguistiques d’ELRA et le troisième est composé de documents extraits de la plateforme OpenEdition.org.</resume>
			<mots_cles>Reconnaissance d'entités nommées, Adaptation au domaine, comparaison d'outils</mots_cles>
			<title></title>
			<abstract>We present a comparative study on the impact of the nature and size of the training corpus on performance in automatic named entities recognition. This evaluation is in the form of multiple modulations on three French corpus. Two corpora are from the catalog of the European Language Resources Association (ELRA) and the third is composed of documents extract from the OpenEdition.org platform.</abstract>
			<keywords>Named entity recognition, Domain adptation, performance comparison</keywords>
		</article>
		<article id="taln-2014-court-022" session="Étiquetage 2">
			<auteurs>
				<auteur>
					<prenom>Meryem</prenom>
					<nom>Talha</nom>
					<email>meriem.talha@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Siham</prenom>
					<nom>Boulaknadel</nom>
					<email>boulaknadel@ircam.ma</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Driss</prenom>
					<nom>Aboutajdine</nom>
					<email>aboutaj@fsr.ac.ma</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LRIT, Unité Associée au CNRST (URAC 29), Faculté des Sciences, Mohammed V-Agdal, Rabat, Maroc</affiliation>
				<affiliation affiliationId="2">IRCAM, Avenue Allal El Fassi, Madinat Al Irfane, Rabat-Instituts, Maroc</affiliation>
			</affiliations>
			<titre>RENAM: Système de Reconnaissance des Entités Nommées Amazighes</titre>
			<type>court</type>
			<pages>517-524</pages>
			<resume>La reconnaissance des Entités Nommées (REN) en langue amazighe est un prétraitement potentiellement utile pour de nombreuses applications du traitement de la langue amazighe. Cette tâche représente toutefois un sévère challenge, compte tenu des particularités de cette langue. Dans cet article, nous présentons le premier système d’extraction d’entités nommées amazighes (RENAM) fondé sur une approche symbolique qui utilise le principe de transducteur à états finis disponible sous la plateforme GATE.</resume>
			<mots_cles>Reconnaissance des entités nommées (REN), Langue Amazighe, Règles d’annotation, JAPE, GATE</mots_cles>
			<title></title>
			<abstract>Named Entity Recognition (NER) for Amazigh language is a potentially useful pretreatment for many processing applications for the Amazigh language. However, this task represents a tough challenge, given the specificities of this language. In this paper, we present (NERAM) the first named entity system for the Amazigh language based on a symbolic approach that uses linguistic rules built manually by using an information extraction tool available within the platform GATE.</abstract>
			<keywords>Named Entities Recognition (NER), Amazigh Language, Annotation Rules, JAPE, GATE</keywords>
		</article>
		<article id="taln-2014-court-023" session="Étiquetage 2">
			<auteurs>
				<auteur>
					<prenom>Ingrid</prenom>
					<nom>Falk</nom>
					<email>ifalk@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>dbernhard@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Gérard</nom>
					<email>christophegerard@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LiLPa, Université de Strasbourg</affiliation>
			</affiliations>
			<titre>De la quenelle culinaire à la quenelle politique : identification de changements sémantiques à l’aide des Topic Models</titre>
			<type>court</type>
			<pages>525-530</pages>
			<resume>Dans cet article nous employons le « topic modeling » pour explorer des chemins vers la détection automatique de l’apparition de nouveaux sens pour des mots connus. Nous appliquons les méthodes introduites dans (Lau et al., 2012, 2014) à un cas de néologie sémantique récent, l’apparition du nouveau sens de geste pour le mot « quenelle ». Nos expériences mettent en évidence le potentiel de cette approche pour l’apprentissage des sens du mot, l’alignement des topics à des sens de dictionnaire et enfin la détection de nouveaux sens.</resume>
			<mots_cles>topic models, induction de sens, néologie sémantique</mots_cles>
			<title></title>
			<abstract>In this study we explore topic modeling for the automatic detection of new senses of known words. We apply methods developed in previous work for English (Lau et al., 2012, 2014) on a recent case of new word sense induction in French, namely the appearence of the new meaning of gesture for the word « quenelle ». Our experiments illustrate the potential of this approach at learning word senses, aligning the topics with dictionary senses and finally at detecting the new senses.</abstract>
			<keywords>topic models, word sense induction, semantic neologism</keywords>
		</article>
		<article id="taln-2014-court-024" session="Langue des signes">
			<auteurs>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Filhol</nom>
					<email>michael.filhol@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI–CNRS, B. P. 133, 91403 Orsay cedex</affiliation>
			</affiliations>
			<titre>Grammaire récursive non linéaire pour les langues des signes</titre>
			<type>court</type>
			<pages>531-536</pages>
			<resume>Cet article propose une approche pour la formalisation de grammaires pour les langues des signes, rendant compte de leurs particularités linguistiques. Comparable aux grammaires génératives en termes de récursivité productive, le système présente des propriétés nouvelles comme la multi-linéarité permettant la spécification simultanée des articulateurs. Basé sur l’analyse des liens entre formes produites/observées et fonctions linguistiques au sens large, on observe un décloisonnement des niveaux traditionnels de construction de la langue, inhérent à la méthodologie employée. Nous présentons un ensemble de règles trouvées suivant la démarche présentée et concluons avec une perspective intéressante en traduction automatique vers la langue des signes.</resume>
			<mots_cles>Grammaire formelle, multi-linéarité, langue des signes</mots_cles>
			<title></title>
			<abstract>This article presents a formal approach to Sign Language grammars, with the aim of capturing their specificities. The system is comparable to generative grammar in the sense that it is recursively productive, but it has quite different properties such as multilinearity, enabling simultaneous articulator specification. As it is based on the analysis of systematic links between observable form features and interpreted linguistic functions in the general sense, the traditionally separate linguistic levels all end up covered by the same model. We present the results found for a set of linguistic structures, following the presented methodology, and conclude with an interesting prospect in the field of text-to-Sign machine translation.</abstract>
			<keywords>Formal grammar, multilinearity, Sign Language</keywords>
		</article>
		<article id="taln-2014-court-025" session="Langue des signes">
			<auteurs>
				<auteur>
					<prenom>Rémi</prenom>
					<nom>Dubot</nom>
					<email>remi.dubot@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Arturo</prenom>
					<nom>Curiel</nom>
					<email>arturo.curiel@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Collet</nom>
					<email>christophe.collet@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université de Toulouse, 118 route de Narbonne, 31062 Toulouse Cedex 9, France</affiliation>
			</affiliations>
			<titre>Vers un traitement automatique en soutien d’une linguistique exploratoire des Langues des Signes</titre>
			<type>court</type>
			<pages>537-542</pages>
			<resume>Les langues des signes sont les langues naturelles utilisées dans les communautés sourdes. Elles ont suscitées, ces dernières années, de l’intérêt dans le domaine du traitement automatique des langues naturelles. Néanmoins, il y a un manque général de données de terrain homogènes. Notre réflexion porte sur les moyens de soutenir la maturation des modèles linguistiques avant d’entamer un large effort d’annotation. Cet article présente des pré-requis pour la réalisation d’outils s’inscrivant dans cette démarche. L’exposé est illustré avec deux outils développés pour les langues des signes : le premier utilise une logique adaptée pour la représentation de modèles phonologiques et le second utilise des grammaires formelles pour la représentation de modèles syntaxiques.</resume>
			<mots_cles>Langues des Signes, Formalismes de modélisation, Reconnaissance, Annotation semi-automatique</mots_cles>
			<title></title>
			<abstract>Sign Languages (SLs) are the vernaculars of deaf communities, they have drawn interests in the natural language processing research in recent years. However, the field suffers a general lack of homogeneous information. Our reflexion is about how to support the maturation of models before starting a consequent annotation effort. In this paper, we describe the requirements of tools supporting such an approach. It is illustrated with two examples of work developed following these guidelines. The first one aims at phonological representation using logic and the second one targets supra-lexical features recognition.</abstract>
			<keywords>Sign Languages, Modeling formalisms, Recognition, semi-automatic annotation</keywords>
		</article>
		<article id="taln-2014-court-026" session="Résumé automatique">
			<auteurs>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Oufaida</nom>
					<email>h_oufaida@esi.dz</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Omar</prenom>
					<nom>Nouali</nom>
					<email>onouali@mail.cerist.dz</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>blache@lpl-aix.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Ecole Nationale Supérieure d’Informatique ESI, BP 68M Oued Smar, 16270, El Harrach Alger Algérie</affiliation>
				<affiliation affiliationId="2">Centre de Recherche sur l'Information Scientifique et Technique CERIST, Rue Des 3 Frères Aissou, Ben Aknoun Alger Algérie</affiliation>
				<affiliation affiliationId="3">LPL, AMU, CNRS, 5 avenue Pasteur, 13100 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Résumé Automatique Multilingue Expérimentations sur l’Anglais, l’Arabe et le Français</titre>
			<type>court</type>
			<pages>543-549</pages>
			<resume>La tâche du résumé multilingue vise à concevoir des systèmes de résumé très peu dépendants de la langue. L’approche par extraction est au coeur de ces systèmes, elle permet à l’aide de méthodes statistiques de sélectionner les phrases les plus pertinentes dans la limite de la taille du résumé. Dans cet article, nous proposons une approche de résumé multilingue, elle extrait les phrases dont les termes sont des plus discriminants. De plus, nous étudions l'impact des différents traitements linguistiques de base : le découpage en phrases, l'analyse lexicale, le filtrage des mots vides et la racinisation sur la couverture ainsi que la notation des phrases. Nous évaluons les performances de notre approche dans un contexte multilingue : l'anglais, l'arabe et le français en utilisant le jeu de données TAC MultiLing 2011.</resume>
			<mots_cles>Résumé multilingue, analyse discriminante, TAL, évaluation multilingue</mots_cles>
			<title></title>
			<abstract>The task of multilingual summarization aims to design free-from language systems. Extractive methods are in the core of multilingual summarization systems. In this paper, we discuss the influence of various basic NLP tasks: sentence splitting, tokenization, stop words removal and stemming on sentence scoring and summaries' coverage. Hence, we propose a statistical method which extracts most relevant sentences on the basis of their terms discriminant power. We conduct several experimentations in a multilingual context: English, Arabic and French using the TAC MultiLing 2011 dataset.</abstract>
			<keywords>Multilingual summarization, Discriminant analysis, NLP, Multilingual evaluation</keywords>
		</article>
		<article id="taln-2014-court-027" session="Résumé automatique">
			<auteurs>
				<auteur>
					<prenom>Rémi</prenom>
					<nom>Bois</nom>
					<email>remi.bois@etu.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Johannes</prenom>
					<nom>Leveling</nom>
					<email>jleveling@computing.dcu.ie</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lorraine</prenom>
					<nom>Goeuriot</nom>
					<email>lgoeuriot@computing.dcu.ie</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gareth J. F.</prenom>
					<nom>Jones</nom>
					<email>gjones@computing.dcu.ie</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Liadh</prenom>
					<nom>Kelly</nom>
					<email>lkelly@computing.dcu.ie</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, Université de Nantes, France</affiliation>
				<affiliation affiliationId="2">CNGL, School of Computing, Dublin City University, Dublin 9, Ireland</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>550-555</pages>
			<resume>Nous présentons dans cet article l’adaptation de l’outil de résumé automatique REZIME à la langue française. REZIME est un outil de résumé automatique mono-document destiné au domaine médical et s’appuyant sur des critères statistiques, syntaxiques et lexicaux pour extraire les phrases les plus pertinentes. Nous décrivons dans cet article le système REZIME tel qu’il a été conçu et les différentes étapes de son adaptation à la langue française. Les performances de l’outil adapté au français sont mesurées et comparées à celle de la version anglaise. Les résultats montrent que l’adaptation au français ne dégrade pas les performances de REZIME, qui donne des résultats équivalents dans les deux langues.</resume>
			<mots_cles>Résumé automatique, multilangue, domaine médical</mots_cles>
			<title>Porting a Summarizer to the French Language</title>
			<abstract>We describe the porting of the English language REZIME text summarizer to the French language. REZIME is a single-document summarizer particularly focused on summarization of medical documents. Summaries are created by extracting key sentences from the original document. The sentence selection employs machine learning techniques, using statistical, syntactic and lexical features which are computed based on specialized language resources. The REZIME system was initially developed for English documents. In this paper we present the summarizer architecture, and describe the steps required to adapt it to the French language. The summarizer performance is evaluated for English and French datasets. Results show that the adaptation to French results in system performance comparable to the initial English system.</abstract>
			<keywords>single-document summarization, multilingual, medical domain</keywords>
		</article>
		<article id="taln-2014-court-028" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Bigi</nom>
					<email>brigitte.bigi@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Tatsuya</prenom>
					<nom>Watanabe</nom>
					<email>tatsuya.watanabe@atilf.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage, AMU, CNRS, 5 avenue Pasteur, 13100 Aix-en-Provence</affiliation>
				<affiliation affiliationId="2">Ortolang</affiliation>
			</affiliations>
			<titre>Extraction de données orales multi-annotées</titre>
			<type>court</type>
			<pages>556-561</pages>
			<resume>Cet article aborde le problème de l’extraction de données orales multi-annotées : nous proposons une solution intermédiaire, entre d’une part les systèmes de requêtages très évolués mais qui nécessitent des données structurées, d’autre part les données (multi-)annotées des utilisateurs qui sont hétérogènes. Notre proposition s’appuie sur 2 fonctions principales : une fonction booléenne pour filtrer sur le contenu, et une fonction de relation qui implémente l’algèbre de Allen. Le principal avantage de cette approche réside dans sa généricité : le fonctionnement sera identique que les annotations proviennent de Praat, Transcriber, Elan ou tout autre logiciel d’annotation. De plus, deux niveaux d’utilisation ont été développés : une interface graphique qui ne nécessite aucune compétence ou connaissance spécifique de la part de l’utilisateur, et un interrogation par scripts en langage Python. L’approche a été implémentée dans le logiciel SPPAS, distribué sous licence GPL.</resume>
			<mots_cles>multimodalité, corpus, extraction</mots_cles>
			<title></title>
			<abstract>This paper addresses the problem of extracting multimodal annotated data in the linguistic field ranging from general linguistic to domain specific information. Our proposal can be considered as a solution or a least an intermediary solution that can link together requesting systems and expert data from various annotation tools. The system is partly based on the Allen algebra and consists in creating filters based on two functions : a boolean function and a relation function. The main advantage of this approach lies in its genericity : it will work identically with annotations from Praat, Transcriber, Elan or from any other annotation software. Furthermore, two levels of usage have been developed : a graphical user interface graph that not requires any skill or knowledge, and a query form in Python. This system is included in the software SPPAS and is distributed under the terms of the GPL license.</abstract>
			<keywords>multimodality, corpus, extraction</keywords>
		</article>
		<article id="taln-2014-court-029" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Anaïs</prenom>
					<nom>Lefeuvre</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Yves</prenom>
					<nom>Antoine</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Agata</prenom>
					<nom>Savary</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Schang</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lotfi</prenom>
					<nom>Abouda</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Denis</prenom>
					<nom>Maurel</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Iris</prenom>
					<nom>Eshkol</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais de Tours, laboratoire LI</affiliation>
				<affiliation affiliationId="2">Université d’Orléans, laboratoire LLL, UMR 7270</affiliation>
			</affiliations>
			<titre>Annotation de la temporalité en corpus : contribution à l'amélioration de la norme TimeML</titre>
			<type>court</type>
			<pages>562-567</pages>
			<resume>Cet article propose une analyse critique de la norme TimeML à la lumière de l’expérience d’annotation temporelle d’un corpus de français parlé. Il montre que certaines adaptations de la norme seraient conseillées pour répondre aux besoins du TAL et des sciences du langage. Sont étudiées ici les questions de séparation des niveaux d’annotation, de délimitation des éventualités dans le texte et de l’ajout d’une relation temporelle de type associative.</resume>
			<mots_cles>annotation temporelle, TimeML, éventualités, relations temporelles, expressions temporelles</mots_cles>
			<title></title>
			<abstract>This paper reports a critical analysis of the TimeML standard, in the light of a temporal annotation that was conducted on spoken French. It shows that the norm suffers from weaknesses that must be corrected to fit the needs of NLP and corpus linguistics. These limitations concern mainly 1) the separation of different levels of linguistic annotation, 2) the delimitation in the text of the events, and 3) the absence of a bridging temporal relation in the norm.</abstract>
			<keywords>temporal annotation, TimeML, eventualities, temporal relations, time expressions</keywords>
		</article>
		<article id="taln-2014-court-030" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Louise</prenom>
					<nom>Deléger</nom>
					<email>louise.deleger@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Névéol</nom>
					<email>aurelie.neveol@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI – CNRS UPR 3251, Orsay, France</affiliation>
			</affiliations>
			<titre>Identification automatique de zones dans des documents pour la constitution d’un corpus médical en français</titre>
			<type>court</type>
			<pages>568-573</pages>
			<resume>De nombreuses informations cliniques sont contenues dans le texte des dossiers électroniques de patients et ne sont pas directement accessibles à des fins de traitement automatique. Pour pallier cela, nous préparons un large corpus annoté de documents cliniques. Une première étape de ce travail consiste à séparer le contenu médical des documents et les informations administratives contenues dans les en-têtes et pieds de page. Nous présentons un système d’identification automatique de zones dans les documents cliniques qui offre une F-mesure de 0,97, équivalente à l’accord inter-annoteur de 0,98. Notre étude montre que le contenu médical ne représente que 60% du contenu total de notre corpus, ce qui justifie la nécessité d’une segmentation en zones. Le travail d’annotation en cours porte sur les sections médicales identifiées.</resume>
			<mots_cles>Traitement Automatique de la Langue Biomédicale, segmentation de documents, identification de zones</mots_cles>
			<title></title>
			<abstract>Much clinical information is contained in the free text of Electronic Health Records (EHRs) and is not available for automatic processing. To advance Natural Language Processing of the French clinical narrative, we are building a richly annotated large-scale corpus of French clinical documents. To access the most medically relevant content of EHRs we develop an automatic system to separate the core medical content from other document sections, such as headers and footers. The performance of automatic content extraction achieves 96.6% F-measure, on par with human inter-annotator agreement of 98%. We find that medically relevant content covers only 60% of clinical documents in our corpus. Future annotation work will focus on these sections.</abstract>
			<keywords>BioNLP, Automatic document segmentation, section identification</keywords>
		</article>
		<article id="taln-2014-court-031" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Perrier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie</prenom>
					<nom>Candito</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Corentin </prenom>
					<nom>Ribeyre</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Djamé</prenom>
					<nom>Seddah</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lorraine/LORIA</affiliation>
				<affiliation affiliationId="2">Université Paris Diderot/INRIA</affiliation>
				<affiliation affiliationId="3">Inria Nancy Grand-Est/LORIA</affiliation>
				<affiliation affiliationId="4">Université Paris Sorbonne/INRIA</affiliation>
			</affiliations>
			<titre>Un schéma d’annotation en dépendances syntaxiques profondes pour le français</titre>
			<type>court</type>
			<pages>574-579</pages>
			<resume>À partir du schéma d’annotation en dépendances syntaxiques de surface du corpus Sequoia, nous proposons un schéma en dépendances syntaxiques profondes qui en est une abstraction exprimant les relations grammaticales entre mots sémantiquement pleins. Quand ces relations grammaticales sont partie prenante de diathèses verbales, ces diathèses sont vues comme le résultat de redistributions à partir d’une diathèse canonique et c’est cette dernière qui est retenue dans notre schéma d’annotation syntaxique profonde.</resume>
			<mots_cles>schéma d’annotation, syntaxe profonde, grammaires de dépendance</mots_cles>
			<title></title>
			<abstract>We describe in this article an annotation scheme for deep dependency syntax, built from the surface annotation scheme of the Sequoia corpus, abstracting away from it and expressing the grammatical relations between content words. When these grammatical relations take part into verbal diatheses, we consider the diatheses as resulting from redistributions from the canonical diathesis, which we retain in our annotation scheme.</abstract>
			<keywords>annotation scheme, deep syntax, dependency grammar</keywords>
		</article>
		<article id="taln-2014-court-032" session="Traitement de corpus 2">
			<auteurs>
				<auteur>
					<prenom>Elisa</prenom>
					<nom>Omodei</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yufan</prenom>
					<nom>Guo</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Cointet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Poibeau</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
			</affiliations>
			<titre>Analyse argumentative du corpus de l’ACL (ACL Anthology)</titre>
			<type>court</type>
			<pages>580-585</pages>
			<resume>Cet article présente un essai d’application de l’analyse argumentative (text zoning) à l’ACL Anthology. Il s’agit ainsi de mieux caractériser le contenu des articles du domaine de la linguistique informatique afin de pouvoir en faire une analyse fine par la suite. Nous montrons que des technique récentes d’analyse argumentative fondées sur l’apprentissage faiblement supervisé permettent d’obtenir de bons résultats.</resume>
			<mots_cles>Analyse argumentative, corpus de textes scientifiques, ACL Anthology</mots_cles>
			<title></title>
			<abstract>This paper presents an application of Text Zoning to the ACL Anthology. Text Zoning is known to be useful to characterize the content of papers, especially in the scientific domain. We show that recent techniques based on weakly supervised learning obtain excellent results on the ACL Anthology. Although these kinds of techniques is known in the domain, it is the first time it is applied to the whole ACL Anthology.</abstract>
			<keywords>Text Zoning, Corpus of scientific texts, ACL Anthology</keywords>
		</article>
		<article id="taln-2014-court-033" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Veronika</prenom>
					<nom>Lux-Pogodalla</nom>
					<email>Veronika.Lux@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS, ATILF, UMR 7118 Nancy, F-54000, France</affiliation>
			</affiliations>
			<titre>Intégration relationnelle des exemples lexicographiques dans un réseau lexical</titre>
			<type>court</type>
			<pages>586-591</pages>
			<resume>Nous présentons un ensemble d’exemples lexicographiques intégré dans le Réseau Lexical du Français et explorons son intérêt potentiel en tant que corpus annoté pour la recherche en désambiguisation sémantique automatique.</resume>
			<mots_cles>Réseau Lexical du Français, exemples lexicographiques, corpus annoté sémantiquement</mots_cles>
			<title></title>
			<abstract>This paper presents a set of lexicographic examples which is being developped along the French Lexical Network. The possibility of using this set as an annotated corpus for research on automatic Word Sense Disambiguation is examined.</abstract>
			<keywords>French Lexical Network, lexicographic examples, semantically annotated corpus</keywords>
		</article>
		<article id="taln-2014-court-034" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>mathieu.lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Le Brun</nom>
					<email>imaginat@imaginat.name</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Virginie</prenom>
					<nom>Zampa</nom>
					<email>virginie.zampa@u-grenoble3.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lirmm, Université Montpellier 2, France</affiliation>
				<affiliation affiliationId="2">Imagin@t, 34400 Lunel</affiliation>
				<affiliation affiliationId="3">Lidilem, Grenoble 3 BP25, 38040 Grenoble cedex 9, France</affiliation>
			</affiliations>
			<titre>Les couleurs des gens</titre>
			<type>court</type>
			<pages>592-597</pages>
			<resume>En TAL et plus particulièrement en analyse sémantique, les informations sur la couleur peuvent être importantes pour traiter correctement des informations textuelles (sens des mots, désambiguïsation et indexation). Plus généralement, connaître la ou les couleurs habituellement associée(s) à un terme est une information cruciale. Dans cet article, nous montrons comment le crowdsourcing, à travers un jeu, peut être une bonne stratégie pour collecter ces données lexico-sémantiques.</resume>
			<mots_cles>association couleur-mot, réseau lexical, crowdsourcing</mots_cles>
			<title></title>
			<abstract>In Natural Language Processing and semantic analysis in particular, color information may be important in order to properly process textual information (word sense disambiguation, and indexing). More specifically, knowing which colors are generally associated to terms is a crucial information. In this paper, we explore how crowdsourcing through a game with a purpose (GWAP) can be an adequate strategy to collect such lexico-semantic data.</abstract>
			<keywords>Word Color Associations, Lexical Network, Crowdsourcing</keywords>
		</article>
		<article id="taln-2014-court-035" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Mohammad</prenom>
					<nom>Nasiruddin</nom>
					<email>Mohammad.Nasiruddin@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Schwab</nom>
					<email>Didier.Schwab@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Andon</prenom>
					<nom>Tchechmedjiev</nom>
					<email>Andon.Tchechmedjiev@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gilles</prenom>
					<nom>Sérasset</nom>
					<email>Gilles.Serasset@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hervé</prenom>
					<nom>Blanchon</nom>
					<email>Hervé.Blanchon@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Univ. Grenoble Alpes</affiliation>
			</affiliations>
			<titre>Induction de sens pour enrichir des ressources lexicales</titre>
			<type>court</type>
			<pages>598-603</pages>
			<resume>En traitement automatique des langues, les ressources lexico-sémantiques ont été incluses dans un grand nombre d’applications. La création manuelle de telles ressources est consommatrice de temps humain et leur couverture limitée ne permet pas toujours de couvrir les besoins des applications. Ce problème est encore plus important pour les langues moins dotées que le français ou l’anglais. L’induction de sens présente dans ce cadre une piste intéressante. À partir d’un corpus de texte, il s’agit d’inférer les sens possibles pour chacun des mots qui le composent. Nous étudions dans cet article une approche basée sur une représentation vectorielle pour chaque occurrence d’un mot correspondant à ses voisins. À partir de cette représentation, construite sur un corpus en bengali, nous comparons plusieurs approches de classification non-supervisées (k-moyennes, regroupement hiérarchique et espérance-maximisation) des occurrences d’un mot pour déterminer les différents sens qu’il peut prendre. Nous comparons nos résultats au Bangla WordNet ainsi qu’à une référence établie pour l’occasion. Nous montrons que cette méthode permet de trouver des sens qui ne se trouvent pas dans le Bangla WordNet.</resume>
			<mots_cles>Induction de sens, bengali, Weka, Classification non-supervisée</mots_cles>
			<title></title>
			<abstract>In natural language processing, lexico-semantic resources are used in many applications. The manual creation of such resources is very time consuming and their limited coverage does not always satisfy the needs of applications. This problem is further exacerbated with lesser resourced languages. However, in that context, Word Sense Induction (WSI) offers an interesting avenue towards a solution. The purpose of WSI is, from a text corpus, to infer the possible senses for each word contained therein. In this paper, we study an approach based on a vectorial representation of the cooccurrence of word with their neighbours across each usage context. We first build the vectorial representation on a Bangla (also known as Bengali) corpus and then apply and compare three clustering algorithms (k-Means, Hierarchical Clustering and Expectation Maximisation) that elicit clusters corresponding to the different senses of each word as used within a corpus. We wanted to use Bangla WordNet to evaluate the clusters, however, the coverage of Bangla WordNet being restrictive compared to Princeton WordNet ( 23.65%), we find that the clustering algorithms induce correct senses that are not present in Bangla WordNet. Therefore we created a gold standard that we manually extended to include the senses not covered in Bangla WordNet.</abstract>
			<keywords>Word Sense Induction, Bangla, Weka, Clustering</keywords>
		</article>
		<article id="taln-2014-court-036" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Trouilleux</nom>
					<email>francois.trouilleux@univ-bpclermont.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Clermont Université, Université Blaise-Pascal, EA 999, LRL</affiliation>
			</affiliations>
			<titre>Un dictionnaire et une grammaire de composés français</titre>
			<type>court</type>
			<pages>604-609</pages>
			<resume>L’article présente deux ressources pour le TAL, distribuées sous licence GPL : un dictionnaire de mots composés français et une grammaire NooJ spécifiant un sous-ensemble des schémas de composés.</resume>
			<mots_cles>open source, ressources, dictionnaire, grammaire, mots composés</mots_cles>
			<title></title>
			<abstract>The paper introduces two resources for NLP, available with a GPL license: a dictionary of French compound words and a NooJ grammar which specifies a subset of compound patterns.</abstract>
			<keywords>open source, resources, dictionary, grammar, compound words</keywords>
		</article>
		<article id="taln-2014-demo-001" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Jean-Marie</prenom>
					<nom>Pierrel</nom>
					<email>Jean-Marie.Pierrel@atilf.fr</email>
					<email>contact@ortolang.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lorraine, ATILF, 44 avenue de la Libération 54063 Nancy Cedex</affiliation>
				<affiliation affiliationId="2">CNRS, ATILF, 44 avenue de la Libération 54063 Nancy Cedex</affiliation>
			</affiliations>
			<titre>ORTOLANG : une infrastructure de mutualisation de ressources linguistiques écrites et orales</titre>
			<type>démonstration</type>
			<pages>1-2</pages>
			<resume>Nous proposons une démonstration de la Plateforme de l’Equipex ORTOLANG (Open Resources and Tools for LANGuage : www.ortolang.fr) en cours de mise en place dans le cadre du programme d’investissements d’avenir (PIA) lancé par le gouvernement français. S’appuyant entre autres sur l’existant des centres de ressources CNRTL (Centre National de Ressources Textuelles et Lexicales : www.cnrtl.fr) et SLDR (Speech and Language Data Repository : http://sldr.org/), cette infrastructure a pour objectif d’assurer la gestion, la mutualisation, la diffusion et la pérennisation de ressources linguistiques de type corpus, dictionnaires, lexiques et outils de traitement de la langue, avec une focalisation particulière sur le français et les langues de France.</resume>
			<mots_cles>Ortolang, plateforme, mutualisation, corpus, ressources linguistiques</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2014-demo-002" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Baptiste</prenom>
					<nom>Chardon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Louis</prenom>
					<nom>Saint-Maxent</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Séguéla</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
			</affiliations>
			<titre>Utilisabilité d'une ressource propriétaire riche dans le cadre de la classification de documents</titre>
			<type>démonstration</type>
			<pages>3-8</pages>
			<resume>Dans ce papier, nous nous intéressons à l’utilisation d’une ressource linguistique propriétaire riche pour une tâche de classification. L'objectif est ici de mesurer l'impact de l'ajout de ces ressources sur cette tâche en termes de performances. Nous montrons que l’utilisation de cette ressource en temps que traits supplémentaires de classification apporte un réel avantage pour un ajout très modéré en termes de nombre de traits.</resume>
			<mots_cles>classification de documents, classification automatique, ressources</mots_cles>
			<title></title>
			<abstract>In this paper, we focus on the use of a proprietary resource for a document classification task. The objective is here to measure the impact of the addition of this resource as input for classification features. We show that the use of this resource impacts positively the classification results, for a limited impact on the feature number.</abstract>
			<keywords>document level classification, automatic classification, resources</keywords>
		</article>
		<article id="taln-2014-demo-003" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Laroche</nom>
					<email>romain.laroche@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 38-40 avenue du Général Leclerc, 92130 Issy-les-Moulineaux / France</affiliation>
			</affiliations>
			<titre></titre>
			<type>démonstration</type>
			<pages>9-10</pages>
			<resume>Cette démonstration de CFAsT s’intéresse à “comment concevoir un système de dialogue avec un effort minimal”. Cet assistant virtuel repose sur un nouveau modèle pour la génération automatique de système de dialogue construite à partir de contenus. Cette approche utilise un moteur de recherche auquel on a ajouté des fonctionnalités de dialogue : à chaque tour, le système propose trois mots-clefs de manière à optimiser l’espérance de gain d’information.</resume>
			<mots_cles>Systèmes de dialogue, Traitement automatique des langues naturelles, Assistant virtuel</mots_cles>
			<title>CFAsT: Content-Finder AssistanT</title>
			<abstract>This CFAsT demonstration focuses on “how to design and develop a dialogue system with a minimal effort”. This virtual assistant embeds a novel model for automatic generation of dialogue systems built from contents. This approach is similar to and relies on a search engine, but with augmented dialogue capabilities : at each dialogue turn, the system propose three keywords, in order to optimise the information gain expectation.</abstract>
			<keywords>Dialogue systems, Natural language processing, Virtual assistant</keywords>
		</article>
		<article id="taln-2014-demo-004" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>André</prenom>
					<nom>Jaccarini</nom>
					<email>jaccarini@mmsh.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Gaubert</nom>
					<email>cgaubert@ifao.egnet.net</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">MMSH, CNRS, 5 rue du Château de l’Horloge, 13094 Aix-en-Provence</affiliation>
				<affiliation affiliationId="2">IFAO, 37 rue Cheikh Aly Yousef, Qasr al Ayni, Le Caire, Egypte</affiliation>
			</affiliations>
			<titre>Démonstration de Kawâkib, outil permettant d’assurer le feedback entre grammaire et corpus arabe pour l’élaboration d’un modèle théorique</titre>
			<type>démonstration</type>
			<pages>11-12</pages>
			<resume>Kawâkib est un outil assurant le feedback entre corpus arabe et grammaire. Ce logiciel interactif en ligne démontre le bien fondé de la méthode de variation des grammaires arabes pour l'obtention de l'algorithme optimal tant au niveau de l'analyse morphologique, cruciale étant donnée la structure du système sémitique, que syntaxique ou dans le domaine de la recherche de critères pertinents et discriminants pour le filtrage des textes.</resume>
			<mots_cles>arabe, automates, analyseurs, opérateurs linguistiques, mots-outils, filtrage de corpus</mots_cles>
			<title></title>
			<abstract>Kawâkib is a tool allowing feedback between arabic corpus and grammar. As far as methodology is concerned, this interactive online software implements and illustrates the grammar variation method that aims to determine the optimal algorithm, either for morphology – which is essential in semitic languages - or for syntax. The software also permits the search for criteria for text filtering.</abstract>
			<keywords>arabic, automata, parsers, lingistic operators, tool words, corpus filtering</keywords>
		</article>
		<article id="taln-2014-demo-005" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Dany</nom>
					<email>christophe.dany@owi-tech.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ilhème</prenom>
					<nom>Ghalamallah</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">OWI, 31 avenue du Général Leclerc, 92340 Bourg-la-Reine</affiliation>
			</affiliations>
			<titre>OWI.Chat : Assistance sémantique pour un conseiller Chat, grâce à la théorie OWI</titre>
			<type>démonstration</type>
			<pages>13-14</pages>
			<resume>La canal chat permet aux entreprises de transformer leur site web en un véritable lieu d’achat et de service. OWI a développé un outil d’assistance aux conversations en ligne (OWI.Chat), qui analyse les messages des internautes et conseille les conseillers en temps réel.</resume>
			<mots_cles>Analyse sémantique, moteur sémantique Chat, Live Chat, Conversation en ligne, Traitement Automatique du Langage (TAL), Base de connaissance, Relation client</mots_cles>
			<title></title>
			<abstract>Chat channel enables companies to transform their website into a real place of purchase and service. OWI developed an online conversations assistance solution (OWI.Chat). Its main task is to analyze the Chat requests and help agents in real time.</abstract>
			<keywords>Semantic analysis, semantic engine, Chat, Live Chat, Online conversation, Natural Language Procession (NLP), Agent Knowledge Base, Customer relationship</keywords>
		</article>
		<article id="taln-2014-demo-006" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email>karen.fort@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email>bruno.guillaume@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Valentin</prenom>
					<nom>Stern</nom>
					<email>valentin.stern@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA, Université de Lorraine</affiliation>
				<affiliation affiliationId="2">LORIA, Inria Nancy Grand-Est</affiliation>
			</affiliations>
			<titre>ZOMBILINGO : manger des têtes pour annoter en syntaxe de dépendances</titre>
			<type>démonstration</type>
			<pages>15-16</pages>
			<resume>Cet article présente ZOMBILINGO un jeu ayant un but (Game with a purpose) permettant d’annoter des corpus en syntaxe de dépendances. Les annotations créées sont librement disponibles sur le site du jeu.</resume>
			<mots_cles>jeux ayant un but, complexité, annotation, syntaxe en dépendances</mots_cles>
			<title></title>
			<abstract>This paper presents ZOMBILINGO, a Game With A Purpose (GWAP) that allows for the dependency syntax annotation of French corpora. The created resource is freely available on the game Web site.</abstract>
			<keywords>GWAP, complexity, annotation, dependency syntax</keywords>
		</article>
		<article id="taln-2014-demo-007" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>François-Régis</prenom>
					<nom>Chaumartin</nom>
					<email>frc@proxem.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Proxem, 19 boulevard de Magenta, 75010 Paris</affiliation>
			</affiliations>
			<titre>Ubiq : une plateforme de collecte, analyse et valorisation des corpus</titre>
			<type>démonstration</type>
			<pages>17-18</pages>
			<resume>Proxem édite Ubiq, une plateforme de collecte de documents et d’analyse sémantique, capable d'extraire des informations pertinentes à partir du contenu de vastes corpus. Les documents analysés sont d’une grande diversité : opinions collectées sur des sites web, emails de réclamation ou de demande d’information, réponse à des questions ouvertes dans des sondages, offres ou demandes d’emploi, etc. La reconnaissance des entités nommées joue un rôle central car c’est un préalable à d’autres traitements sémantiques. La conception d’un module de reconnaissance d’entités nommées nécessite généralement un investissement important en amont, avec une adaptation de domaine. Ubiq propose une approche d’apprentissage faiblement supervisé de l’extraction d’entités nommées qui tient compte du corpus collecté et de ressources externes (Wikipédia). La méthode et l’outillage développés permettent de déterminer à la volée, en interaction avec l’utilisateur, la granularité des types d’entités adaptée à un corpus de texte tout-venant.</resume>
			<mots_cles>entités nommées, désambiguïsation, apprentissage, Wikipédia, catégorisation</mots_cles>
			<title></title>
			<abstract>Proxem publishes Ubiq, a platform for web crawling and semantic analysis, which can extract relevant information from large corpus. Documents are of great variety: reviews crawled from websites, emails about complaints or requests for information, answers to open questions in surveys, employment offers or job applications, etc. Named Entity Recognition plays a key role since it is a prerequisite to further semantic processing. The design of a NER module generally requires a significant upfront investment with some domain adaptation. Ubiq proposes a semi-supervised approach to NER that takes into account the crawled corpus and external resources (Wikipedia). The proposed method and tools allow to get on the fly, with some user interaction, the type granularity of entities suitable for a given corpus.</abstract>
			<keywords>named entities, disambiguation, machine learning, Wikipedia, categorization</keywords>
		</article>
		<article id="taln-2014-demo-008" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Fabrizio</prenom>
					<nom>Gotti</nom>
					<email>gottif@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI, Université de Montréal, CP 6128 Succursale Centre-Ville, Montréal, Canada, H3C 3J7</affiliation>
			</affiliations>
			<titre>Zodiac : Insertion automatique des signes diacritiques du français</titre>
			<type>démonstration</type>
			<pages>19-20</pages>
			<resume>Nous proposons dans cette démonstration de présenter le logiciel Zodiac, permettant l’insertion automatique de diacritiques (accents, cédilles, etc.) dans un texte français. Zodiac prend la forme d’un complément Microsoft Word sous Windows permettant des corrections automatiques du texte au cours de la frappe. Sous Linux et Mac OS X, il est implémenté comme un programme sur ligne de commande, se prêtant naturellement à lire ses entrées sur un « pipeline » et écrire ses sorties sur la sortie standard. Implémenté en UTF-8, il met en oeuvre diverses librairies C++ utiles à certaines tâches du TAL, incluant la manipulation de modèles de langue statistiques.</resume>
			<mots_cles>aide à la rédaction, diacritiques, modèles de langue probabilistes</mots_cles>
			<title></title>
			<abstract>In this demo session, we propose to show how the software module Zodiac works. It allows the automatic insertion of diacritical marks (accents, cedillas, etc.) in text written in French. Zodiac is implemented as a Microsoft Word add-in under Windows, allowing automatic corrections as the user is typing. Under Linux and Mac OS X, it is implemented as a command-line utility, lending itself naturally to be used in a text-processing pipeline. Zodiac handles UTF-8, and showcases some useful C++ libraries for natural language processing, including statistical language modeling.</abstract>
			<keywords>text editing, diacritical marks, statistical language models</keywords>
		</article>
		<article id="taln-2014-demo-009" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Mehdi</prenom>
					<nom>Embarek</nom>
					<email>embarekm@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">MK SOFT, 11 rue des fossés St Marcel, 75005 Paris</affiliation>
			</affiliations>
			<titre>Le système STAM</titre>
			<type>démonstration</type>
			<pages>21-22</pages>
			<resume>Le projet STAM aborde la problématique de la transcription automatique du langage texto (SMS) et plus particulièrement la traduction des messages écrits en arabe dialectal. L’objectif du système STAM est de traduire automatiquement des textes rédigés en langage SMS dans un dialecte parlé dans le monde arabe (langue source) en un texte facilement interprétable, compréhensible et en bon français (langue cible).</resume>
			<mots_cles>Dialecte, SMS, Transcription, STAM</mots_cles>
			<title></title>
			<abstract>The STAM project addresses the problem of automatic transcription of SMS language and especially the translation of messages written in Arabic dialect. The objective of STAM system is to automatically translate texts written in SMS language in a dialect spoken in the Arab World (source language) into a French text (target language), interpretable and understandable.</abstract>
			<keywords>Dialect, SMS, Transcription, STAM</keywords>
		</article>
		<article id="taln-2014-demo-010" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Hatim</prenom>
					<nom>Khouzaimi</nom>
					<email>hatim.khouzaimi@orange.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Laroche</nom>
					<email>romain.laroche@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 38-40 Avenue du Général Leclerc, 92794 Issy-les-Moulineaux, France</affiliation>
				<affiliation affiliationId="2">Laboratoire Informatique d’Avignon, 339 Chemin des Meinajaries, 84911 Avignon, France</affiliation>
			</affiliations>
			<titre>DictaNum : système de dialogue incrémental pour la dictée de numéros.</titre>
			<type>démonstration</type>
			<pages>23-25</pages>
			<resume>Les stratégies de dialogue incrémentales offrent une meilleure réactivité, une expérience utilisateur plus aboutie et une réduction du risque de désynchronisation. Cependant, les systèmes de dialogue incrémentaux sont basés sur une architecture logicielle dont l’implantation est longue, difficile et donc coûteuse. Pour faciliter cette évolution d’architecture, nous proposons de simuler un comportement incrémental en ajoutant une surcouche à un service de dialogue traditionnel existant. DictaNum est un démonstrateur de dialogue incrémental mettant en oeuvre cette démarche. Sa tâche consiste à recueillir des numéros auprès des utilisateurs. Grâce à son fonctionnement incrémental, il autorise une correction rapide des erreurs au fil de la dictée.</resume>
			<mots_cles>Systèmes de Dialogue, Traitement Incrémental, Architecture des Systèmes de Dialogue</mots_cles>
			<title></title>
			<abstract>Incremental dialogue strategies are more reactive, offer a better user experience and reduce desynchronisation risks. However, incremental dialogue systems are based on architectures that are long, difficult and hence costly to implement. In order to make this architecture evolution easier, we suggest to simulate incremental behavior by adding a new layer to an existing traditional service. DictaNum is an incremental dialogue demonstrator that uses this approach. It collects numbers dictated by the user. Thanks to its incremental behavior, it makes it possible to rapidly correct errors on the fly.</abstract>
			<keywords>Dialogue Systems, Incremental Processing, Dialogue Systems Architectures</keywords>
		</article>
		<article id="taln-2014-demo-011" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Li</prenom>
					<nom>Gong</nom>
					<email>li.gong@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>francois.yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Orsay, France</affiliation>
				<affiliation affiliationId="2">Univ. Paris Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Construction (très) rapide de tables de traduction à partir de grands bi-textes</titre>
			<type>démonstration</type>
			<pages>26-27</pages>
			<resume>Dans cet article de démonstration, nous introduisons un logiciel permettant de construire des tables de traduction de manière beaucoup plus rapide que ne le font les techniques à l’état de l’art. Cette accélération notable est obtenue par le biais d’un double échantillonnage : l’un permet la sélection d’un nombre limité de bi-phrases contenant les segments à traduire, l’autre réalise un alignement à la volée de ces bi-phrases pour extraire des exemples de traduction.</resume>
			<mots_cles>traduction automatique statistique, développement efficace, temps de calcul</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords>statistical machine translation, efficient development, computation time</keywords>
		</article>
		<article id="taln-2014-demo-012" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Tatiana</prenom>
					<nom>Ekeinhor-Komi</nom>
					<email>Tatiana.EkeinhorKomi@orange.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hajar</prenom>
					<nom>Falih</nom>
					<email>Hajar.Falih@orange.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christine</prenom>
					<nom>Chardenon</nom>
					<email>Christine.Chardenon@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Laroche</nom>
					<email>Romain.Laroche@orange.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 2 Avenue Pierre Marzin, 22300 Lannion</affiliation>
				<affiliation affiliationId="2">Orange Labs, 38-40 Rue du Général Leclerc, 92130 Issy les Moulineaux</affiliation>
				<affiliation affiliationId="3">LIA-CERI, Université d’Avignon, France</affiliation>
			</affiliations>
			<titre>Un assistant vocal personnalisable</titre>
			<type>démonstration</type>
			<pages>28-29</pages>
			<resume>Nous proposons la démonstration d’un assistant personnel basé sur une architecture distribuée. Un portail vocal relie l’utilisateur à des applications. Celles-ci sont installées par l’utilisateur qui compose de ce fait son propre assistant personnel selon ses besoins.</resume>
			<mots_cles>Système de dialogue, applications du traitement automatique du langage naturel, assistant personnel</mots_cles>
			<title></title>
			<abstract>We introduce a personal assistant based on a distributed architecture. A portal connects user to applications. Applications are installed by a user who compose his own assistant according to his needs.</abstract>
			<keywords>Dialogue system, natural language processing applications, personal assistant</keywords>
		</article>
		<article id="taln-2014-demo-013" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Gaume</nom>
					<email>gaume@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Navarro</nom>
					<email>navarro@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Desalle</nom>
					<email>yann.desalle@gmail.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Gaillard</nom>
					<email>benoit.gd@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS, CNRS, Universié de Toulouse</affiliation>
				<affiliation affiliationId="2">IRIT, CNRS, Université de Toulouse</affiliation>
				<affiliation affiliationId="3">ATILF, CNRS, Université de Lorraine</affiliation>
			</affiliations>
			<titre>Mesurer la similarité structurelle entre réseaux lexicaux</titre>
			<type>démonstration</type>
			<pages>30-39</pages>
			<resume>Dans cet article, nous comparons la structure topologique des réseaux lexicaux avec une méthode fondée sur des marches aléatoires. Au lieu de caractériser les paires de sommets selon un critère binaire de connectivité, nous mesurons leur proximité structurelle par la probabilité relative d’atteindre un sommet depuis l’autre par une courte marche aléatoire. Parce que cette proximité rapproche les sommets d’une même zone dense en arêtes, elle permet de comparer la structure topologique des réseaux lexicaux.</resume>
			<mots_cles>Réseaux lexicaux, réseaux petits mondes, comparaison de graphes, marches aléatoires</mots_cles>
			<title></title>
			<abstract>In this paper, we compare the topological structure of lexical networks with a method based on random walks. Instead of characterising pairs of vertices according only to whether they are connected or not, we measure their structural proximity by evaluating the relative probability of reaching one vertex from the other via a short random walk. This proximity between vertices is the basis on which we can compare the topological structure of lexical networks because it outlines the similar dense zones of the graphs.</abstract>
			<keywords>Lexical networks, small worlds, comparison graphs, random walks</keywords>
		</article>
		<article id="taln-2014-demo-014" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Yoann</prenom>
					<nom>Dupont</nom>
					<email>yoann.dupont@etud.sorbonne-nouvelle.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 3 Sorbonne Nouvelle, 13, rue de Santeuil - 75231 Paris Cedex 05</affiliation>
			</affiliations>
			<titre>Un reconnaisseur d’entités nommées du Français</titre>
			<type>démonstration</type>
			<pages>40-41</pages>
			<resume>Nous proposons une démonstration d’un reconnaisseur d’entités nommées du Français appris automatiquement sur le French TreeBank annoté en entités nommées.</resume>
			<mots_cles>REN, POS, apprentissage automatique, French Treebank, extraction d’information, CRF</mots_cles>
			<title></title>
			<abstract>We propose to demonstrate a french named entity recognizer trained on the French TreeBank enriched with named entity annotations.</abstract>
			<keywords>NER, POS, machine learning, French Treebank, information extraction, CRF</keywords>
		</article>
	</articles>
</conference>