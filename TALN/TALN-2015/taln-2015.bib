@proceedings{TALN:2015,
  editor    = {Lecarpentier, Jean-Marc and Lucas, Nadine},
  title     = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015}
}

@inproceedings{navigli:2015:TALN,
  author    = {Navigli, Roberto},
  title     = {},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {i-i},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-invite-001},
  language  = {french},
  note      = {Multilinguality at Your Fingertips : BabelNet, Babelfy and Beyond !},
  resume    = {},
  abstract  = {Multilinguality is a key feature of today's Web, and it is this feature that we leverage and exploit in our research work at the Sapienza University of Rome's Linguistic Computing Laboratory, which I am going to overview and showcase in this talk. I will start by presenting BabelNet 3.0, available at http://babelnet.org, a very large multilingual encyclopedic dictionary and semantic network, which covers 271 languages and provides both lexicographic and encyclopedic knowledge for all the open-class parts of speech, thanks to the seamless integration of WordNet, Wikipedia, Wiktionary, OmegaWiki, Wikidata and the Open Multilingual WordNet. Next, I will present Babelfy, available at http://babelfy.org, a unified approach that leverages BabelNet to jointly perform word sense disambiguation and entity linking in arbitrary languages, with performance on both tasks on a par with, or surpassing, those of task-specific state-of-the-art supervised systems. Finally I will describe the Wikipedia Bitaxonomy, available at http://wibitaxonomy.org, a new approach to the construction of a Wikipedia bitaxonomy, that is, the largest and most accurate currently available taxonomy of Wikipedia pages and taxonomy of categories, aligned to each other. I will also give an outline of future work on multilingual resources and processing, including state-of-the-art semantic similarity with sense embeddings.},
  motscles  = {multilingualité, dictionnaire, sémantique, désambiguïsation, taxonomie},
  keywords  = {multilinguality, dictionary, semantic, disambiguation, taxonomy},
}

@inproceedings{l'homme:2015:TALN,
  author    = {L'Homme, Marie-Claude},
  title     = {Pourquoi construire des ressources terminologiques et pourquoi le faire différemment ?},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {ii-ii},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-invite-002},
  language  = {french},
  note      = {Why compile terminological resources and why do it differently?},
  resume    = {Dans cette présentation, je défendrai l'idée selon laquelle des ressources terminologiques décrivant les propriétés lexico-sémantiques des termes constituent un complément nécessaire, voire indispensable, à d'autres types de ressources, À partir d'exemples anglais et français empruntés au domaine de l'environnement, je montrerai, d'une part, que les ressources lexicales générales (y compris celles qui ont une large couverture) n'offrent pas un portait complet du sens des termes ou de la structure lexicale observée du point de vue d'un domaine de spécialité. Je montrerai, d'autre part, que les ressources terminologiques (thésaurus, ontologies, banques de terminologie) souvent d'obédience conceptuelle, se concentrent sur le lien entre les termes et les connaissances dénotées par eux et s'attardent peu sur leur fonctionnement linguistique. Je présenterai un type de ressource décrivant les propriétés lexico-sémantiques des termes d'un domaine (structure actantielle, liens lexicaux, annotations contextuelles, etc.) et des éléments méthodologiques présidant à son élaboration.},
  abstract  = {In this talk, I will argue that terminological resources that account for the lexico-semantic properties of terms are a necessary – perhaps even indispensable – complement to other kinds of resources. Using examples taken from the field of the environment, I will first show that general lexical resources (including those that have a large coverage) do not give a complete picture of the meaning of terms or the lexical structure observed from the point of view of a special subject field. I will then proceed to show that terminological resources (thesauri, ontologies, term banks), that often take a conceptual approach, focus on the relationship between terms and the knowledge they convey and give little information about the linguistic behavior of terms. I will present a type of resource that describes the lexico-semantic properties of terms (argument structure, lexical relationships, contextual annotations, etc.) and some methodological considerations about its compilation.},
  motscles  = {ressources terminologiques, ressources lexicales, liens lexicaux, corpus spécialisé, structure actancielle, annotations contextuelles},
  keywords  = {terminological resources, lexical resources, lexical relationships, specialized corpora, argument structure, contextual annotations},
}

@inproceedings{knyazeva-wisniewski-yvon:2015:TALN,
  author    = {Knyazeva, Elena and Wisniewski, Guillaume and Yvon, François},
  title     = {Apprentissage par imitation pour l'étiquetage de séquences : vers une formalisation des méthodes d'étiquetage « easy-first »},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {1--12},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-001},
  language  = {french},
  note      = {Imitation learning for sequence labeling: towards a formalization of easy-first labeling methods},
  resume    = {De nombreuses méthodes ont été proposées pour accélérer la prédiction d'objets structurés (tels que les arbres ou les séquences), ou pour permettre la prise en compte de dépendances plus riches afin d'améliorer les performances de la prédiction. Ces méthodes reposent généralement sur des techniques d'inférence approchée et ne bénéficient d'aucune garantie théorique aussi bien du point de vue de la qualité de la solution trouvée que du point de vue de leur critère d'apprentissage. Dans ce travail, nous étudions une nouvelle formulation de l'apprentissage structuré qui consiste à voir celui-ci comme un processus incrémental au cours duquel la sortie est construite de façon progressive. Ce cadre permet de formaliser plusieurs approches de prédiction structurée existantes. Grâce au lien que nous faisons entre apprentissage structuré et apprentissage par renforcement, nous sommes en mesure de proposer une méthode théoriquement bien justifiée pour apprendre des méthodes d'inférence approchée. Les expériences que nous réalisons sur quatre tâches de TAL valident l'approche proposée.},
  abstract  = {Structured learning techniques, aimed at modeling structured objects such as labeled trees or strings, are computationally expensive. Many attempts have been made to reduce their complexity, either to speed up learning and inference, or to take richer dependencies into account. These attempts typically rely on approximate inference techniques and usually provide very little theoretical guarantee regarding the optimality of the solutions they find. In this work we study a new formulation of structured learning where inference is primarily viewed as an incremental process along which a solution is progressively computed. This framework generalizes several structured learning ap- proaches. Building on the connections between this framework and reinforcement learning, we propose a theoretically sound method to learn to perform approximate inference. Experiments on four sequence labeling tasks show that our approach is very competitive when compared to several strong baselines.},
  motscles  = {Apprentissage par Imitation, Apprentissage Structuré, Étiquetage de Séquences},
  keywords  = {Imitation Learning, Structured Learning, Sequence Models},
}

@inproceedings{claveau-kijak:2015:TALN,
  author    = {Claveau, Vincent and Kijak, Ewa},
  title     = {Stratégies de sélection des exemples pour l'apprentissage actif avec des champs aléatoires conditionnels},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {13--24},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-002},
  language  = {french},
  note      = {Strategies to select examples for Active Learning with Conditional Random Fields},
  resume    = {Beaucoup de problèmes de TAL sont désormais modélisés comme des tâches d'apprentissage supervisé. De ce fait, le coût des annotations des exemples par l'expert représente un problème important. L'apprentissage actif (active learning) apporte un cadre à ce problème, permettant de contrôler le coût d'annotation tout en maximisant, on l'espère, la performance de la tâche visée, mais repose sur le choix difficile des exemples à soumettre à l'expert. Dans cet article, nous examinons et proposons des stratégies de sélection des exemples pour le cas spécifique des champs aléatoires conditionnels (Conditional Random Fields, CRF), outil largement utilisé en TAL. Nous proposons d'une part une méthode simple corrigeant un biais de certaines méthodes de l'état de l'art. D'autre part, nous détaillons une méthode originale de sélection s'appuyant sur un critère de respect des proportions dans les jeux de données manipulés. Le bien- fondé de ces propositions est vérifié au travers de plusieurs tâches et jeux de données, incluant reconnaissance d'entités nommées, chunking, phonétisation, désambiguïsation de sens.},
  abstract  = {Nowadays, many NLP problems are modelized as supervised machine learning tasks. Consequently, the cost of the ex- pertise needed to annotate the examples is a widespread issue. Active learning offers a framework to that issue, allowing to control the annotation cost while maximizing the classifier performance, but it relies on the key step of choosing which example will be proposed to the expert. In this paper, we examine and propose such selection strategies in the specific case of Conditional Random Fields (CRF) which are largely used in NLP. On the one hand, we propose a simple method to correct a bias of certain state-of-the-art selection techniques. On the other hand, we detail an original approach to select the examples, based on the respect of proportions in the datasets. These contributions are validated over a large range of experiments implying several tasks and datasets, including named entity recognition, chunking, phonetization, word sens disambiguation.},
  motscles  = {CRF, champs aléatoires conditionnels, apprentissage actif, apprentissage semi-supervisé, test statistique de proportion},
  keywords  = {CRF, conditional random fields, active learning, semi-supervised learning, statistical test of proportion},
}

@inproceedings{grouin-EtAl:2015:TALN,
  author    = {Grouin, Cyril and Moriceau, Véronique and Rosset, Sophie and Zweigenbaum, Pierre},
  title     = {Identification de facteurs de risque pour des patients diabétiques à partir de comptes-rendus cliniques par des approches hybrides},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {25--36},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-003},
  language  = {french},
  note      = {Risk factor identification for diabetic patients from clinical records using hybrid approaches},
  resume    = {Dans cet article, nous présentons les méthodes que nous avons développées pour analyser des comptes- rendus hospitaliers rédigés en anglais. L'objectif de cette étude consiste à identifier les facteurs de risque de décès pour des patients diabétiques et à positionner les événements médicaux décrits par rapport à la date de création de chaque document. Notre approche repose sur (i) HeidelTime pour identifier les expressions temporelles, (ii) des CRF complétés par des règles de post-traitement pour identifier les traitements, les maladies et facteurs de risque, et (iii) des règles pour positionner temporellement chaque événement médical. Sur un corpus de 514 documents, nous obtenons une F-mesure globale de 0,8451. Nous observons que l'identification des informations directement mentionnées dans les documents se révèle plus performante que l'inférence d'informations à partir de résultats de laboratoire.},
  abstract  = {In this paper, we present the methods we designed to process clinical records written in English. The aim of this study consists in identifying risk factors for diabetic patients and to define the temporal relation of those medical events wrt. the document creation time. Our approach relies (i) on HeidelTime to identify temporal expressions, (ii) on CRF and post-processing rules to identify treatments, diseases and risk factors, and (iii) on rules to determine the temporal relation of each medical event. On a corpus of 514 documents, we achieved a 0.8451 global F-measure. We observe we performed best on the identification of information mentionned in the text than information inference from lab results.},
  motscles  = {Comptes-rendus hospitaliers, extraction d'information, apprentissage statistique},
  keywords  = {Electronic Health Records, Information Extraction, Machine Learning},
}

@inproceedings{pecheux-EtAl:2015:TALN,
  author    = {Pécheux, Nicolas and Allauzen, Alexandre and Lavergne, Thomas and Wisniewski, Guillaume and Yvon, François},
  title     = {Oublier ce qu'on sait, pour mieux apprendre ce qu'on ne sait pas : une étude sur les contraintes de type dans les modèles CRF},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {37--48},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-004},
  language  = {french},
  note      = {Ignore what you know to better learn what you don't : a case study on type constraints for CRFs},
  resume    = {Quand on dispose de connaissances a priori sur les sorties possibles d'un problème d'étiquetage, il semble souhaitable d'inclure cette information lors de l'apprentissage pour simplifier la tâche de modélisation et accélérer les traitements. Pourtant, même lorsque ces contraintes sont correctes et utiles au décodage, leur utilisation lors de l'apprentissage peut dégrader sévèrement les performances. Dans cet article, nous étudions ce paradoxe et montrons que le manque de contraste induit par les connaissances entraîne une forme de sous-apprentissage qu'il est cependant possible de limiter.},
  abstract  = {When information about the possible outputs of a sequence labeling task is available, it may seem appropriate to include this knowledge into the system, so as to facilitate and speed-up learning and inference. However, we show in this paper that using such constraints at training time is likely to drastically reduce performance, even when they are both correct and useful at decoding. In this paper, we study this paradox and show that the lack of contrast induced by constraints leads to a form of under-fitting, that it is however possible to partially overcome.},
  motscles  = {Étiquetage Morpho-Syntaxique, Apprentissage Statistique, Champs Markoviens Aléatoires},
  keywords  = {Part-of-Speech Tagging, Statistical Machine Learning, Conditional Random Fields},
}

@inproceedings{tapinzali-EtAl:2015:TALN,
  author    = {Tapi Nzali, Mike Donald and Névéol, Aurélie and Tannier, Xavier and Vilnat, Anne},
  title     = {Analyse d'expressions temporelles dans les dossiers électroniques patients},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {49--58},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-005},
  language  = {french},
  note      = {An analysis of temporal expressions in Electronic Health Records in French},
  resume    = {Les références à des phénomènes du monde réel et à leur caractérisation temporelle se retrouvent dans beaucoup de types de discours en langue naturelle. Ainsi, l'analyse temporelle apparaît comme un élément important en traitement automatique de la langue. Cet article présente une analyse de textes en domaine de spécialité du point de vue temporel. En s'appuyant sur un corpus de documents issus de plusieurs dossiers électroniques patient désidentifiés, nous décrivons la construction d'une ressource annotée en expressions temporelles selon la norme TimeML. Par suite, nous utilisons cette ressource pour évaluer plusieurs méthodes d'extraction automatique d'expressions temporelles adaptées au domaine médical. Notre meilleur système statistique offre une performance de 0,91 de F-mesure, surpassant pour l'identification le système état de l'art HeidelTime. La comparaison de notre corpus de travail avec le corpus journalistique FR-Timebank permet également de caractériser les différences d'utilisation des expressions temporelles dans deux domaines de spécialité.},
  abstract  = {References to phenomena ocurring in the world and their temporal caracterization can be found in a variety of natural language utterances. For this reason, temporal analysis is a key issue in natural language processing. This article presents a temporal analysis of specialized documents. We use a corpus of documents contained in several de-identified Electronic Health Records to develop an annotated resource of temporal expressions relying on the TimeML standard. We then use this corpus to evaluate several methods for the automatic extraction of temporal expressions. Our best statistical model yields 0.91 F-measure, which provides significant improvement on extraction, over the state-of-the-art system Heidel-Time. We also compare our medical corpus to FR-Timebank in order to characterize the uses of temporal expressions in two different subdomains},
  motscles  = {Extraction d'Information, Analyse Temporelle, Développement d'un Corpus Annoté},
  keywords  = {Information Extraction, Temporal Analysis, Development of Annotated Corpus},
}

@inproceedings{ferreira-jabaian-lefevre:2015:TALN,
  author    = {Ferreira, Emmanuel and Jabaian, Bassam and Lefèvre, Fabrice},
  title     = {Compréhension automatique de la parole sans données de référence},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {59--70},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-006},
  language  = {french},
  note      = {Spoken language understanding without reference data},
  resume    = {La majorité des méthodes état de l'art en compréhension automatique de la parole ont en commun de devoir être apprises sur une grande quantité de données annotées. Cette dépendance aux données constitue un réel obstacle lors du développement d'un système pour une nouvelle tâche/langue. Aussi, dans cette étude, nous présentons une méthode visant à limiter ce besoin par un mécanisme d'apprentissage sans données de référence (zero-shot learning). Cette méthode combine une description ontologique minimale de la tâche visée avec l'utilisation d'un espace sémantique continu appris par des approches à base de réseaux de neurones à partir de données génériques non-annotées. Nous montrons que le modèle simple et peu coûteux obtenu peut atteindre, dès le démarrage, des performances comparables à celles des systèmes état de l'art reposant sur des règles expertes ou sur des approches probabilistes sur des tâches de compréhension de la parole de référence (tests des Dialog State Tracking Challenges, DSTC2 et DSTC3). Nous proposons ensuite une stratégie d'adaptation en ligne permettant d'améliorer encore les performances de notre approche à l'aide d'une supervision faible et ajustable par l'utilisateur.},
  abstract  = {Most recent state-of-the-art spoken language understanding models have in common to be trained on a potentially large amount of data. However, the required annotated corpora are not available for a variety of tasks and languages of interest. In this work, we present a novel zero-shot learning method for spoken language understanding which alleviate the need of any annotated or in-context data. Instead, it combines an ontological description of the target domain and the use of a continuous semantic space trained on large amounts of unannotated and unstructured found data with neural network algorithms. We show that this very low cost model can reach instantly performance comparable to those obtained by either state-of-the-art carefully hand crafted rule-based or trained statistical models on reference spoken language understanding tasks (test sets of the second and the third Dialog State Tracking Challenge, DSTC2,DSTC3). Eventually we extend the approach with an online adaptative strategy allowing to refine progressively the initial model with only a light and adjustable supervision.},
  motscles  = {Compréhension automatique de la parole, espace sémantique continu, apprentissage sans données de référence, données d'apprentissage hors domaine},
  keywords  = {Spoken language understanding, continuous semantic space, zero-shot learning, out-of-domain training data},
}

@inproceedings{nguyen-EtAl:2015:TALN,
  author    = {Nguyen, Kiem-Hieu and Tannier, Xavier and Ferret, Olivier and Besançon, Romaric},
  title     = {Désambiguïsation d'entités pour l'induction non supervisée de schémas événementiels},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {71--82},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-007},
  language  = {french},
  note      = {Entity disambiguation for event template induction},
  resume    = {Cet article présente un modèle génératif pour l'induction non supervisée d'événements. Les précédentes méthodes de la littérature utilisent uniquement les têtes des syntagmes pour représenter les entités. Pourtant, le groupe complet (par exemple, "un homme armé") apporte une information plus discriminante (que "homme"). Notre modèle tient compte de cette information et la représente dans la distribution des schémas d'événements. Nous montrons que ces relations jouent un rôle important dans l'estimation des paramètres, et qu'elles conduisent à des distributions plus cohérentes et plus discriminantes. Les résultats expérimentaux sur le corpus de MUC-4 confirment ces progrès.},
  abstract  = {In this paper, we present an approach for event induction with a generative model. This model makes possible to consider more relational information than previous models, and has been applied to noun attributes. By their inﬂuence on parameter estimation, this new information make probabilistic topic distribution more discriminative and more robust. We evaluated different versions of our model on MUC-4 datasets.},
  motscles  = {Événements, modèle génératif, désambiguïsation d'entités, échantillonnage de Gibbs},
  keywords  = {Event Induction, Generative Model, Entity Disambiguation, Gibbs Sampling},
}

@inproceedings{nasiruddin-EtAl:2015:TALN,
  author    = {Nasiruddin, Mohammad and Tchechmedjiev, Andon and Blanchon, Hervé and Schwab, Didier},
  title     = {Création rapide et efficace d'un système de désambiguïsation lexicale pour une langue peu dotée},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {83--94},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-008},
  language  = {french},
  note      = {Rapid Construction of Supervised Word Sense Disambiguation System for Lesser-resourced Languages},
  resume    = {Nous présentons une méthode pour créer rapidement un système de désambiguïsation lexicale (DL) pour une langue L peu dotée pourvu que l'on dispose d'un système de traduction automatique statistique (TAS) d'une langue riche en corpus annotés en sens (ici l'anglais) vers L. Il est, en effet, plus facile de disposer des ressources nécessaires à la création d'un système de TAS que des ressources dédiées nécessaires à la création d'un système de DL pour la langue L. Notre méthode consiste à traduire automatiquement un corpus annoté en sens vers la langue L, puis de créer le système de désambiguïsation pour L par des méthodes supervisées classiques. Nous montrons la faisabilité de la méthode et sa généricité en traduisant le SemCor, un corpus en anglais annoté grâce au Princeton WordNet, de l'anglais vers le bangla et de l'anglais vers le français. Nous montrons la validité de l'approche en évaluant les résultats sur la tâche de désambiguïsation lexicale multilingue de Semeval 2013.},
  abstract  = {We introduce a method to quickly build a Word Sense Disambiguation (WSD) system for a lesser-resourced language L, under the condition that a Statistical Machine Transation system (SMT) is available from a well resourced language where semantically annotated corpora are available (here, English) towards L. We argue that it is less difficult to obtain the resources mandatory for the development of an SMT system (parallel-corpora) than it is to create the resources necessary for a WSD system (semantically annotated corpora, lexical resources). In the present work, we propose to translate a semantically annotated corpus from English to L and then to create a WSD system for L following the classical supervised WSD paradigm. We demonstrate the feasibility and genericity of our proposed method by translating SemCor from English to Bangla and from English to French. SemCor is an English corpus annotated with Princeton WordNet sense tags. We show the feasibility of the approach using the Multilingual WSD task from Semeval 2013.},
  motscles  = {clarification de texte, désambiguïsation lexicale, langues peu dotées, traduction automatique, portage d'annotations},
  keywords  = {clarification of texts, word sens disambiguation, under resourced languages, machine translation, annotation transfert},
}

@inproceedings{besancon:2015:TALN,
  author    = {Besançon, Romaric},
  title     = {Méthode faiblement supervisée pour l'extraction d'opinion ciblée dans un domaine spécifique},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {95--106},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-009},
  language  = {french},
  note      = {A Weakly Supervised Approach for Aspect-Based Opinion Mining in a Specific Domain},
  resume    = {La détection d'opinion ciblée a pour but d'attribuer une opinion à une caractéristique particulière d'un produit donné. La plupart des méthodes existantes envisagent pour cela une approche non supervisée. Or, les utilisateurs ont souvent une idée a priori des caractéristiques sur lesquelles ils veulent découvrir l'opinion des gens. Nous proposons dans cet article une méthode pour une extraction d'opinion ciblée, qui exploite cette information minimale sur les caractéristiques d'intérêt. Ce modèle s'appuie sur une segmentation automatique des textes, un enrichissement des données disponibles par similarité sémantique, et une annotation de l'opinion par classification supervisée. Nous montrons l'intérêt de l'approche sur un cas d'étude dans le domaine des jeux vidéos.},
  abstract  = {The goal of aspect-based opinion mining is to associate an opinion with fine-grain aspects of a given product. Most approaches designed in this purpose use unsupervised techniques, whereas the information of the desired targeted aspects can often be given by the end-users. We propose in this paper a new approach for targeted opinion detection that uses this minimal information, enriched using several semantic similarty measures, along with topical segmentation and supervised classification. We prove the interest of the approach on an evaluation corpus in the specific domain of video games.},
  motscles  = {Analyse d'opinion, classification supervisée, similarité sémantique},
  keywords  = {Opinion analysis, classification, semantic similarity},
}

@inproceedings{eensoo-valette:2015:TALN,
  author    = {Eensoo, Egle and Valette, Mathieu},
  title     = {Une méthodologie de sémantique de corpus appliquée à des tâches de fouille d'opinion et d'analyse des sentiments : étude sur l'impact de marqueurs dialogiques et dialectiques dans l'expression de la subjectivité},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {107--118},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-010},
  language  = {french},
  note      = {A method of corpus semantics applied to opinion mining and sentiment analysis: the impact of dialogical and dialectical features on the expression of subjectivity},
  resume    = {Cet article entend dresser, dans un premier temps, un panorama critique des relations entre TAL et linguistique. Puis, il esquisse une discussion sur l'apport possible d'une sémantique de corpus dans un contexte applicatif en s'appuyant sur plusieurs expériences en fouille de textes subjectifs (analyse de sentiments et fouille d'opinions). Ces expériences se démarquent des approches traditionnelles fondées sur la recherche de marqueurs axiologiques explicites par l'utilisation de critères relevant des représentations des acteurs (composante dialogique) et des structures argumentatives et narratives des textes (composante dialectique). Nous souhaitons de cette façon mettre en lumière le bénéfice d'un dialogue méthodologique entre une théorie (la sémantique textuelle), des méthodes de linguistique de corpus orientées vers l'analyse du sens (la textométrie) et les usages actuels du TAL en termes d'algorithmiques (apprentissage automatique) mais aussi de méthodologie d'évaluation des résultats.},
  abstract  = {This paper first aims to provide a critical overview of the relationship between NLP and linguistics, and then to sketch out a discussion on the possible contribution of corpus semantics in an application-based context based on several subjective text mining studies (sentiment analysis and opinion mining). These studies break away from traditional approaches founded on the detection of axiological markers. Instead, they use explicit criteria related to the representation of actors (dialogical component) and argumentative or narrative structures (dialectical component). We hope to highlight the benefit of a methodological dialogue between theory (text semantics), meaning-oriented methods of corpus linguistics (i.e. textometrics) and NLP current practices in terms of algorithmic (machine learning) and assessment methodology.},
  motscles  = {Textométrie, Sémantique de corpus, Fouille d'opinion, Analyse des sentiments},
  keywords  = {Textometry, corpus semantics, opinion mining, sentiment analysis},
}

@inproceedings{pho-ligozat-grau:2015:TALN,
  author    = {Pho, Van-Minh and Ligozat, Anne-Laure and Grau, Brigitte},
  title     = {Estimation de l'homogénéité sémantique pour les Questionnaires à Choix Multiples},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {119--130},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-011},
  language  = {french},
  note      = {Semantic homogeneity estimation for MCQs},
  resume    = {L'homogénéité sémantique stipule que des termes sont sémantiquement proches mais non similaires. Cette notion est au cœur de travaux relatifs à la génération automatique de questionnaires à choix multiples, et particulièrement à la sélection automatique de distracteurs. Dans cet article, nous présentons une méthode d'estimation de l'homogénéité sémantique dans un cadre de validation automatique de distracteurs. Cette méthode est fondée sur une combinaison de plusieurs critères de voisinage et de similarité sémantique entre termes, par apprentissage automatique. Nous montrerons que notre méthode permet d'obtenir une meilleure estimation de l'homogénéité sémantique que les méthodes proposées dans l'état de l'art.},
  abstract  = {Semantic homogeneity states that terms are semantically close but not similar. This notion is the focus of work related to multiple-choice test generation, and especially to automatic distractor selection. In this paper, we present a method to estimate semantic homogeneity within a framework of automatic distractor validation. This method is based on a combination of several criteria of semantic relatedness and similarity between terms, by machine learning. We will show that our method allows to obtain a better estimation of semantic homogeneity than methods proposed in related work.},
  motscles  = {similarité, voisinage sémantique, classification de termes},
  keywords  = {similarity, semantic relatedness, term ranking},
}

@inproceedings{cartier:2015:TALN,
  author    = {Cartier, Emmanuel},
  title     = {Extraction automatique de relations sémantiques dans les dé finitions : approche hybride, construction d'un corpus de relations sémantiques pour le français},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {131--145},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-012},
  language  = {french},
  note      = {Automatic Extraction of Semantic Relations from Definitions : en experiment in French with an Hybrid Approach},
  resume    = {Cet article présente une expérimentation visant à construire une ressource sémantique pour le français contemporain à partir d'un corpus d'environ un million de définitions tirées de deux ressources lexicographiques (Trésor de la Langue Française, Wiktionary) et d'une ressource encyclopédique (Wikipedia). L'objectif est d'extraire automatiquement dans les définitions différentes relations sémantiques : hyperonymie, synonymie, méronymie, autres relations sémantiques. La méthode suivie combine la précision des patrons lexico-syntaxiques et le rappel des méthodes statistiques, ainsi qu'un traitement inédit de canonisation et de décomposition des énoncés. Après avoir présenté les différentes approches et réalisations existantes, nous détaillons l'architecture du système et présentons les résultats : environ 900 000 relations d'hyperonymie et près de 100 000 relations de synonymie, avec un taux de précision supérieur à 90% sur un échantillon aléatoire de 500 relations. Plus de 2 millions de prédications définitoires ont également été extraites.},
  abstract  = {This article presents an experiment to extract semantic relations from definitions. It is based on approximately one million definitions from two general dictionaries (Trésor de la Langue Française, French Wiktionary) and from the collaborative Wikipedia. We aim at extracting from these data several semantic relations : hyperonymy, synonymy, meronymy and other semantic relations. The methodological approach combines the precision of lexico-syntactic patterns and the recall of statistical analysis. After a survey of the state-of-the-art methods in this area, we detail our system and give the overall outcomes : about 900 000 hypernymy and 100 000 synonymy relations are extracted with a precision above 90% on a sample of 500 pairs for each relation. About 2 millions of definitory predicates are also extracted.},
  motscles  = {relations sémantiques, patrons lexico-syntaxiques, distributionnalisme, prédication, hyperonymie, synonymie, méronymie, définition},
  keywords  = {semantic relations, lexico-syntactic patterns, distributionnalism, predication, hypernymy, synonymy, meronymy, definition},
}

@inproceedings{ferret:2015:TALN,
  author    = {Ferret, Olivier},
  title     = {Déclasser les voisins non sémantiques pour améliorer les thésaurus distributionnels},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {146--157},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-013},
  language  = {french},
  note      = {Downgrading non-semantic neighbors for improving distributional thesauri},
  resume    = {La plupart des méthodes d'amélioration des thésaurus distributionnels se focalisent sur les moyens – représentations ou mesures de similarité – de mieux détecter la similarité sémantique entre les mots. Dans cet article, nous proposons un point de vue inverse : nous cherchons à détecter les voisins sémantiques associés à une entrée les moins susceptibles d'être liés sémantiquement à elle et nous utilisons cette information pour réordonner ces voisins. Pour détecter les faux voisins sémantiques d'une entrée, nous adoptons une approche s'inspirant de la désambiguïsation sémantique en construisant un classifieur permettant de différencier en contexte cette entrée des autres mots. Ce classifieur est ensuite appliqué à un échantillon des occurrences des voisins de l'entrée pour repérer ceux les plus éloignés de l'entrée. Nous évaluons cette méthode pour des thésaurus construits à partir de cooccurrents syntaxiques et nous montrons l'intérêt de la combiner avec les méthodes décrites dans (Ferret, 2013b) selon une stratégie de type vote.},
  abstract  = {Most of the methods for improving distributional thesauri focus on the means – representations or similarity measures – to detect better semantic similarity between words. In this article, we propose a more indirect approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate more particularly the interest of this method for thesauri built from syntactic co-occurrents and we show the interest of associating this method with those of (Ferret, 2013b) following an ensemble strategy.},
  motscles  = {Sémantique lexicale, similarité sémantique, thésaurus distributionnels},
  keywords  = {Lexical semantics, semantic similarity, distributional thesauri},
}

@inproceedings{danlos-maskharashvili-pogodalla:2015:TALN,
  author    = {Danlos, Laurence and Maskharashvili, Aleksandre and Pogodalla, Sylvain},
  title     = {Grammaires phrastiques et discursives fondées sur les TAG : une approche de D-STAG avec les ACG},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {158--169},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-014},
  language  = {french},
  note      = {Sentential and Discourse TAG-Based Grammars: An ACG Approach to D-STAG},
  resume    = {Nous présentons une méthode pour articuler grammaire de phrase et grammaire de discours qui évite de recourir à une étape de traitement intermédiaire. Cette méthode est suffisamment générale pour construire des structures discursives qui ne soient pas des arbres mais des graphes orientés acycliques (DAG). Notre analyse s'appuie sur une approche de l'analyse discursive, Discourse Synchronous TAG (D-STAG), qui utilise les Grammaires d'Arbres Adjoint (TAG). Nous utilisons pour ce faire un encodage des TAG dans les Grammaires Catégorielles Abstraites (ACG). Cet encodage permet d'une part d'utiliser l'ordre supérieur pour l'interprétation sémantique afin de construire des structures qui soient des DAG et non des arbres, et d'autre part d'utiliser les propriétés de composition d'ACG pour réaliser naturellement l'interface entre grammaire phrastique et grammaire discursive. Tous les exemples proposés pour illustrer la méthode ont été implantés et peuvent être testés avec le logiciel approprié.},
  abstract  = {This article presents a method to interface a sentential grammar and a discourse grammar without resorting to an intermediate processing step. The method is general enough to build discourse structures that are direct acyclic graphs (DAG) and not only trees. Our analysis is based on Discourse Synchronous TAG (D-STAG), a Tree-Adjoining Grammar (TAG)-based approach to discourse. We also use an encoding of TAG into Abstract Categorial Grammar (ACG). This encoding allows us to express a higher-order semantic interpretation that enables building DAG discourse structures on the one hand, and to smoothly integrate the sentential and the discourse grammar thanks to the modular capability of ACG. All the examples of the article have been implemented and may be run and tested with the appropriate software.},
  motscles  = {Syntaxe, sémantique, discours, grammaire, grammaire d'arbres adjoints, TAG, D-LTAG, D-STAG, grammaire catégorielle abstraite, ACG},
  keywords  = {Syntax, semantics, discourse, grammar, Tree-Adjoining Grammar, TAG, D-LTAG, D-STAG, Abstract Categorial Grammar, ACG},
}

@inproceedings{gleize-grau:2015:TALN,
  author    = {Gleize, Martin and Grau, Brigitte},
  title     = {Noyaux de réécriture de phrases munis de types lexico-sémantiques},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {170--181},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-015},
  language  = {french},
  note      = {Enriching String Rewriting Kernels With Lexico-semantic Types},
  resume    = {De nombreux problèmes en traitement automatique des langues requièrent de déterminer si deux phrases sont des réécritures l'une de l'autre. Une solution efficace consiste à apprendre les réécritures en se fondant sur des méthodes à noyau qui mesurent la similarité entre deux réécritures de paires de phrases. Toutefois, ces méthodes ne permettent généralement pas de prendre en compte des variations sémantiques entre mots, qui permettraient de capturer un plus grand nombre de règles de réécriture. Dans cet article, nous proposons la définition et l'implémentation d'une nouvelle classe de fonction noyau, fondée sur la réécriture de phrases enrichie par un typage pour combler ce manque. Nous l'évaluons sur deux tâches, la reconnaissance de paraphrases et d'implications textuelles.},
  abstract  = {Many high level natural language processing problems can be framed as determining if two given sentences are a rewriting of each other. One way to solve this problem is to learn the way a sentence rewrites into another with kernel-based methods, relying on a kernel function to measure the similarity between two rewritings. While a wide range of rewriting kernels has been developed in the past, they often do not allow the user to provide lexico-semantic variations of words, which could help capturing a wider class of rewriting rules. In this paper, we propose and implement a new class of kernel functions, referred to as type-enriched string rewriting kernel, to address this lack. We experiment with various typing schemes on two natural sentence rewriting tasks, paraphrase identification and recognizing textual entailment.},
  motscles  = {fonction noyau, variations sémantiques, réécriture de phrase, reconnaissance de paraphrases, implication textuelle},
  keywords  = {kernel methods, semantic variations, sentence rewriting, paraphrase identification, textual entailment},
}

@inproceedings{grabar-hamon:2015:TALN,
  author    = {Grabar, Natalia and Hamon, Thierry},
  title     = {Extraction automatique de paraphrases grand public pour les termes médicaux},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {182--193},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-016},
  language  = {french},
  note      = {Automatic extraction of layman paraphrases for medical terms},
  resume    = {Nous sommes tous concernés par notre état de santé et restons sensibles aux informations de santé disponibles dans la société moderne à travers par exemple les résultats des recherches scientifiques, les médias sociaux de santé, les documents cliniques, les émissions de télé et de radio ou les nouvelles. Cependant, il est commun de rencontrer dans le domaine médical des termes très spécifiques (e.g., blépharospasme, alexitymie, appendicectomie), qui restent difficiles à comprendre par les non spécialistes. Nous proposons une méthode automatique qui vise l'acquisition de paraphrases pour les termes médicaux, qui soient plus faciles à comprendre que les termes originaux. La méthode est basée sur l'analyse morphologique des termes, l'analyse syntaxique et la fouille de textes non spécialisés. L'analyse et l'évaluation des résultats indiquent que de telles paraphrases peuvent être trouvées dans les documents non spécialisés et présentent une compréhension plus facile. En fonction des paramètres de la méthode, la précision varie entre 86 et 55 %. Ce type de ressources est utile pour plusieurs applications de TAL (e.g., recherche d'information grand public, lisibilité et simplification de textes, systèmes de question-réponses).},
  abstract  = {We all have health concerns and sensibility to health information available in the modern society through modern media, such as scientific research, health social media, clinical documents, TV and radio broadcast, or novels. However, medical area conveys very specific notions (e.g., blepharospasm, alexitymia, appendicectomy), which are difficult to understand by people without medical training. We propose an automatic method for the acquisition of paraphrases for technical medical terms. We expect that such paraphrases are easier to understand than the original terms. The method is based on the morphological analysis of terms, syntactic analysis of texts, and text mining of non specialized texts. An analysis of the results and their evaluation indicate that such paraphrases can indeed be found in non specialized documents and show easier understanding level. According to the setting of the method, precision of the extractions ranges between 86 and 55%. This kind of resources is useful for several Natural Language Processing applications (e.g., information retrieval for lay people, text readability and simplification, question and answering systems).},
  motscles  = {Domaines de spécialité, terminologie médicale, composition, analyse morphologique, paraphrase, compréhension},
  keywords  = {Specialized Area, Medical Terminology, Compounds, Morphological Analysis, Paraphrasis, Understanding},
}

@inproceedings{guibon-EtAl:2015:TALN,
  author    = {Guibon, Gaël and Tellier, Isabelle and Prévost, Sophie and Constant, Matthieu and Gerdes, Kim},
  title     = {Analyse syntaxique de l'ancien français : quelles propriétés de la langue inﬂuent le plus sur la qualité de l'apprentissage ?},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {194--207},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-017},
  language  = {french},
  note      = {Old French parsing : Which language properties have the greatest inﬂuence on learning quality ?},
  resume    = {L'article présente des résultats d'expériences d'apprentissage automatique pour l'étiquetage morpho-syntaxique et l'analyse syntaxique en dépendance de l'ancien français. Ces expériences ont pour objectif de servir une exploration de corpus pour laquelle le corpus arboré SRCMF sert de données de référence. La nature peu standardisée de la langue qui y est utilisée implique des données d'entraînement hétérogènes et quantitativement limitées. Nous explorons donc diverses stratégies, fondées sur différents critères (variabilité du lexique, forme Vers/Prose des textes, dates des textes), pour constituer des corpus d'entrainement menant aux meilleurs résultats possibles.},
  abstract  = {This paper presents machine learning experiments for part-of-speech labelling and dependency parsing of Old French. Machine learning methods are used for the purpose of corpus exploration. The SRCMF Treebank is our reference data. The poorly standardised nature of the language used in this corpus implies that training data is heterogenous and quantitatively limited. We explore various strategies, based on different criteria (variability of the lexicon, Verse/Prose form, date of writing) to build training corpora leading to the best possible results.},
  motscles  = {étiquetage morpho-syntaxique, analyse en dépendance, ancien français, apprentissage automatique, exploration de corpus},
  keywords  = {POS labelling, Dependency Parsing, Old French, machine learning, corpus exploration},
}

@inproceedings{brixtel-lecluze-lejeune:2015:TALN,
  author    = {Brixtel, Romain and Lecluze, Charlotte and Lejeune, Gaël},
  title     = {Attribution d'Auteur : approche multilingue fondée sur les répétitions maximales},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {208--219},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-018},
  language  = {french},
  note      = {Authorship Attribution through Character Substrings (and vise versa)},
  resume    = {Cet article s'attaque à la tâche d'Attribution d'Auteur en contexte multilingue. Nous proposons une alternative aux méthodes supervisées fondées sur les n-grammes de caractères de longueurs variables : les répétitions maximales. Pour un texte donné, la liste de ses n-grammes de caractères contient des informations redondantes. A contrario, les répétitions maximales représentent l'ensemble des répétitions de ce texte de manière condensée. Nos expériences montrent que la redondance des n-grammes contribue à l'efficacité des techniques d'Attribution d'Auteur exploitant des sous-chaînes de caractères. Ce constat posé, nous proposons une fonction de pondération sur les traits donnés en entrée aux classifieurs, en introduisant les répétitions maximales du nème ordre (c'est-à-dire des répétitions maximales détectées dans un ensemble de répétitions maximales). Les résultats expérimentaux montrent de meilleures performances avec des répétitions maximales, avec moins de données que pour les approches fondées sur les n-grammes.},
  abstract  = {This article tackles the Authorship Attribution task according to the language independence issue. We propose an alternative of variable length character n-gram features in supervised methods : maximal repeats in strings. When character n-grams are by essence redundant, maximal repeats are a condensed way to represent any substring of a corpus. Our experiments show that the redundant aspect of character n-grams contributes to the efficiency of character-based Authorship Attribution techniques. Therefore, we introduce a new way to weight features in vector based classifier by introducing n-th order maximal repeats (maximal repeats detected in a set of maximal repeats). The experimental results show higher performance with maximal repeats, with less data than n-grams based approach.},
  motscles  = {attribution d'auteur, multilinguisme, classification, chaînes de caractères, répétitions maximales},
  keywords  = {authorship attribution, multilinguism, classification, character substrings, maximal repeats},
}

@inproceedings{vu-EtAl:2015:TALN,
  author    = {Vu, Hai Hieu and Villaneau, Jeanne and Saïd, Farida and Marteau, Pierre-François},
  title     = {Mesurer la similarité entre phrases grâce à Wikipédia en utilisant une indexation aléatoire},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {220--231},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-019},
  language  = {french},
  note      = {Semantic similarity between sentences based on Wikipedia and Random Indexing},
  resume    = {Cet article présente une méthode pour mesurer la similarité sémantique entre phrases qui utilise Wikipédia comme unique ressource linguistique et qui est, de ce fait, utilisable pour un grand nombre de langues. Basée sur une représentation vectorielle, elle utilise une indexation aléatoire pour réduire la dimension des espaces manipulés. En outre, elle inclut une technique de calcul des vecteurs de termes qui corrige les défauts engendrés par l'utilisation d'un corpus aussi général que Wikipédia. Le système a été évalué sur les données de SemEval 2014 en anglais avec des résultats très encourageants, au-dessus du niveau moyen des systèmes en compétition. Il a également été testé sur un ensemble de paires de phrases en français, à partir de ressources que nous avons construites et qui seront mises à la libre disposition de la communauté scientifique.},
  abstract  = {This paper proposes a semantic similarity measure for sentence comparison based on the exploitation of Wikipedia as the only language resource. Such similarity measure is therefore usable for a wide range of languages, basically those covered by Wikipedia. Random Indexing is used to cope with the great dimensionality and the spareness of the data vectorial representations. Furthermore, a statistical weight function is used to reduce the noise generated by the use of a multi domain corpus such as Wikipedia. This semantic similarity measure has been evaluated on SemEval 2014 dataset for English language leading to very promising results, basically above the average level of the competing systems that exploit Wikipédia in conjunction with other sources of semantic information. It has been also evaluated on a set of pairs of sentences in French that we have build specifically for the task, and made freely available for the research community.},
  motscles  = {Similarité sémantique, Indexation aléatoire, Wikipédia, Relation sémantique},
  keywords  = {Semantic Textual Similarity, Random indexing, Wikipédia, Semantic Relatedness},
}

@inproceedings{blache-demontcheuil-rauzy:2015:TALN,
  author    = {Blache, Philippe and de Montcheuil, Grégroie and Rauzy, Stéphane},
  title     = {Typologie automatique des langues à partir de treebanks},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {232--243},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-020},
  language  = {french},
  note      = {Automatic Linguistic Typology from Treebanks},
  resume    = {La typologie des langues repose sur l'étude de la réalisation de propriétés ou phénomènes linguistiques dans plusieurs langues ou familles de langues. Nous abordons dans cet article la question de la typologie syntaxique et proposons une méthode permettant d'extraire automatiquement ces propriétés à partir de treebanks, puis de les analyser en vue de dresser une telle typologie. Nous décrivons cette méthode ainsi que les outils développés pour la mettre en œuvre. Celle-ci a été appliquée à l'analyse de 10 langues décrites dans le Universal Dependencies Treebank. Nous validons ces résultats en montrant comment une technique de classification permet, sur la base des informations extraites, de reconstituer des familles de langues.},
  abstract  = {Linguistic typology studies different linguistic properties or phenomena in order to compare several languages or language families. We address in this paper the question of syntactic typology and propose a method for extracting automatically from treebanks syntactic properties, and bring them into a typology perspective. We present here the method and the different tools for inferring such information. The approach has been applied to 10 languages of the Universal Dependencies Treebank. We validate the results in showing how automatic classification corrélâtes with language families.},
  motscles  = {Typologie, syntaxe, treebank, inférence de grammaire, Grammaire de Propriétés},
  keywords  = {Typology, syntax, grammar inference, Property Grammars},
}

@inproceedings{besacier-lecouteux-ngocquang:2015:TALN,
  author    = {Besacier, Laurent and Lecouteux, Benjamin and Ngoc Quang, Luong},
  title     = {Utilisation de mesures de confiance pour améliorer le décodage en traduction de parole},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {244--254},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-021},
  language  = {french},
  note      = {Word confidence estimation for re-decoding speech translation graphs},
  resume    = {Les mesures de confiance au niveau mot (Word Confidence Estimation - WCE) pour la traduction auto- matique (TA) ou pour la reconnaissance automatique de la parole (RAP) attribuent un score de confiance à chaque mot dans une hypothèse de transcription ou de traduction. Dans le passé, l'estimation de ces mesures a le plus souvent été traitée séparément dans des contextes RAP ou TA. Nous proposons ici une estimation conjointe de la confiance associée à un mot dans une hypothèse de traduction automatique de la parole (TAP). Cette estimation fait appel à des paramètres issus aussi bien des systèmes de transcription de la parole (RAP) que des systèmes de traduction automatique (TA). En plus de la construction de ces estimateurs de confiance robustes pour la TAP, nous utilisons les informations de confiance pour re-décoder nos graphes d'hypothèses de traduction. Les expérimentations réalisées montrent que l'utilisation de ces mesures de confiance au cours d'une seconde passe de décodage permettent d'obtenir une amélioration significative des performances de traduction (évaluées avec la métrique BLEU - gains de deux points par rapport à notre système de traduc- tion de parole de référence). Ces expériences sont faites pour une tâche de TAP (français-anglais) pour laquelle un corpus a été spécialement conçu (ce corpus, mis à la disposition de la communauté TALN, est aussi décrit en détail dans l'article).},
  abstract  = {Word Confidence Estimation (WCE) for machine translation (MT) or automatic speech recognition (ASR) assigns a confidence score to each word in the MT or ASR hypothesis. In the past, this task has been treated separately in ASR or MT contexts and we propose here a joint estimation of word confidence for a spoken language translation (SLT) task involving both ASR and MT. We build robust word confidence estimators for SLT, based on joint ASR and MT features. Using these word confidence measures to re-decode the spoken language translation graph leads to a significant BLEU improvement (2 points) compared to the SLT baseline. These experiments are done for a French-English SLT task for which a corpus was specifically designed (this corpus being made available to the NLP community).},
  motscles  = {Mesures de confiance, traduction automatique de la parole, paramètres joints, redécodage de graphe},
  keywords  = {Word confidence estimation (WCE), spoken language translation (SLT), joint features, search graph re-decoding},
}

@inproceedings{kraif:2015:TALN,
  author    = {Kraif, Olivier},
  title     = {Multi­alignement vs bi­alignement : à plusieurs, c'est mieux !},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {255--266},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-022},
  language  = {french},
  note      = {Multi­alignment vs bi­alignment: the more languages the better},
  resume    = {Dans cet article, nous proposons une méthode originale destinée à effectuer l'alignement d'un corpus multi­parallèle, i.e. comportant plus de deux langues, en prenant en compte toutes les langues simultanément (et non en composant une série de bi­alignements indépendants). Pour ce faire, nous nous appuyons sur les réseaux de correspondances lexicales constitués par les transfuges (chaînes identiques) et cognats (mots apparentés), et nous montrons comment divers tuilages des couples de langues permettent d'exploiter au mieux les ressemblances superficielles liées aux relations génétiques interlinguistiques. Nous évaluons notre méthode par rapport à une méthode de bi­alignement classique, et montrons en quoi le multi­alignement permet d'obtenir des résultats à la fois plus précis et plus robustes.},
  abstract  = {In this paper, we propose an original method for performing the alignment of a multi­parallel corpus, ie a parallel corpus involving more than two languages, taking into account all the languages simultaneously (and not by merging a series of independent bi­alignments). To do this, we rely on the networks of lexical correspondences formed by identical chains and cognates (related words, and we show how various tiling of language pairs allow to exploit the surface similarities due to genetic relationships between languages. We evaluate our method compared to a conventional method of bi­alignment, and show how the multi­alignement achieves both more accurate and robust results.},
  motscles  = {Alignement multilingue, corpus parallèles, cognats},
  keywords  = {Multilingual alignment, parallel corpora, cognates},
}

@inproceedings{do-allauzen-yvon:2015:TALN,
  author    = {Do, Quoc-Khanh and Allauzen, Alexandre and Yvon, François},
  title     = {Apprentissage discriminant des modèles continus de traduction},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {267--278},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-023},
  language  = {french},
  note      = {Discriminative Learning of Continuous Translation Models},
  resume    = {Alors que les réseaux neuronaux occupent une place de plus en plus importante dans le traitement automatique des langues, les méthodes d'apprentissage actuelles utilisent pour la plupart des critères qui sont décorrélés de l'application. Cet article propose un nouveau cadre d'apprentissage discriminant pour l'estimation des modèles continus de traduction. Ce cadre s'appuie sur la définition d'un critère d'optimisation permettant de prendre en compte d'une part la métrique utilisée pour l'évaluation de la traduction et d'autre part l'intégration de ces modèles au sein des systèmes de traduction automatique. De plus, cette méthode d'apprentissage est comparée aux critères existants d'estimation que sont le maximum de vraisemblance et l'estimation contrastive bruitée. Les expériences menées sur la tâches de traduction des séminaires TED Talks de l'anglais vers le français montrent la pertinence d'un cadre discriminant d'apprentissage, dont les performances restent toutefois très dépendantes du choix d'une stratégie d'initialisation idoine. Nous montrons qu'avec une initialisation judicieuse des gains significatifs en termes de scores BLEU peuvent être obtenus.},
  abstract  = {This paper proposes a new discriminative framework to train translation models based on neural network. This framework relies on the definition of a new objective function that allows us to introduce the evaluation metric in the learning process as well as to consider how the model interacts with the translation system. Moreover, this approach is compared with the state of the art estimation methods, such as the maximum likelihood criterion and the noise contrastive estimation. Experiments are carried out on the English to French translation task of TED Talks . The results show the efficiency of the proposed approach, whereas the initialization has a strong impact. We show that with a tailored initialization scheme significant improvements can be obtained in terms of BLEU scores.},
  motscles  = {Modèle neuronal de traduction, traduction automatique par approche statistique, apprentissage discriminant},
  keywords  = {Neural network based translation model, statistical machine translation, discriminative learning},
}

@inproceedings{fraisse-paroubek:2015:TALN,
  author    = {Fraisse, Amel and Paroubek, Patrick},
  title     = {Utiliser les interjections pour détecter les émotions},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {279--292},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-024},
  language  = {french},
  note      = {Using interjections for emotion detection},
  resume    = {Bien que les interjections soient un phénomène linguistique connu, elles ont été peu étudiées et cela continue d'être le cas pour les travaux sur les microblogs. Des travaux en analyse de sentiments ont montré l'intérêt des émoticônes et récemment des mots-dièses, qui s'avèrent être très utiles pour la classification en polarité. Mais malgré leur statut grammatical et leur richesse sémantique, les interjections sont restées marginalisées par les systèmes d'analyse de sentiments. Nous montrons dans cet article l'apport majeur des interjections pour la détection des émotions. Nous détaillons la production automatique, basée sur les interjections, d'un corpus étiqueté avec les émotions. Nous expliquons ensuite comment nous avons utilisé ce corpus pour en déduire, automatiquement, un lexique affectif pour le français. Ce lexique a été évalué sur une tâche de détection des émotions, qui a montré un gain en mesure F1 allant, selon les émotions, de +0,04 à +0,21.},
  abstract  = {Although interjections have been recognized as linguistic phenomena for a long time, they have somehow been rarely studied and continue to be left aside in works dealing with microblogs. Users of this new kind of communication plateforms have popularized widely the use of linguistic constructions, like emoticons or interjections. In spite of their grammatic status and semantic richness for describing emotional states, interjections have been mostly ignored. In this article we show the importance of the role that interjections can play for detecting emotions. We detail how using interjections we have tagged automatically a French microblog corpus with emotion labels. Then we describe how we did deduce automatically from this corpus a fine-grained affective lexicon. The usefulness of the lexicon was evaluated in an emotion recognition task where, depending on the emotion, the F1-measure improvement ranged from +0.04 to +0.21.},
  motscles  = {Interjections, détection des émotions, lexique affectif, analyse de sentiments, fouille d'opinions},
  keywords  = {Interjections, emotion recognition, affective lexicon, sentiment analysis, opinion mining},
}

@inproceedings{coavoux-crabbe:2015:TALN,
  author    = {Coavoux, Maximin and Crabbé, Benoît},
  title     = {Comparaison d'architectures neuronales pour l'analyse syntaxique en constituants},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {293--304},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-025},
  language  = {french},
  note      = {A Comparison of Neural Network Architectures for Constituent Parsing},
  resume    = {L'article traite de l'analyse syntaxique lexicalisée pour les grammaires de constituants. On se place dans le cadre de l'analyse par transitions. Les modèles statistiques généralement utilisés pour cette tâche s'appuient sur une représentation non structurée du lexique. Les mots du vocabulaire sont représentés par des symboles discrets sans liens entre eux. À la place, nous proposons d'utiliser des représentations denses du type plongements (embeddings) qui permettent de modéliser la similarité entre symboles, c'est-à-dire entre mots, entre parties du discours et entre catégories syntagmatiques. Nous proposons d'adapter le modèle statistique sous-jacent à ces nouvelles représentations. L'article propose une étude de 3 architectures neuronales de complexité croissante et montre que l'utilisation d'une couche cachée non-linéaire permet de tirer parti des informations données par les plongements.},
  abstract  = {The article deals with lexicalized constituent parsing in a transition-based framework. Typical statistical approaches for this task are based on an unstructured representation of the lexicon. Words are represented by discrete unrelated symbols. Instead, our proposal relies on dense vector representations (embeddings) that are able to encode similarity between symbols : words, part-of-speech tags and phrase structure symbols. The article studies and compares 3 increasingly complex neural network architectures, which are fed symbol embeddings. The experiments suggest that the information given by embeddings is best captured by a deep architecture with a non-linear layer.},
  motscles  = {Analyse syntaxique en constituants lexicalisée, plongements, réseaux de neurones},
  keywords  = {Lexicalized constituent parsing, embeddings, neural networks},
}

@inproceedings{grabar-eshkol:2015:TALN,
  author    = {Grabar, Natalia and Eshkol, Iris},
  title     = {...des conférences enfin disons des causeries... Détection automatique de segments en relation de paraphrase dans les reformulations de corpus oraux},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {305--316},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-long-026},
  language  = {french},
  note      = {...des conférences enfin disons des causeries... Automatic detection of segments with paraphrase relation in spoken corpora rephrasings},
  resume    = {Notre travail porte sur la détection automatique des segments en relation de reformulation paraphrastique dans les corpus oraux. L'approche proposée est une approche syntagmatique qui tient compte des marqueurs de reformulation paraphrastique et des spécificités de l'oral. Les données de référence sont consensuelles. Une méthode automatique fondée sur l'apprentissage avec les CRF est proposée afin de détecter les segments paraphrasés. Différents descripteurs sont exploités dans une fenêtre de taille variable. Les tests effectués montrent que les segments en relation de paraphrase sont assez difficiles à détecter, surtout avec leurs frontières correctes. Les meilleures moyennes atteignent 0,65 de F-mesure, 0,75 de précision et 0,63 de rappel. Nous avons plusieurs perspectives à ce travail pour améliorer la détection des segments en relation de paraphrase et pour étudier les données depuis d'autres points de vue.},
  abstract  = {Our work addresses automatic detection of segments with paraphrastic rephrasing relation in spoken corpus. The proposed approach is syntagmatic. It is based on paraphrastic rephrasing markers and the specificities of the spoken language. The reference data used are consensual. Automatic method based on machine learning using CRFs is proposed in order to detect the segments that are paraphrased. Different descriptors are exploited within a window with various sizes. The tests performed indicate that the segments that are in paraphrastic relation are quite difficult to detect. Our best average reaches up to 0.65 F-measure, 0.75 precision, and 0.63 recall. We have several perspectives to this work for improving the detection of segments that are in paraphrastic relation and for studying the data from other points of view.},
  motscles  = {Corpus oraux, Paraphrase, Reformulation, Marqueur de reformulation paraphrastique, Apprentissage supervisé},
  keywords  = {Spoken Corpora, Paraphrase, Reformulation, Paraphrastic Reformulation Marker, Supervised Learning},
}

@inproceedings{xia-EtAl:2015:TALN,
  author    = {Xia, Tian and Zhai, Shaodan and Li, Zhongliang and Wang, Shaojun},
  title     = {Une méthode discriminant formation simple pour la traduction automatique avec Grands Caractéristiques},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {317--322},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-001},
  language  = {french},
  note      = {A Simple Discriminative Training Method for Machine Translation with Large-Scale Features},
  resume    = {Marge infusé algorithmes détendus (MIRAS) dominent modèle de tuning dans la traduction automatique statistique dans le cas des grandes caractéristiques de l'échelle, mais ils sont également célèbres pour la complexité de mise en œuvre. Nous introduisons une nouvelle méthode, qui concerne une liste des N meilleures comme une permutation et minimise la perte Plackett-Luce de permutations rez-de-vérité. Des expériences avec des caractéristiques à grande échelle démontrent que, la nouvelle méthode est plus robuste que MERT ; si ce est seulement à rattacher avec Miras, il a un avantage comparativement, plus facile à mettre en œuvre.},
  abstract  = {The margin infused relaxed algorithm (MIRAs) dominates model tuning in statistical machine translation in the case of large scale features, but also they are famous for the complexity in implementation. We introduce a new method, which regards an N-best list as a permutation and minimizes the Plackett-Luce loss of ground-truth permutations. Experiments with large-scale features demonstrate that, the new method is more robust than MERT ; though it is only matchable with MIRAs, it has a comparatively advantage, easier to implement.},
  motscles  = {Traduction automatique, ajustement du modèle, caractéristiques à grande échelle},
  keywords  = {machine translation, model tuning, large-scale features},
}

@inproceedings{chatzikyriakidis:2015:TALN,
  author    = {Chatzikyriakidis, Stergios},
  title     = {Natural Language Reasoning using Coq: Interaction and Automation},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {323--329},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-002},
  language  = {french},
  resume    = {Dans cet article, nous présentons une utilisation des assistants des preuves pour traiter l'inférence en Language Naturel (NLI). D' abord, nous proposons d'utiliser les theories des types modernes comme langue dans laquelle traduire la sémantique du langage naturel. Ensuite, nous implémentons cette sémantique dans l'assistant de preuve Coq pour raisonner sur ceux-ci. En particulier, nous évaluons notre proposition sur un sous-ensemble de la suite de tests FraCas, et nous montrons que 95.2% des exemples peuvent être correctement prédits. Nous discutons ensuite la question de l'automatisation et il est démontré que le langage de tactiques de Coq permet de construire des tactiques qui peuvent automatiser entièrement les preuves, au moins pour les cas qui nous intéressent.},
  abstract  = {In this paper, we present the use of proof-assistant technology in order to deal with Natural Language Inference. We first propose the use of modern type theories as the language in which we translate natural language semantics to. Then, we implement these semantics in the proof-assistant Coq in order to reason about them. In particular we evaluate against a subset of the FraCas test suite and show a 95.2% accuracy and also precision levels that outperform existing approaches at least for the comparable parts. We then discuss the issue of automation, showing that Coq's tactical language allows one to build tactics that can fully automate proofs, at least for the cases we have looked at.},
  motscles  = {Inference en Langage Naturel, Théorie des Types, Sémantique Formelle, FraCas, Coq, Automatisation des Preuves},
  keywords  = {Natural Language Inference, Type Theory, Formal Semantics, FraCas, Coq, Proof automation},
}

@inproceedings{lafourcade-lebrun-joubert:2015:TALN,
  author    = {Lafourcade, Mathieu and Le Brun, Nathalie and Joubert, Alain},
  title     = {Vous aimez ?...ou pas ? LikeIt, un jeu pour construire une ressource lexicale de polarité},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {330--336},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-003},
  language  = {french},
  note      = {Do you like it? or not? LikeIt, a game to build a polarity lexical resource},
  resume    = {En analyse de discours ou d'opinion, savoir caractériser la connotation générale d'un texte, les sentiments qu'il véhicule, est une aptitude recherchée, qui suppose la constitution préalable d'une ressource lexicale de polarité. Au sein du réseau lexical JeuxDeMots, nous avons mis au point LikeIt, un jeu qui permet d'affecter une valeur positive, négative, ou neutre à un terme, et de constituer ainsi pour chaque terme, à partir des votes, une polarité résultante. Nous présentons ici l'analyse quantitative des données de polarité obtenues, ainsi que la méthode pour les valider qualitativement.},
  abstract  = {The ability to analyze the feelings that emerge from a text requires having a polarity lexical resource. In the lexical network JeuxDeMots we designed LikeIt, a GWAP that allows attributing a positive, negative or neutral value to a term, and thus obtaining for each term a resulting polarity. We present a quantitative analysis of polarity data obtained, together with the comparison method we developed to validate them qualitatively.},
  motscles  = {polarité, sentiment, réseau lexical, crowdsourcing, GWAP},
  keywords  = {polarity, feelings, lexical network, crowdsourcing, GWAP},
}

@inproceedings{morlanehondere-grouin-zweigenbaum:2015:TALN,
  author    = {Morlane-Hondère, François and Grouin, Cyril and Zweigenbaum, Pierre},
  title     = {Étude des verbes introducteurs de noms de médicaments dans les forums de santé},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {337--343},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-004},
  language  = {french},
  note      = {Study of Drug-Introducing Verbs on Health Forums},
  resume    = {Dans cet article, nous combinons annotations manuelle et automatique pour identifier les verbes utilisés pour introduire un médicament dans les messages sur les forums de santé. Cette information est notamment utile pour identifier la relation entre un médicament et un effet secondaire. La mention d'un médicament dans un message ne garantit pas que l'utilisateur a pris ce traitement mais qu'il effectue un retour. Nous montrons ensuite que ces verbes peuvent servir pour extraire automatiquement des variantes de noms de médicaments. Nous estimons que l'analyse de ces variantes pourrait permettre de modéliser les erreurs faites par les usagers des forums lorsqu'ils écrivent les noms de médicaments, et améliorer en conséquence les systèmes de recherche d'information.},
  abstract  = {In this paper, we combine manual/automatic annotation to identify the verbs used by the users of a health forum to say that they are taking a drug. This information is important in many aspects, one of them being the identification of the relation between drugs and side effects : the mere mention of a drug in a message is not enough to assess that the user is taking this drug, and is thus likely to provide a feedback on it. In a second part of the study, we show how the set of verbs that we identified can be used to automatically extract variants of drug names. We assume that the analysis of the variants could shed light on patterns of mistakes that users make when spelling drug names and thus, improve medical information retrieval systems.},
  motscles  = {contenu généré par l'utilisateur, forum, verbes, noms de médicaments},
  keywords  = {user-generated content, forum, verbs, drug names},
}

@inproceedings{morchid-dufour-linares:2015:TALN,
  author    = {Morchid, Mohamed and Dufour, Richard and Linarès, Georges},
  title     = {Initialisation de Réseaux de Neurones à l'aide d'un Espace Thématique},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {344--349},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-005},
  language  = {french},
  note      = {Neural Network Initialization using a Topic Space},
  resume    = {Ce papier présente une méthode de traitement de documents parlés intégrant une représentation fondée sur un espace thématique dans un réseau de neurones artificiels (ANN) employé comme classifieur de document. La méthode proposée consiste à configurer la topologie d'un ANN ainsi que d'initialiser les connexions de celui-ci à l'aide des espaces thématiques appris précédemment. Il est attendu que l'initialisation fondée sur les probabilités thématiques permette d'optimiser le processus d'optimisation des poids du réseau ainsi qu'à accélérer la phase d'apprentissage tout en amélioration la précision de la classification d'un document de test. Cette méthode est évaluée lors d'une tâche de catégorisation de dialogues parlés entre des utilisateurs et des agents du service d'appels de la Régie Autonome Des Transports Parisiens (RATP). Les résultats montrent l'intérêt de la méthode proposée d'initialisation d'un réseau, avec un gain observé de plus de 4 points en termes de bonne classification comparativement à l'initialisation aléatoire. De plus, les expérimentations soulignent que les performances sont faiblement dépendantes de la topologie du ANN lorsque les poids de la couche cachée sont initialisés au moyen des espaces de thèmes issus d'une allocation latente de Dirichlet ou latent Dirichlet Allocation (LDA) en comparaison à une initialisation empirique.},
  abstract  = {This paper presents a method for speech analytics that integrates topic-space based representation into an artificial neural network (ANN), working as a document classifier. The proposed method consists in configuring the ANN's topology and in initializing the weights according to a previously estimated topic-space. Setup based on thematic priors is expected to improve the efficiency of the ANN's weight optimization process, while speeding-up the training process and improving the classification accuracy. This method is evaluated on a spoken dialogue categorization task which is composed of customer-agent dialogues from the call-centre of Paris Public Transportation Company. Results show the interest of the proposed setup method, with a gain of more than 4 points in terms of classification accuracy, compared to the baseline. Moreover, experiments highlight that performance is weakly dependent to ANN's topology with the LDA-based configuration, in comparison to classical empirical setup.},
  motscles  = {Réseau de neurones artificiels, Allocation latente de Dirichlet, Initialisation de poids},
  keywords  = {Artificial neural network, Latent Dirichlet allocation, Weights initialization},
}

@inproceedings{steinlin-colinet-danlos:2015:TALN,
  author    = {Steinlin, Jacques and Colinet, Margot and Danlos, Laurence},
  title     = {FDTB1: Repérage des connecteurs de discours en corpus},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {350--356},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-006},
  language  = {french},
  note      = {FDTB1 : Identification of discourse connectives in a French corpus},
  resume    = {Cet article présente le repérage manuel des connecteurs de discours dans le corpus FTB (French Treebank) déjà annoté pour la morpho-syntaxe. C'est la première étape de l'annotation discursive complète de ce corpus. Il s'agit de projeter sur le corpus les éléments répertoriés dans LexConn, lexique des connecteurs du français, et de filtrer les occurrences de ces éléments qui n'ont pas un emploi discursif mais par exemple un emploi d'adverbe de manière ou de préposition introduisant un complément sous-catégorisé. Plus de 10 000 connecteurs ont ainsi été repérés.},
  abstract  = {This paper presents the manual identification of discourse connectives in the corpus FTB (French Treebank) already annotated for morpho-syntax. This is the first step in the full discursive annotation of this corpus. The method consists in projecting on the corpus the items that are listed in LexConn, a lexicon of French connectives, and then filtering the occurrences of these elements that do not have a discursive use. More than 10K connectives have been identified.},
  motscles  = {connecteurs de discours, annotation discursive de corpus, grammaire et discours},
  keywords  = {discourse connectives, discourse annotation, grammar and discourse},
}

@inproceedings{bossard-rodrigues:2015:TALN,
  author    = {Bossard, Aurélien and Rodrigues, Christophe},
  title     = {ROBO, an edit distance for sentence comparison Application to automatic summarization},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {357--363},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-007},
  language  = {french},
  resume    = {Dans cet article, nous proposons une mesure de distance entre phrases fondée sur la distance de Levenshtein, doublement pondérée par la fréquence des mots et par le type d'opération réalisée. Nous l'évaluons au sein d'un système de résumé automatique dont la méthode de calcul est volontairement limitée à une approche fondée sur la similarité entre phrases. Nous sommes donc ainsi en mesure d'évaluer indirectement la performance de cette nouvelle mesure de distance.},
  abstract  = {We here propose a sentence edit distance metric, ROBO, based on Levenshtein distance. This metric distance is weighted by words frequency and operation type. We apply ROBO on an automatic summarization system whose sentence selection metrics are on purpose restricted to sentence similarity approaches. ROBO performance can then be evaluated indirectly.},
  motscles  = {résumé automatique, similarité sémantique, distance d'édition},
  keywords  = {automatic summarization, semantic similarity, edit distance},
}

@inproceedings{collin-guerraz:2015:TALN,
  author    = {Collin, Olivier and Guerraz, Aleksandra},
  title     = {Classification d'entités nommées de type « film »},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {364--370},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-008},
  language  = {french},
  note      = {Named Entity Classification for Movie Titles},
  resume    = {Dans cet article, nous nous intéressons à la classification contextuelle d'entités nommées de type « film ». Notre travail s'inscrit dans un cadre applicatif dont le but est de repérer, dans un texte, un titre de film contenu dans un catalogue (par exemple catalogue de films disponibles en VoD). Pour ce faire, nous combinons deux approches : nous partons d'un système à base de règles, qui présente une bonne précision, que nous couplons avec un modèle de langage permettant d'augmenter le rappel. La génération peu coûteuse de données d'apprentissage pour le modèle de langage à partir de Wikipedia est au coeur de ce travail. Nous montrons, à travers l'évaluation de notre système, la difficulté de classification des entités nommées de type « film » ainsi que la complémentarité des approches que nous utilisons pour cette tâche.},
  abstract  = {In this article, we focus on contextual classification of named entities for « movie » type. Our work is part of an application framework which aims to identify, in a text, a movie title contained in a catalog (e.g. VoD catalog). To do this, we combine two approaches : we use a rule-based system, which has good accuracy. To increase recall we couple our system with a language model. The generation of training data for the language model from Wikipedia is a crucial part of this work. We show, through the evaluation of our system, the complementarity of approaches we use.},
  motscles  = {reconnaissance d'entités nommées, films, classification, règles, modèle de langage, Wikipedia},
  keywords  = {named entity recognition, movies, classification, rules, language model, Wikipedia},
}

@inproceedings{schluter:2015:TALN,
  author    = {Schluter, Natalie},
  title     = {},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {371--376},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-009},
  language  = {french},
  note      = {A critical survey on measuring success in rank-based keyword assignment to documents},
  resume    = {},
  abstract  = {Evaluation approaches for unsupervised rank-based keyword assignment are nearly as numerous as are the existing systems. The prolific production of each newly used metric (or metric twist) seems to stem from general dis-satisfaction with the previous one and the source of that dissatisfaction has not previously been discussed in the literature. The difficulty may stem from a poor specification of the keyword assignment task in view of the rank-based approach. With a more complete specification of this task, we aim to show why the previous evaluation metrics fail to satisfy researchers' goals to distinguish and detect good rank-based keyword assignment systems. We put forward a characterisation of an ideal evaluation metric, and discuss the consistency of the evaluation metrics with this ideal, finding that the average standard normalised cumulative gain metric is most consistent with this ideal.},
  motscles  = {},
  keywords  = {rank-based keyword assignment},
}

@inproceedings{schluter:2015:TALN,
  author    = {Schluter, Natalie},
  title     = {},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {377--383},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-010},
  language  = {french},
  note      = {Effects of Graph Generation for Unsupervised Non-Contextual Single Document Keyword Extraction},
  resume    = {},
  abstract  = {This paper presents an exhaustive study on the generation of graph input to unsupervised graph-based non-contextual single document keyword extraction systems. A concrete hypothesis on concept coordination for documents that are scientific articles is put forward, consistent with two separate graph models : one which is based on word adjacency in the linear text–an approach forming the foundation of all previous graph-based keyword extraction methods, and a novel one that is based on word adjacency modulo their modifiers. In doing so, we achieve a best reported NDCG score to date of 0.431 for any system on the same data. In terms of a best parameter f-score, we achieve the highest reported to date (0.714) at a reasonable ranked list cut-off of n = 6, which is also the best reported f-score for any keyword extraction or generation system in the literature on the same data. The best-parameter f-score corresponds to a reduction in error of 12.6% conservatively.},
  motscles  = {},
  keywords  = {Keyword Extraction},
}

@inproceedings{servan-dymetman:2015:TALN,
  author    = {Servan, Christophe and Dymetman, Marc},
  title     = {Adaptation par enrichissement terminologique en traduction automatique statistique fondée sur la génération et le filtrage de bi-segments virtuels},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {384--390},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-011},
  language  = {french},
  note      = {Statistical machine translation adaptation through terminological enrichment based on virtual phrase generation and filtering},
  resume    = {Nous présentons des travaux préliminaires sur une approche permettant d'ajouter des termes bilingues à un système de Traduction Automatique Statistique (TAS) à base de segments. Les termes sont non seulement inclus individuellement, mais aussi avec des contextes les englobant. Tout d'abord nous générons ces contextes en généralisant des motifs (ou patrons) observés pour des mots de même nature syntaxique dans un corpus bilingue. Enfin, nous filtrons les contextes qui n'atteignent pas un certain seuil de confiance, à l'aide d'une méthode de sélection de bi-segments inspirée d'une approche de sélection de données, précédemment appliquée à des textes bilingues alignés.},
  abstract  = {We propose a technique for adding bilingual terms to a phrase-based SMT system which includes not only individual words, but also induces phrasal contexts around these words. We first generate these contexts by generalizing patterns observed for similar words in a bilingual corpus, but then filter out those contexts that fall below a certain confidence threshold, based on an original phrase-pair selection process inspired by existing sentence selection techniques.},
  motscles  = {Traduction Automatique Statistique, Génération Automatique de Texte, contexte phrastique, terminologie bilingue},
  keywords  = {Statistical Machine Translation, Natural Language Generation, phrasal context, bilingual terminology},
}

@inproceedings{boukhaled-frontini-ganascia:2015:TALN,
  author    = {Boukhaled, Mohamed Amine and Frontini, Francesca and Ganascia, Jean-Gabriel},
  title     = {Une mesure d'intérêt à base de surreprésentation pour l'extraction des motifs syntaxiques stylistiques},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {391--396},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-012},
  language  = {french},
  note      = {An Overrepresentation-based Interestingness Measure for Syntactic Stylistic Pattern Extraction},
  resume    = {Dans cette contribution, nous présentons une étude sur la stylistique computationnelle des textes de la littérature classiques française fondée sur une approche conduite par données, où la découverte des motifs linguistiques intéressants se fait sans aucune connaissance préalable. Nous proposons une mesure objective capable de capturer et d'extraire des motifs syntaxiques stylistiques significatifs à partir d'un œuvre d'un auteur donné. Notre hypothèse de travail est fondée sur le fait que les motifs syntaxiques les plus pertinents devraient refléter de manière significative le choix stylistique de l'auteur, et donc ils doivent présenter une sorte de comportement de surreprésentation contrôlé par les objectifs de l'auteur. Les résultats analysés montrent l'efficacité dans l'extraction de motifs syntaxiques intéressants dans le texte littéraire français classique, et semblent particulièrement prometteurs pour les analyses de ce type particulier de texte.},
  abstract  = {In this contribution, we present a computational stylistic study of the French classic literature texts based on a data-driven approach where discovering interesting linguistic patterns is done without any prior knowledge. We propose an objective measure capable of capturing and extracting meaningful stylistic syntactic patterns from a given author's work. Our hypothesis is based on the fact that the most relevant syntactic patterns should significantly reflect the author's stylistic choice and thus they should exhibit some kind of overrepresentation behavior controlled by the author's purpose. The analysed results show the effectiveness in extracting interesting syntactic patterns from classic French literary text, and seem particularly promising for the analyses of such particular text.},
  motscles  = {Stylistique computationnelle, fouille de texte, motifs syntaxiques, mesure d'intérêt},
  keywords  = {Computational stylistic, text mining, syntactic patterns, interestingness measure},
}

@inproceedings{bossard-rodrigues:2015:TALN,
  author    = {Bossard, Aurélien and Rodrigues, Christophe},
  title     = {Une Approche évolutionnaire pour le résumé automatique},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {397--403},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-013},
  language  = {french},
  note      = {Automatic summarization using a genetic algorithm},
  resume    = {Dans cet article, nous proposons une méthode de résumé automatique fondée sur l'utilisation d'un algorithme génétique pour parcourir l'espace des résumés candidats couplé à un calcul de divergence de distribution de probabilités de n-grammes entre résumés candidats et documents source. Cette méthode permet de considérer un résumé non plus comme une accumulation de phrases indépendantes les unes des autres, mais comme un texte vu dans sa globalité. Nous la comparons à une des meilleures méthodes existantes fondée sur la programmation linéaire en nombre entier, et montrons son efficacité sur le corpus TAC 2009.},
  abstract  = {This paper proposes a novel method for automatic summarization based on a genetic algorithm that explores candidate summaries space following an objective function computed over ngrams probability distributions of the candidate summary and the source documents. This method does not consider a summary as a stack of independant sentences but as a whole text. We compare this method to one of the best existing methods which is based on integer linear programming, and show its efficiency on TAC 2009 corpus.},
  motscles  = {Résumé automatique, algorithme génétique, modèles probabilistes},
  keywords  = {automatic summarization, genetic algorithm, probabilistic models},
}

@inproceedings{berrahou-EtAl:2015:TALN,
  author    = {Berrahou, Soumia Lilia and Buche, Patrice and Dibie-Barthélemy, Juliette and Roche, Mathieu},
  title     = {Identification des unités de mesure dans les textes scientifiques},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {404--410},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-014},
  language  = {french},
  note      = {Identification of units of measures in scientific texts},
  resume    = {Le travail présenté dans cet article se situe dans le cadre de l'identification de termes spécialisés (unités de mesure) à partir de données textuelles pour enrichir une Ressource Termino-Ontologique (RTO). La première étape de notre méthode consiste à prédire la localisation des variants d'unités de mesure dans les documents. Nous avons utilisé une méthode reposant sur l'apprentissage supervisé. Cette méthode permet de réduire sensiblement l'espace de recherche des variants tout en restant dans un contexte optimal de recherche (réduction de 86% de l'espace de recherché sur le corpus étudié). La deuxième étape du processus, une fois l'espace de recherche réduit aux variants d'unités, utilise une nouvelle mesure de similarité permettant d'identifier automatiquement les variants découverts par rapport à un terme d'unité déjà référencé dans la RTO avec un taux de précision de 82% pour un seuil au dessus de 0.6 sur le corpus étudié.},
  abstract  = {The work presented in this paper consists in identifying specialized terms (units of measures) in textual documents in order to enrich a onto-terminological resource (OTR). The first step permits to predict the localization of unit of measure variants in the documents. We have used a method based on supervised learning. This method permits to reduce significantly the variant search space staying in an optimal search context (reduction of 86% of the search space on the studied set of documents). The second step uses a new similarity measure identifying automatically variants associated with term denoting a unit of measure already present in the OTR with a precision rate of 82% for a threshold above 0.6 on the studied corpus .},
  motscles  = {ressource termino-ontologique, apprentissage, similarité},
  keywords  = {onto-terminological resource, learning, similarity},
}

@inproceedings{lejeune-brixtel-lecluze:2015:TALN,
  author    = {Lejeune, Gaël and Brixtel, Romain and Lecluze, Charlotte},
  title     = {Évaluation intrinsèque et extrinsèque du nettoyage de pages Web},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {411--417},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-015},
  language  = {french},
  note      = {Intrinsic and extrinsic evaluation of boilerplate removal tools},
  resume    = {Le nettoyage de documents issus du web est une tâche importante pour le TAL en général et pour la constitution de corpus en particulier. Cette phase est peu traitée dans la littérature, pourtant elle n'est pas sans inﬂuence sur la qualité des informations extraites des corpus. Nous proposons deux types d'évaluation de cette tâche de détourage : (I) une évaluation intrinsèque fondée sur le contenu en mots, balises et caractères ; (II) une évaluation extrinsèque fondée sur la tâche, en examinant l'effet du détourage des documents sur le système placé en aval de la chaîne de traitement. Nous montrons que les résultats ne sont pas cohérents entre ces deux évaluations ainsi qu'entre les différentes langues. Ainsi, le choix d'un outil de détourage devrait être guidé par la tâche visée plutôt que par la simple évaluation intrinsèque.},
  abstract  = {In this article, we tackle the problem of evaluation of web page cleaning tools. This task is seldom studied in the literature although it has consequences on the linguistic processing performed on web-based corpora. We propose two types of evaluation : (I) an intrinsic (content-based) evaluation with measures on words, tags and characters ; (II) an extrinsic (task-based) evaluation on the same corpus by studying the effects of the cleaning step on the performances of an NLP pipeline. We show that the results are not consistent in both evaluations. We show as well that there are important differences in the results between the studied languages. We conclude that the choice of a web page cleaning tool should be made in view of the aimed task rather than on the performances of the tools in an intrinsic evaluation.},
  motscles  = {Nettoyage de pages Web, collecte de corpus, évaluation intrinsèque, évaluation extrinsèque, détourage},
  keywords  = {Web page cleaning, corpus collecting, intrinsic evaluation, extrinsic evaluation, web scraping},
}

@inproceedings{lark-morin-penasaldarriaga:2015:TALN,
  author    = {Lark, Joseph and Morin, Emmanuel and Peña Saldarriaga, Sebastian},
  title     = {CANÉPHORE : un corpus français pour la fouille d'opinion ciblée},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {418--424},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-016},
  language  = {french},
  note      = {CANÉPHORE : a French corpus for aspect-based sentiment analysis evaluation},
  resume    = {La fouille d'opinion ciblée (aspect-based sentiment analysis) fait l'objet ces dernières années d'un intérêt particulier, visible dans les sujets des récentes campagnes d'évaluation comme SemEval 2014 et 2015 ou bien DEFT 2015. Cependant les corpus annotés et publiquement disponibles permettant l'évaluation de cette tâche sont rares. Dans ce travail nous présentons en premier lieu un corpus français librement accessible de 10 000 tweets manuellement annotés. Nous accompagnons ce corpus de résultats de référence pour l'extraction de marqueurs d'opinion non supervisée. Nous présentons ensuite une méthode améliorant les résultats de cette extraction, en suivant une approche semi-supervisée.},
  abstract  = {Aspect-based sentiment analysis knows a renewed interest these last years, according to recent opinion mining evaluation series (SemEval 2014 and 2015, DEFT 2015). However, publicly available evaluation resources are scarse. This work firstly introduces a publicly available annotated French Twitter corpus for sentiment analysis evaluation on aspect, subject and opinion word levels (10 000 documents). We present baseline results on this corpus for the task of opinion word extraction and then show that these results can be improved with simple semi-supervised methods.},
  motscles  = {Fouille d'opinion, web social, corpus annoté, extraction d'information semi-supervisée},
  keywords  = {Opinion mining, social web, annotated corpus, semi-supervised information extraction},
}

@inproceedings{hmida-morin-daille:2015:TALN,
  author    = {Hmida, Firas and Morin, Emmanuel and Daille, Béatrice},
  title     = {Extraction de Contextes Riches en Connaissances en corpus spécialisés},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {425--431},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-017},
  language  = {french},
  note      = {Knowledge-Rich Contexts Extraction in Specialized Corpora},
  resume    = {Les banques terminologiques et les dictionnaires sont des ressources précieuses qui facilitent l'accès aux connaissances des domaines spécialisés. Ces ressources sont souvent assez pauvres et ne proposent pas toujours pour un terme à illustrer des exemples permettant d'appréhender le sens et l'usage de ce terme. Dans ce contexte, nous proposons de mettre en œuvre la notion de Contextes Riches en Connaissances (CRC) pour extraire directement de corpus spécialisés des exemples de contextes illustrant son usage. Nous définissons un cadre unifié pour exploiter tout à la fois des patrons de connaissances et des collocations avec une qualité acceptable pour une révision humaine.},
  abstract  = {The term banks and dictionaries are valuable resources that improve access to knowledge in specialized domains. These resources are often relatively poor and do not always provide, for a given term, examples of its typicall use. In this context, we implement Knowledge-Rich Contexts (KRCs) to extract examples of contexts providing illustration of terms in specialized domain. We propose a unified framework to apply at the same time knowledge pattern and collocations with acceptable quality for human review.},
  motscles  = {corpus spécialisé, CRC, patrons de connaisssances, collocations},
  keywords  = {specialized corpus, KRC, knowledge patterns, collocations},
}

@inproceedings{delente-renault:2015:TALN,
  author    = {Delente, Eliane and Renault, Richard},
  title     = {Traitement automatique des formes métriques des textes versifiés},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {432--438},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-018},
  language  = {french},
  note      = {Automatic Processing of Metrical Forms in Verse Texts},
  resume    = {L'objectif de cet article est de présenter tout d'abord dans ses grandes lignes le projet Anamètre qui a pour objet le traitement automatique des formes métriques de la poésie et du théâtre français du début du XVIIe au début du XXe siècle. Nous présenterons ensuite un programme de calcul automatique des mètres appliqué à notre corpus dans le cadre d'une approche déterministe en nous appuyant sur la méthode métricométrique de B. de Cornulier ainsi que la procédure d'appariement des rimes et la détermination des schémas de strophes dans les suites périodiques et les formes fixes.},
  abstract  = {The purpose of this paper is to present the project Anamètre. The project proposes automatic processing of metrical forms in French poetry and drama from the early seventeenth to the early twentieth century. Then we present the calculation program of meters on our corpus using a deterministic approach relying on the "métricométric" method of B. de Cornulier. Finally, we present the procedure of maching rimes and determination of rhyme schemes in periodic sequences and specific forms.},
  motscles  = {métrique française, corpus poétique, calcul du mètre, appariement des rimes, détermination des schémas de strophes},
  keywords  = {French metrics, poetic corpus, calculation of meter, matching rhymes, determining of rhyme schemes},
}

@inproceedings{desoyer-landragin-tellier:2015:TALN,
  author    = {Désoyer, Adèle and Landragin, Frédéric and Tellier, Isabelle},
  title     = {Apprentissage automatique d'un modèle de résolution de la coréférence à partir de données orales transcrites du français : le système CROC},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {439--445},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-019},
  language  = {french},
  note      = {Machine Learning for Coreference Resolution of Transcribed Oral French Data : the CROC System},
  resume    = {Cet article présente CROC 1 (Coreference Resolution for Oral Corpus), un premier système de résolution des coréférences en français reposant sur des techniques d'apprentissage automatique. Une des spécificités du système réside dans son apprentissage sur des données exclusivement orales, à savoir ANCOR (anaphore et coréférence dans les corpus oraux), le premier corpus de français oral transcrit annoté en relations anaphoriques. En l'état actuel, le système CROC nécessite un repérage préalable des mentions. Nous détaillons les choix des traits – issus du corpus ou calculés – utilisés par l'apprentissage, et nous présentons un ensemble d'expérimentations avec ces traits. Les scores obtenus sont très proches de ceux de l'état de l'art des systèmes conçus pour l'écrit. Nous concluons alors en donnant des perspectives sur la réalisation d'un système end-to-end valable à la fois pour l'oral transcrit et l'écrit.},
  abstract  = {We present CROC (Coreference Resolution for Oral Corpus), the first machine learning system for coreference resolution in French. One specific aspect of the system is that it has been trained on data that are exclusively oral, namely ANCOR (ANaphora and Coreference in ORal corpus), the first corpus in oral French with anaphorical relations annotations. In its current state, the CROC system requires pre-annotated mentions. We detail the features that we chose to be used by the learning algorithms, and we present a set of experiments with these features. The scores we obtain are close to those of state-of-the-art systems for written English. Then we give future works on the design of an end-to-end system for oral and written French.},
  motscles  = {corpus de dialogues, détection de coréférences, apprentissage, paires de mentions},
  keywords  = {Dialogue corpus, Coreference resolution, Machine learning, Mention-pair model},
}

@inproceedings{lejeune-daille:2015:TALN,
  author    = {Lejeune, Gaël and Daille, Béatrice},
  title     = {Vers un diagnostic d'ambiguïté des termes candidats d'un texte},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {446--452},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-020},
  language  = {french},
  note      = {Towards diagnosing ambiguity of candidate terms},
  resume    = {Les recherches autour de la désambiguïsation sémantique traitent de la question du sens à accorder à différentes occurrences d'un mot ou plus largement d'une unité lexicale. Dans cet article, nous nous intéressons à l'ambiguïté d'un terme en domaine de spécialité. Nous posons les premiers jalons de nos recherches sur une question connexe que nous nommons le diagnostic d'ambiguïté. Cette tâche consiste à décider si une occurrence d'un terme est ou n'est pas ambiguë. Nous mettons en œuvre une approche d'apprentissage supervisée qui exploite un corpus d'articles de sciences humaines rédigés en français dans lequel les termes ambigus ont été détectés par des experts. Le diagnostic s'appuie sur deux types de traits : syntaxiques et positionnels. Nous montrons l'intérêt de la structuration du texte pour établir le diagnostic d'ambiguïté.},
  abstract  = {Researches in the field of Word Sense Disambiguation focus on identifying the precise meaning of a lexical unit found in a text. This article tackles another kind of problem : assessing the ambiguity of a lexical unit. In other words, we try to identify if a particular unit is ambiguous or not, we define this task as ambiguity diagnosis. Our evaluation dataset contains scientific articles where ambiguous words have been tagged by experts. In order to give an ambiguity diagnosis for each term, we use two types of features : POS tags and positions in the text. We show that the position of an occurrence in the text is a strong hint for such a task.},
  motscles  = {diagnostic d'ambiguïté, extraction de mot-clés, terminologie},
  keywords  = {ambiguity diagnosis, keyword extraction, terminology},
}

@inproceedings{lafourcade-ramadier:2015:TALN,
  author    = {Lafourcade, Mathieu and Ramadier, Lionel},
  title     = {Augmentation d'index par propagation sur un réseau lexical Application aux comptes rendus de radiologie},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {453--459},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-021},
  language  = {french},
  note      = {Index augmentation through propagation over a lexical network – application to radiological reports},
  resume    = {Les données médicales étant de plus en plus informatisées, le traitement sémantiquement efficace des rapports médicaux est devenu une nécessité. La recherche d'images radiologiques peut être grandement facilitée grâce à l'indexation textuelle des comptes rendus associés. Nous présentons un algorithme d'augmentation d'index de comptes rendus fondé sur la propagation d'activation sur un réseau lexico-sémantique généraliste.},
  abstract  = {Medical data being increasingly computerized, semantically effective treatment of medical reports has become a necessity. The search of radiological images can be greatly facilitated through textual indexing of the associated reports. We present here an index enlargement algorithm based on spreading activations over a general lexical-semantic network.},
  motscles  = {réseau lexico-sémantique, propagation, indexation, recherche d'information, imagerie médicale},
  keywords  = {lexico-semantic network, propagation, indexation, information retrieval, medical imaging},
}

@inproceedings{karoui-EtAl:2015:TALN,
  author    = {Karoui, Jihen and Benamara Zitoune, Farah and Moriceau, Véronique and Aussenac-Gilles, Nathalie and Hadrich Belguith, Lamia},
  title     = {Détection automatique de l'ironie dans les tweets en français},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {460--465},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-022},
  language  = {french},
  note      = {Automatic Irony Detection in French tweets},
  resume    = {Cet article présente une méthode par apprentissage supervisé pour la détection de l'ironie dans les tweets en français. Un classifieur binaire utilise des traits de l'état de l'art dont les performances sont reconnues, ainsi que de nouveaux traits issus de notre étude de corpus. En particulier, nous nous sommes intéressés à la négation et aux oppositions explicites/implicites entre des expressions d'opinion ayant des polarités différentes. Les résultats obtenus sont encourageants.},
  abstract  = {This paper presents a supervised learning method for irony detection in tweets in French. A binary classifier uses both state of the art features whose efficiency has been empirically proved and new groups of features observed in our corpus. We focused on negation and explicit/implicit oppositions of opinions with different polarities. Results are encouraging.},
  motscles  = {Analyse d'opinion, détection de l'ironie, apprentissage supervisé},
  keywords  = {Opinion analysis, irony detection, supervised learning},
}

@inproceedings{mathieucolas-cartier-grezka:2015:TALN,
  author    = {Mathieu-Colas, Michel and Cartier, Emmanuel and Grezka, Aude},
  title     = {Dictionnaires morphologiques du français contemporain : présentation de Morfetik, éléments d'un modèle pour le TAL},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {466--472},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-023},
  language  = {french},
  note      = {French Contemporary Morphological Dictionaries : Morfetik Database, Elements of a Model for Computational Linguistics},
  resume    = {Dans cet article, nous présentons une ressource linguistique, Morfetik, développée au LDI. Après avoir présenté le modèle sous-jacent et spécifié les modalités de sa construction, nous comparons cette ressource avec d'autres ressources du français : le GLAFF, le LEFF, Morphalou et Dicolecte. Nous étudions ensuite la couverture lexicale de ces dictionnaires sur trois corpus, le Wikipedia français, la version française de Wacky et les dix ans du Monde. Nous concluons par un programme de travail permettant de mettre à jour de façon continue la ressource lexicographique du point de vue des formes linguistiques, en connectant la ressource à un corpus continu.},
  abstract  = {In this article, we present a morphological linguistic resource for Contemporary French called Morfetik. We first detail its composition, features and coverage. We compare it to other available morphological dictionaries for French (GLAFF, LEFF, Morphalou and Dicolecte). We then study its coverage on big corpora (French Wikipedia, French version of Wacky and Le Monde 10 years). We conclude with a proposition for updating the dictionary by connecting the resource with a continuously live corpus.},
  motscles  = {dictionnaire, morphologie, français, ressource linguistique, corpus},
  keywords  = {dictionary, morphology, French language, linguistic resource, corpus},
}

@inproceedings{petitjean-samih-lichte:2015:TALN,
  author    = {Petitjean, Simon and Samih, Younes and Lichte, Timm},
  title     = {Une métagrammaire de l'interface morpho-sémantique dans les verbes en arabe},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {473--479},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-024},
  language  = {french},
  note      = {A metagrammar of the morphology-semantics interface in Arabic verbs},
  resume    = {Dans cet article, nous présentons une modélisation de la morphologie dérivationnelle de l'arabe utilisant le cadre métagrammatical oﬀert par XMG. Nous démontrons que l'utilisation de racines et patrons abstraits comme morphèmes atomiques sous-spécifiés oﬀre une manière élégante de traiter l'interaction entre morphologie et sémantique.},
  abstract  = {In this article we propose to model the derivational morphology of Arabic using the metagrammatical framework of XMG. We demonstrate that treating abstract roots and patterns as semantically underspecified atomic morphemes oﬀers an elegant way to account for the interaction between morphology and semantics.},
  motscles  = {Morphologie, arabe, metagrammaire, frame semantics},
  keywords  = {Morphology, Arabic, metagrammar, frame semantics},
}

@inproceedings{blache-EtAl:2015:TALN,
  author    = {Blache, Philippe and Moncheuil, Grégoire and Rauzy, Stéphane and Guénot, Marie-Laure},
  title     = {Création d'un nouveau treebank à partir de quatrièmes de couverture},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {480--486},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-025},
  language  = {french},
  note      = {Creation of a new treebank with backcovers},
  resume    = {Nous présentons ici 4-couv, un nouveau corpus arboré d'environ 3 500 phrases, constitué d'un ensemble de quatrièmes de couverture, étiqueté et analysé automatiquement puis corrigé et validé à la main. Il répond à des besoins spécifiques pour des projets de linguistique expérimentale, et vise à rester compatible avec les autres treebanks existants pour le français. Nous présentons ici le corpus lui-même ainsi que les outils utilisés pour les différentes étapes de son élaboration : choix des textes, étiquetage, parsing, correction manuelle.},
  abstract  = {We introduce 4-couv, a treebank of approximatively 3 500 trees, built from a set of literacy backcovers. It has been automatically tagged and parsed, then manually corrected and validated. It was developed in the perspective of linguistic expriment projects, and aims to be compatible with other standard treebanks for french. We present in the following the corpus itself, then the tools we used or developed for the different stages of its elaboration : texts' selection, tagging, parsing, and manual correction.},
  motscles  = {Corpus arboré, Étiquetage automatique, Analyse syntaxique automatique, Parsing stochastique, Conventions d'annotation, Outils d'annotation, Linguistique expérimentale},
  keywords  = {Treebank, Tagging, Parsing, Stochastic parsing, Annotation scheme, Annotation tools, Experimental linguistics},
}

@inproceedings{damnati-guerraz-charlet:2015:TALN,
  author    = {Damnati, Géraldine and Guerraz, Aleksandra and Charlet, Delphine},
  title     = {Entre écrit et oral ? Analyse comparée de conversations de type tchat et de conversations téléphoniques dans un centre de contact client},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {487--493},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-026},
  language  = {french},
  note      = {Comparing Written and Spoken Languages: a Descriptive Study of Chat and Phone Conversations from an Assistance Contact Center},
  resume    = {Dans cet article nous proposons une première étude descriptive d'un corpus de conversations de type tchat issues d'un centre de contact d'assistance. Les dimensions lexicales, syntaxiques et interactionnelles sont analysées. L'étude parallèle de transcriptions de conversations téléphoniques issues d'un centre d'appel dans le même domaine de l'assistance permet d'établir des comparaisons entre ces deux modes d'interaction. L'analyse révèle des différences marquées en termes de déroulement de la conversation, avec une plus grande efficacité pour les conversations de type tchat malgré un plus grand étalement temporel. L'analyse lexicale et syntaxique révèle également des différences de niveaux de langage avec une plus grande proximité entre le client et le téléconseiller à l'oral que pour les tchats où le décalage entre le style adopté par le téléconseiller et l'expression du client est plus important.},
  abstract  = {In this article we propose a first descriptive study of a chat conversations corpus from an assistance contact center. The analysis includes lexical, syntactic and interactional dimensions. Transcriptions of telephone conversations from an assistance call center are also studied, allowing comparisons between these two interaction modes to be drawn. The study reveals significant differences in terms of conversation flow, with an increased efficiency for chat conversations in spite of longer temporal span. The lexical and syntactic analyses also reveal differences in terms of language level with a tighter proximity between agent and customers on the phone than for chats where the style adopted by agents is different from the style adopted by customers.},
  motscles  = {Centre de contact, conversations tchat, interaction, analyse lexicale et syntaxique},
  keywords  = {Contact center, chat conversations, interaction, lexical and syntactic analysis},
}

@inproceedings{planes:2015:TALN,
  author    = {Planes, Laurie},
  title     = {Construction et maintenance d'une ressource lexicale basées sur l'usage},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {494--500},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-027},
  language  = {french},
  note      = {Lexical resource building and maintenance based on the use},
  resume    = {Notre société développe un moteur de recherche (MR) sémantique basé sur la reformulation de requête. Notre MR s'appuie sur un lexique que nous avons construit en nous inspirant de la Théorie Sens-Texte (TST). Nous présentons ici notre ressource lexicale et indiquons comment nous l'enrichissons et la maintenons en fonction des besoins détectés à l'usage. Nous abordons également la question de l'adaptation de la TST à nos besoins.},
  abstract  = {Our company develops a semantic search engine based on queries rephrasing. Our search engine relies on a lexicon we built on the basis of the Meaning-Text theory. We introduce our lexical resource and explain how we enrich and update it according to the needs we detect. We also mention the customization of the Meaning Text Theory to our needs.},
  motscles  = {Ressources lexicales, Théorie Sens-Texte, Recherche d'Information},
  keywords  = {Lexical Ressources, Meaning-Text Theory, Information Retrieval},
}

@inproceedings{lailler-EtAl:2015:TALN,
  author    = {Lailler, Carole and EstÈve, Yannick and De Mori, Renato and BouallÈgue, Mohamed and Morchid, Mohamed},
  title     = {Utilisation d'annotations sémantiques pour la validation automatique d'hypothèses dans des conversations téléphoniques},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {501--507},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-028},
  language  = {french},
  note      = {Use of Semantic Annotations for Validating Mentions of Semantic Hypotheses in Telephone Conversations},
  resume    = {Les travaux présentés portent sur l'extraction automatique d'unités sémantiques et l'évaluation de leur pertinence pour des conversations téléphoniques. Le corpus utilisé est le corpus français DECODA. L'objectif de la tâche est de permettre l'étiquetage automatique en thème de chaque conversation. Compte tenu du caractère spontané de ce type de conversations et de la taille du corpus, nous proposons de recourir à une stratégie semi-supervisée fondée sur la construction d'une ontologie et d'un apprentissage actif simple : un annotateur humain analyse non seulement les listes d'unités sémantiques candidates menant au thème mais étudie également une petite quantité de conversations. La pertinence de la relation unissant les unités sémantiques conservées, le sous-thème issu de l'ontologie et le thème annoté est évaluée par un DNN, prenant en compte une représentation vectorielle du document. L'intégration des unités sémantiques retenues dans le processus de classification en thème améliore les performances.},
  abstract  = {The presented work focuses on the automatic extraction of semantic units and evaluation of their relevance to telephone conversations. The corpus used is DECODA corpus. The objective of the task is to enable automatic labeling theme of each conversation. Given the spontaneous nature of this type of conversations and the size of the corpus, we propose to use a semi-supervised strategy based on the construction of an ontology and a simple active learning : a human annotator analyses not only the lists of semantic units leading to the theme, but also studying a small amount of conversations. The relevance of the relationship between the conserved semantic units, sub-theme from the ontology and annotated theme is assessed by DNN, taking into account a vector representation of the document. The integration of semantic units included in the theme classification process improves performance.},
  motscles  = {analyse de conversation humain/humain, extraction automatique d'unités sémantiques pertinentes, validation d'une ontologie},
  keywords  = {human/human conversation analysis, automatic extraction of relevant semantic units, ontology validation},
}

@inproceedings{rabary-lavergne-neveol:2015:TALN,
  author    = {Rabary, Christelle and Lavergne, Thomas and Névéol, Aurélie},
  title     = {Etiquetage morpho-syntaxique en domaine de spécialité: le domaine médical},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {508--514},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-029},
  language  = {french},
  note      = {Part of Speech tagging for specialized domains : a case study with clinical documents in French},
  resume    = {L'étiquetage morpho-syntaxique est une tâche fondamentale du Traitement Automatique de la Langue, sur laquelle reposent souvent des traitements plus complexes tels que l'extraction d'information ou la traduction automatique. L'étiquetage en domaine de spécialité est limité par la disponibilité d'outils et de corpus annotés spécifiques au domaine. Dans cet article, nous présentons le développement d'un corpus clinique du français annoté morpho-syntaxiquement à l'aide d'un jeu d'étiquettes issus des guides d'annotation French Treebank et Multitag. L'analyse de ce corpus nous permet de caractériser le domaine clinique et de dégager les points clés pour l'adaptation d'outils d'analyse morpho-syntaxique à ce domaine. Nous montrons également les limites d'un outil entraîné sur un corpus journalistique appliqué au domaine clinique. En perspective de ce travail, nous envisageons une application du corpus clinique annoté pour améliorer l'étiquetage morpho-syntaxique des documents cliniques en français.},
  abstract  = {Part-of-Speech (PoS) tagging is a core task in Natural Language Processing, often used as a stepping stone to perform more complex tasks such as information extraction or machine translation. PoS tagging of specialized documents is often challenging due to the limited availability of tools and annotated corpora dedicated to specialized domains. Herein, we present the development of a PoS annotated corpus of clinical documents in French, using annotation guidelines from the FrenchTree Bank and Multitag datasets. Through analysis of the annotated corpus, we characterize the clinical domain, including specific targets for domain adaptation. We also show the limitations of a PoS tagger trained on news documents when applied to clinical text. We expect that the domain-specific resource presented in this paper will contribute to improve PoS tagging for clinical documents in French.},
  motscles  = {adaptation, analyse morpho-syntaxique, langue de spécialité, dossier électronique patient},
  keywords  = {domain adaptation, part-of-speech tagging, specialized domain, EHR},
}

@inproceedings{bois-EtAl:2015:TALN,
  author    = {Bois, Remi and Gravier, Guillaume and Morin, Emmanuel and Sébillot, Pascale},
  title     = {Vers une typologie de liens entre contenus journalistiques},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {515--521},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-030},
  language  = {french},
  note      = {Towards a typology for linking newswire contents},
  resume    = {Nous présentons une typologie de liens pour un corpus multimédia ancré dans le domaine journalistique. Bien que plusieurs typologies aient été créées et utilisées par la communauté, aucune ne permet de répondre aux enjeux de taille et de variété soulevés par l'utilisation d'un corpus large comprenant des textes, des vidéos, ou des émissions radiophoniques. Nous proposons donc une nouvelle typologie, première étape visant à la création et la catégorisation automatique de liens entre des fragments de documents afin de proposer de nouveaux modes de navigation au sein d'un grand corpus. Plusieurs exemples d'instanciation de la typologie sont présentés afin d'illustrer son intérêt.},
  abstract  = {In this paper, we introduce a typology of possible links between contents of a multimedia news corpus. While several typologies have been proposed and used by the community, we argue that they are not adapted to rich and large corpora which can contain texts, videos, or radio stations recordings. We propose a new typology, as a first step towards automatically creating and categorizing links between documents' fragments in order to create new ways to navigate, explore, and extract knowledge from large collections. Several examples of links in a large corpus are given.},
  motscles  = {typologie, liens inter-documents, hypertexte, multimédia, presse},
  keywords  = {typology, linking documents, hypertext, multimedia, newswire},
}

@inproceedings{bechet-lacroix:2015:TALN,
  author    = {Béchet, Denis and Lacroix, Ophélie},
  title     = {CDGFr, un corpus en dépendances non-projectives pour le français},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {522--528},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-031},
  language  = {french},
  note      = {CDGFr, a Non-projective Dependency Corpus for French},
  resume    = {Dans le cadre de l'analyse en dépendances du français, le phénomène de la non-projectivité est peu pris en compte, en majeure partie car les donneés sur lesquelles sont entraînés les analyseurs représentent peu ou pas ces cas particuliers. Nous présentons, dans cet article, un nouveau corpus en dépendances pour le français, librement disponible, contenant un nombre substantiel de dépendances non-projectives. Ce corpus permettra d'étudier et de mieux prendre en compte les cas de non-projectivité dans l'analyse du français.},
  abstract  = {The non-projective cases, as a part of the dependency parsing of French, are often disregarded, mainly because the tree- banks on which parsers are trained contain little or no non-projective dependencies. In this paper, we present a new freely available dependency treebank for French that includes a substantial number of non-projective dependencies. This corpus can be used to study and process non-projectivity more effectively within the context of French dependency parsing.},
  motscles  = {Corpus français, annotation en dépendances, dépendances non-projectives},
  keywords  = {Treebank for French, dependency annotation, non-projective dependencies},
}

@inproceedings{zennaki-semmar-besacier:2015:TALN,
  author    = {Zennaki, Othman and Semmar, Nasredine and Besacier, Laurent},
  title     = {Utilisation des réseaux de neurones récurrents pour la projection interlingue d'étiquettes morpho-syntaxiques à partir d'un corpus parallèle},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {529--536},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-032},
  language  = {french},
  note      = {Use of Recurrent Neural Network for Part-Of-Speech tags projection from a parallel corpus},
  resume    = {La construction d'outils d'analyse linguistique pour les langues faiblement dotées est limitée, entre autres, par le manque de corpus annotés. Dans cet article, nous proposons une méthode pour construire automatiquement des outils d'analyse via une projection interlingue d'annotations linguistiques en utilisant des corpus parallèles. Notre approche n'utilise pas d'autres sources d'information, ce qui la rend applicable à un large éventail de langues peu dotées. Nous proposons d'utiliser les réseaux de neurones récurrents pour projeter les annotations d'une langue à une autre (sans utiliser d'information d'alignement des mots). Dans un premier temps, nous explorons la tâche d'annotation morpho-syntaxique. Notre méthode combinée avec une méthode de projection d'annotation basique (utilisant l'alignement mot à mot), donne des résultats comparables à ceux de l'état de l'art sur une tâche similaire.},
  abstract  = {In this paper, we propose a method to automatically induce linguistic analysis tools for languages that have no labeled training data. This method is based on cross-language projection of linguistic annotations from parallel corpora. Our method does not assume any knowledge about foreign languages, making it applicable to a wide range of resource-poor languages. No word alignment information is needed in our approach. We use Recurrent Neural Networks (RNNs) as cross-lingual analysis tool. To illustrate the potential of our approach, we firstly investigate Part-Of-Speech (POS) tagging. Combined with a simple projection method (using word alignment information), it achieves performance comparable to the one of recently published approaches for cross-lingual projection.},
  motscles  = {Multilinguisme, transfert crosslingue, étiquetage morpho-syntaxique, réseaux de neurones récurrents},
  keywords  = {Multilingualism, cross-Lingual transfer, part-of-speech tagging, recurrent neural network},
}

@inproceedings{bouchekif-EtAl:2015:TALN,
  author    = {Bouchekif, Abdessalam and Damnati, Géraldine and Camelin, Nathalie and Estève, Yannick and Charlet, Delphine},
  title     = {Segmentation et Titrage Automatique de Journaux Télévisés},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {537--543},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-033},
  language  = {french},
  note      = {Automatic Topic Segmentation and Title Assignment in TV Broadcast News},
  resume    = {Dans cet article, nous nous intéressons au titrage automatique des segments issus de la segmentation thématique de journaux télévisés. Nous proposons d'associer un segment à un article de presse écrite collecté le jour même de la diffusion du journal. La tâche consiste à apparier un segment à un article de presse à l'aide d'une mesure de similarité. Cette approche soulève plusieurs problèmes, comme la sélection des articles candidats, une bonne représentation du segment et des articles, le choix d'une mesure de similarité robuste aux imprécisions de la segmentation. Des expériences sont menées sur un corpus varié de journaux télévisés français collectés pendant une semaine, conjointement avec des articles aspirés à partir de la page d'accueil de Google Actualités. Nous introduisons une métrique d'évaluation reﬂétant la qualité de la segmentation, du titrage ainsi que la qualité conjointe de la segmentation et du titrage. L'approche donne de bonnes performances et se révèle robuste à la segmentation thématique.},
  abstract  = {This paper addresses the task of assigning a title to topic segments automatically extracted from TV Broadcast News video recordings. We propose to associate to a topic segment the title of a newspaper article collected on the web at the same date. The task implies pairing newspaper articles and topic segments by maximising a given similarity measure. This approach raises several issues, such as the selection of candidate newspaper articles, the vectorial representation of both the segment and the articles, the choice of a suitable similarity measure, and the robustness to automatic segmentation errors. Experiments were made on various French TV Broadcast News shows recorded during one week, in conjunction with text articles collected through the Google News homepage at the same period. We introduce a full evaluation framework allowing to measure the quality of topic segment retrieval, topic title assignment and also joint retrieval and titling. The approach yields good titling performance and reveals to be robust to automatic segmentation.},
  motscles  = {Segmentation thématique, Titrage automatique, Pondération Okapi, Mesures de similarité},
  keywords  = {Topic segmentation, Title assignation, Okapi weighting, Similarity measures},
}

@inproceedings{brun-popa-roux:2015:TALN,
  author    = {Brun, Caroline and Popa, Diana Nicoleta and Roux, Claude},
  title     = {Un système hybride pour l'analyse de sentiments associés aux aspects},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {544--550},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-034},
  language  = {french},
  note      = {An Hybrid System for Aspect-Based Sentiment Analysis},
  resume    = {Cet article présente en détails notre participation à la tâche 4 de SemEval2014 (Analyse de Sentiments associés aux Aspects). Nous présentons la tâche et décrivons précisément notre système qui consiste en une combinaison de composants linguistiques et de modules de classification. Nous exposons ensuite les résultats de son évaluation, ainsi que les résultats des meilleurs systèmes. Nous concluons par la présentation de quelques nouvelles expériences réalisées en vue de l'amélioration de ce système.},
  abstract  = {This paper details our participation to the SemEval2014 task 4 (Aspect Based Sentiment Analysis). We present the shared task, and then describe precisely our system, which is a combination of natural processing components and classificaton modules. We also present its evaluation results and the best system results. We finally expose some new experiments aiming at improving the system.},
  motscles  = {Analyse de sentiments associés aux aspects, SemEval2014, système hybride},
  keywords  = {Aspect Based Sentiment Analysis, SemEval2014, hybrid system},
}

@inproceedings{atallah:2015:TALN,
  author    = {Atallah, Caroline},
  title     = {La ressource EXPLICADIS, un corpus annoté spécifiquement pour l'étude des relations de discours causales},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {551--557},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-035},
  language  = {french},
  note      = {A corpus specifically annotated for causal discourse relations studies : the EXPLICADIS resource},
  resume    = {Dans le but de proposer une caractérisation des relations de discours liées à la causalité, nous avons été amenée à constituer et annoter notre propre corpus d'étude : la ressource EXPLICADIS (EXPlication et Argumentation en DIScours). Cette ressource a été construite dans la continuité d'une ressource déjà disponible, le corpus ANNODIS. Proposant une annotation plus précise des relations causales sur un ensemble de textes diversifiés en genres textuels, EXPLICADIS est le premier corpus de ce type constitué spécifiquement pour l'étude des relations de discours causales.},
  abstract  = {In order to offer a characterization of causal discourse relations, we created and annotated our own corpus : EXPLICADIS (EXPlanation and Argumentation in DIScourse). This corpus was built in the continuity of a readily available corpus, the ANNODIS corpus. Providing a more precise annotation of causal relations in a set of texts that are representative of multiple textual genres, EXPLICADIS is the first corpus of its kind built specifically for causal discourse relations studies.},
  motscles  = {annotation de corpus, discours, relations causales},
  keywords  = {corpus annotation, discourse, causal relations},
}

@inproceedings{lareau-berniercolborne-drouin:2015:TALN,
  author    = {Lareau, François and Bernier-Colborne, Gabriel and Drouin, Patrick},
  title     = {La séparation des composantes lexicale et ﬂexionnelle des vecteurs de mots},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {558--564},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-036},
  language  = {french},
  note      = {Separating the lexical and grammatical components of semantic vectors},
  resume    = {En sémantique distributionnelle, le sens des mots est modélisé par des vecteurs qui représentent leur distribution en corpus. Les modèles étant souvent calculés sur des corpus sans pré-traitement linguistique poussé, ils ne permettent pas de rendre bien compte de la compositionnalité morphologique des mots-formes. Nous proposons une méthode pour décomposer les vecteurs de mots en vecteurs lexicaux et ﬂexionnels.},
  abstract  = {In distributional semantics, the meaning of words is modelled by vectors that represent their distribution in a corpus. Vectorial models being often built from corpora with little linguistic pre-treatment, they do not represent very well the morphological compositionality of words. We propose here a method to decompose semantic vectors into lexical and inﬂectional vectors.},
  motscles  = {Sémantique distributionnelle, compositionnalité, ﬂexion},
  keywords  = {Distributional semantics, compositionality, inﬂection},
}

@inproceedings{diwersy-EtAl:2015:TALN,
  author    = {Diwersy, Sascha and Falaise, Achille and Lay, Marie-Hélène and Souvay, Gilles},
  title     = {Traitements pour l'analyse du français préclassique},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {565--571},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-037},
  language  = {french},
  note      = {Treatments for Preclassic French parsing},
  resume    = {La période « préclassique » du français s'étend sur tout le XVIe siècle et la première moitié du XVIIe siècle. Cet état de langue écrite, qui accompagne les débuts de l'imprimerie, est relativement proche du français moderne, mais se caractérise par une grande variabilité graphique. Il s'agit de l'un des moins bien dotés en termes de ressources. Nous présentons ici la construction d'un lexique, d'un corpus d'apprentissage et d'un modèle de langage pour la période préclassique, à partir de ressources du français moderne.},
  abstract  = {The "Preclassical" French language period extends throughout the sixteenth century and the first half of the seventeenth century. This state of the written French language, which accompanies the beginnings of printing, is relatively close to the modern French, but is characterized by a large graphic variability. It is one of the most underresourced state of the French language. Here we present the construction of a lexicon, a training corpus and a language model for the Preclassic period, built from modern French resources.},
  motscles  = {construction de lexique morphologique, annotation et étiquetage de corpus, linguistique diachronique},
  keywords  = {morphological lexicon construction, corpus annotation and tagging, diachronic linguistics},
}

@inproceedings{holat-tomeh-charnois:2015:TALN,
  author    = {Holat, Pierre and Tomeh, Nadi and Charnois, Thierry},
  title     = {Classification de texte enrichie à l'aide de motifs séquentiels},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {572--578},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-038},
  language  = {french},
  note      = {Sequential pattern mining for text classification},
  resume    = {En classification de textes, la plupart des méthodes fondées sur des classifieurs statistiques utilisent des mots, ou des combinaisons de mots contigus, comme descripteurs. Si l'on veut prendre en compte plus d'informations le nombre de descripteurs non contigus augmente exponentiellement. Pour pallier à cette croissance, la fouille de motifs séquentiels permet d'extraire, de façon efficace, un nombre réduit de descripteurs qui sont à la fois fréquents et pertinents grâce à l'utilisation de contraintes. Dans ce papier, nous comparons l'utilisation de motifs fréquents sous contraintes et l'utilisation de motifs δ-libres, comme descripteurs. Nous montrons les avantages et inconvénients de chaque type de motif.},
  abstract  = {Most methods in text classification rely on contiguous sequences of words as features. Indeed, if we want to take non-contiguous (gappy) patterns into account, the number of features increases exponentially with the size of the text. Furthermore, most of these patterns will be mere noise. To overcome both issues, sequential pattern mining can be used to efficiently extract a smaller number of relevant, non-contiguous, features. In this paper, we compare the use of constrained frequent pattern mining and δ-free patterns as features for text classification. We show experimentally the advantages and disadvantages of each type of patterns.},
  motscles  = {Fouille de séquences, motifs libres, classification de texte, sélection de descripteurs},
  keywords  = {Sequence mining, free patterns, text classification, feature selection},
}

@inproceedings{lambrey-lareau:2015:TALN,
  author    = {Lambrey, Florie and Lareau, François},
  title     = {Le traitement des collocations en génération de texte multilingue},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {579--585},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-039},
  language  = {french},
  note      = {The treatment of collocations in multilingual text generation},
  resume    = {Pour concevoir des générateurs automatiques de texte génériques qui soient facilement réutilisables d'une langue et d'une application à l'autre, il faut modéliser les principaux phénomènes linguistiques qu'on retrouve dans les langues en général. Un des phénomènes fondamentaux qui demeurent problématiques pour le TAL est celui des collocations, comme grippe carabinée, peur bleue ou désir ardent, où un sens (ici, l'intensité) ne s'exprime pas de la même façon selon l'unité lexicale qu'il modifie. Dans la lexicographie explicative et combinatoire, on modélise les collocations au moyen de fonctions lexicales qui correspondent à des patrons récurrents de collocations. Par exemple, les expressions mentionnées ici se décrivent au moyen de la fonction Magn : Magn(PEUR) = BLEUE, Magn(GRIPPE) = CARABINÉE, etc. Il existe des centaines de fonctions lexicales. Dans cet article, nous nous intéressons à l'implémentation d'un sous-ensemble de fonctions qui décrivent les verbes supports et certains types de modificateurs.},
  abstract  = {In order to develop generic natural language generators that could be reused across languages and applications, it is necessary to model the core linguistic phenomena that one finds in language. One central phenomenon that remains problematic in NLP is collocations, such as heavy rain, strong preference or intense ﬂavour, where the same idea (here, intensity), is expressed differently depending on the lexical unit it modifies. In explicative combinatory lexicography, collocations are modelled via lexical functions, which correspond to recurrent patterns of collocations. For instance, the three expressions above are all described with the function Magn : Magn(RAIN) = HEAVY, Magn(PREFERENCE) = STRONG , etc. There are hundreds of lexical functions. In this paper, we focus on the implementation of a subset of them that are used to model support verbs and some modifiers.},
  motscles  = {génération automatique de texte multilingue ; collocations ; fonctions lexicales ; théorie sens-texte},
  keywords  = {multilingual natural language generation ; collocations ; lexical functions ; meaning-text theory},
}

@inproceedings{morlanehondere-EtAl:2015:TALN,
  author    = {Morlane-Hondère, François and Grouin, Cyril and Moriceau, Véronique and Zweigenbaum, Pierre},
  title     = {Médicaments qui soignent, médicaments qui rendent malades : étude des relations causales pour identifier les effets secondaires},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {586--592},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-040},
  language  = {french},
  note      = {Drugs that cure, drugs that make you sick : study of causal links to identify drug side effects},
  resume    = {Dans cet article, nous nous intéressons à la manière dont sont exprimés les liens qui existent entre un traitement médical et un effet secondaire. Parce que les patients se tournent en priorité vers internet, nous fondons cette étude sur un corpus annoté de messages issus de forums de santé en français. L'objectif de ce travail consiste à mettre en évidence des éléments linguistiques (connecteurs logiques et expressions temporelles) qui pourraient être utiles pour des systèmes automatiques de repérage des effets secondaires. Nous observons que les modalités d'écriture sur les forums ne permettent pas de se fonder sur les expressions temporelles. En revanche, les connecteurs logiques semblent utiles pour identifier les effets secondaires.},
  abstract  = {In this paper, we study the textual manifestations of the relation between drugs and side effects in online health forums. Our goal is to find relevant linguistic cues in order to improve the automatic identification of side effects by leveraging the ambiguity between actual side effects and indications (the reason for drug use). We find that the use of discourse markers can be relevant for the identification of indications – a third of indication mentions follow markers like ‘pour' (‘for') or ‘dans le but de' (‘with the aim of') – while temporal informations are not as discriminating.},
  motscles  = {Pharmacovigilance, forums de santé, relations causales},
  keywords  = {Pharmacovigilance, Health Forums, Causal Links},
}

@inproceedings{berniercolborne:2015:TALN,
  author    = {Bernier-Colborne, Gabriel},
  title     = {Exploration de modèles distributionnels au moyen de graphes 1-PPV},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {593--599},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-041},
  language  = {french},
  note      = {Exploring distributional semantic models using a 1-NN graph},
  resume    = {Dans cet article, nous montrons qu'un graphe à 1 plus proche voisin (graphe 1-PPV) offre différents moyens d'explorer les voisinages sémantiques captés par un modèle distributionnel. Nous vérifions si les composantes connexes de ce graphe, qui représentent des ensembles de mots apparaissant dans des contextes similaires, permettent d'identifier des ensembles d'unités lexicales qui évoquent un même cadre sémantique. Nous illustrons également différentes façons d'exploiter le graphe 1-PPV afin d'explorer un modèle ou de comparer différents modèles.},
  abstract  = {We show how a 1-NN graph can be used to explore the semantic neighbourhoods modeled by distributional models of semantics. We check whether the connected components of the graph, which represent sets of words that occur in similar contexts, can be used to identify sets of lexical units that evoke the same semantic frame. We then illustrate different ways in which the 1-NN graph can be used to explore a model or compare different models.},
  motscles  = {Sémantique distributionnelle, sémantique lexicale, graphe, terminologie, sémantique des cadres},
  keywords  = {Distributional semantics, lexical semantics, graph, terminology, frame semantics},
}

@inproceedings{janod-EtAl:2015:TALN,
  author    = {Janod, Killian and Morchid, Mohamed and Dufour, Richard and Linares, Georges},
  title     = {Apport de l'information temporelle des contextes pour la représentation vectorielle continue des mots},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {600--606},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-042},
  language  = {french},
  note      = {Contribution of temporal context information to a continuous vector representation of words},
  resume    = {Les représentations vectorielles continues des mots sont en plein essor et ont déjà été appliquées avec succès à de nombreuses tâches en traitement automatique de la langue (TAL). Dans cet article, nous proposons d'intégrer l'information temporelle issue du contexte des mots au sein des architectures fondées sur les sacs-de-mots continus (continuous bag-of-words ou CBOW) ou sur les Skip-Grams. Ces approches sont manipulées au travers d'un réseau de neurones, l'architecture CBOW cherchant alors à prédire un mot sachant son contexte, alors que l'architecture Skip-Gram prédit un contexte sachant un mot. Cependant, ces modèles, au travers du réseau de neurones, s'appuient sur des représentations en sac-de-mots et ne tiennent pas compte, explicitement, de l'ordre des mots. En conséquence, chaque mot a potentiellement la même inﬂuence dans le réseau de neurones. Nous proposons alors une méthode originale qui intègre l'information temporelle des contextes des mots en utilisant leur position relative. Cette méthode s'inspire des modèles contextuels continus. L'information temporelle est traitée comme coefficient de pondération, en entrée du réseau de neurones par le CBOW et dans la couche de sortie par le Skip-Gram. Les premières expériences ont été réalisées en utilisant un corpus de test mesurant la qualité de la relation sémantique-syntactique des mots. Les résultats préliminaires obtenus montrent l'apport du contexte des mots, avec des gains de 7 et 7,7 points respectivement avec l'architecture Skip-Gram et l'architecture CBOW.},
  abstract  = {Word embedding representations are gaining a lot of attention from researchers and have been successfully applied to various Natural Language Processing (NLP) tasks. In this paper, we propose to integrate temporal context information of words into the continuous bag-of-words (CBOW) and Skip-gram architectures for computing word-vector representations. Those architectures are shallow neural-networks. The CBOW architecture predicts a word given its context while the Skip-gram architecture predicts a context given a word. However, in those neural-networks, context windows are represented as bag-of-words. According to this representation, every word in the context is treated equally : the word order is not taken into account explicitly. As a result, each word will have the same inﬂuence on the network. We then propose an original method that integrates temporal information of word contexts using their relative position. This method is inspired from Continuous Context Models. The temporal information is treated as weights, in input by the CBOW and in the output layer by the Skip-Gram. The quality of the obtained models has been measured using a Semantic-Syntactic Word Relationship test set. Results showed that the incorporation of temporal information allows a substantial quality gain of 5 and 0.9 points respectively in comparison to the classical use of the CBOW and Skip-gram architectures.},
  motscles  = {Réseau de neurones, Représentation vectorielle continue, Information contextuelle, Word2vec , Modèle de langue},
  keywords  = {Neural network, Continuous vectorial representation, Contextual information, Word2vec, language model},
}

@inproceedings{tian-EtAl:2015:TALN,
  author    = {Tian, Tian and Marco, Dinarelli and Isabelle, Tellier and Pedro, Cardoso},
  title     = {Etiquetage morpho-syntaxique de tweets avec des CRF},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {607--613},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-043},
  language  = {french},
  note      = {Part-of-speech Tagging for Tweets with CRFs},
  resume    = {Nous nous intéressons dans cet article à l'apprentissage automatique d'un étiqueteur mopho-syntaxique pour les tweets en anglais. Nous proposons tout d'abord un jeu d'étiquettes réduit avec 17 étiquettes différentes, qui permet d'obtenir de meilleures performances en exactitude par rapport au jeu d'étiquettes traditionnel qui contient 45 étiquettes. Comme nous disposons de peu de tweets étiquetés, nous essayons ensuite de compenser ce handicap en ajoutant dans l'ensemble d'apprentissage des données issues de textes bien formés. Les modèles mixtes obtenus permettent d'améliorer les résultats par rapport aux modèles appris avec un seul corpus, qu'il soit issu de Twitter ou de textes journalistiques.},
  abstract  = {We are insterested in this paper in training a part-of-speach tagger for tweets in English. We first propose a reduced tagset with 17 different tags, which allows better results in accuracy than traditional tagsets which contain 45 tags. Since we have few annoted tweets, we then try and overcome this difficulty by adding data from other more standard texts into the training set. The obtained models reach better results compared to models trained with only one corpus, whether coming of Twitter or of journalistic texts.},
  motscles  = {tweets, CRF, étiquettage morpho-syntaxique},
  keywords  = {tweets, CRFs, part-of-speech tagging},
}

@inproceedings{todirascu-sanchezcardenas:2015:TALN,
  author    = {Todirascu, Amalia and Sanchez Cardenas, Beatriz},
  title     = {Caractériser les discours académiques et de vulgarisation : quelles propriétés ?},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {614--620},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-044},
  language  = {french},
  note      = {Characterizing scientific genre for academia and general public: which properties should be taken into account?},
  resume    = {L'article présente une étude des propriétés linguistiques (lexicales, morpho-syntaxiques, syntaxiques) permettant la classification automatique de documents selon leur genre (articles scientifiques et articles de vulgarisation), dans deux domaines différentes (médecine et informatique). Notre analyse, effectuée sur des corpus comparables en genre et en thèmes disponibles en français, permet de valider certaines propriétés identifiées dans la littérature comme caractéristiques des discours académiques ou de vulgarisation scientifique. Les premières expériences de classification évaluent l'influence de ces propriétés pour l'identification automatique du genre pour le cas spécifique des textes scientifiques ou de vulgarisation.},
  abstract  = {The article focuses on the study of a set of morpho-syntactic properties for audience-based classification. The linguistic analysis of academic discourse and of popular science discourse reveals that both discourse types are characterized by specific linguistic and textual properties. This research used two French comparable corpora in regards to genre and subject matter. The corpora was composed of scientific articles and popular science texts in the domains of medicine and computer science. The experiments performed as part of our study evaluated the influence of discourse-specific morpho-syntactic properties on genre-based classification, for scientific and popular science texts.},
  motscles  = {analyse linguistique, discours scientifique et de vulgarisation, corpus comparables, classification selon le genre},
  keywords  = {linguistic analysis, academic and popular science discourse, comparable corpora, genre-based classification},
}

@inproceedings{mpouli-ganascia:2015:TALN,
  author    = {Mpouli, Suzanne and Ganascia, Jean-Gabriel},
  title     = {Extraction et analyse automatique des comparaisons et des pseudo-comparaisons pour la détection des comparaisons figuratives},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {621--627},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-045},
  language  = {french},
  note      = {Extraction and automatic analysis of comparative and pseudo-comparative structures for simile detection},
  resume    = {Le présent article s'intéresse à la détection et à la désambiguïsation des comparaisons figuratives. Il décrit un algorithme qui utilise un analyseur syntaxique de surface (chunker) et des règles manuelles afin d'extraire et d'analyser les (pseudo-)comparaisons présentes dans un texte. Cet algorithme, évalué sur un corpus de textes littéraires, donne de meilleurs résultats qu'un système reposant sur une analyse syntaxique profonde.},
  abstract  = {This article is focused on automatic simile detection and disambiguation. It describes an algorithm which uses syntactic chunks and handcrafted rules to extract and analyse similes in a given text. This algorithm, which was evaluated on a corpus of literary texts, performs better than a system based on dependency parsing.},
  motscles  = {comparaisons figuratives, comparé, comparant, analyse syntaxique de surface, règles manuelles, analyse syntaxique profonde},
  keywords  = {simile, tenor, vehicle, chunking, handcrafted rules, dependency parsing},
}

@inproceedings{ferguth-EtAl:2015:TALN,
  author    = {Ferguth, Johan and Jouannet, Aurélie and Zamiti, Asma and Wu, Yunhe and Li, Jia and Bondarenko, Antonina and Nouvel, Damien and Valette, Mathieu},
  title     = {Proposition méthodologique pour la détection automatique de Community Manager. Étude multilingue sur un corpus relatif à la Junk Food},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {628--634},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-court-046},
  language  = {french},
  note      = {Methodological Proposal for Automatic Detection of Community Manager. Multilingual Study based on a Junk Food corpus},
  resume    = {Dans cet article, nous présentons une méthodologie pour l'identification de messages suspectés d'être produits par des Community Managers à des fins commerciales déguisées dans des documents du Web 2.0. Le champ d'application est la malbouffe (junkfood) et le corpus est multilingue (anglais, chinois, français). Nous exposons dans un premier temps la stratégie de constitution et d'annotation de nos corpus, en explicitant notamment notre guide d'annotation, puis nous développons la méthode adoptée, basée sur la combinaison d'une analyse textométrique et d'un apprentissage supervisé.},
  abstract  = {This article describes the methodology for identifying a certain kind of speech in internet forums. The detection of the speech of a Community Manager combines recent issues in the domain of Natural Language Processing, including opinion mining and sentiment analysis, with another more abstract problem. Going beyond detecting the polarity of a message, this project targets the underlying intentions and identity of the author of the message on the forum.},
  motscles  = {Community Management, Textométrie, Multilinguisme, Fouille de texte},
  keywords  = {Community Management, Textometry, Multilingualism, Data Mining},
}

@inproceedings{sellami-ganascia-boukhaled:2015:TALN,
  author    = {Sellami, Zied and Ganascia, Jean-Gabriel and Boukhaled, Mohamed Amine},
  title     = {MEDITE : logiciel d'alignement de textes pour l'étude de la génétique textuelle},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {635--636},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-001},
  language  = {french},
  note      = {MEDITE: text alignment software for the study of textual genetics},
  resume    = {MEDITE est un logiciel d'alignement de textes permettant l'identification de transformations entre une version et une autre d'un même texte. Dans ce papier nous présentons les aspects théoriques et techniques de MEDITE.},
  abstract  = {MEDITE is an alignment software able to identifying transformations between two versions of a same text. In this paper we show the theoretical and technical aspects of this tool.},
  motscles  = {Alignement de textes, Génétique textuelle, Détection d'homologies dans les séquences textuelles},
  keywords  = {Text alignment, Textual genetics, Homology detection in text sequences},
}

@inproceedings{boukhaled-sellami-ganascia:2015:TALN,
  author    = {Boukhaled, Mohamed Amine and Sellami, Zied and Ganascia, Jean-Gabriel},
  title     = {Phœbus : un Logiciel d'Extraction de Réutilisations dans des Textes Littéraires},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {637--639},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-002},
  language  = {french},
  note      = {Phoebus: a Reuse Extraction Software for Literary Text},
  resume    = {Phœbus est un logiciel d'extraction de réutilisations dans des textes littéraires. Il a été développé comme un outil d'analyse littéraire assistée par ordinateur. Dans ce contexte, ce logiciel détecte automatiquement et explore des réseaux de réutilisation textuelle dans la littérature classique.},
  abstract  = {Phoebus is a reuse extraction software for literary text. It was developed as a computer-assisted literary analysis tool. In this context, the software automatically detects and explores textual reuse networks in classical literature.},
  motscles  = {Extraction de réutilisations, empreintes digitales textuelles, analyse littéraire assistée par ordinateur},
  keywords  = {Reuse Extraction, textual fingerprintng, computer-assisted literary analysis},
}

@inproceedings{lehuen-lailler-stenzhorn:2015:TALN,
  author    = {Lehuen, Jérôme and Lailler, Carole and Stenzhorn, Julien},
  title     = {YADTK : Une plateforme open-source à base de règles pour développer des systèmes de dialogue oral},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {640--641},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-003},
  language  = {french},
  note      = {YADTK: An open-source, rule-based platform for speech dialogue system development},
  resume    = {YADTK est une plateforme open-source pour développer des systèmes de dialogue oral. De part son caractère déclaratif et unifié, le modèle de représentation des connaissances permet un développement rapide et facilité.},
  abstract  = {YADTK is an open-source, rule-based framework to build spoken dialogue systems. The declarative and unified nature of the model of knowledge representation allows a rapid and easier development process.},
  motscles  = {},
  keywords  = {},
}

@inproceedings{foret:2015:TALN,
  author    = {Foret, Annie},
  title     = {TermLis : un contexte d'information logique pour des ressources terminologiques},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {642--643},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-004},
  language  = {french},
  note      = {TermLis : a logical information context for terminological resources},
  resume    = {Nous présentons TermLis un contexte d'information logique construit à partir de ressources terminologiques disponibles en xml (FranceTerme), pour une utilisation ﬂexible avec un logiciel de contexte logique (CAMELIS). Une vue en contexte logique permet d'explorer des informations de manière ﬂexible, sans rédaction de requête a priori, et d'obtenir aussi des indications sur la qualité des données. Un tel contexte peut être enrichi par d'autres informations (de natures diverses), mais aussi en le reliant à d'autres applications (par des actions associées selon des arguments fournis par le contexte). Nous montrons comment utiliser TermLis et nous illustrons, à travers cette réalisation concrète sur des données de FranceTerme, les avantages d'une telle approche pour des données terminologiques.},
  abstract  = {We present TermLis a logical information context constructed from terminological resources available in XML (FranceTerme), for a ﬂexible use with a logical context system (CAMELIS). A logical view of a context allows to explore information in a ﬂexible way, without writing explicit queries, it may also provide insights on the quality of the data. Such a context can be enriched by other information (of diverse natures), it can also be linked with other applications (according to arguments supplied by the context). We show how to use TermLis and we illustrate, through this concrete realization from FranceTerme data, the advantages of such an approach with terminological data.},
  motscles  = {Applications multilingues, Classification, Extraction d'information, Fouilles de données textuelles, Recherche d'information, Ressources du langage, Données Ouvertes, Qualité des données, Données légales},
  keywords  = {Multilingual applications, Classification, Information extraction, Textual data mining, Information retrieval, Linguistic resources, Open Data, Information Quality, Legal Information},
}

@inproceedings{khouas-EtAl:2015:TALN,
  author    = {Khouas, Leila and Brun, Caroline and Peradotto, Anne and Cossu, Jean-Valère and Boyadjian, Julien and Velcin, Julien},
  title     = {Etude de l'image de marque d'entités dans le cadre d'une plateforme de veille sur le Web social},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {644--645},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-005},
  language  = {french},
  note      = {Study the brand image of entities as part of a social media-monitoring platform},
  resume    = {Ce travail concerne l'intégration à une plateforme de veille sur internet d'outils permettant l'analyse des opinions émises par les internautes à propos d'une entité, ainsi que la manière dont elles évoluent dans le temps. Les entités considérées peuvent être des personnes, des entreprises, des marques, etc. Les outils implémentés sont le produit d'une collaboration impliquant plusieurs partenaires industriels et académiques dans le cadre du projet ANR ImagiWeb.},
  abstract  = {The work presented here is about a Web monitoring software providing powerful tools for the analysis of opinions expressed in social media about an entity, such as a celebrity, a company or a brand. The implemented tools result from the research ANR project ImagiWeb involving several industrial and academic partners.},
  motscles  = {Plateforme de veille sur internet, médias sociaux, analyse d'opinion, fouille de données},
  keywords  = {Web monitoring software, social media, opinion analysis, data mining},
}

@inproceedings{le-sadat:2015:TALN,
  author    = {Le, Ngoc Tan and Sadat, Fatiha},
  title     = {Building a Bilingual Vietnamese-French Named Entity Annotated Corpus through Cross-Linguistic Projection},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {646--647},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-006},
  language  = {french},
  note      = {Building a Bilingual Vietnamese-French Named Entity Annotated Corpus through Cross-Linguistic Projection},
  resume    = {La création de ressources linguistiques de bonne qualité annotées en entités nommées est très coûteuse en temps et en main d'œuvre. La plupart des corpus standards sont disponibles pour l'anglais mais pas pour les langues peu dotées, comme le vietnamien. Pour les langues asiatiques, cette tâche reste très difficile. Le présent article concerne la création automatique de corpus annotés en entités nommées pour le vietnamien-français, une paire de langues peu dotée. L'application d'une méthode basée sur la projection cross-lingue en utilisant des corpus parallèles. Les évaluations ont montré une bonne performance (F-score de 94.90%) lors de la reconnaissance des paires d'entités nommées dans les corpus parallèles et ainsi la construction d'un corpus bilingue annoté en entités nommées.},
  abstract  = {The creation of high-quality named entity annotated resources is time-consuming and an expensive process. Most of the gold standard corpora are available for English but not for less-resourced languages such as Vietnamese. In Asian languages, this task is remained problematic. This paper focuses on an automatic construction of named entity annotated corpora for Vietnamese-French, a less-resourced pair of languages. We incrementally apply different cross-projection methods using parallel corpora, such as perfect string matching and edit distance similarity. Evaluations on Vietnamese –French pair of languages show a good accuracy (F-score of 94.90%) when identifying named entities pairs and building a named entity annotated parallel corpus.},
  motscles  = {Entité nommée, corpus parallèle, projection cross-lingue},
  keywords  = {Named entity, parallel corpus, cross-projection},
}

@inproceedings{guillaume:2015:TALN,
  author    = {Guillaume, Bruno},
  title     = {Recherche de motifs de graphe en ligne},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {648--649},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-007},
  language  = {french},
  note      = {Online Graph Matching},
  resume    = {Nous présentons un outil en ligne de recherche de graphes dans des corpus annotés en syntaxe.},
  abstract  = {We present an online tool for graph pattern matching in syntactically annotated corpora.},
  motscles  = {Syntaxe de dépendances, Corpus, Graphes},
  keywords  = {Dependency Syntax, Corpus, Graph matching},
}

@inproceedings{campillos-EtAl:2015:TALN,
  author    = {Campillos, Leonardo and Bouamor, Dhouha and Bilinski, Éric and Ligozat, Anne-Laure and Zweigenbaum, Pierre and Rosset, Sophie},
  title     = {Un patient virtuel dialogant},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {650--651},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-008},
  language  = {french},
  note      = {An Interactive Virtual Patient},
  resume    = {Le démonstrateur que nous décrivons ici est un prototype de système de dialogue dont l'objectif est de simuler un patient. Nous décrivons son fonctionnement général en insistant sur les aspects concernant la langue et surtout le rapport entre langue médicale de spécialité et langue générale.},
  abstract  = {This paper describes the work-in-progress prototype of a dialog system that simulates a virtual patient consultation. We describe the general architecture and specifically the mapping between technical and lay terms in the medical domain.},
  motscles  = {Patient virtuel, système de dialogue, langage spécialisé, langage grand public},
  keywords  = {Virtual patient, dialog system, specialised language, lay language},
}

@inproceedings{falaise:2015:TALN,
  author    = {Falaise, Achille},
  title     = {Intégration du corpus des actes de TALN à la plateforme ScienQuest},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {652--653},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-009},
  language  = {french},
  note      = {Integration of the TALN proceedings treebank to the ScienQuest platform},
  resume    = {Cette démonstration présente l'intégration du corpus arboré des Actes de TALN à la plateforme ScienQuest. Cette plateforme fut initialement créée pour l'étude du corpus de textes scientifiques Scientext. Cette intégration tient compte des méta­données propres au corpus TALN, et a été effectuée en s'efforçant de rapprocher les jeux d'étiquettes de ces deux corpus, et en convertissant pour le corpus TALN les requêtes prédéfinies conçues pour le corpus Scientext, de manière à permettre d'effectuer facilement des recherches similaires sur les deux corpus.},
  abstract  = {This demonstration shows the integration of the TALN proceedings Treebank to the ScienQuest platform. This platform was initially created for the study of the Scientext scientific texts corpus. This integration takes into account the metadata to the TALN corpus, and was done in an effort to reconcile these two corpora's sets of labels, and to convert for the TALN corpus the predefined queries designed for the Scientext corpus, in order to easily perform similar queries on the two corpora.},
  motscles  = {corpus, corpus arborés, environnement d'étude de corpus},
  keywords  = {corpora, treebanks, corpus study environment},
}

@inproceedings{merlo:2015:TALN,
  author    = {Merlo, Aurélie},
  title     = {Une aide à la communication par pictogrammes avec prédiction sémantique},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {654--656},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-010},
  language  = {french},
  note      = {An Augmentative and Alternative Communication with semantic prediction},
  resume    = {Cette démonstration présente une application mobile (pour tablette et smartphone) pour des personnes souffrant de troubles du langage et/ou de la parole permettant de générer des phrases à partir de la combinaison de pictogrammes puis de verbaliser le texte généré en Text-To-Speech (TTS). La principale critique adressée par les patients utilisant les solutions existantes est le temps de composition trop long d'une phrase. Cette limite ne permet pas ou très difficilement d'utiliser les solutions actuelles en condition dialogique. Pour pallier cela, nous avons développé un moteur de génération de texte avec prédiction sémantique ne proposant à l'utilisateur que les pictogrammes pertinents au regard de la saisie en cours (e.g. après le pictogramme [manger], l'application propose les pictogrammes [pomme] ou encore [viande] correspondant à des concepts comestibles). Nous avons ainsi multiplié de 5 à 10 la vitesse de composition d'une phrase par rapport aux solutions existantes.},
  abstract  = {This demo shows a mobile app (for smartphone and tablet) for people with language and/or speech disorders for generating sentences from the combination of icons and verbalize text generated by Text-To-Speech (TTS). The main criticism expressed by patients using existing solutions is the time too long required to compose a sentence. Hence, existing solutions are hard to use in dialogic conditions. To allieviate this problem, we have developed a text generation engine with semantic prediction. This engine only proposes to user the relevant pictograms in view of the current entry (e.g. after the pictogram [eat], the application offers pictograms [apple] or [meat] corresponding to edible concepts). We have multiplied from 5 to 10 the speed of composing a sentence compared to existing solutions.},
  motscles  = {génération de texte, prédiction sémantique, frame semantics, handicap, aide à la communication},
  keywords  = {natural language generation, semantic prediction, frame semantics, disability, AAC},
}

@inproceedings{lopez-EtAl:2015:TALN,
  author    = {Lopez, Cédric and Ponomareva, Aleksandra and Robin, Cécile and Bittar, André and Larrucea, Xabier and Segond, Frédérique and Metzger, Marie-Hélène},
  title     = {Un système expert fondé sur une analyse sémantique pour l'identification de menaces d'ordre biologique},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {657--658},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-011},
  language  = {french},
  note      = {An Expert System Based on a Semantic Analysis for Identifying Biological Threats},
  resume    = {Le projet européen TIER (Integrated strategy for CBRN – Chemical, Biological, Radiological and Nuclear – Threat Identification and Emergency Response) vise à intégrer une stratégie complète et intégrée pour la réponse d'urgence dans un contexte de dangers biologiques, chimiques, radiologiques, nucléaires, ou liés aux explosifs, basée sur l'identification des menaces et d'évaluation des risques. Dans cet article, nous nous focalisons sur les risques biologiques. Nous présentons notre système expert fondé sur une analyse sémantique, permettant l'extraction de données structurées à partir de données non structurées dans le but de raisonner.},
  abstract  = {The European project TIER (Integrated strategy for CBRN – Chemical, Biological, Radiological and Nuclear - Threat Identification and Emergency Response) aims at developing a comprehensive and integrated strategy for emergency response in case of chemical, biological, radiological and nuclear danger, as well as explosives use, based on threat identification and risk assessment. In this article, we focus on the biological risks. We introduce our business rules management system based on a semantic analysis, that enables the extraction of structured data from unstructured data with the aim to make reasoning.},
  motscles  = {TIER, SGRM, Système de Gestion de Règles Métier, analyse sémantique},
  keywords  = {TIER, BRMS, Business Rules Management System, semantic analysis},
}

@inproceedings{christodoulides-barreca-avanzi:2015:TALN,
  author    = {Christodoulides, George and Barreca, Giulia and Avanzi, Mathieu},
  title     = {DisMo : un annotateur multi-niveaux pour les corpus oraux},
  booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2015},
  address   = {Caen, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {659--661},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2015/taln-2015-demo-012},
  language  = {french},
  note      = {DisMo: a multi-level annotator for spoken language},
  resume    = {Dans cette démonstration, nous présentons l'annotateur multi-niveaux DisMo, un outil conçu pour faire face aux spécificités des corpus oraux. Il fournit une annotation morphosyntaxique, une lemmatisation, une détection des unités poly-lexicales, une détection des phénomènes de disfluence et des marqueurs de discours.},
  abstract  = {In this demonstration we present the multi-level automatic annotator DisMo which is specifically designed for the challenges posed by spoken language corpora. Its output comprises of part-of-speech tagging, lemmatization, multi-word unit detection, detection of disfluency phenomena and discourse markers.},
  motscles  = {annotation morphosyntaxique, corpus oraux, disfluences, unités poly-lexicales},
  keywords  = {part-of-speech tagging, spoken corpora, disfluencies, multi-word expressions},
}