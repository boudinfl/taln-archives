<?xml version="1.0" encoding="UTF-8"?>

<conference>
	<edition>
		<acronyme>TALN'2015</acronyme>
		<titre>22e conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Caen</ville>
		<pays>France</pays>
		<dateDebut>2015-06-22</dateDebut>
		<dateFin>2015-06-25</dateFin>
		<presidents>
			<president>
				<prenom>Jean-Marc</prenom>
				<nom>Lecarpentier</nom>
			</president>
			<president>
				<prenom>Nadine</prenom>
				<nom>Lucas</nom>
			</president>
		</presidents>
		<editeurs>
			<editeur>
				<prenom>Jose G</prenom>
				<nom>Moreno</nom>
			</editeur>
		</editeurs>
		<typeArticles>
			<type id="invite">Conférenciers invités</type>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<siteWeb>http://taln2015.greyc.fr</siteWeb>
	</edition>
	<articles>
		<article id="taln-2015-invite-001" session="keynote">
			<auteurs>
				<auteur>
					<prenom>Roberto</prenom>
					<nom>Navigli</nom>
					<email>navigli@di.uniroma1.it</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Dipartimento di Informatica - Sapienza Università di Roma - Viale Regina Elena 295, 00161 Roma, Italy</affiliation>
			</affiliations>
			<titre></titre>
			<type>invite</type>
			<pages>i-i</pages>
			<resume></resume>
			<mots_cles>multilingualité, dictionnaire, sémantique, désambiguïsation, taxonomie</mots_cles>
			<title>Multilinguality at Your Fingertips : BabelNet, Babelfy and Beyond !</title>
			<abstract>Multilinguality is a key feature of today's Web, and it is this feature that we leverage and exploit in our research work at the Sapienza University of Rome's Linguistic Computing Laboratory, which I am going to overview and showcase in this talk. I will start by presenting BabelNet 3.0, available at http://babelnet.org, a very large multilingual encyclopedic dictionary and semantic network, which covers 271 languages and provides both lexicographic and encyclopedic knowledge for all the open-class parts of speech, thanks to the seamless integration of WordNet, Wikipedia, Wiktionary, OmegaWiki, Wikidata and the Open Multilingual WordNet. Next, I will present Babelfy, available at http://babelfy.org, a unified approach that leverages BabelNet to jointly perform word sense disambiguation and entity linking in arbitrary languages, with performance on both tasks on a par with, or surpassing, those of task-specific state-of-the-art supervised systems. Finally I will describe the Wikipedia Bitaxonomy, available at http://wibitaxonomy.org, a new approach to the construction of a Wikipedia bitaxonomy, that is, the largest and most accurate currently available taxonomy of Wikipedia pages and taxonomy of categories, aligned to each other. I will also give an outline of future work on multilingual resources and processing, including state-of-the-art semantic similarity with sense embeddings.</abstract>
			<keywords>multilinguality, dictionary, semantic, disambiguation, taxonomy</keywords>
		</article>
		<article id="taln-2015-invite-002" session="keynote">
			<auteurs>
				<auteur>
					<prenom>Marie-Claude</prenom>
					<nom>L'Homme</nom>
					<email>mc.lhomme@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Observatoire de linguistique Sens-Texte (OLST) - Université de Montréal, C.P. 6128, succ. Centre-ville - Montréal (Québec), H1T 3L9, Canada</affiliation>
			</affiliations>
			<titre>Pourquoi construire des ressources terminologiques et pourquoi le faire différemment ?</titre>
			<type>invite</type>
			<pages>ii-ii</pages>
			<resume>Dans cette présentation, je défendrai l'idée selon laquelle des ressources terminologiques décrivant les propriétés lexico-sémantiques des termes constituent un complément nécessaire, voire indispensable, à d'autres types de ressources, À partir d'exemples anglais et français empruntés au domaine de l'environnement, je montrerai, d'une part, que les ressources lexicales générales (y compris celles qui ont une large couverture) n'offrent pas un portait complet du sens des termes ou de la structure lexicale observée du point de vue d'un domaine de spécialité. Je montrerai, d'autre part, que les ressources terminologiques (thésaurus, ontologies, banques de terminologie) souvent d'obédience conceptuelle, se concentrent sur le lien entre les termes et les connaissances dénotées par eux et s'attardent peu sur leur fonctionnement linguistique. Je présenterai un type de ressource décrivant les propriétés lexico-sémantiques des termes d'un domaine (structure actantielle, liens lexicaux, annotations contextuelles, etc.) et des éléments méthodologiques présidant à son élaboration.</resume>
			<mots_cles>ressources terminologiques, ressources lexicales, liens lexicaux, corpus spécialisé, structure actancielle, annotations contextuelles</mots_cles>
			<title>Why compile terminological resources and why do it differently?</title>
			<abstract>In this talk, I will argue that terminological resources that account for the lexico-semantic properties of terms are a necessary – perhaps even indispensable – complement to other kinds of resources. Using examples taken from the field of the environment, I will first show that general lexical resources (including those that have a large coverage) do not give a complete picture of the meaning of terms or the lexical structure observed from the point of view of a special subject field. I will then proceed to show that terminological resources (thesauri, ontologies, term banks), that often take a conceptual approach, focus on the relationship between terms and the knowledge they convey and give little information about the linguistic behavior of terms. I will present a type of resource that describes the lexico-semantic properties of terms (argument structure, lexical relationships, contextual annotations, etc.) and some methodological considerations about its compilation.</abstract>
			<keywords>terminological resources, lexical resources, lexical relationships, specialized corpora, argument structure, contextual annotations</keywords>
		</article>
		<article id="taln-2015-long-001" session="Extraction d'information">
			<auteurs>
				<auteur>
					<prenom>Elena</prenom>
					<nom>Knyazeva</nom>
					<email>knyazeva@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Wisniewski</nom>
					<email>wisniews@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Sud, 91 403 Orsay CEDEX</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, 91 403 Orsay CEDEX</affiliation>					
			</affiliations>
			<titre>Apprentissage par imitation pour l'étiquetage de séquences : vers une formalisation des méthodes d'étiquetage « easy-first »</titre>
			<type>long</type>
			<pages>1-12</pages>
			<resume>De nombreuses méthodes ont été proposées pour accélérer la prédiction d'objets structurés (tels que les arbres ou les séquences), ou pour permettre la prise en compte de dépendances plus riches afin d'améliorer les performances de la prédiction. Ces méthodes reposent généralement sur des techniques d'inférence approchée et ne bénéficient d'aucune garantie théorique aussi bien du point de vue de la qualité de la solution trouvée que du point de vue de leur critère d'apprentissage. Dans ce travail, nous étudions une nouvelle formulation de l'apprentissage structuré qui consiste à voir celui-ci comme un processus incrémental au cours duquel la sortie est construite de façon progressive. Ce cadre permet de formaliser plusieurs approches de prédiction structurée existantes. Grâce au lien que nous faisons entre apprentissage structuré et apprentissage par renforcement, nous sommes en mesure de proposer une méthode théoriquement bien justifiée pour apprendre des méthodes d'inférence approchée. Les expériences que nous réalisons sur quatre tâches de TAL valident l'approche proposée.</resume>
			<mots_cles>Apprentissage par Imitation, Apprentissage Structuré, Étiquetage de Séquences</mots_cles>
			<title>Imitation learning for sequence labeling: towards a formalization of easy-first labeling methods</title>
			<abstract>Structured learning techniques, aimed at modeling structured objects such as labeled trees or strings, are computationally expensive. Many attempts have been made to reduce their complexity, either to speed up learning and inference, or to take richer dependencies into account. These attempts typically rely on approximate inference techniques and usually provide very little theoretical guarantee regarding the optimality of the solutions they find. In this work we study a new formulation of structured learning where inference is primarily viewed as an incremental process along which a solution is progressively computed. This framework generalizes several structured learning ap- proaches. Building on the connections between this framework and reinforcement learning, we propose a theoretically sound method to learn to perform approximate inference. Experiments on four sequence labeling tasks show that our approach is very competitive when compared to several strong baselines.</abstract>
			<keywords>Imitation Learning, Structured Learning, Sequence Models</keywords>
		</article>	
		<article id="taln-2015-long-002" session="Extraction d'information">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Claveau</nom>
					<email>vincent.claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ewa</prenom>
					<nom>Kijak</nom>
					<email>ekijak@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA – CNRS – Univ. Rennes 1, Campus de Beaulieu, 35042 Rennes cedex</affiliation>
			</affiliations>
			<titre>Stratégies de sélection des exemples pour l'apprentissage actif avec des champs aléatoires conditionnels</titre>
			<type>long</type>
			<pages>13-24</pages>
			<resume>Beaucoup de problèmes de TAL sont désormais modélisés comme des tâches d'apprentissage supervisé. De ce fait, le coût des annotations des exemples par l'expert représente un problème important. L'apprentissage actif (active learning) apporte un cadre à ce problème, permettant de contrôler le coût d'annotation tout en maximisant, on l'espère, la performance de la tâche visée, mais repose sur le choix difficile des exemples à soumettre à l'expert. Dans cet article, nous examinons et proposons des stratégies de sélection des exemples pour le cas spécifique des champs aléatoires conditionnels (Conditional Random Fields, CRF), outil largement utilisé en TAL. Nous proposons d'une part une méthode simple corrigeant un biais de certaines méthodes de l'état de l'art. D'autre part, nous détaillons une méthode originale de sélection s'appuyant sur un critère de respect des proportions dans les jeux de données manipulés. Le bien- fondé de ces propositions est vérifié au travers de plusieurs tâches et jeux de données, incluant reconnaissance d'entités nommées, chunking, phonétisation, désambiguïsation de sens.</resume>
			<mots_cles>CRF, champs aléatoires conditionnels, apprentissage actif, apprentissage semi-supervisé, test statistique de proportion</mots_cles>
			<title>Strategies to select examples for Active Learning with Conditional Random Fields</title>
			<abstract>Nowadays, many NLP problems are modelized as supervised machine learning tasks. Consequently, the cost of the ex- pertise needed to annotate the examples is a widespread issue. Active learning offers a framework to that issue, allowing to control the annotation cost while maximizing the classifier performance, but it relies on the key step of choosing which example will be proposed to the expert. In this paper, we examine and propose such selection strategies in the specific case of Conditional Random Fields (CRF) which are largely used in NLP. On the one hand, we propose a simple method to correct a bias of certain state-of-the-art selection techniques. On the other hand, we detail an original approach to select the examples, based on the respect of proportions in the datasets. These contributions are validated over a large range of experiments implying several tasks and datasets, including named entity recognition, chunking, phonetization, word sens disambiguation.</abstract>
			<keywords>CRF, conditional random fields, active learning, semi-supervised learning, statistical test of proportion</keywords>
		</article>	
		<article id="taln-2015-long-003" session="Extraction d'information">
			<auteurs>
				<auteur>
					<prenom>Cyril</prenom>
					<nom>Grouin</nom>
					<email>cyril.grouin@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Moriceau</nom>
					<email>veronique.moriceau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Rosset</nom>
					<email>rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email>pz@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI–CNRS, UPR 3251, rue John von Neumann, 91400 Orsay</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, Campus universitaire d'Orsay, 91400 Orsay</affiliation>			
			</affiliations>
			<titre>Identification de facteurs de risque pour des patients diabétiques à partir de comptes-rendus cliniques par des approches hybrides</titre>
			<type>long</type>
			<pages>25-36</pages>
			<resume>Dans cet article, nous présentons les méthodes que nous avons développées pour analyser des comptes- rendus hospitaliers rédigés en anglais. L'objectif de cette étude consiste à identifier les facteurs de risque de décès pour des patients diabétiques et à positionner les événements médicaux décrits par rapport à la date de création de chaque document. Notre approche repose sur (i) HeidelTime pour identifier les expressions temporelles, (ii) des CRF complétés par des règles de post-traitement pour identifier les traitements, les maladies et facteurs de risque, et (iii) des règles pour positionner temporellement chaque événement médical. Sur un corpus de 514 documents, nous obtenons une F-mesure globale de 0,8451. Nous observons que l'identification des informations directement mentionnées dans les documents se révèle plus performante que l'inférence d'informations à partir de résultats de laboratoire.</resume>
			<mots_cles>Comptes-rendus hospitaliers, extraction d'information, apprentissage statistique</mots_cles>
			<title>Risk factor identification for diabetic patients from clinical records using hybrid approaches</title>
			<abstract>In this paper, we present the methods we designed to process clinical records written in English. The aim of this study consists in identifying risk factors for diabetic patients and to define the temporal relation of those medical events wrt. the document creation time. Our approach relies (i) on HeidelTime to identify temporal expressions, (ii) on CRF and post-processing rules to identify treatments, diseases and risk factors, and (iii) on rules to determine the temporal relation of each medical event. On a corpus of 514 documents, we achieved a 0.8451 global F-measure. We observe we performed best on the identification of information mentionned in the text than information inference from lab results.</abstract>
			<keywords>Electronic Health Records, Information Extraction, Machine Learning</keywords>
		</article>	
		<article id="taln-2015-long-004" session="Extraction d'information">
			<auteurs>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Pécheux</nom>
					<email>nicolas.pecheux@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Allauzen</nom>
					<email>alexandre.allauzen@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thomas</prenom>
					<nom>Lavergne</nom>
					<email>thomas.lavergne@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Wisniewski</nom>
					<email>guillaume.wisniewski@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>											
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris-Sud, 91 403 Orsay CEDEX</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, 91 403 Orsay CEDEX</affiliation>			
			</affiliations>
			<titre>Oublier ce qu'on sait, pour mieux apprendre ce qu'on ne sait pas : une étude sur les contraintes de type dans les modèles CRF</titre>
			<type>long</type>
			<pages>37-48</pages>
			<resume>Quand on dispose de connaissances a priori sur les sorties possibles d'un problème d'étiquetage, il semble souhaitable d'inclure cette information lors de l'apprentissage pour simplifier la tâche de modélisation et accélérer les traitements. Pourtant, même lorsque ces contraintes sont correctes et utiles au décodage, leur utilisation lors de l'apprentissage peut dégrader sévèrement les performances. Dans cet article, nous étudions ce paradoxe et montrons que le manque de contraste induit par les connaissances entraîne une forme de sous-apprentissage qu'il est cependant possible de limiter.</resume>
			<mots_cles>Étiquetage Morpho-Syntaxique, Apprentissage Statistique, Champs Markoviens Aléatoires</mots_cles>
			<title>Ignore what you know to better learn what you don't : a case study on type constraints for CRFs</title>
			<abstract>When information about the possible outputs of a sequence labeling task is available, it may seem appropriate to include this knowledge into the system, so as to facilitate and speed-up learning and inference. However, we show in this paper that using such constraints at training time is likely to drastically reduce performance, even when they are both correct and useful at decoding. In this paper, we study this paradox and show that the lack of contrast induced by constraints leads to a form of under-fitting, that it is however possible to partially overcome.</abstract>
			<keywords>Part-of-Speech Tagging, Statistical Machine Learning, Conditional Random Fields</keywords>
		</article>								
		<article id="taln-2015-long-005" session="Compréhension et paraphrase">
			<auteurs>
				<auteur>
					<prenom>Mike Donald</prenom>
					<nom>Tapi Nzali</nom>
					<email>mike-donald.tapi-nzali@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Névéol</nom>
					<email>aurelie.neveol@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xavier</prenom>
					<nom>Tannier</nom>
					<email>xtannier@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>anne.vilnat@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Campus Universitaire d'Orsay, bât 508, 91405 ORSAY, FRANCE</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, 91403 ORSAY, FRANCE</affiliation>
			</affiliations>
			<titre>Analyse d'expressions temporelles dans les dossiers électroniques patients</titre>
			<type>long</type>
			<pages>49-58</pages>
			<resume>Les références à des phénomènes du monde réel et à leur caractérisation temporelle se retrouvent dans beaucoup de types de discours en langue naturelle. Ainsi, l'analyse temporelle apparaît comme un élément important en traitement automatique de la langue. Cet article présente une analyse de textes en domaine de spécialité du point de vue temporel. En s'appuyant sur un corpus de documents issus de plusieurs dossiers électroniques patient désidentifiés, nous décrivons la construction d'une ressource annotée en expressions temporelles selon la norme TimeML. Par suite, nous utilisons cette ressource pour évaluer plusieurs méthodes d'extraction automatique d'expressions temporelles adaptées au domaine médical. Notre meilleur système statistique offre une performance de 0,91 de F-mesure, surpassant pour l'identification le système état de l'art HeidelTime. La comparaison de notre corpus de travail avec le corpus journalistique FR-Timebank permet également de caractériser les différences d'utilisation des expressions temporelles dans deux domaines de spécialité.</resume>
			<mots_cles>Extraction d'Information, Analyse Temporelle, Développement d'un Corpus Annoté</mots_cles>
			<title>An analysis of temporal expressions in Electronic Health Records in French</title>
			<abstract>References to phenomena ocurring in the world and their temporal caracterization can be found in a variety of natural language utterances. For this reason, temporal analysis is a key issue in natural language processing. This article presents a temporal analysis of specialized documents. We use a corpus of documents contained in several de-identified Electronic Health Records to develop an annotated resource of temporal expressions relying on the TimeML standard. We then use this corpus to evaluate several methods for the automatic extraction of temporal expressions. Our best statistical model yields 0.91 F-measure, which provides significant improvement on extraction, over the state-of-the-art system Heidel-Time. We also compare our medical corpus to FR-Timebank in order to characterize the uses of temporal expressions in two different subdomains</abstract>
			<keywords>Information Extraction, Temporal Analysis, Development of Annotated Corpus</keywords>
		</article>
		<article id="taln-2015-long-006" session="Compréhension et paraphrase">
			<auteurs>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Ferreira</nom>
					<email>emmanuel.ferreira@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bassam</prenom>
					<nom>Jabaian</nom>
					<email>bassam.jabaian@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>											
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université d'Avignon, CERI-LIA, France</affiliation>
			</affiliations>
			<titre>Compréhension automatique de la parole sans données de référence</titre>
			<type>long</type>
			<pages>59-70</pages>
			<resume>La majorité des méthodes état de l'art en compréhension automatique de la parole ont en commun de devoir être apprises sur une grande quantité de données annotées. Cette dépendance aux données constitue un réel obstacle lors du développement d'un système pour une nouvelle tâche/langue. Aussi, dans cette étude, nous présentons une méthode visant à limiter ce besoin par un mécanisme d'apprentissage sans données de référence (zero-shot learning). Cette méthode combine une description ontologique minimale de la tâche visée avec l'utilisation d'un espace sémantique continu appris par des approches à base de réseaux de neurones à partir de données génériques non-annotées. Nous montrons que le modèle simple et peu coûteux obtenu peut atteindre, dès le démarrage, des performances comparables à celles des systèmes état de l'art reposant sur des règles expertes ou sur des approches probabilistes sur des tâches de compréhension de la parole de référence (tests des Dialog State Tracking Challenges, DSTC2 et DSTC3). Nous proposons ensuite une stratégie d'adaptation en ligne permettant d'améliorer encore les performances de notre approche à l'aide d'une supervision faible et ajustable par l'utilisateur.</resume>
			<mots_cles>Compréhension automatique de la parole, espace sémantique continu, apprentissage sans données de référence, données d'apprentissage hors domaine</mots_cles>
			<title>Spoken language understanding without reference data</title>
			<abstract>Most recent state-of-the-art spoken language understanding models have in common to be trained on a potentially large amount of data. However, the required annotated corpora are not available for a variety of tasks and languages of interest. In this work, we present a novel zero-shot learning method for spoken language understanding which alleviate the need of any annotated or in-context data. Instead, it combines an ontological description of the target domain and the use of a continuous semantic space trained on large amounts of unannotated and unstructured found data with neural network algorithms. We show that this very low cost model can reach instantly performance comparable to those obtained by either state-of-the-art carefully hand crafted rule-based or trained statistical models on reference spoken language understanding tasks (test sets of the second and the third Dialog State Tracking Challenge, DSTC2,DSTC3). Eventually we extend the approach with an online adaptative strategy allowing to refine progressively the initial model with only a light and adjustable supervision.</abstract>
			<keywords>Spoken language understanding, continuous semantic space, zero-shot learning, out-of-domain training data</keywords>
		</article>	
		<article id="taln-2015-long-007" session="Désambiguïsation">
			<auteurs>
				<auteur>
					<prenom>Kiem-Hieu</prenom>
					<nom>Nguyen</nom>
					<email>nguyen@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xavier</prenom>
					<nom>Tannier</nom>
					<email>Xavier.Tannier@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
				<affiliation affiliationId="2">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, F-91191, Gif-sur-Yvette</affiliation>
				<affiliation affiliationId="3">Univ. Paris-Sud</affiliation>
			</affiliations>
			<titre>Désambiguïsation d'entités pour l'induction non supervisée de schémas événementiels</titre>
			<type>long</type>
			<pages>71-82</pages>
			<resume>Cet article présente un modèle génératif pour l'induction non supervisée d'événements. Les précédentes méthodes de la littérature utilisent uniquement les têtes des syntagmes pour représenter les entités. Pourtant, le groupe complet (par exemple, "un homme armé") apporte une information plus discriminante (que "homme"). Notre modèle tient compte de cette information et la représente dans la distribution des schémas d'événements. Nous montrons que ces relations jouent un rôle important dans l'estimation des paramètres, et qu'elles conduisent à des distributions plus cohérentes et plus discriminantes. Les résultats expérimentaux sur le corpus de MUC-4 confirment ces progrès.</resume>
			<mots_cles>Événements, modèle génératif, désambiguïsation d'entités, échantillonnage de Gibbs</mots_cles>
			<title>Entity disambiguation for event template induction</title>
			<abstract>In this paper, we present an approach for event induction with a generative model. This model makes possible to consider more relational information than previous models, and has been applied to noun attributes. By their inﬂuence on parameter estimation, this new information make probabilistic topic distribution more discriminative and more robust. We evaluated different versions of our model on MUC-4 datasets.</abstract>
			<keywords>Event Induction, Generative Model, Entity Disambiguation, Gibbs Sampling</keywords>
		</article>
		<article id="taln-2015-long-008" session="Désambiguïsation">
			<auteurs>
				<auteur>
					<prenom>Mohammad</prenom>
					<nom>Nasiruddin</nom>
					<email>mohammad.nasiruddin@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Andon</prenom>
					<nom>Tchechmedjiev</nom>
					<email>Andon.Tchechmedjiev@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hervé</prenom>
					<nom>Blanchon</nom>
					<email>Herve.Blanchon@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Schwab</nom>
					<email>didier.schwab@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG-GETALP Univ. Grenoble Alpes</affiliation>
			</affiliations>				
			<titre>Création rapide et efficace d'un système de désambiguïsation lexicale pour une langue peu dotée</titre>
			<type>long</type>
			<pages>83-94</pages>
			<resume>Nous présentons une méthode pour créer rapidement un système de désambiguïsation lexicale (DL) pour une langue L peu dotée pourvu que l'on dispose d'un système de traduction automatique statistique (TAS) d'une langue riche en corpus annotés en sens (ici l'anglais) vers L. Il est, en effet, plus facile de disposer des ressources nécessaires à la création d'un système de TAS que des ressources dédiées nécessaires à la création d'un système de DL pour la langue L. Notre méthode consiste à traduire automatiquement un corpus annoté en sens vers la langue L, puis de créer le système de désambiguïsation pour L par des méthodes supervisées classiques. Nous montrons la faisabilité de la méthode et sa généricité en traduisant le SemCor, un corpus en anglais annoté grâce au Princeton WordNet, de l'anglais vers le bangla et de l'anglais vers le français. Nous montrons la validité de l'approche en évaluant les résultats sur la tâche de désambiguïsation lexicale multilingue de Semeval 2013.</resume>
			<mots_cles>clarification de texte, désambiguïsation lexicale, langues peu dotées, traduction automatique, portage d'annotations</mots_cles>
			<title>Rapid Construction of Supervised Word Sense Disambiguation System for Lesser-resourced Languages</title>
			<abstract>We introduce a method to quickly build a Word Sense Disambiguation (WSD) system for a lesser-resourced language L, under the condition that a Statistical Machine Transation system (SMT) is available from a well resourced language where semantically annotated corpora are available (here, English) towards L. We argue that it is less difficult to obtain the resources mandatory for the development of an SMT system (parallel-corpora) than it is to create the resources necessary for a WSD system (semantically annotated corpora, lexical resources). In the present work, we propose to translate a semantically annotated corpus from English to L and then to create a WSD system for L following the classical supervised WSD paradigm. We demonstrate the feasibility and genericity of our proposed method by translating SemCor from English to Bangla and from English to French. SemCor is an English corpus annotated with Princeton WordNet sense tags. We show the feasibility of the approach using the Multilingual WSD task from Semeval 2013.</abstract>
			<keywords>clarification of texts, word sens disambiguation, under resourced languages, machine translation, annotation transfert</keywords>
		</article>
		<article id="taln-2015-long-009" session="Opinions et sentiments">
			<auteurs>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus</affiliation>
				<affiliation affiliationId="2">CEA Saclay Nano-INNOV, PC no 173, 91191 Gif-sur-Yvette CEDEX</affiliation>
			</affiliations>
			<titre>Méthode faiblement supervisée pour l'extraction d'opinion ciblée dans un domaine spécifique</titre>
			<type>long</type>
			<pages>95-106</pages>
			<resume>La détection d'opinion ciblée a pour but d'attribuer une opinion à une caractéristique particulière d'un produit donné. La plupart des méthodes existantes envisagent pour cela une approche non supervisée. Or, les utilisateurs ont souvent une idée a priori des caractéristiques sur lesquelles ils veulent découvrir l'opinion des gens. Nous proposons dans cet article une méthode pour une extraction d'opinion ciblée, qui exploite cette information minimale sur les caractéristiques d'intérêt. Ce modèle s'appuie sur une segmentation automatique des textes, un enrichissement des données disponibles par similarité sémantique, et une annotation de l'opinion par classification supervisée. Nous montrons l'intérêt de l'approche sur un cas d'étude dans le domaine des jeux vidéos.</resume>
			<mots_cles>Analyse d'opinion, classification supervisée, similarité sémantique</mots_cles>
			<title>A Weakly Supervised Approach for Aspect-Based Opinion Mining in a Specific Domain</title>
			<abstract>The goal of aspect-based opinion mining is to associate an opinion with fine-grain aspects of a given product. Most approaches designed in this purpose use unsupervised techniques, whereas the information of the desired targeted aspects can often be given by the end-users. We propose in this paper a new approach for targeted opinion detection that uses this minimal information, enriched using several semantic similarty measures, along with topical segmentation and supervised classification. We prove the interest of the approach on an evaluation corpus in the specific domain of video games.</abstract>
			<keywords>Opinion analysis, classification, semantic similarity</keywords>
		</article>
		<article id="taln-2015-long-010" session="Opinions et sentiments">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Valette</nom>
					<email>mvalette@inalco.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Egle</prenom>
					<nom>Eensoo</nom>
					<email>egle.eensoo@inalco.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERTIM, INALCO, 2 rue de Lille, 75343 PARIS cedex 07</affiliation>												
			</affiliations>
			<titre>Une méthodologie de sémantique de corpus appliquée à des tâches de fouille d'opinion et d'analyse des sentiments : étude sur l'impact de marqueurs dialogiques et dialectiques dans l'expression de la subjectivité</titre>
			<type>long</type>
			<pages>107-118</pages>
			<resume>Cet article entend dresser, dans un premier temps, un panorama critique des relations entre TAL et linguistique. Puis, il esquisse une discussion sur l'apport possible d'une sémantique de corpus dans un contexte applicatif en s'appuyant sur plusieurs expériences en fouille de textes subjectifs (analyse de sentiments et fouille d'opinions). Ces expériences se démarquent des approches traditionnelles fondées sur la recherche de marqueurs axiologiques explicites par l'utilisation de critères relevant des représentations des acteurs (composante dialogique) et des structures argumentatives et narratives des textes (composante dialectique). Nous souhaitons de cette façon mettre en lumière le bénéfice d'un dialogue méthodologique entre une théorie (la sémantique textuelle), des méthodes de linguistique de corpus orientées vers l'analyse du sens (la textométrie) et les usages actuels du TAL en termes d'algorithmiques (apprentissage automatique) mais aussi de méthodologie d'évaluation des résultats.</resume>
			<mots_cles>Textométrie, Sémantique de corpus, Fouille d'opinion, Analyse des sentiments</mots_cles>
			<title>A method of corpus semantics applied to opinion mining and sentiment analysis: the impact of dialogical and dialectical features on the expression of subjectivity</title>
			<abstract>This paper first aims to provide a critical overview of the relationship between NLP and linguistics, and then to sketch out a discussion on the possible contribution of corpus semantics in an application-based context based on several subjective text mining studies (sentiment analysis and opinion mining). These studies break away from traditional approaches founded on the detection of axiological markers. Instead, they use explicit criteria related to the representation of actors (dialogical component) and argumentative or narrative structures (dialectical component). We hope to highlight the benefit of a methodological dialogue between theory (text semantics), meaning-oriented methods of corpus linguistics (i.e. textometrics) and NLP current practices in terms of algorithmic (machine learning) and assessment methodology.</abstract>
			<keywords>Textometry, corpus semantics, opinion mining, sentiment analysis</keywords>
		</article>	
		<article id="taln-2015-long-011" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Van-Minh</prenom>
					<nom>Pho</nom>
					<email>pho@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Ligozat</nom>
					<email>annlor@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>Brigitte.Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Orsay</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, Orsay</affiliation>
				<affiliation affiliationId="3">ENSIIE, Evry</affiliation>								
			</affiliations>
			<titre>Estimation de l'homogénéité sémantique pour les Questionnaires à Choix Multiples</titre>
			<type>long</type>
			<pages>119-130</pages>
			<resume>L'homogénéité sémantique stipule que des termes sont sémantiquement proches mais non similaires. Cette notion est au cœur de travaux relatifs à la génération automatique de questionnaires à choix multiples, et particulièrement à la sélection automatique de distracteurs. Dans cet article, nous présentons une méthode d'estimation de l'homogénéité sémantique dans un cadre de validation automatique de distracteurs. Cette méthode est fondée sur une combinaison de plusieurs critères de voisinage et de similarité sémantique entre termes, par apprentissage automatique. Nous montrerons que notre méthode permet d'obtenir une meilleure estimation de l'homogénéité sémantique que les méthodes proposées dans l'état de l'art.</resume>
			<mots_cles>similarité, voisinage sémantique, classification de termes</mots_cles>
			<title>Semantic homogeneity estimation for MCQs</title>
			<abstract>Semantic homogeneity states that terms are semantically close but not similar. This notion is the focus of work related to multiple-choice test generation, and especially to automatic distractor selection. In this paper, we present a method to estimate semantic homogeneity within a framework of automatic distractor validation. This method is based on a combination of several criteria of semantic relatedness and similarity between terms, by machine learning. We will show that our method allows to obtain a better estimation of semantic homogeneity than methods proposed in related work.</abstract>
			<keywords>similarity, semantic relatedness, term ranking</keywords>
		</article>	
		<article id="taln-2015-long-012" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Cartier</nom>
					<email>emmanuel.cartier@univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 13 Sorbonne Paris Cité, LIPN UMR 7030, équipe RCLN</affiliation>
			</affiliations>
			<titre>Extraction automatique de relations sémantiques dans les dé finitions : approche hybride, construction d'un corpus de relations sémantiques pour le français</titre>
			<type>long</type>
			<pages>131-145</pages>
			<resume>Cet article présente une expérimentation visant à construire une ressource sémantique pour le français contemporain à partir d'un corpus d'environ un million de définitions tirées de deux ressources lexicographiques (Trésor de la Langue Française, Wiktionary) et d'une ressource encyclopédique (Wikipedia). L'objectif est d'extraire automatiquement dans les définitions différentes relations sémantiques : hyperonymie, synonymie, méronymie, autres relations sémantiques. La méthode suivie combine la précision des patrons lexico-syntaxiques et le rappel des méthodes statistiques, ainsi qu'un traitement inédit de canonisation et de décomposition des énoncés. Après avoir présenté les différentes approches et réalisations existantes, nous détaillons l'architecture du système et présentons les résultats : environ 900 000 relations d'hyperonymie et près de 100 000 relations de synonymie, avec un taux de précision supérieur à 90% sur un échantillon aléatoire de 500 relations. Plus de 2 millions de prédications définitoires ont également été extraites.</resume>
			<mots_cles>relations sémantiques, patrons lexico-syntaxiques, distributionnalisme, prédication, hyperonymie, synonymie, méronymie, définition</mots_cles>
			<title>Automatic Extraction of Semantic Relations from Definitions : en experiment in French with an Hybrid Approach</title>
			<abstract>This article presents an experiment to extract semantic relations from definitions. It is based on approximately one million definitions from two general dictionaries (Trésor de la Langue Française, French Wiktionary) and from the collaborative Wikipedia. We aim at extracting from these data several semantic relations : hyperonymy, synonymy, meronymy and other semantic relations. The methodological approach combines the precision of lexico-syntactic patterns and the recall of statistical analysis. After a survey of the state-of-the-art methods in this area, we detail our system and give the overall outcomes : about 900 000 hypernymy and 100 000 synonymy relations are extracted with a precision above 90% on a sample of 500 pairs for each relation. About 2 millions of definitory predicates are also extracted.</abstract>
			<keywords>semantic relations, lexico-syntactic patterns, distributionnalism, predication, hypernymy, synonymy, meronymy, definition</keywords>
		</article>	
		<article id="taln-2015-long-013" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Gif-sur-Yvette, F-91191 France</affiliation>
			</affiliations>
			<titre>Déclasser les voisins non sémantiques pour améliorer les thésaurus distributionnels</titre>
			<type>long</type>
			<pages>146-157</pages>
			<resume>La plupart des méthodes d'amélioration des thésaurus distributionnels se focalisent sur les moyens – représentations ou mesures de similarité – de mieux détecter la similarité sémantique entre les mots. Dans cet article, nous proposons un point de vue inverse : nous cherchons à détecter les voisins sémantiques associés à une entrée les moins susceptibles d'être liés sémantiquement à elle et nous utilisons cette information pour réordonner ces voisins. Pour détecter les faux voisins sémantiques d'une entrée, nous adoptons une approche s'inspirant de la désambiguïsation sémantique en construisant un classifieur permettant de différencier en contexte cette entrée des autres mots. Ce classifieur est ensuite appliqué à un échantillon des occurrences des voisins de l'entrée pour repérer ceux les plus éloignés de l'entrée. Nous évaluons cette méthode pour des thésaurus construits à partir de cooccurrents syntaxiques et nous montrons l'intérêt de la combiner avec les méthodes décrites dans (Ferret, 2013b) selon une stratégie de type vote.</resume>
			<mots_cles>Sémantique lexicale, similarité sémantique, thésaurus distributionnels</mots_cles>
			<title>Downgrading non-semantic neighbors for improving distributional thesauri</title>
			<abstract>Most of the methods for improving distributional thesauri focus on the means – representations or similarity measures – to detect better semantic similarity between words. In this article, we propose a more indirect approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate more particularly the interest of this method for thesauri built from syntactic co-occurrents and we show the interest of associating this method with those of (Ferret, 2013b) following an ensemble strategy.</abstract>
			<keywords>Lexical semantics, semantic similarity, distributional thesauri</keywords>
		</article>	
		<article id="taln-2015-long-014" session="Syntaxe et paraphrase">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>laurence.danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aleksandre</prenom>
					<nom>Maskharashvili</nom>
					<email>aleksandre.maskharashvili@inria.fr</email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
					<affiliationId>6</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sylvain</prenom>
					<nom>Pogodalla</nom>
					<email>sylvain.pogodalla@loria.fr</email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
					<affiliationId>6</affiliationId>
					<affiliationId>7</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Diderot (Paris 7), Paris, F-75013, France</affiliation>
				<affiliation affiliationId="2">ALPAGE, INRIA Paris–Rocquencourt, Paris, F-75013, France</affiliation>
				<affiliation affiliationId="3">Institut Universitaire de France, Paris, F-75005, France</affiliation>
				<affiliation affiliationId="4">INRIA, Villers-lès-Nancy, F-54600, France</affiliation>	
				<affiliation affiliationId="5">Université de Lorraine, LORIA, UMR 7503, Vandœuvre-lès-Nancy, F-54500, France</affiliation>	
				<affiliation affiliationId="6">CNRS, LORIA, UMR 7503, Vandœuvre-lès-Nancy, F-54500, France</affiliation>	
				<affiliation affiliationId="7">Heinrich Heine Universität, Düsseldorf, Allemagne</affiliation>	
			</affiliations>
			<titre>Grammaires phrastiques et discursives fondées sur les TAG : une approche de D-STAG avec les ACG</titre>
			<type>long</type>
			<pages>158-169</pages>
			<resume>Nous présentons une méthode pour articuler grammaire de phrase et grammaire de discours qui évite de recourir à une étape de traitement intermédiaire. Cette méthode est suffisamment générale pour construire des structures discursives qui ne soient pas des arbres mais des graphes orientés acycliques (DAG). Notre analyse s'appuie sur une approche de l'analyse discursive, Discourse Synchronous TAG (D-STAG), qui utilise les Grammaires d'Arbres Adjoint (TAG). Nous utilisons pour ce faire un encodage des TAG dans les Grammaires Catégorielles Abstraites (ACG). Cet encodage permet d'une part d'utiliser l'ordre supérieur pour l'interprétation sémantique afin de construire des structures qui soient des DAG et non des arbres, et d'autre part d'utiliser les propriétés de composition d'ACG pour réaliser naturellement l'interface entre grammaire phrastique et grammaire discursive. Tous les exemples proposés pour illustrer la méthode ont été implantés et peuvent être testés avec le logiciel approprié.</resume>
			<mots_cles>Syntaxe, sémantique, discours, grammaire, grammaire d'arbres adjoints, TAG, D-LTAG, D-STAG, grammaire catégorielle abstraite, ACG</mots_cles>
			<title>Sentential and Discourse TAG-Based Grammars: An ACG Approach to D-STAG</title>
			<abstract>This article presents a method to interface a sentential grammar and a discourse grammar without resorting to an intermediate processing step. The method is general enough to build discourse structures that are direct acyclic graphs (DAG) and not only trees. Our analysis is based on Discourse Synchronous TAG (D-STAG), a Tree-Adjoining Grammar (TAG)-based approach to discourse. We also use an encoding of TAG into Abstract Categorial Grammar (ACG). This encoding allows us to express a higher-order semantic interpretation that enables building DAG discourse structures on the one hand, and to smoothly integrate the sentential and the discourse grammar thanks to the modular capability of ACG. All the examples of the article have been implemented and may be run and tested with the appropriate software.</abstract>
			<keywords>Syntax, semantics, discourse, grammar, Tree-Adjoining Grammar, TAG, D-LTAG, D-STAG, Abstract Categorial Grammar, ACG</keywords>
		</article>
		<article id="taln-2015-long-015" session="Syntaxe et paraphrase">
			<auteurs>
				<auteur>
					<prenom>Martin</prenom>
					<nom>Gleize</nom>
					<email>gleize@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>Brigitte.Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>											
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Rue John von Neumann, 91405 Orsay CEDEX, France</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, Orsay</affiliation>
				<affiliation affiliationId="3">ENSIIE, Evry</affiliation>								
			</affiliations>
			<titre>Noyaux de réécriture de phrases munis de types lexico-sémantiques</titre>
			<type>long</type>
			<pages>170-181</pages>
			<resume>De nombreux problèmes en traitement automatique des langues requièrent de déterminer si deux phrases sont des réécritures l'une de l'autre. Une solution efficace consiste à apprendre les réécritures en se fondant sur des méthodes à noyau qui mesurent la similarité entre deux réécritures de paires de phrases. Toutefois, ces méthodes ne permettent généralement pas de prendre en compte des variations sémantiques entre mots, qui permettraient de capturer un plus grand nombre de règles de réécriture. Dans cet article, nous proposons la définition et l'implémentation d'une nouvelle classe de fonction noyau, fondée sur la réécriture de phrases enrichie par un typage pour combler ce manque. Nous l'évaluons sur deux tâches, la reconnaissance de paraphrases et d'implications textuelles.</resume>
			<mots_cles>fonction noyau, variations sémantiques, réécriture de phrase, reconnaissance de paraphrases, implication textuelle</mots_cles>
			<title>Enriching String Rewriting Kernels With Lexico-semantic Types</title>
			<abstract>Many high level natural language processing problems can be framed as determining if two given sentences are a rewriting of each other. One way to solve this problem is to learn the way a sentence rewrites into another with kernel-based methods, relying on a kernel function to measure the similarity between two rewritings. While a wide range of rewriting kernels has been developed in the past, they often do not allow the user to provide lexico-semantic variations of words, which could help capturing a wider class of rewriting rules. In this paper, we propose and implement a new class of kernel functions, referred to as type-enriched string rewriting kernel, to address this lack. We experiment with various typing schemes on two natural sentence rewriting tasks, paraphrase identification and recognizing textual entailment.</abstract>
			<keywords>kernel methods, semantic variations, sentence rewriting, paraphrase identification, textual entailment</keywords>
		</article>	
		<article id="taln-2015-long-016" session="Syntaxe et paraphrase">
			<auteurs>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email>natalia.grabar@univ-lille3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Hamon</nom>
					<email>hamon@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS UMR 8163 STL, Université Lille 3, 59653 Villeneuve d'Ascq, France</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, BP133, Orsay; Université Paris 13, Sorbonne Paris Cité, France</affiliation>													
			</affiliations>
			<titre>Extraction automatique de paraphrases grand public pour les termes médicaux</titre>
			<type>long</type>
			<pages>182-193</pages>
			<resume>Nous sommes tous concernés par notre état de santé et restons sensibles aux informations de santé disponibles dans la société moderne à travers par exemple les résultats des recherches scientifiques, les médias sociaux de santé, les documents cliniques, les émissions de télé et de radio ou les nouvelles. Cependant, il est commun de rencontrer dans le domaine médical des termes très spécifiques (e.g., blépharospasme, alexitymie, appendicectomie), qui restent difficiles à comprendre par les non spécialistes. Nous proposons une méthode automatique qui vise l'acquisition de paraphrases pour les termes médicaux, qui soient plus faciles à comprendre que les termes originaux. La méthode est basée sur l'analyse morphologique des termes, l'analyse syntaxique et la fouille de textes non spécialisés. L'analyse et l'évaluation des résultats indiquent que de telles paraphrases peuvent être trouvées dans les documents non spécialisés et présentent une compréhension plus facile. En fonction des paramètres de la méthode, la précision varie entre 86 et 55 %. Ce type de ressources est utile pour plusieurs applications de TAL (e.g., recherche d'information grand public, lisibilité et simplification de textes, systèmes de question-réponses).</resume>
			<mots_cles>Domaines de spécialité, terminologie médicale, composition, analyse morphologique, paraphrase, compréhension</mots_cles>
			<title>Automatic extraction of layman paraphrases for medical terms</title>
			<abstract>We all have health concerns and sensibility to health information available in the modern society through modern media, such as scientific research, health social media, clinical documents, TV and radio broadcast, or novels. However, medical area conveys very specific notions (e.g., blepharospasm, alexitymia, appendicectomy), which are difficult to understand by people without medical training. We propose an automatic method for the acquisition of paraphrases for technical medical terms. We expect that such paraphrases are easier to understand than the original terms. The method is based on the morphological analysis of terms, syntactic analysis of texts, and text mining of non specialized texts. An analysis of the results and their evaluation indicate that such paraphrases can indeed be found in non specialized documents and show easier understanding level. According to the setting of the method, precision of the extractions ranges between 86 and 55%. This kind of resources is useful for several Natural Language Processing applications (e.g., information retrieval for lay people, text readability and simplification, question and answering systems).</abstract>
			<keywords>Specialized Area, Medical Terminology, Compounds, Morphological Analysis, Paraphrasis, Understanding</keywords>
		</article>		
		<article id="taln-2015-long-017" session="Syntaxe et paraphrase">
			<auteurs>
				<auteur>
					<prenom>Gaël</prenom>
					<nom>Guibon</nom>
					<email>gael.guibon@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Prévost</nom>
					<email>sophie.prevost@ens.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Matthieu</prenom>
					<nom>Constant</nom>
					<email>Mathieu.Constant@u-pem.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kim</prenom>
					<nom>Gerdes</nom>
					<email>kim@gerdes.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice CNRS</affiliation>
				<affiliation affiliationId="2">université Paris 3 - Sorbonne Nouvelle</affiliation>
				<affiliation affiliationId="3">université Paris-Est, LIGM</affiliation>
				<affiliation affiliationId="4">LPP CNRS</affiliation>																	
			</affiliations>
			<titre>Analyse syntaxique de l'ancien français : quelles propriétés de la langue inﬂuent le plus sur la qualité de l'apprentissage ?</titre>
			<type>long</type>
			<pages>194-207</pages>
			<resume>L'article présente des résultats d'expériences d'apprentissage automatique pour l'étiquetage morpho-syntaxique et l'analyse syntaxique en dépendance de l'ancien français. Ces expériences ont pour objectif de servir une exploration de corpus pour laquelle le corpus arboré SRCMF sert de données de référence. La nature peu standardisée de la langue qui y est utilisée implique des données d'entraînement hétérogènes et quantitativement limitées. Nous explorons donc diverses stratégies, fondées sur différents critères (variabilité du lexique, forme Vers/Prose des textes, dates des textes), pour constituer des corpus d'entrainement menant aux meilleurs résultats possibles.</resume>
			<mots_cles>étiquetage morpho-syntaxique, analyse en dépendance, ancien français, apprentissage automatique, exploration de corpus</mots_cles>
			<title>Old French parsing : Which language properties have the greatest inﬂuence on learning quality ?</title>
			<abstract>This paper presents machine learning experiments for part-of-speech labelling and dependency parsing of Old French. Machine learning methods are used for the purpose of corpus exploration. The SRCMF Treebank is our reference data. The poorly standardised nature of the language used in this corpus implies that training data is heterogenous and quantitatively limited. We explore various strategies, based on different criteria (variability of the lexicon, Verse/Prose form, date of writing) to build training corpora leading to the best possible results.</abstract>
			<keywords>POS labelling, Dependency Parsing, Old French, machine learning, corpus exploration</keywords>
		</article>	
		<article id="taln-2015-long-018" session="Classification et Alignement">
			<auteurs>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Brixtel</nom>
					<email>romain.brixtel@unil.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Charlotte</prenom>
					<nom>Lecluze</nom>
					<email>charlotte.lecluze@unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gaël</prenom>
					<nom>Lejeune</nom>
					<email>gael.lejeune@univ-nantes.fr</email>
					<affiliationId>3</affiliationId></auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lausanne – HEC - Département de comportement organisationnel, Quartier Dorigny, 1015 Lausanne, Suisse</affiliation>
				<affiliation affiliationId="2">GREYC, Campus Côte de Nacre, Boulevard du Maréchal Juin, 14032 CAEN cedex 5, France</affiliation>
				<affiliation affiliationId="3">LINA, 2 rue de la Houssinière, 44322 Nantes, France</affiliation>
			</affiliations>
			<titre>Attribution d'Auteur : approche multilingue fondée sur les répétitions maximales</titre>
			<type>long</type>
			<pages>208-219</pages>
			<resume>Cet article s'attaque à la tâche d'Attribution d'Auteur en contexte multilingue. Nous proposons une alternative aux méthodes supervisées fondées sur les n-grammes de caractères de longueurs variables : les répétitions maximales. Pour un texte donné, la liste de ses n-grammes de caractères contient des informations redondantes. A contrario, les répétitions maximales représentent l'ensemble des répétitions de ce texte de manière condensée. Nos expériences montrent que la redondance des n-grammes contribue à l'efficacité des techniques d'Attribution d'Auteur exploitant des sous-chaînes de caractères. Ce constat posé, nous proposons une fonction de pondération sur les traits donnés en entrée aux classifieurs, en introduisant les répétitions maximales du nème ordre (c'est-à-dire des répétitions maximales détectées dans un ensemble de répétitions maximales). Les résultats expérimentaux montrent de meilleures performances avec des répétitions maximales, avec moins de données que pour les approches fondées sur les n-grammes.</resume>
			<mots_cles>attribution d'auteur, multilinguisme, classification, chaînes de caractères, répétitions maximales</mots_cles>
			<title>Authorship Attribution through Character Substrings (and vise versa)</title>
			<abstract>This article tackles the Authorship Attribution task according to the language independence issue. We propose an alternative of variable length character n-gram features in supervised methods : maximal repeats in strings. When character n-grams are by essence redundant, maximal repeats are a condensed way to represent any substring of a corpus. Our experiments show that the redundant aspect of character n-grams contributes to the efficiency of character-based Authorship Attribution techniques. Therefore, we introduce a new way to weight features in vector based classifier by introducing n-th order maximal repeats (maximal repeats detected in a set of maximal repeats). The experimental results show higher performance with maximal repeats, with less data than n-grams based approach.</abstract>
			<keywords>authorship attribution, multilinguism, classification, character substrings, maximal repeats</keywords>
		</article>
		<article id="taln-2015-long-019" session="Classification et Alignement">
			<auteurs>
				<auteur>
					<prenom>Hai Hieu</prenom>
					<nom>Vu</nom>
					<email>hai-hieu.vu@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jeanne</prenom>
					<nom>Villaneau</nom>
					<email>Jeanne.Villaneau@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Farida</prenom>
					<nom>Saïd</nom>
					<email>Farida.Said@univ-ubs.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre-François</prenom>
					<nom>Marteau</nom>
					<email>pierre-francois.marteau@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>											
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Bretagne Sud, laboratoire IRISA</affiliation>
				<affiliation affiliationId="2">Université de Bretagne Sud, laboratoire LMBA</affiliation>	
			</affiliations>
			<titre>Mesurer la similarité entre phrases grâce à Wikipédia en utilisant une indexation aléatoire</titre>
			<type>long</type>
			<pages>220-231</pages>
			<resume>Cet article présente une méthode pour mesurer la similarité sémantique entre phrases qui utilise Wikipédia comme unique ressource linguistique et qui est, de ce fait, utilisable pour un grand nombre de langues. Basée sur une représentation vectorielle, elle utilise une indexation aléatoire pour réduire la dimension des espaces manipulés. En outre, elle inclut une technique de calcul des vecteurs de termes qui corrige les défauts engendrés par l'utilisation d'un corpus aussi général que Wikipédia. Le système a été évalué sur les données de SemEval 2014 en anglais avec des résultats très encourageants, au-dessus du niveau moyen des systèmes en compétition. Il a également été testé sur un ensemble de paires de phrases en français, à partir de ressources que nous avons construites et qui seront mises à la libre disposition de la communauté scientifique.</resume>
			<mots_cles>Similarité sémantique, Indexation aléatoire, Wikipédia, Relation sémantique</mots_cles>
			<title>Semantic similarity between sentences based on Wikipedia and Random Indexing</title>
			<abstract>This paper proposes a semantic similarity measure for sentence comparison based on the exploitation of Wikipedia as the only language resource. Such similarity measure is therefore usable for a wide range of languages, basically those covered by Wikipedia. Random Indexing is used to cope with the great dimensionality and the spareness of the data vectorial representations. Furthermore, a statistical weight function is used to reduce the noise generated by the use of a multi domain corpus such as Wikipedia. This semantic similarity measure has been evaluated on SemEval 2014 dataset for English language leading to very promising results, basically above the average level of the competing systems that exploit Wikipédia in conjunction with other sources of semantic information. It has been also evaluated on a set of pairs of sentences in French that we have build specifically for the task, and made freely available for the research community.</abstract>
			<keywords>Semantic Textual Similarity, Random indexing, Wikipédia, Semantic Relatedness</keywords>
		</article>	
		<article id="taln-2015-long-020" session="Classification et Alignement">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>blache@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Grégroie</prenom>
					<nom>de Montcheuil</nom>
					<email>gregoire.montcheuil@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Rauzy</nom>
					<email>stephane.rauzy@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix-Marseille Université &amp; CNRS, LPL, Aix-en-Provence</affiliation>
				<affiliation affiliationId="2">Equipex ORTOLANG</affiliation>						
			</affiliations>
			<titre>Typologie automatique des langues à partir de treebanks</titre>
			<type>long</type>
			<pages>232-243</pages>
			<resume>La typologie des langues repose sur l'étude de la réalisation de propriétés ou phénomènes linguistiques dans plusieurs langues ou familles de langues. Nous abordons dans cet article la question de la typologie syntaxique et proposons une méthode permettant d'extraire automatiquement ces propriétés à partir de treebanks, puis de les analyser en vue de dresser une telle typologie. Nous décrivons cette méthode ainsi que les outils développés pour la mettre en œuvre. Celle-ci a été appliquée à l'analyse de 10 langues décrites dans le Universal Dependencies Treebank. Nous validons ces résultats en montrant comment une technique de classification permet, sur la base des informations extraites, de reconstituer des familles de langues.</resume>
			<mots_cles>Typologie, syntaxe, treebank, inférence de grammaire, Grammaire de Propriétés</mots_cles>
			<title>Automatic Linguistic Typology from Treebanks</title>
			<abstract>Linguistic typology studies different linguistic properties or phenomena in order to compare several languages or language families. We address in this paper the question of syntactic typology and propose a method for extracting automatically from treebanks syntactic properties, and bring them into a typology perspective. We present here the method and the different tools for inferring such information. The approach has been applied to 10 languages of the Universal Dependencies Treebank. We validate the results in showing how automatic classification corrélâtes with language families.</abstract>
			<keywords>Typology, syntax, grammar inference, Property Grammars</keywords>
		</article>	
		<article id="taln-2015-long-021" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email>Laurent.Besacier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benjamin</prenom>
					<nom>Lecouteux</nom>
					<email>Benjamin.Lecouteux@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Luong</prenom>
					<nom>Ngoc Quang</nom>
					<email>quangngocluong@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG, Univ. Grenoble-Alpes, France</affiliation>
			</affiliations>
			<titre>Utilisation de mesures de confiance pour améliorer le décodage en traduction de parole</titre>
			<type>long</type>
			<pages>244-254</pages>
			<resume>Les mesures de confiance au niveau mot (Word Confidence Estimation - WCE) pour la traduction auto- matique (TA) ou pour la reconnaissance automatique de la parole (RAP) attribuent un score de confiance à chaque mot dans une hypothèse de transcription ou de traduction. Dans le passé, l'estimation de ces mesures a le plus souvent été traitée séparément dans des contextes RAP ou TA. Nous proposons ici une estimation conjointe de la confiance associée à un mot dans une hypothèse de traduction automatique de la parole (TAP). Cette estimation fait appel à des paramètres issus aussi bien des systèmes de transcription de la parole (RAP) que des systèmes de traduction automatique (TA). En plus de la construction de ces estimateurs de confiance robustes pour la TAP, nous utilisons les informations de confiance pour re-décoder nos graphes d'hypothèses de traduction. Les expérimentations réalisées montrent que l'utilisation de ces mesures de confiance au cours d'une seconde passe de décodage permettent d'obtenir une amélioration significative des performances de traduction (évaluées avec la métrique BLEU - gains de deux points par rapport à notre système de traduc- tion de parole de référence). Ces expériences sont faites pour une tâche de TAP (français-anglais) pour laquelle un corpus a été spécialement conçu (ce corpus, mis à la disposition de la communauté TALN, est aussi décrit en détail dans l'article).</resume>
			<mots_cles>Mesures de confiance, traduction automatique de la parole, paramètres joints, redécodage de graphe</mots_cles>
			<title>Word confidence estimation for re-decoding speech translation graphs</title>
			<abstract>Word Confidence Estimation (WCE) for machine translation (MT) or automatic speech recognition (ASR) assigns a confidence score to each word in the MT or ASR hypothesis. In the past, this task has been treated separately in ASR or MT contexts and we propose here a joint estimation of word confidence for a spoken language translation (SLT) task involving both ASR and MT. We build robust word confidence estimators for SLT, based on joint ASR and MT features. Using these word confidence measures to re-decode the spoken language translation graph leads to a significant BLEU improvement (2 points) compared to the SLT baseline. These experiments are done for a French-English SLT task for which a corpus was specifically designed (this corpus being made available to the NLP community).</abstract>
			<keywords>Word confidence estimation (WCE), spoken language translation (SLT), joint features, search graph re-decoding</keywords>
		</article>
		<article id="taln-2015-long-022" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Kraif</nom>
					<email>Olivier.Kraif@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIDILEM, Univ. Grenoble Alpes, BP 25, 38040 Grenoble cedex 9</affiliation>
			</affiliations>
			<titre>Multi­alignement vs bi­alignement : à plusieurs, c'est mieux !</titre>
			<type>long</type>
			<pages>255-266</pages>
			<resume>Dans cet article, nous proposons une méthode originale destinée à effectuer l'alignement d'un corpus multi­parallèle, i.e. comportant plus de deux langues, en prenant en compte toutes les langues simultanément (et non en composant une série de bi­alignements indépendants). Pour ce faire, nous nous appuyons sur les réseaux de correspondances lexicales constitués par les transfuges (chaînes identiques) et cognats (mots apparentés), et nous montrons comment divers tuilages des couples de langues permettent d'exploiter au mieux les ressemblances superficielles liées aux relations génétiques interlinguistiques. Nous évaluons notre méthode par rapport à une méthode de bi­alignement classique, et montrons en quoi le multi­alignement permet d'obtenir des résultats à la fois plus précis et plus robustes.</resume>
			<mots_cles>Alignement multilingue, corpus parallèles, cognats</mots_cles>
			<title>Multi­alignment vs bi­alignment: the more languages the better</title>
			<abstract>In this paper, we propose an original method for performing the alignment of a multi­parallel corpus, ie a parallel corpus involving more than two languages, taking into account all the languages simultaneously (and not by merging a series of independent bi­alignments). To do this, we rely on the networks of lexical correspondences formed by identical chains and cognates (related words, and we show how various tiling of language pairs allow to exploit the surface similarities due to genetic relationships between languages. We evaluate our method compared to a conventional method of bi­alignment, and show how the multi­alignement achieves both more accurate and robust results.</abstract>
			<keywords>Multilingual alignment, parallel corpora, cognates</keywords>
		</article>
		<article id="taln-2015-long-023" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Quoc-Khanh</prenom>
					<nom>Do</nom>
					<email>dokhanh@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Allauzen</nom>
					<email>allauzen@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI/CNRS, rue John von Neumann, Campus Universitaire Orsay 91 403 Orsay</affiliation>
				<affiliation affiliationId="2">Université Paris Sud, 91 403 Orsay</affiliation>														
			</affiliations>
			<titre>Apprentissage discriminant des modèles continus de traduction</titre>
			<type>long</type>
			<pages>267-278</pages>
			<resume>Alors que les réseaux neuronaux occupent une place de plus en plus importante dans le traitement automatique des langues, les méthodes d'apprentissage actuelles utilisent pour la plupart des critères qui sont décorrélés de l'application. Cet article propose un nouveau cadre d'apprentissage discriminant pour l'estimation des modèles continus de traduction. Ce cadre s'appuie sur la définition d'un critère d'optimisation permettant de prendre en compte d'une part la métrique utilisée pour l'évaluation de la traduction et d'autre part l'intégration de ces modèles au sein des systèmes de traduction automatique. De plus, cette méthode d'apprentissage est comparée aux critères existants d'estimation que sont le maximum de vraisemblance et l'estimation contrastive bruitée. Les expériences menées sur la tâches de traduction des séminaires TED Talks de l'anglais vers le français montrent la pertinence d'un cadre discriminant d'apprentissage, dont les performances restent toutefois très dépendantes du choix d'une stratégie d'initialisation idoine. Nous montrons qu'avec une initialisation judicieuse des gains significatifs en termes de scores BLEU peuvent être obtenus.</resume>
			<mots_cles>Modèle neuronal de traduction, traduction automatique par approche statistique, apprentissage discriminant</mots_cles>
			<title>Discriminative Learning of Continuous Translation Models</title>
			<abstract>This paper proposes a new discriminative framework to train translation models based on neural network. This framework relies on the definition of a new objective function that allows us to introduce the evaluation metric in the learning process as well as to consider how the model interacts with the translation system. Moreover, this approach is compared with the state of the art estimation methods, such as the maximum likelihood criterion and the noise contrastive estimation. Experiments are carried out on the English to French translation task of TED Talks . The results show the efficiency of the proposed approach, whereas the initialization has a strong impact. We show that with a tailored initialization scheme significant improvements can be obtained in terms of BLEU scores.</abstract>
			<keywords>Neural network based translation model, statistical machine translation, discriminative learning</keywords>
		</article>	
		<article id="taln-2015-long-024" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Amel</prenom>
					<nom>Fraisse</nom>
					<email>fraisse@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Paroubek</nom>
					<email>pap@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Bât. 508 Université Paris-Sud, 91403 Orsay Cedex, France</affiliation>								
			</affiliations>
			<titre>Utiliser les interjections pour détecter les émotions</titre>
			<type>long</type>
			<pages>279-292</pages>
			<resume>Bien que les interjections soient un phénomène linguistique connu, elles ont été peu étudiées et cela continue d'être le cas pour les travaux sur les microblogs. Des travaux en analyse de sentiments ont montré l'intérêt des émoticônes et récemment des mots-dièses, qui s'avèrent être très utiles pour la classification en polarité. Mais malgré leur statut grammatical et leur richesse sémantique, les interjections sont restées marginalisées par les systèmes d'analyse de sentiments. Nous montrons dans cet article l'apport majeur des interjections pour la détection des émotions. Nous détaillons la production automatique, basée sur les interjections, d'un corpus étiqueté avec les émotions. Nous expliquons ensuite comment nous avons utilisé ce corpus pour en déduire, automatiquement, un lexique affectif pour le français. Ce lexique a été évalué sur une tâche de détection des émotions, qui a montré un gain en mesure F1 allant, selon les émotions, de +0,04 à +0,21.</resume>
			<mots_cles>Interjections, détection des émotions, lexique affectif, analyse de sentiments, fouille d'opinions</mots_cles>
			<title>Using interjections for emotion detection</title>
			<abstract>Although interjections have been recognized as linguistic phenomena for a long time, they have somehow been rarely studied and continue to be left aside in works dealing with microblogs. Users of this new kind of communication plateforms have popularized widely the use of linguistic constructions, like emoticons or interjections. In spite of their grammatic status and semantic richness for describing emotional states, interjections have been mostly ignored. In this article we show the importance of the role that interjections can play for detecting emotions. We detail how using interjections we have tagged automatically a French microblog corpus with emotion labels. Then we describe how we did deduce automatically from this corpus a fine-grained affective lexicon. The usefulness of the lexicon was evaluated in an emotion recognition task where, depending on the emotion, the F1-measure improvement ranged from +0.04 to +0.21.</abstract>
			<keywords>Interjections, emotion recognition, affective lexicon, sentiment analysis, opinion mining</keywords>
		</article>	
		<article id="taln-2015-long-025" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Maximin</prenom>
					<nom>Coavoux</nom>
					<email>maximin.coavoux@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Crabbé</nom>
					<email>bcrabbe@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, INRIA, Université Paris Diderot, Place Paul Ricœur, 75013 Paris</affiliation>
				<affiliation affiliationId="2">Institut Universitaire de France</affiliation>					
			</affiliations>
			<titre>Comparaison d'architectures neuronales pour l'analyse syntaxique en constituants</titre>
			<type>long</type>
			<pages>293-304</pages>
			<resume>L'article traite de l'analyse syntaxique lexicalisée pour les grammaires de constituants. On se place dans le cadre de l'analyse par transitions. Les modèles statistiques généralement utilisés pour cette tâche s'appuient sur une représentation non structurée du lexique. Les mots du vocabulaire sont représentés par des symboles discrets sans liens entre eux. À la place, nous proposons d'utiliser des représentations denses du type plongements (embeddings) qui permettent de modéliser la similarité entre symboles, c'est-à-dire entre mots, entre parties du discours et entre catégories syntagmatiques. Nous proposons d'adapter le modèle statistique sous-jacent à ces nouvelles représentations. L'article propose une étude de 3 architectures neuronales de complexité croissante et montre que l'utilisation d'une couche cachée non-linéaire permet de tirer parti des informations données par les plongements.</resume>
			<mots_cles>Analyse syntaxique en constituants lexicalisée, plongements, réseaux de neurones</mots_cles>
			<title>A Comparison of Neural Network Architectures for Constituent Parsing</title>
			<abstract>The article deals with lexicalized constituent parsing in a transition-based framework. Typical statistical approaches for this task are based on an unstructured representation of the lexicon. Words are represented by discrete unrelated symbols. Instead, our proposal relies on dense vector representations (embeddings) that are able to encode similarity between symbols : words, part-of-speech tags and phrase structure symbols. The article studies and compares 3 increasingly complex neural network architectures, which are fed symbol embeddings. The experiments suggest that the information given by embeddings is best captured by a deep architecture with a non-linear layer.</abstract>
			<keywords>Lexicalized constituent parsing, embeddings, neural networks</keywords>
		</article>	
		<article id="taln-2015-long-026" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email>natalia.grabar@univ-lille3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Iris</prenom>
					<nom>Eshkol</nom>
					<email>iris.eshkol@univ-orleans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS UMR 8163 STL, Université Lille 3, 59653 Villeneuve d'Ascq, France</affiliation>
				<affiliation affiliationId="2">CNRS UMR 7270 LLL, Université d'Orléans, 45100 Orléans, France</affiliation>							
			</affiliations>
			<titre>...des conférences enfin disons des causeries... Détection automatique de segments en relation de paraphrase dans les reformulations de corpus oraux</titre>
			<type>long</type>
			<pages>305-316</pages>
			<resume>Notre travail porte sur la détection automatique des segments en relation de reformulation paraphrastique dans les corpus oraux. L'approche proposée est une approche syntagmatique qui tient compte des marqueurs de reformulation paraphrastique et des spécificités de l'oral. Les données de référence sont consensuelles. Une méthode automatique fondée sur l'apprentissage avec les CRF est proposée afin de détecter les segments paraphrasés. Différents descripteurs sont exploités dans une fenêtre de taille variable. Les tests effectués montrent que les segments en relation de paraphrase sont assez difficiles à détecter, surtout avec leurs frontières correctes. Les meilleures moyennes atteignent 0,65 de F-mesure, 0,75 de précision et 0,63 de rappel. Nous avons plusieurs perspectives à ce travail pour améliorer la détection des segments en relation de paraphrase et pour étudier les données depuis d'autres points de vue.</resume>
			<mots_cles>Corpus oraux, Paraphrase, Reformulation, Marqueur de reformulation paraphrastique, Apprentissage supervisé</mots_cles>
			<title>...des conférences enfin disons des causeries... Automatic detection of segments with paraphrase relation in spoken corpora rephrasings</title>
			<abstract>Our work addresses automatic detection of segments with paraphrastic rephrasing relation in spoken corpus. The proposed approach is syntagmatic. It is based on paraphrastic rephrasing markers and the specificities of the spoken language. The reference data used are consensual. Automatic method based on machine learning using CRFs is proposed in order to detect the segments that are paraphrased. Different descriptors are exploited within a window with various sizes. The tests performed indicate that the segments that are in paraphrastic relation are quite difficult to detect. Our best average reaches up to 0.65 F-measure, 0.75 precision, and 0.63 recall. We have several perspectives to this work for improving the detection of segments that are in paraphrastic relation and for studying the data from other points of view.</abstract>
			<keywords>Spoken Corpora, Paraphrase, Reformulation, Paraphrastic Reformulation Marker, Supervised Learning</keywords>
		</article>
		<article id="taln-2015-court-001" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Tian</prenom>
					<nom>Xia</nom>
					<email>SummerRainET2008@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Shaodan</prenom>
					<nom>Zhai</nom>
					<email>zhai.6@wright.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Zhongliang</prenom>
					<nom>Li</nom>
					<email>li.141@wright.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Shaojun</prenom>
					<nom>Wang</nom>
					<email>shaojun.wang@wright.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Wright State University</affiliation>
			</affiliations>
			<titre>Une méthode discriminant formation simple pour la traduction automatique avec Grands Caractéristiques</titre>
			<type>court</type>
			<pages>317-322</pages>
			<resume>Marge infusé algorithmes détendus (MIRAS) dominent modèle de tuning dans la traduction automatique statistique dans le cas des grandes caractéristiques de l'échelle, mais ils sont également célèbres pour la complexité de mise en œuvre. Nous introduisons une nouvelle méthode, qui concerne une liste des N meilleures comme une permutation et minimise la perte Plackett-Luce de permutations rez-de-vérité. Des expériences avec des caractéristiques à grande échelle démontrent que, la nouvelle méthode est plus robuste que MERT ; si ce est seulement à rattacher avec Miras, il a un avantage comparativement, plus facile à mettre en œuvre.</resume>
			<mots_cles>Traduction automatique, ajustement du modèle, caractéristiques à grande échelle</mots_cles>
			<title>A Simple Discriminative Training Method for Machine Translation with Large-Scale Features</title>
			<abstract>The margin infused relaxed algorithm (MIRAs) dominates model tuning in statistical machine translation in the case of large scale features, but also they are famous for the complexity in implementation. We introduce a new method, which regards an N-best list as a permutation and minimizes the Plackett-Luce loss of ground-truth permutations. Experiments with large-scale features demonstrate that, the new method is more robust than MERT ; though it is only matchable with MIRAs, it has a comparatively advantage, easier to implement.</abstract>
			<keywords>machine translation, model tuning, large-scale features</keywords>
		</article>
		<article id="taln-2015-court-002" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Stergios</prenom>
					<nom>Chatzikyriakidis</nom>
					<email>kafouroutsos@hotmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, University of Montpellier</affiliation>
			</affiliations>
			<titre>Natural Language Reasoning using Coq: Interaction and Automation</titre>
			<type>court</type>
			<pages>323-329</pages>
			<resume>Dans cet article, nous présentons une utilisation des assistants des preuves pour traiter l'inférence en Language Naturel (NLI). D' abord, nous proposons d'utiliser les theories des types modernes comme langue dans laquelle traduire la sémantique du langage naturel. Ensuite, nous implémentons cette sémantique dans l'assistant de preuve Coq pour raisonner sur ceux-ci. En particulier, nous évaluons notre proposition sur un sous-ensemble de la suite de tests FraCas, et nous montrons que 95.2% des exemples peuvent être correctement prédits. Nous discutons ensuite la question de l'automatisation et il est démontré que le langage de tactiques de Coq permet de construire des tactiques qui peuvent automatiser entièrement les preuves, au moins pour les cas qui nous intéressent.</resume>
			<mots_cles>Inference en Langage Naturel, Théorie des Types, Sémantique Formelle, FraCas, Coq, Automatisation des Preuves</mots_cles>
			<title></title>
			<abstract>In this paper, we present the use of proof-assistant technology in order to deal with Natural Language Inference. We first propose the use of modern type theories as the language in which we translate natural language semantics to. Then, we implement these semantics in the proof-assistant Coq in order to reason about them. In particular we evaluate against a subset of the FraCas test suite and show a 95.2% accuracy and also precision levels that outperform existing approaches at least for the comparable parts. We then discuss the issue of automation, showing that Coq's tactical language allows one to build tactics that can fully automate proofs, at least for the cases we have looked at.</abstract>
			<keywords> Natural Language Inference, Type Theory, Formal Semantics, FraCas, Coq, Proof automation</keywords>
		</article>
		<article id="taln-2015-court-003" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>mathieu.lafourcade@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Le Brun</nom>
					<email>imaginat@imaginat.name</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alain</prenom>
					<nom>Joubert</nom>
					<email>joubert@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lirmm, Université Montpellier, France</affiliation>
				<affiliation affiliationId="2">Imagin@t, 34400 Lunel, France</affiliation>
			</affiliations>
			<titre>Vous aimez ?...ou pas ? LikeIt, un jeu pour construire une ressource lexicale de polarité</titre>
			<type>court</type>
			<pages>330-336</pages>
			<resume>En analyse de discours ou d'opinion, savoir caractériser la connotation générale d'un texte, les sentiments qu'il véhicule, est une aptitude recherchée, qui suppose la constitution préalable d'une ressource lexicale de polarité. Au sein du réseau lexical JeuxDeMots, nous avons mis au point LikeIt, un jeu qui permet d'affecter une valeur positive, négative, ou neutre à un terme, et de constituer ainsi pour chaque terme, à partir des votes, une polarité résultante. Nous présentons ici l'analyse quantitative des données de polarité obtenues, ainsi que la méthode pour les valider qualitativement.</resume>
			<mots_cles>polarité, sentiment, réseau lexical, crowdsourcing, GWAP</mots_cles>
			<title>Do you like it? or not? LikeIt, a game to build a polarity lexical resource</title>
			<abstract>The ability to analyze the feelings that emerge from a text requires having a polarity lexical resource. In the lexical network JeuxDeMots we designed LikeIt, a GWAP that allows attributing a positive, negative or neutral value to a term, and thus obtaining for each term a resulting polarity. We present a quantitative analysis of polarity data obtained, together with the comparison method we developed to validate them qualitatively.</abstract>
			<keywords>polarity, feelings, lexical network, crowdsourcing, GWAP</keywords>
		</article>
		<article id="taln-2015-court-004" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Morlane-Hondère</nom>
					<email>morlanehondere@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cyril</prenom>
					<nom>Grouin</nom>
					<email>cyril.grouin@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email>pz@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI–CNRS, UPR 3251, rue John von Neumann, 91400 Orsay</affiliation>
			</affiliations>
			<titre>Étude des verbes introducteurs de noms de médicaments dans les forums de santé</titre>
			<type>court</type>
			<pages>337-343</pages>
			<resume>Dans cet article, nous combinons annotations manuelle et automatique pour identifier les verbes utilisés pour introduire un médicament dans les messages sur les forums de santé. Cette information est notamment utile pour identifier la relation entre un médicament et un effet secondaire. La mention d'un médicament dans un message ne garantit pas que l'utilisateur a pris ce traitement mais qu'il effectue un retour. Nous montrons ensuite que ces verbes peuvent servir pour extraire automatiquement des variantes de noms de médicaments. Nous estimons que l'analyse de ces variantes pourrait permettre de modéliser les erreurs faites par les usagers des forums lorsqu'ils écrivent les noms de médicaments, et améliorer en conséquence les systèmes de recherche d'information.</resume>
			<mots_cles>contenu généré par l'utilisateur, forum, verbes, noms de médicaments</mots_cles>
			<title>Study of Drug-Introducing Verbs on Health Forums</title>
			<abstract>In this paper, we combine manual/automatic annotation to identify the verbs used by the users of a health forum to say that they are taking a drug. This information is important in many aspects, one of them being the identification of the relation between drugs and side effects : the mere mention of a drug in a message is not enough to assess that the user is taking this drug, and is thus likely to provide a feedback on it. In a second part of the study, we show how the set of verbs that we identified can be used to automatically extract variants of drug names. We assume that the analysis of the variants could shed light on patterns of mistakes that users make when spelling drug names and thus, improve medical information retrieval systems.</abstract>
			<keywords>user-generated content, forum, verbs, drug names</keywords>
		</article>
		<article id="taln-2015-court-005" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Morchid</nom>
					<email>mohamed.morchid@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Dufour</nom>
					<email>richard.dufour@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Georges</prenom>
					<nom>Linarès</nom>
					<email>georges.linares@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Initialisation de Réseaux de Neurones à l'aide d'un Espace Thématique</titre>
			<type>court</type>
			<pages>344-349</pages>
			<resume>Ce papier présente une méthode de traitement de documents parlés intégrant une représentation fondée sur un espace thématique dans un réseau de neurones artificiels (ANN) employé comme classifieur de document. La méthode proposée consiste à configurer la topologie d'un ANN ainsi que d'initialiser les connexions de celui-ci à l'aide des espaces thématiques appris précédemment. Il est attendu que l'initialisation fondée sur les probabilités thématiques permette d'optimiser le processus d'optimisation des poids du réseau ainsi qu'à accélérer la phase d'apprentissage tout en amélioration la précision de la classification d'un document de test. Cette méthode est évaluée lors d'une tâche de catégorisation de dialogues parlés entre des utilisateurs et des agents du service d'appels de la Régie Autonome Des Transports Parisiens (RATP). Les résultats montrent l'intérêt de la méthode proposée d'initialisation d'un réseau, avec un gain observé de plus de 4 points en termes de bonne classification comparativement à l'initialisation aléatoire. De plus, les expérimentations soulignent que les performances sont faiblement dépendantes de la topologie du ANN lorsque les poids de la couche cachée sont initialisés au moyen des espaces de thèmes issus d'une allocation latente de Dirichlet ou latent Dirichlet Allocation (LDA) en comparaison à une initialisation empirique.</resume>
			<mots_cles>Réseau de neurones artificiels, Allocation latente de Dirichlet, Initialisation de poids</mots_cles>
			<title>Neural Network Initialization using a Topic Space</title>
			<abstract>This paper presents a method for speech analytics that integrates topic-space based representation into an artificial neural network (ANN), working as a document classifier. The proposed method consists in configuring the ANN's topology and in initializing the weights according to a previously estimated topic-space. Setup based on thematic priors is expected to improve the efficiency of the ANN's weight optimization process, while speeding-up the training process and improving the classification accuracy. This method is evaluated on a spoken dialogue categorization task which is composed of customer-agent dialogues from the call-centre of Paris Public Transportation Company. Results show the interest of the proposed setup method, with a gain of more than 4 points in terms of classification accuracy, compared to the baseline. Moreover, experiments highlight that performance is weakly dependent to ANN's topology with the LDA-based configuration, in comparison to classical empirical setup.</abstract>
			<keywords>Artificial neural network, Latent Dirichlet allocation, Weights initialization</keywords>
		</article>
		<article id="taln-2015-court-006" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Jacques</prenom>
					<nom>Steinlin</nom>
					<email>jacques.steinlin@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Margot</prenom>
					<nom>Colinet</nom>
					<email>margotcolinet@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>laurence.danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, INRIA et Université Paris Diderot, 75013 Paris</affiliation>
				<affiliation affiliationId="2">IUF</affiliation>
			</affiliations>
			<titre>FDTB1: Repérage des connecteurs de discours en corpus</titre>
			<type>court</type>
			<pages>350-356</pages>
			<resume>Cet article présente le repérage manuel des connecteurs de discours dans le corpus FTB (French Treebank) déjà annoté pour la morpho-syntaxe. C'est la première étape de l'annotation discursive complète de ce corpus. Il s'agit de projeter sur le corpus les éléments répertoriés dans LexConn, lexique des connecteurs du français, et de filtrer les occurrences de ces éléments qui n'ont pas un emploi discursif mais par exemple un emploi d'adverbe de manière ou de préposition introduisant un complément sous-catégorisé. Plus de 10 000 connecteurs ont ainsi été repérés.</resume>
			<mots_cles>connecteurs de discours, annotation discursive de corpus, grammaire et discours</mots_cles>
			<title>FDTB1 : Identification of discourse connectives in a French corpus</title>
			<abstract>This paper presents the manual identification of discourse connectives in the corpus FTB (French Treebank) already annotated for morpho-syntax. This is the first step in the full discursive annotation of this corpus. The method consists in projecting on the corpus the items that are listed in LexConn, a lexicon of French connectives, and then filtering the occurrences of these elements that do not have a discursive use. More than 10K connectives have been identified.</abstract>
			<keywords>discourse connectives, discourse annotation, grammar and discourse</keywords>
		</article>
		<article id="taln-2015-court-007" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Bossard</nom>
					<email>aurelien.bossard@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Rodrigues</nom>
					<email>christophe.rodrigues@lipn.univ-paris13.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 8, Laboratoire d'Informatique Avancée de Saint-Denis</affiliation>
				<affiliation affiliationId="2">CNRS, UMR 7030, Laboratoire d'Informatique de Paris Nord</affiliation>
			</affiliations>
			<titre>ROBO, an edit distance for sentence comparison Application to automatic summarization</titre>
			<type>court</type>
			<pages>357-363</pages>
			<resume>Dans cet article, nous proposons une mesure de distance entre phrases fondée sur la distance de Levenshtein, doublement pondérée par la fréquence des mots et par le type d'opération réalisée. Nous l'évaluons au sein d'un système de résumé automatique dont la méthode de calcul est volontairement limitée à une approche fondée sur la similarité entre phrases. Nous sommes donc ainsi en mesure d'évaluer indirectement la performance de cette nouvelle mesure de distance.</resume>
			<mots_cles>résumé automatique, similarité sémantique, distance d'édition</mots_cles>
			<title></title>
			<abstract>We here propose a sentence edit distance metric, ROBO, based on Levenshtein distance. This metric distance is weighted by words frequency and operation type. We apply ROBO on an automatic summarization system whose sentence selection metrics are on purpose restricted to sentence similarity approaches. ROBO performance can then be evaluated indirectly.</abstract>
			<keywords>automatic summarization, semantic similarity, edit distance</keywords>
		</article>
		<article id="taln-2015-court-008" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Collin</nom>
					<email>olivier.collin@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aleksandra</prenom>
					<nom>Guerraz</nom>
					<email>aleksandra.guerraz@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs - 2, avenue Pierre Marzin, 22307 Lannion Cedex, France</affiliation>
			</affiliations>
			<titre>Classification d'entités nommées de type « film »</titre>
			<type>court</type>
			<pages>364-370</pages>
			<resume>Dans cet article, nous nous intéressons à la classification contextuelle d'entités nommées de type « film ». Notre travail s'inscrit dans un cadre applicatif dont le but est de repérer, dans un texte, un titre de film contenu dans un catalogue (par exemple catalogue de films disponibles en VoD). Pour ce faire, nous combinons deux approches : nous partons d'un système à base de règles, qui présente une bonne précision, que nous couplons avec un modèle de langage permettant d'augmenter le rappel. La génération peu coûteuse de données d'apprentissage pour le modèle de langage à partir de Wikipedia est au coeur de ce travail. Nous montrons, à travers l'évaluation de notre système, la difficulté de classification des entités nommées de type « film » ainsi que la complémentarité des approches que nous utilisons pour cette tâche.</resume>
			<mots_cles>reconnaissance d'entités nommées, films, classification, règles, modèle de langage, Wikipedia</mots_cles>
			<title>Named Entity Classification for Movie Titles</title>
			<abstract>In this article, we focus on contextual classification of named entities for « movie » type. Our work is part of an application framework which aims to identify, in a text, a movie title contained in a catalog (e.g. VoD catalog). To do this, we combine two approaches : we use a rule-based system, which has good accuracy. To increase recall we couple our system with a language model. The generation of training data for the language model from Wikipedia is a crucial part of this work. We show, through the evaluation of our system, the complementarity of approaches we use.</abstract>
			<keywords>named entity recognition, movies, classification, rules, language model, Wikipedia</keywords>
		</article>
		<article id="taln-2015-court-009" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Natalie</prenom>
					<nom>Schluter</nom>
					<email>schluten@tcd.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Center for Language Technology, University of Copenhagen, Copenhagen, Denmark</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>371-376</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>A critical survey on measuring success in rank-based keyword assignment to documents</title>
			<abstract>Evaluation approaches for unsupervised rank-based keyword assignment are nearly as numerous as are the existing systems. The prolific production of each newly used metric (or metric twist) seems to stem from general dis-satisfaction with the previous one and the source of that dissatisfaction has not previously been discussed in the literature. The difficulty may stem from a poor specification of the keyword assignment task in view of the rank-based approach. With a more complete specification of this task, we aim to show why the previous evaluation metrics fail to satisfy researchers' goals to distinguish and detect good rank-based keyword assignment systems. We put forward a characterisation of an ideal evaluation metric, and discuss the consistency of the evaluation metrics with this ideal, finding that the average standard normalised cumulative gain metric is most consistent with this ideal.</abstract>
			<keywords>rank-based keyword assignment</keywords>
		</article>
		<article id="taln-2015-court-010" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Natalie</prenom>
					<nom>Schluter</nom>
					<email>schluten@tcd.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Center for Language Technology, University of Copenhagen, Copenhagen, Denmark</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>377-383</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Effects of Graph Generation for Unsupervised Non-Contextual Single Document Keyword Extraction</title>
			<abstract>This paper presents an exhaustive study on the generation of graph input to unsupervised graph-based non-contextual single document keyword extraction systems. A concrete hypothesis on concept coordination for documents that are scientific articles is put forward, consistent with two separate graph models : one which is based on word adjacency in the linear text–an approach forming the foundation of all previous graph-based keyword extraction methods, and a novel one that is based on word adjacency modulo their modifiers. In doing so, we achieve a best reported NDCG score to date of 0.431 for any system on the same data. In terms of a best parameter f-score, we achieve the highest reported to date (0.714) at a reasonable ranked list cut-off of n = 6, which is also the best reported f-score for any keyword extraction or generation system in the literature on the same data. The best-parameter f-score corresponds to a reduction in error of 12.6% conservatively.</abstract>
			<keywords>Keyword Extraction</keywords>
		</article>
		<article id="taln-2015-court-011" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Servan</nom>
					<email>christophe.servan@imag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marc</prenom>
					<nom>Dymetman</nom>
					<email>marc.dymetman@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG équipe GETALP, 41 rue des mathématiques, BP 53 38041 Grenoble Cedex 9</affiliation>
				<affiliation affiliationId="2">Xerox Research Centre Europe, 6 chemin de Maupertuis, 38240 Meylan</affiliation>
			</affiliations>
			<titre>Adaptation par enrichissement terminologique en traduction automatique statistique fondée sur la génération et le filtrage de bi-segments virtuels</titre>
			<type>court</type>
			<pages>384-390</pages>
			<resume>Nous présentons des travaux préliminaires sur une approche permettant d'ajouter des termes bilingues à un système de Traduction Automatique Statistique (TAS) à base de segments. Les termes sont non seulement inclus individuellement, mais aussi avec des contextes les englobant. Tout d'abord nous générons ces contextes en généralisant des motifs (ou patrons) observés pour des mots de même nature syntaxique dans un corpus bilingue. Enfin, nous filtrons les contextes qui n'atteignent pas un certain seuil de confiance, à l'aide d'une méthode de sélection de bi-segments inspirée d'une approche de sélection de données, précédemment appliquée à des textes bilingues alignés.</resume>
			<mots_cles>Traduction Automatique Statistique, Génération Automatique de Texte, contexte phrastique, terminologie bilingue</mots_cles>
			<title>Statistical machine translation adaptation through terminological enrichment based on virtual phrase generation and filtering</title>
			<abstract>We propose a technique for adding bilingual terms to a phrase-based SMT system which includes not only individual words, but also induces phrasal contexts around these words. We first generate these contexts by generalizing patterns observed for similar words in a bilingual corpus, but then filter out those contexts that fall below a certain confidence threshold, based on an original phrase-pair selection process inspired by existing sentence selection techniques.</abstract>
			<keywords>Statistical Machine Translation, Natural Language Generation, phrasal context, bilingual terminology</keywords>
		</article>
		<article id="taln-2015-court-012" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Mohamed Amine</prenom>
					<nom>Boukhaled</nom>
					<email>mohamed.boukhaled@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Francesca</prenom>
					<nom>Frontini</nom>
					<email>francesca.frontini@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Gabriel</prenom>
					<nom>Ganascia</nom>
					<email>jean-gabriel.ganascia@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIP6 (Laboratoire d'Informatique de Paris 6), Université Pierre et Marie Curie and CNRS (UMR7606),ACASA Team, 4, place Jussieu,75252-PARIS Cedex 05 (France)</affiliation>
			</affiliations>
			<titre>Une mesure d'intérêt à base de surreprésentation pour l'extraction des motifs syntaxiques stylistiques</titre>
			<type>court</type>
			<pages>391-396</pages>
			<resume>Dans cette contribution, nous présentons une étude sur la stylistique computationnelle des textes de la littérature classiques française fondée sur une approche conduite par données, où la découverte des motifs linguistiques intéressants se fait sans aucune connaissance préalable. Nous proposons une mesure objective capable de capturer et d'extraire des motifs syntaxiques stylistiques significatifs à partir d'un œuvre d'un auteur donné. Notre hypothèse de travail est fondée sur le fait que les motifs syntaxiques les plus pertinents devraient refléter de manière significative le choix stylistique de l'auteur, et donc ils doivent présenter une sorte de comportement de surreprésentation contrôlé par les objectifs de l'auteur. Les résultats analysés montrent l'efficacité dans l'extraction de motifs syntaxiques intéressants dans le texte littéraire français classique, et semblent particulièrement prometteurs pour les analyses de ce type particulier de texte.</resume>
			<mots_cles>Stylistique computationnelle, fouille de texte, motifs syntaxiques, mesure d'intérêt</mots_cles>
			<title>An Overrepresentation-based Interestingness Measure for Syntactic Stylistic Pattern Extraction</title>
			<abstract>In this contribution, we present a computational stylistic study of the French classic literature texts based on a data-driven approach where discovering interesting linguistic patterns is done without any prior knowledge. We propose an objective measure capable of capturing and extracting meaningful stylistic syntactic patterns from a given author's work. Our hypothesis is based on the fact that the most relevant syntactic patterns should significantly reflect the author's stylistic choice and thus they should exhibit some kind of overrepresentation behavior controlled by the author's purpose. The analysed results show the effectiveness in extracting interesting syntactic patterns from classic French literary text, and seem particularly promising for the analyses of such particular text.</abstract>
			<keywords>Computational stylistic, text mining, syntactic patterns, interestingness measure</keywords>
		</article>
		<article id="taln-2015-court-013" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Bossard</nom>
					<email>aurelien.bossard@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Rodrigues</nom>
					<email>christophe.rodrigues@lipn.univ-paris13.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 8, Laboratoire d'Informatique Avancée de Saint-Denis</affiliation>
				<affiliation affiliationId="2">CNRS, UMR 7030, Laboratoire d'Informatique de Paris Nord</affiliation>
			</affiliations>
			<titre>Une Approche évolutionnaire pour le résumé automatique</titre>
			<type>court</type>
			<pages>397-403</pages>
			<resume>Dans cet article, nous proposons une méthode de résumé automatique fondée sur l'utilisation d'un algorithme génétique pour parcourir l'espace des résumés candidats couplé à un calcul de divergence de distribution de probabilités de n-grammes entre résumés candidats et documents source. Cette méthode permet de considérer un résumé non plus comme une accumulation de phrases indépendantes les unes des autres, mais comme un texte vu dans sa globalité. Nous la comparons à une des meilleures méthodes existantes fondée sur la programmation linéaire en nombre entier, et montrons son efficacité sur le corpus TAC 2009.</resume>
			<mots_cles>Résumé automatique, algorithme génétique, modèles probabilistes</mots_cles>
			<title>Automatic summarization using a genetic algorithm</title>
			<abstract>This paper proposes a novel method for automatic summarization based on a genetic algorithm that explores candidate summaries space following an objective function computed over ngrams probability distributions of the candidate summary and the source documents. This method does not consider a summary as a stack of independant sentences but as a whole text. We compare this method to one of the best existing methods which is based on integer linear programming, and show its efficiency on TAC 2009 corpus.</abstract>
			<keywords>automatic summarization, genetic algorithm, probabilistic models</keywords>
		</article>
		<article id="taln-2015-court-014" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Soumia Lilia</prenom>
					<nom>Berrahou</nom>
					<email>berrahou@lirmm.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Buche</nom>
					<email>Patrice.Buche@supagro.inra.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Juliette</prenom>
					<nom>Dibie-Barthélemy</nom>
					<email>dibie@agroparistech.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Roche</nom>
					<email>mroche@lirmm.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, 161 rue Ada, Montpellier, France</affiliation>
				<affiliation affiliationId="2">IATE, 2 place Viala, Montpellier, France</affiliation>
				<affiliation affiliationId="3">MIA, 16 rue Claude Bernard, Paris, France</affiliation>
				<affiliation affiliationId="4">TETIS, 500 rue Jean-François Breton, Montpellier, France</affiliation>
			</affiliations>
			<titre>Identification des unités de mesure dans les textes scientifiques</titre>
			<type>court</type>
			<pages>404-410</pages>
			<resume>Le travail présenté dans cet article se situe dans le cadre de l'identification de termes spécialisés (unités de mesure) à partir de données textuelles pour enrichir une Ressource Termino-Ontologique (RTO). La première étape de notre méthode consiste à prédire la localisation des variants d'unités de mesure dans les documents. Nous avons utilisé une méthode reposant sur l'apprentissage supervisé. Cette méthode permet de réduire sensiblement l'espace de recherche des variants tout en restant dans un contexte optimal de recherche (réduction de 86% de l'espace de recherché sur le corpus étudié). La deuxième étape du processus, une fois l'espace de recherche réduit aux variants d'unités, utilise une nouvelle mesure de similarité permettant d'identifier automatiquement les variants découverts par rapport à un terme d'unité déjà référencé dans la RTO avec un taux de précision de 82% pour un seuil au dessus de 0.6 sur le corpus étudié.</resume>
			<mots_cles>ressource termino-ontologique, apprentissage, similarité</mots_cles>
			<title>Identification of units of measures in scientific texts</title>
			<abstract>The work presented in this paper consists in identifying specialized terms (units of measures) in textual documents in order to enrich a onto-terminological resource (OTR). The first step permits to predict the localization of unit of measure variants in the documents. We have used a method based on supervised learning. This method permits to reduce significantly the variant search space staying in an optimal search context (reduction of 86% of the search space on the studied set of documents). The second step uses a new similarity measure identifying automatically variants associated with term denoting a unit of measure already present in the OTR with a precision rate of 82% for a threshold above 0.6 on the studied corpus .</abstract>
			<keywords>onto-terminological resource, learning, similarity</keywords>
		</article>
		<article id="taln-2015-court-015" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Gaël</prenom>
					<nom>Lejeune</nom>
					<email>gael.lejeune@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Brixtel</nom>
					<email>romain.brixtel@unil.ch</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Charlotte</prenom>
					<nom>Lecluze</nom>
					<email>charlotte.lecluze@unicaen.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, Université de Nantes, 2 rue de la Houssinière, 44322 Nantes, France</affiliation>
				<affiliation affiliationId="2">Université de Lausanne – HEC - Département de comportement organisationnel, Quartier Dorigny, 1015 Lausanne, Suisse</affiliation>
				<affiliation affiliationId="3">GREYC, Campus Côte de Nacre, Boulevard du Maréchal Juin, 14032 CAEN cedex 5, France</affiliation>
			</affiliations>
			<titre>Évaluation intrinsèque et extrinsèque du nettoyage de pages Web</titre>
			<type>court</type>
			<pages>411-417</pages>
			<resume>Le nettoyage de documents issus du web est une tâche importante pour le TAL en général et pour la constitution de corpus en particulier. Cette phase est peu traitée dans la littérature, pourtant elle n'est pas sans inﬂuence sur la qualité des informations extraites des corpus. Nous proposons deux types d'évaluation de cette tâche de détourage : (I) une évaluation intrinsèque fondée sur le contenu en mots, balises et caractères ; (II) une évaluation extrinsèque fondée sur la tâche, en examinant l'effet du détourage des documents sur le système placé en aval de la chaîne de traitement. Nous montrons que les résultats ne sont pas cohérents entre ces deux évaluations ainsi qu'entre les différentes langues. Ainsi, le choix d'un outil de détourage devrait être guidé par la tâche visée plutôt que par la simple évaluation intrinsèque.</resume>
			<mots_cles>Nettoyage de pages Web, collecte de corpus, évaluation intrinsèque, évaluation extrinsèque, détourage</mots_cles>
			<title>Intrinsic and extrinsic evaluation of boilerplate removal tools</title>
			<abstract>In this article, we tackle the problem of evaluation of web page cleaning tools. This task is seldom studied in the literature although it has consequences on the linguistic processing performed on web-based corpora. We propose two types of evaluation : (I) an intrinsic (content-based) evaluation with measures on words, tags and characters ; (II) an extrinsic (task-based) evaluation on the same corpus by studying the effects of the cleaning step on the performances of an NLP pipeline. We show that the results are not consistent in both evaluations. We show as well that there are important differences in the results between the studied languages. We conclude that the choice of a web page cleaning tool should be made in view of the aimed task rather than on the performances of the tools in an intrinsic evaluation.</abstract>
			<keywords>Web page cleaning, corpus collecting, intrinsic evaluation, extrinsic evaluation, web scraping</keywords>
		</article>
		<article id="taln-2015-court-016" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Joseph</prenom>
					<nom>Lark</nom>
					<email>joseph@dictanova.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sebastian</prenom>
					<nom>Peña Saldarriaga</nom>
					<email>sebastian@dictanova.com</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, UMR CNRS 4261, 2 chemin de la Houssinière, Nantes, France</affiliation>
				<affiliation affiliationId="2">Dictanova, 2 chemin de la Houssinière, Nantes, France</affiliation>
			</affiliations>
			<titre>CANÉPHORE : un corpus français pour la fouille d'opinion ciblée</titre>
			<type>court</type>
			<pages>418-424</pages>
			<resume>La fouille d'opinion ciblée (aspect-based sentiment analysis) fait l'objet ces dernières années d'un intérêt particulier, visible dans les sujets des récentes campagnes d'évaluation comme SemEval 2014 et 2015 ou bien DEFT 2015. Cependant les corpus annotés et publiquement disponibles permettant l'évaluation de cette tâche sont rares. Dans ce travail nous présentons en premier lieu un corpus français librement accessible de 10 000 tweets manuellement annotés. Nous accompagnons ce corpus de résultats de référence pour l'extraction de marqueurs d'opinion non supervisée. Nous présentons ensuite une méthode améliorant les résultats de cette extraction, en suivant une approche semi-supervisée.</resume>
			<mots_cles>Fouille d'opinion, web social, corpus annoté, extraction d'information semi-supervisée</mots_cles>
			<title>CANÉPHORE : a French corpus for aspect-based sentiment analysis evaluation</title>
			<abstract>Aspect-based sentiment analysis knows a renewed interest these last years, according to recent opinion mining evaluation series (SemEval 2014 and 2015, DEFT 2015). However, publicly available evaluation resources are scarse. This work firstly introduces a publicly available annotated French Twitter corpus for sentiment analysis evaluation on aspect, subject and opinion word levels (10 000 documents). We present baseline results on this corpus for the task of opinion word extraction and then show that these results can be improved with simple semi-supervised methods.</abstract>
			<keywords>Opinion mining, social web, annotated corpus, semi-supervised information extraction</keywords>
		</article>
		<article id="taln-2015-court-017" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Firas</prenom>
					<nom>Hmida</nom>
					<email>firas.hmida@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Daille</nom>
					<email>beatrice.daille@lina.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes, LINA UMR 6241, 2 rue de la Houssinière, BP 92208, 44322 Nantes Cedex 3</affiliation>
			</affiliations>
			<titre>Extraction de Contextes Riches en Connaissances en corpus spécialisés</titre>
			<type>court</type>
			<pages>425-431</pages>
			<resume>Les banques terminologiques et les dictionnaires sont des ressources précieuses qui facilitent l'accès aux connaissances des domaines spécialisés. Ces ressources sont souvent assez pauvres et ne proposent pas toujours pour un terme à illustrer des exemples permettant d'appréhender le sens et l'usage de ce terme. Dans ce contexte, nous proposons de mettre en œuvre la notion de Contextes Riches en Connaissances (CRC) pour extraire directement de corpus spécialisés des exemples de contextes illustrant son usage. Nous définissons un cadre unifié pour exploiter tout à la fois des patrons de connaissances et des collocations avec une qualité acceptable pour une révision humaine.</resume>
			<mots_cles>corpus spécialisé, CRC, patrons de connaisssances, collocations</mots_cles>
			<title>Knowledge-Rich Contexts Extraction in Specialized Corpora</title>
			<abstract>The term banks and dictionaries are valuable resources that improve access to knowledge in specialized domains. These resources are often relatively poor and do not always provide, for a given term, examples of its typicall use. In this context, we implement Knowledge-Rich Contexts (KRCs) to extract examples of contexts providing illustration of terms in specialized domain. We propose a unified framework to apply at the same time knowledge pattern and collocations with acceptable quality for human review.</abstract>
			<keywords>specialized corpus, KRC, knowledge patterns, collocations</keywords>
		</article>
		<article id="taln-2015-court-018" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Eliane</prenom>
					<nom>Delente</nom>
					<email>eliane.delente@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Renault</nom>
					<email>richard.renault@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Caen Basse-Normandie</affiliation>
			</affiliations>
			<titre>Traitement automatique des formes métriques des textes versifiés</titre>
			<type>court</type>
			<pages>432-438</pages>
			<resume>L'objectif de cet article est de présenter tout d'abord dans ses grandes lignes le projet Anamètre qui a pour objet le traitement automatique des formes métriques de la poésie et du théâtre français du début du XVIIe au début du XXe siècle. Nous présenterons ensuite un programme de calcul automatique des mètres appliqué à notre corpus dans le cadre d'une approche déterministe en nous appuyant sur la méthode métricométrique de B. de Cornulier ainsi que la procédure d'appariement des rimes et la détermination des schémas de strophes dans les suites périodiques et les formes fixes.</resume>
			<mots_cles>métrique française, corpus poétique, calcul du mètre, appariement des rimes, détermination des schémas de strophes</mots_cles>
			<title>Automatic Processing of Metrical Forms in Verse Texts</title>
			<abstract>The purpose of this paper is to present the project Anamètre. The project proposes automatic processing of metrical forms in French poetry and drama from the early seventeenth to the early twentieth century. Then we present the calculation program of meters on our corpus using a deterministic approach relying on the "métricométric" method of B. de Cornulier. Finally, we present the procedure of maching rimes and determination of rhyme schemes in periodic sequences and specific forms.</abstract>
			<keywords>French metrics, poetic corpus, calculation of meter, matching rhymes, determining of rhyme schemes</keywords>
		</article>
		<article id="taln-2015-court-019" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Adèle</prenom>
					<nom>Désoyer</nom>
					<email>adele.desoyer@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Landragin</nom>
					<email>frederic.landragin@ens.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice, CNRS, ENS et Université de Paris 3 - Sorbonne Nouvelle</affiliation>
				<affiliation affiliationId="2">MoDyCo, CNRS, Université Paris Ouest - Nanterre La Défense</affiliation>
			</affiliations>
			<titre>Apprentissage automatique d'un modèle de résolution de la coréférence à partir de données orales transcrites du français : le système CROC</titre>
			<type>court</type>
			<pages>439-445</pages>
			<resume>Cet article présente CROC 1 (Coreference Resolution for Oral Corpus), un premier système de résolution des coréférences en français reposant sur des techniques d'apprentissage automatique. Une des spécificités du système réside dans son apprentissage sur des données exclusivement orales, à savoir ANCOR (anaphore et coréférence dans les corpus oraux), le premier corpus de français oral transcrit annoté en relations anaphoriques. En l'état actuel, le système CROC nécessite un repérage préalable des mentions. Nous détaillons les choix des traits – issus du corpus ou calculés – utilisés par l'apprentissage, et nous présentons un ensemble d'expérimentations avec ces traits. Les scores obtenus sont très proches de ceux de l'état de l'art des systèmes conçus pour l'écrit. Nous concluons alors en donnant des perspectives sur la réalisation d'un système end-to-end valable à la fois pour l'oral transcrit et l'écrit.</resume>
			<mots_cles>corpus de dialogues, détection de coréférences, apprentissage, paires de mentions</mots_cles>
			<title>Machine Learning for Coreference Resolution of Transcribed Oral French Data : the CROC System</title>
			<abstract>We present CROC (Coreference Resolution for Oral Corpus), the first machine learning system for coreference resolution in French. One specific aspect of the system is that it has been trained on data that are exclusively oral, namely ANCOR (ANaphora and Coreference in ORal corpus), the first corpus in oral French with anaphorical relations annotations. In its current state, the CROC system requires pre-annotated mentions. We detail the features that we chose to be used by the learning algorithms, and we present a set of experiments with these features. The scores we obtain are close to those of state-of-the-art systems for written English. Then we give future works on the design of an end-to-end system for oral and written French.</abstract>
			<keywords>Dialogue corpus, Coreference resolution, Machine learning, Mention-pair model</keywords>
		</article>
		<article id="taln-2015-court-020" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Gaël</prenom>
					<nom>Lejeune</nom>
					<email>gael.lejeune@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Daille</nom>
					<email>beatrice.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, Université de Nantes, 2 rue de la Houssinière, 44322 Nantes, France</affiliation>
			</affiliations>
			<titre>Vers un diagnostic d'ambiguïté des termes candidats d'un texte</titre>
			<type>court</type>
			<pages>446-452</pages>
			<resume>Les recherches autour de la désambiguïsation sémantique traitent de la question du sens à accorder à différentes occurrences d'un mot ou plus largement d'une unité lexicale. Dans cet article, nous nous intéressons à l'ambiguïté d'un terme en domaine de spécialité. Nous posons les premiers jalons de nos recherches sur une question connexe que nous nommons le diagnostic d'ambiguïté. Cette tâche consiste à décider si une occurrence d'un terme est ou n'est pas ambiguë. Nous mettons en œuvre une approche d'apprentissage supervisée qui exploite un corpus d'articles de sciences humaines rédigés en français dans lequel les termes ambigus ont été détectés par des experts. Le diagnostic s'appuie sur deux types de traits : syntaxiques et positionnels. Nous montrons l'intérêt de la structuration du texte pour établir le diagnostic d'ambiguïté.</resume>
			<mots_cles>diagnostic d'ambiguïté, extraction de mot-clés, terminologie</mots_cles>
			<title>Towards diagnosing ambiguity of candidate terms</title>
			<abstract>Researches in the field of Word Sense Disambiguation focus on identifying the precise meaning of a lexical unit found in a text. This article tackles another kind of problem : assessing the ambiguity of a lexical unit. In other words, we try to identify if a particular unit is ambiguous or not, we define this task as ambiguity diagnosis. Our evaluation dataset contains scientific articles where ambiguous words have been tagged by experts. In order to give an ambiguity diagnosis for each term, we use two types of features : POS tags and positions in the text. We show that the position of an occurrence in the text is a strong hint for such a task.</abstract>
			<keywords>ambiguity diagnosis, keyword extraction, terminology</keywords>
		</article>
		<article id="taln-2015-court-021" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>mathieu.lafourcade@gmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lionel</prenom>
					<nom>Ramadier</nom>
					<email>lionel.ramadier@imaios.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>											
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lirmm, Université Montpellier, France</affiliation>
				<affiliation affiliationId="2">Imaios, 34000 MONTPELLIER - France</affiliation>
			</affiliations>
			<titre>Augmentation d'index par propagation sur un réseau lexical Application aux comptes rendus de radiologie</titre>
			<type>court</type>
			<pages>453-459</pages>
			<resume>Les données médicales étant de plus en plus informatisées, le traitement sémantiquement efficace des rapports médicaux est devenu une nécessité. La recherche d'images radiologiques peut être grandement facilitée grâce à l'indexation textuelle des comptes rendus associés. Nous présentons un algorithme d'augmentation d'index de comptes rendus fondé sur la propagation d'activation sur un réseau lexico-sémantique généraliste.</resume>
			<mots_cles>réseau lexico-sémantique, propagation, indexation, recherche d'information, imagerie médicale</mots_cles>
			<title>Index augmentation through propagation over a lexical network – application to radiological reports</title>
			<abstract>Medical data being increasingly computerized, semantically effective treatment of medical reports has become a necessity. The search of radiological images can be greatly facilitated through textual indexing of the associated reports. We present here an index enlargement algorithm based on spreading activations over a general lexical-semantic network.</abstract>
			<keywords>lexico-semantic network, propagation, indexation, information retrieval, medical imaging</keywords>
		</article>
		<article id="taln-2015-court-022" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Jihen</prenom>
					<nom>Karoui</nom>
					<email>jihen.karoui@irit.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Farah</prenom>
					<nom>Benamara Zitoune</nom>
					<email>benamara@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Moriceau</nom>
					<email>moriceau@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Aussenac-Gilles</nom>
					<email>Nathalie.Aussenac-Gilles@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lamia</prenom>
					<nom>Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>3</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, CNRS, Université Paul Sabatier, 118 Route de Narbonne, F-31062 TOULOUSE CEDEX 9</affiliation>
				<affiliation affiliationId="2">LIMSI, CNRS, Université Paris Sud, Rue John von Neumann, 91403 ORSAY CEDEX</affiliation>
				<affiliation affiliationId="3">MIRACL, Pôle technologique de Sfax, Route de Tunis Km 10 B.P. 242, 3021 SFAX</affiliation>
			</affiliations>
			<titre>Détection automatique de l'ironie dans les tweets en français</titre>
			<type>court</type>
			<pages>460-465</pages>
			<resume>Cet article présente une méthode par apprentissage supervisé pour la détection de l'ironie dans les tweets en français. Un classifieur binaire utilise des traits de l'état de l'art dont les performances sont reconnues, ainsi que de nouveaux traits issus de notre étude de corpus. En particulier, nous nous sommes intéressés à la négation et aux oppositions explicites/implicites entre des expressions d'opinion ayant des polarités différentes. Les résultats obtenus sont encourageants.</resume>
			<mots_cles>Analyse d'opinion, détection de l'ironie, apprentissage supervisé</mots_cles>
			<title>Automatic Irony Detection in French tweets</title>
			<abstract>This paper presents a supervised learning method for irony detection in tweets in French. A binary classifier uses both state of the art features whose efficiency has been empirically proved and new groups of features observed in our corpus. We focused on negation and explicit/implicit oppositions of opinions with different polarities. Results are encouraging.</abstract>
			<keywords>Opinion analysis, irony detection, supervised learning</keywords>
		</article>
		<article id="taln-2015-court-023" session="Session 1">
			<auteurs>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Mathieu-Colas</nom>
					<email>michel.mathieu-colas@univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Cartier</nom>
					<email>emmanuel.cartier@univ-paris13.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aude</prenom>
					<nom>Grezka</nom>
					<email>aude.grezka@ldi.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LDI UMR 7187, Université Paris 13 Sorbonne Paris Cité</affiliation>
				<affiliation affiliationId="2">LIPN-RCLN UMR 7030, Université Paris 13 Sorbonne Paris Cité</affiliation>
			</affiliations>
			<titre>Dictionnaires morphologiques du français contemporain : présentation de Morfetik, éléments d'un modèle pour le TAL</titre>
			<type>court</type>
			<pages>466-472</pages>
			<resume>Dans cet article, nous présentons une ressource linguistique, Morfetik, développée au LDI. Après avoir présenté le modèle sous-jacent et spécifié les modalités de sa construction, nous comparons cette ressource avec d'autres ressources du français : le GLAFF, le LEFF, Morphalou et Dicolecte. Nous étudions ensuite la couverture lexicale de ces dictionnaires sur trois corpus, le Wikipedia français, la version française de Wacky et les dix ans du Monde. Nous concluons par un programme de travail permettant de mettre à jour de façon continue la ressource lexicographique du point de vue des formes linguistiques, en connectant la ressource à un corpus continu.</resume>
			<mots_cles>dictionnaire, morphologie, français, ressource linguistique, corpus</mots_cles>
			<title>French Contemporary Morphological Dictionaries : Morfetik Database, Elements of a Model for Computational Linguistics</title>
			<abstract>In this article, we present a morphological linguistic resource for Contemporary French called Morfetik. We first detail its composition, features and coverage. We compare it to other available morphological dictionaries for French (GLAFF, LEFF, Morphalou and Dicolecte). We then study its coverage on big corpora (French Wikipedia, French version of Wacky and Le Monde 10 years). We conclude with a proposition for updating the dictionary by connecting the resource with a continuously live corpus.</abstract>
			<keywords>dictionary, morphology, French language, linguistic resource, corpus</keywords>
		</article>
		<article id="taln-2015-court-024" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Simon</prenom>
					<nom>Petitjean</nom>
					<email>simon.petitjean@univ-orleans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Younes</prenom>
					<nom>Samih</nom>
					<email>samih@phil.hhu.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Timm</prenom>
					<nom>Lichte</nom>
					<email>lichte@phil.hhu.de</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Heinrich-Heine-Universität Düsseldorf, Allemagne</affiliation>
			</affiliations>
			<titre>Une métagrammaire de l'interface morpho-sémantique dans les verbes en arabe</titre>
			<type>court</type>
			<pages>473-479</pages>
			<resume>Dans cet article, nous présentons une modélisation de la morphologie dérivationnelle de l'arabe utilisant le cadre métagrammatical oﬀert par XMG. Nous démontrons que l'utilisation de racines et patrons abstraits comme morphèmes atomiques sous-spécifiés oﬀre une manière élégante de traiter l'interaction entre morphologie et sémantique.</resume>
			<mots_cles>Morphologie, arabe, metagrammaire, frame semantics</mots_cles>
			<title>A metagrammar of the morphology-semantics interface in Arabic verbs</title>
			<abstract>In this article we propose to model the derivational morphology of Arabic using the metagrammatical framework of XMG. We demonstrate that treating abstract roots and patterns as semantically underspecified atomic morphemes oﬀers an elegant way to account for the interaction between morphology and semantics.</abstract>
			<keywords>Morphology, Arabic, metagrammar, frame semantics</keywords>
		</article>
		<article id="taln-2015-court-025" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>philippe.blache@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Grégoire</prenom>
					<nom>Moncheuil</nom>
					<email>gregoire.moncheuil@lpl-aix.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Rauzy</nom>
					<email>stephane.rauzy@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie-Laure</prenom>
					<nom>Guénot</nom>
					<email>marie-laure.guenot@lpl-aix.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix Marseille Université, CNRS, Laboratoire Parole et Langage UMR 7309</affiliation>
				<affiliation affiliationId="2">Equipex ORTOLANG, CNRS</affiliation>
			</affiliations>
			<titre>Création d'un nouveau treebank à partir de quatrièmes de couverture</titre>
			<type>court</type>
			<pages>480-486</pages>
			<resume>Nous présentons ici 4-couv, un nouveau corpus arboré d'environ 3 500 phrases, constitué d'un ensemble de quatrièmes de couverture, étiqueté et analysé automatiquement puis corrigé et validé à la main. Il répond à des besoins spécifiques pour des projets de linguistique expérimentale, et vise à rester compatible avec les autres treebanks existants pour le français. Nous présentons ici le corpus lui-même ainsi que les outils utilisés pour les différentes étapes de son élaboration : choix des textes, étiquetage, parsing, correction manuelle.</resume>
			<mots_cles>Corpus arboré, Étiquetage automatique, Analyse syntaxique automatique, Parsing stochastique, Conventions d'annotation, Outils d'annotation, Linguistique expérimentale</mots_cles>
			<title>Creation of a new treebank with backcovers</title>
			<abstract>We introduce 4-couv, a treebank of approximatively 3 500 trees, built from a set of literacy backcovers. It has been automatically tagged and parsed, then manually corrected and validated. It was developed in the perspective of linguistic expriment projects, and aims to be compatible with other standard treebanks for french. We present in the following the corpus itself, then the tools we used or developed for the different stages of its elaboration : texts' selection, tagging, parsing, and manual correction.</abstract>
			<keywords>Treebank, Tagging, Parsing, Stochastic parsing, Annotation scheme, Annotation tools, Experimental linguistics</keywords>
		</article>
		<article id="taln-2015-court-026" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Géraldine</prenom>
					<nom>Damnati</nom>
					<email>geraldine.damnati@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aleksandra</prenom>
					<nom>Guerraz</nom>
					<email>aleksandra.guerraz@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Charlet</nom>
					<email>delphine.charlet@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, OPENSERV/CONTENT/FAST, 2 av. Pierre Marzin, 22307 LANNION cedex</affiliation>
			</affiliations>
			<titre>Entre écrit et oral ? Analyse comparée de conversations de type tchat et de conversations téléphoniques dans un centre de contact client</titre>
			<type>court</type>
			<pages>487-493</pages>
			<resume>Dans cet article nous proposons une première étude descriptive d'un corpus de conversations de type tchat issues d'un centre de contact d'assistance. Les dimensions lexicales, syntaxiques et interactionnelles sont analysées. L'étude parallèle de transcriptions de conversations téléphoniques issues d'un centre d'appel dans le même domaine de l'assistance permet d'établir des comparaisons entre ces deux modes d'interaction. L'analyse révèle des différences marquées en termes de déroulement de la conversation, avec une plus grande efficacité pour les conversations de type tchat malgré un plus grand étalement temporel. L'analyse lexicale et syntaxique révèle également des différences de niveaux de langage avec une plus grande proximité entre le client et le téléconseiller à l'oral que pour les tchats où le décalage entre le style adopté par le téléconseiller et l'expression du client est plus important.</resume>
			<mots_cles>Centre de contact, conversations tchat, interaction, analyse lexicale et syntaxique</mots_cles>
			<title>Comparing Written and Spoken Languages: a Descriptive Study of Chat and Phone Conversations from an Assistance Contact Center</title>
			<abstract>In this article we propose a first descriptive study of a chat conversations corpus from an assistance contact center. The analysis includes lexical, syntactic and interactional dimensions. Transcriptions of telephone conversations from an assistance call center are also studied, allowing comparisons between these two interaction modes to be drawn. The study reveals significant differences in terms of conversation flow, with an increased efficiency for chat conversations in spite of longer temporal span. The lexical and syntactic analyses also reveal differences in terms of language level with a tighter proximity between agent and customers on the phone than for chats where the style adopted by agents is different from the style adopted by customers.</abstract>
			<keywords>Contact center, chat conversations, interaction, lexical and syntactic analysis</keywords>
		</article>
		<article id="taln-2015-court-027" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Laurie</prenom>
					<nom>Planes</nom>
					<email>lplanes@inbenta.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Inbenta France, 164 route de Revel, 31400 TOULOUSE</affiliation>
			</affiliations>
			<titre>Construction et maintenance d'une ressource lexicale basées sur l'usage</titre>
			<type>court</type>
			<pages>494-500</pages>
			<resume>Notre société développe un moteur de recherche (MR) sémantique basé sur la reformulation de requête. Notre MR s'appuie sur un lexique que nous avons construit en nous inspirant de la Théorie Sens-Texte (TST). Nous présentons ici notre ressource lexicale et indiquons comment nous l'enrichissons et la maintenons en fonction des besoins détectés à l'usage. Nous abordons également la question de l'adaptation de la TST à nos besoins.</resume>
			<mots_cles>Ressources lexicales, Théorie Sens-Texte, Recherche d'Information</mots_cles>
			<title>Lexical resource building and maintenance based on the use</title>
			<abstract>Our company develops a semantic search engine based on queries rephrasing. Our search engine relies on a lexicon we built on the basis of the Meaning-Text theory. We introduce our lexical resource and explain how we enrich and update it according to the needs we detect. We also mention the customization of the Meaning Text Theory to our needs.</abstract>
			<keywords>Lexical Ressources, Meaning-Text Theory, Information Retrieval</keywords>
		</article>
		<article id="taln-2015-court-028" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Carole</prenom>
					<nom>Lailler</nom>
					<email>carole.lailler@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yannick</prenom>
					<nom>EstÈve</nom>
					<email>yannick.esteve@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Renato</prenom>
					<nom>De Mori</nom>
					<email>rdemori@cs.mcgill.ca</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>BouallÈgue</nom>
					<email>mohamed.bouallegue@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Morchid</nom>
					<email>mohamed.morchid@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIUM — Université du Maine, France</affiliation>
				<affiliation affiliationId="2">LIA — Université d'Avignon et des Pays de Vaucluse, France</affiliation>
				<affiliation affiliationId="3">McGill University, Montréal, Canada</affiliation>
			</affiliations>
			<titre>Utilisation d'annotations sémantiques pour la validation automatique d'hypothèses dans des conversations téléphoniques</titre>
			<type>court</type>
			<pages>501-507</pages>
			<resume>Les travaux présentés portent sur l'extraction automatique d'unités sémantiques et l'évaluation de leur pertinence pour des conversations téléphoniques. Le corpus utilisé est le corpus français DECODA. L'objectif de la tâche est de permettre l'étiquetage automatique en thème de chaque conversation. Compte tenu du caractère spontané de ce type de conversations et de la taille du corpus, nous proposons de recourir à une stratégie semi-supervisée fondée sur la construction d'une ontologie et d'un apprentissage actif simple : un annotateur humain analyse non seulement les listes d'unités sémantiques candidates menant au thème mais étudie également une petite quantité de conversations. La pertinence de la relation unissant les unités sémantiques conservées, le sous-thème issu de l'ontologie et le thème annoté est évaluée par un DNN, prenant en compte une représentation vectorielle du document. L'intégration des unités sémantiques retenues dans le processus de classification en thème améliore les performances.</resume>
			<mots_cles>analyse de conversation humain/humain, extraction automatique d'unités sémantiques pertinentes, validation d'une ontologie</mots_cles>
			<title>Use of Semantic Annotations for Validating Mentions of Semantic Hypotheses in Telephone Conversations</title>
			<abstract>The presented work focuses on the automatic extraction of semantic units and evaluation of their relevance to telephone conversations. The corpus used is DECODA corpus. The objective of the task is to enable automatic labeling theme of each conversation. Given the spontaneous nature of this type of conversations and the size of the corpus, we propose to use a semi-supervised strategy based on the construction of an ontology and a simple active learning : a human annotator analyses not only the lists of semantic units leading to the theme, but also studying a small amount of conversations. The relevance of the relationship between the conserved semantic units, sub-theme from the ontology and annotated theme is assessed by DNN, taking into account a vector representation of the document. The integration of semantic units included in the theme classification process improves performance.</abstract>
			<keywords>human/human conversation analysis, automatic extraction of relevant semantic units, ontology validation</keywords>
		</article>
		<article id="taln-2015-court-029" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Christelle</prenom>
					<nom>Rabary</nom>
					<email>christelle.rabary@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thomas</prenom>
					<nom>Lavergne</nom>
					<email>lavergne@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Névéol</nom>
					<email>aurelie.neveol@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Campus Universitaire d'Orsay, bât 508, 91405 ORSAY, FRANCE</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, 91403 ORSAY, FRANCE</affiliation>
			</affiliations>
			<titre>Etiquetage morpho-syntaxique en domaine de spécialité: le domaine médical</titre>
			<type>court</type>
			<pages>508-514</pages>
			<resume>L'étiquetage morpho-syntaxique est une tâche fondamentale du Traitement Automatique de la Langue, sur laquelle reposent souvent des traitements plus complexes tels que l'extraction d'information ou la traduction automatique. L'étiquetage en domaine de spécialité est limité par la disponibilité d'outils et de corpus annotés spécifiques au domaine. Dans cet article, nous présentons le développement d'un corpus clinique du français annoté morpho-syntaxiquement à l'aide d'un jeu d'étiquettes issus des guides d'annotation French Treebank et Multitag. L'analyse de ce corpus nous permet de caractériser le domaine clinique et de dégager les points clés pour l'adaptation d'outils d'analyse morpho-syntaxique à ce domaine. Nous montrons également les limites d'un outil entraîné sur un corpus journalistique appliqué au domaine clinique. En perspective de ce travail, nous envisageons une application du corpus clinique annoté pour améliorer l'étiquetage morpho-syntaxique des documents cliniques en français.</resume>
			<mots_cles>adaptation, analyse morpho-syntaxique, langue de spécialité, dossier électronique patient</mots_cles>
			<title>Part of Speech tagging for specialized domains : a case study with clinical documents in French</title>
			<abstract>Part-of-Speech (PoS) tagging is a core task in Natural Language Processing, often used as a stepping stone to perform more complex tasks such as information extraction or machine translation. PoS tagging of specialized documents is often challenging due to the limited availability of tools and annotated corpora dedicated to specialized domains. Herein, we present the development of a PoS annotated corpus of clinical documents in French, using annotation guidelines from the FrenchTree Bank and Multitag datasets. Through analysis of the annotated corpus, we characterize the clinical domain, including specific targets for domain adaptation. We also show the limitations of a PoS tagger trained on news documents when applied to clinical text. We expect that the domain-specific resource presented in this paper will contribute to improve PoS tagging for clinical documents in French.</abstract>
			<keywords>domain adaptation, part-of-speech tagging, specialized domain, EHR</keywords>
		</article>
		<article id="taln-2015-court-030" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Remi</prenom>
					<nom>Bois</nom>
					<email>remi.bois@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Gravier</nom>
					<email>guillaume.gravier@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascale</prenom>
					<nom>Sébillot</nom>
					<email>pascale.sebillot@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS, IRISA &amp; INRIA, Campus de Beaulieu, 35000 Rennes</affiliation>
				<affiliation affiliationId="2">INSA, IRISA &amp; INRIA, Campus de Beaulieu, 35000 Rennes</affiliation>
				<affiliation affiliationId="3">Université de Nantes, LINA, 2 Rue de la Houssinière, 44300 Nantes</affiliation>
			</affiliations>
			<titre>Vers une typologie de liens entre contenus journalistiques</titre>
			<type>court</type>
			<pages>515-521</pages>
			<resume>Nous présentons une typologie de liens pour un corpus multimédia ancré dans le domaine journalistique. Bien que plusieurs typologies aient été créées et utilisées par la communauté, aucune ne permet de répondre aux enjeux de taille et de variété soulevés par l'utilisation d'un corpus large comprenant des textes, des vidéos, ou des émissions radiophoniques. Nous proposons donc une nouvelle typologie, première étape visant à la création et la catégorisation automatique de liens entre des fragments de documents afin de proposer de nouveaux modes de navigation au sein d'un grand corpus. Plusieurs exemples d'instanciation de la typologie sont présentés afin d'illustrer son intérêt.</resume>
			<mots_cles>typologie, liens inter-documents, hypertexte, multimédia, presse</mots_cles>
			<title>Towards a typology for linking newswire contents</title>
			<abstract>In this paper, we introduce a typology of possible links between contents of a multimedia news corpus. While several typologies have been proposed and used by the community, we argue that they are not adapted to rich and large corpora which can contain texts, videos, or radio stations recordings. We propose a new typology, as a first step towards automatically creating and categorizing links between documents' fragments in order to create new ways to navigate, explore, and extract knowledge from large collections. Several examples of links in a large corpus are given.</abstract>
			<keywords>typology, linking documents, hypertext, multimedia, newswire</keywords>
		</article>
		<article id="taln-2015-court-031" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Denis</prenom>
					<nom>Béchet</nom>
					<email>Denis.Bechet@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ophélie</prenom>
					<nom>Lacroix</nom>
					<email>ophelie.lacroix@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, 44322 Nantes Cedex 3, France</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, 91403 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre>CDGFr, un corpus en dépendances non-projectives pour le français</titre>
			<type>court</type>
			<pages>522-528</pages>
			<resume>Dans le cadre de l'analyse en dépendances du français, le phénomène de la non-projectivité est peu pris en compte, en majeure partie car les donneés sur lesquelles sont entraînés les analyseurs représentent peu ou pas ces cas particuliers. Nous présentons, dans cet article, un nouveau corpus en dépendances pour le français, librement disponible, contenant un nombre substantiel de dépendances non-projectives. Ce corpus permettra d'étudier et de mieux prendre en compte les cas de non-projectivité dans l'analyse du français.</resume>
			<mots_cles>Corpus français, annotation en dépendances, dépendances non-projectives</mots_cles>
			<title>CDGFr, a Non-projective Dependency Corpus for French</title>
			<abstract>The non-projective cases, as a part of the dependency parsing of French, are often disregarded, mainly because the tree- banks on which parsers are trained contain little or no non-projective dependencies. In this paper, we present a new freely available dependency treebank for French that includes a substantial number of non-projective dependencies. This corpus can be used to study and process non-projectivity more effectively within the context of French dependency parsing.</abstract>
			<keywords>Treebank for French, dependency annotation, non-projective dependencies</keywords>
		</article>
		<article id="taln-2015-court-032" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Othman</prenom>
					<nom>Zennaki</nom>
					<email>Othman.ZENNAKI@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nasredine</prenom>
					<nom>Semmar</nom>
					<email>nasredine.semmar@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email>laurent.besacier@imag.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie de Contenus, F-91191, Gif-sur-Yvette, France</affiliation>
				<affiliation affiliationId="2">Laboratoire d'Informatique de Grenoble, Univ. Grenoble-Alpes, Grenoble, France</affiliation>
			</affiliations>
			<titre>Utilisation des réseaux de neurones récurrents pour la projection interlingue d'étiquettes morpho-syntaxiques à partir d'un corpus parallèle</titre>
			<type>court</type>
			<pages>529-536</pages>
			<resume>La construction d'outils d'analyse linguistique pour les langues faiblement dotées est limitée, entre autres, par le manque de corpus annotés. Dans cet article, nous proposons une méthode pour construire automatiquement des outils d'analyse via une projection interlingue d'annotations linguistiques en utilisant des corpus parallèles. Notre approche n'utilise pas d'autres sources d'information, ce qui la rend applicable à un large éventail de langues peu dotées. Nous proposons d'utiliser les réseaux de neurones récurrents pour projeter les annotations d'une langue à une autre (sans utiliser d'information d'alignement des mots). Dans un premier temps, nous explorons la tâche d'annotation morpho-syntaxique. Notre méthode combinée avec une méthode de projection d'annotation basique (utilisant l'alignement mot à mot), donne des résultats comparables à ceux de l'état de l'art sur une tâche similaire.</resume>
			<mots_cles>Multilinguisme, transfert crosslingue, étiquetage morpho-syntaxique, réseaux de neurones récurrents</mots_cles>
			<title>Use of Recurrent Neural Network for Part-Of-Speech tags projection from a parallel corpus</title>
			<abstract>In this paper, we propose a method to automatically induce linguistic analysis tools for languages that have no labeled training data. This method is based on cross-language projection of linguistic annotations from parallel corpora. Our method does not assume any knowledge about foreign languages, making it applicable to a wide range of resource-poor languages. No word alignment information is needed in our approach. We use Recurrent Neural Networks (RNNs) as cross-lingual analysis tool. To illustrate the potential of our approach, we firstly investigate Part-Of-Speech (POS) tagging. Combined with a simple projection method (using word alignment information), it achieves performance comparable to the one of recently published approaches for cross-lingual projection.</abstract>
			<keywords>Multilingualism, cross-Lingual transfer, part-of-speech tagging, recurrent neural network</keywords>
		</article>
		<article id="taln-2015-court-033" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Abdessalam</prenom>
					<nom>Bouchekif</nom>
					<email>abdessalam.bouchekif@orange.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Géraldine</prenom>
					<nom>Damnati</nom>
					<email>geraldine.damnati@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Camelin</nom>
					<email>nathalie.camelin@lium.univ-lemans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yannick</prenom>
					<nom>Estève</nom>
					<email>yannick.esteve@lium.univ-lemans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Charlet</nom>
					<email>delphine.charlet@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 2 avenue Pierre Marzin 22300, Lannion, France</affiliation>
				<affiliation affiliationId="2">Laboratoire d'Informatique de l'Universite du Maine, LIUM - France</affiliation>
			</affiliations>
			<titre>Segmentation et Titrage Automatique de Journaux Télévisés</titre>
			<type>court</type>
			<pages>537-543</pages>
			<resume>Dans cet article, nous nous intéressons au titrage automatique des segments issus de la segmentation thématique de journaux télévisés. Nous proposons d'associer un segment à un article de presse écrite collecté le jour même de la diffusion du journal. La tâche consiste à apparier un segment à un article de presse à l'aide d'une mesure de similarité. Cette approche soulève plusieurs problèmes, comme la sélection des articles candidats, une bonne représentation du segment et des articles, le choix d'une mesure de similarité robuste aux imprécisions de la segmentation. Des expériences sont menées sur un corpus varié de journaux télévisés français collectés pendant une semaine, conjointement avec des articles aspirés à partir de la page d'accueil de Google Actualités. Nous introduisons une métrique d'évaluation reﬂétant la qualité de la segmentation, du titrage ainsi que la qualité conjointe de la segmentation et du titrage. L'approche donne de bonnes performances et se révèle robuste à la segmentation thématique.</resume>
			<mots_cles>Segmentation thématique, Titrage automatique, Pondération Okapi, Mesures de similarité</mots_cles>
			<title>Automatic Topic Segmentation and Title Assignment in TV Broadcast News</title>
			<abstract>This paper addresses the task of assigning a title to topic segments automatically extracted from TV Broadcast News video recordings. We propose to associate to a topic segment the title of a newspaper article collected on the web at the same date. The task implies pairing newspaper articles and topic segments by maximising a given similarity measure. This approach raises several issues, such as the selection of candidate newspaper articles, the vectorial representation of both the segment and the articles, the choice of a suitable similarity measure, and the robustness to automatic segmentation errors. Experiments were made on various French TV Broadcast News shows recorded during one week, in conjunction with text articles collected through the Google News homepage at the same period. We introduce a full evaluation framework allowing to measure the quality of topic segment retrieval, topic title assignment and also joint retrieval and titling. The approach yields good titling performance and reveals to be robust to automatic segmentation.</abstract>
			<keywords>Topic segmentation, Title assignation, Okapi weighting, Similarity measures</keywords>
		</article>
		<article id="taln-2015-court-034" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Brun</nom>
					<email>Caroline.Brun@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Diana Nicoleta</prenom>
					<nom>Popa</nom>
					<email>Diana.Popa@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>Roux</nom>
					<email>Claude.Roux@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">XRCE, 6 chemin de Maupertuis, 38240 Meylan France</affiliation>
			</affiliations>
			<titre>Un système hybride pour l'analyse de sentiments associés aux aspects</titre>
			<type>court</type>
			<pages>544-550</pages>
			<resume>Cet article présente en détails notre participation à la tâche 4 de SemEval2014 (Analyse de Sentiments associés aux Aspects). Nous présentons la tâche et décrivons précisément notre système qui consiste en une combinaison de composants linguistiques et de modules de classification. Nous exposons ensuite les résultats de son évaluation, ainsi que les résultats des meilleurs systèmes. Nous concluons par la présentation de quelques nouvelles expériences réalisées en vue de l'amélioration de ce système.</resume>
			<mots_cles>Analyse de sentiments associés aux aspects, SemEval2014, système hybride</mots_cles>
			<title>An Hybrid System for Aspect-Based Sentiment Analysis</title>
			<abstract>This paper details our participation to the SemEval2014 task 4 (Aspect Based Sentiment Analysis). We present the shared task, and then describe precisely our system, which is a combination of natural processing components and classificaton modules. We also present its evaluation results and the best system results. We finally expose some new experiments aiming at improving the system.</abstract>
			<keywords>Aspect Based Sentiment Analysis, SemEval2014, hybrid system</keywords>
		</article>
		<article id="taln-2015-court-035" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Atallah</nom>
					<email>caroline.atallah@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS (UMR 5263), Université de Toulouse</affiliation>
			</affiliations>
			<titre>La ressource EXPLICADIS, un corpus annoté spécifiquement pour l'étude des relations de discours causales</titre>
			<type>court</type>
			<pages>551-557</pages>
			<resume>Dans le but de proposer une caractérisation des relations de discours liées à la causalité, nous avons été amenée à constituer et annoter notre propre corpus d'étude : la ressource EXPLICADIS (EXPlication et Argumentation en DIScours). Cette ressource a été construite dans la continuité d'une ressource déjà disponible, le corpus ANNODIS. Proposant une annotation plus précise des relations causales sur un ensemble de textes diversifiés en genres textuels, EXPLICADIS est le premier corpus de ce type constitué spécifiquement pour l'étude des relations de discours causales.</resume>
			<mots_cles>annotation de corpus, discours, relations causales</mots_cles>
			<title>A corpus specifically annotated for causal discourse relations studies : the EXPLICADIS resource</title>
			<abstract>In order to offer a characterization of causal discourse relations, we created and annotated our own corpus : EXPLICADIS (EXPlanation and Argumentation in DIScourse). This corpus was built in the continuity of a readily available corpus, the ANNODIS corpus. Providing a more precise annotation of causal relations in a set of texts that are representative of multiple textual genres, EXPLICADIS is the first corpus of its kind built specifically for causal discourse relations studies.</abstract>
			<keywords>corpus annotation, discourse, causal relations</keywords>
		</article>
		<article id="taln-2015-court-036" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Lareau</nom>
					<email>francois.lareau@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gabriel</prenom>
					<nom>Bernier-Colborne</nom>
					<email>gabriel.bernier-colborne@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Drouin</nom>
					<email>patrick.drouin@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">OLST, Université de Montréal, C.P. 6128, succ. Centre-Ville, Montréal QC H3C 3J7, Canada</affiliation>
			</affiliations>
			<titre>La séparation des composantes lexicale et ﬂexionnelle des vecteurs de mots</titre>
			<type>court</type>
			<pages>558-564</pages>
			<resume>En sémantique distributionnelle, le sens des mots est modélisé par des vecteurs qui représentent leur distribution en corpus. Les modèles étant souvent calculés sur des corpus sans pré-traitement linguistique poussé, ils ne permettent pas de rendre bien compte de la compositionnalité morphologique des mots-formes. Nous proposons une méthode pour décomposer les vecteurs de mots en vecteurs lexicaux et ﬂexionnels.</resume>
			<mots_cles>Sémantique distributionnelle, compositionnalité, ﬂexion</mots_cles>
			<title>Separating the lexical and grammatical components of semantic vectors</title>
			<abstract>In distributional semantics, the meaning of words is modelled by vectors that represent their distribution in a corpus. Vectorial models being often built from corpora with little linguistic pre-treatment, they do not represent very well the morphological compositionality of words. We propose here a method to decompose semantic vectors into lexical and inﬂectional vectors.</abstract>
			<keywords>Distributional semantics, compositionality, inﬂection</keywords>
		</article>
		<article id="taln-2015-court-037" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Sascha</prenom>
					<nom>Diwersy</nom>
					<email>sascha.diwersy@uni-koeln.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Achille</prenom>
					<nom>Falaise</nom>
					<email>achillefalaise@hotmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie-Hélène</prenom>
					<nom>Lay</nom>
					<email>marie-helene.lay@univ-poitiers.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gilles</prenom>
					<nom>Souvay</nom>
					<email>gilles.souvay@atilf.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Cologne, Albertus­Magnus­Platz, D­50923 Köln, Allemagne</affiliation>
				<affiliation affiliationId="2">ICAR, ENS de Lyon, 15 Parvis René Descartes, 69342 Lyon, France</affiliation>
				<affiliation affiliationId="3">FoReLL, Université de Poitiers, 5 rue Théodore Lefebvre, 86073 Poitiers, France</affiliation>
				<affiliation affiliationId="3">ATILF­CNRS, Université Nancy 2, 44 avenue de la Libération, 54063 Nancy, France</affiliation>
			</affiliations>
			<titre>Traitements pour l'analyse du français préclassique</titre>
			<type>court</type>
			<pages>565-571</pages>
			<resume>La période « préclassique » du français s'étend sur tout le XVIe siècle et la première moitié du XVIIe siècle. Cet état de langue écrite, qui accompagne les débuts de l'imprimerie, est relativement proche du français moderne, mais se caractérise par une grande variabilité graphique. Il s'agit de l'un des moins bien dotés en termes de ressources. Nous présentons ici la construction d'un lexique, d'un corpus d'apprentissage et d'un modèle de langage pour la période préclassique, à partir de ressources du français moderne.</resume>
			<mots_cles>construction de lexique morphologique, annotation et étiquetage de corpus, linguistique diachronique</mots_cles>
			<title>Treatments for Preclassic French parsing</title>
			<abstract>The "Preclassical" French language period extends throughout the sixteenth century and the first half of the seventeenth century. This state of the written French language, which accompanies the beginnings of printing, is relatively close to the modern French, but is characterized by a large graphic variability. It is one of the most underresourced state of the French language. Here we present the construction of a lexicon, a training corpus and a language model for the Preclassic period, built from modern French resources.</abstract>
			<keywords>morphological lexicon construction, corpus annotation and tagging, diachronic linguistics</keywords>
		</article>
		<article id="taln-2015-court-038" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Holat</nom>
					<email>pierre.holat@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nadi</prenom>
					<nom>Tomeh</nom>
					<email>nadi.tomeh@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Charnois</nom>
					<email>thierry.charnois@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 13, Sorbonne Paris Cité, CNRS, LIPN UMR7030, 93430, France</affiliation>
			</affiliations>
			<titre>Classification de texte enrichie à l'aide de motifs séquentiels</titre>
			<type>court</type>
			<pages>572-578</pages>
			<resume>En classification de textes, la plupart des méthodes fondées sur des classifieurs statistiques utilisent des mots, ou des combinaisons de mots contigus, comme descripteurs. Si l'on veut prendre en compte plus d'informations le nombre de descripteurs non contigus augmente exponentiellement. Pour pallier à cette croissance, la fouille de motifs séquentiels permet d'extraire, de façon efficace, un nombre réduit de descripteurs qui sont à la fois fréquents et pertinents grâce à l'utilisation de contraintes. Dans ce papier, nous comparons l'utilisation de motifs fréquents sous contraintes et l'utilisation de motifs δ-libres, comme descripteurs. Nous montrons les avantages et inconvénients de chaque type de motif.</resume>
			<mots_cles>Fouille de séquences, motifs libres, classification de texte, sélection de descripteurs</mots_cles>
			<title>Sequential pattern mining for text classification</title>
			<abstract>Most methods in text classification rely on contiguous sequences of words as features. Indeed, if we want to take non-contiguous (gappy) patterns into account, the number of features increases exponentially with the size of the text. Furthermore, most of these patterns will be mere noise. To overcome both issues, sequential pattern mining can be used to efficiently extract a smaller number of relevant, non-contiguous, features. In this paper, we compare the use of constrained frequent pattern mining and δ-free patterns as features for text classification. We show experimentally the advantages and disadvantages of each type of patterns.</abstract>
			<keywords>Sequence mining, free patterns, text classification, feature selection</keywords>
		</article>
		<article id="taln-2015-court-039" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Florie</prenom>
					<nom>Lambrey</nom>
					<email>florie.lambrey@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Lareau</nom>
					<email>francois.lareau@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">OLST, Université de Montréal, C.P. 6128, succ. Centre-Ville, Montréal QC H3C 3J7, Canada</affiliation>
			</affiliations>
			<titre>Le traitement des collocations en génération de texte multilingue</titre>
			<type>court</type>
			<pages>579-585</pages>
			<resume>Pour concevoir des générateurs automatiques de texte génériques qui soient facilement réutilisables d'une langue et d'une application à l'autre, il faut modéliser les principaux phénomènes linguistiques qu'on retrouve dans les langues en général. Un des phénomènes fondamentaux qui demeurent problématiques pour le TAL est celui des collocations, comme grippe carabinée, peur bleue ou désir ardent, où un sens (ici, l'intensité) ne s'exprime pas de la même façon selon l'unité lexicale qu'il modifie. Dans la lexicographie explicative et combinatoire, on modélise les collocations au moyen de fonctions lexicales qui correspondent à des patrons récurrents de collocations. Par exemple, les expressions mentionnées ici se décrivent au moyen de la fonction Magn : Magn(PEUR) = BLEUE, Magn(GRIPPE) = CARABINÉE, etc. Il existe des centaines de fonctions lexicales. Dans cet article, nous nous intéressons à l'implémentation d'un sous-ensemble de fonctions qui décrivent les verbes supports et certains types de modificateurs.</resume>
			<mots_cles>génération automatique de texte multilingue ; collocations ; fonctions lexicales ; théorie sens-texte</mots_cles>
			<title>The treatment of collocations in multilingual text generation</title>
			<abstract>In order to develop generic natural language generators that could be reused across languages and applications, it is necessary to model the core linguistic phenomena that one finds in language. One central phenomenon that remains problematic in NLP is collocations, such as heavy rain, strong preference or intense ﬂavour, where the same idea (here, intensity), is expressed differently depending on the lexical unit it modifies. In explicative combinatory lexicography, collocations are modelled via lexical functions, which correspond to recurrent patterns of collocations. For instance, the three expressions above are all described with the function Magn : Magn(RAIN) = HEAVY, Magn(PREFERENCE) = STRONG , etc. There are hundreds of lexical functions. In this paper, we focus on the implementation of a subset of them that are used to model support verbs and some modifiers.</abstract>
			<keywords>multilingual natural language generation ; collocations ; lexical functions ; meaning-text theory</keywords>
		</article>
		<article id="taln-2015-court-040" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Morlane-Hondère</nom>
					<email>morlanehondere@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cyril</prenom>
					<nom>Grouin</nom>
					<email>cyril.grouin@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Moriceau</nom>
					<email>moriceau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email>pz@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI–CNRS, UPR 3251, rue John von Neumann, 91400 Orsay</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, Campus universitaire d'Orsay, 91400 Orsay</affiliation>
			</affiliations>
			<titre>Médicaments qui soignent, médicaments qui rendent malades : étude des relations causales pour identifier les effets secondaires</titre>
			<type>court</type>
			<pages>586-592</pages>
			<resume>Dans cet article, nous nous intéressons à la manière dont sont exprimés les liens qui existent entre un traitement médical et un effet secondaire. Parce que les patients se tournent en priorité vers internet, nous fondons cette étude sur un corpus annoté de messages issus de forums de santé en français. L'objectif de ce travail consiste à mettre en évidence des éléments linguistiques (connecteurs logiques et expressions temporelles) qui pourraient être utiles pour des systèmes automatiques de repérage des effets secondaires. Nous observons que les modalités d'écriture sur les forums ne permettent pas de se fonder sur les expressions temporelles. En revanche, les connecteurs logiques semblent utiles pour identifier les effets secondaires.</resume>
			<mots_cles>Pharmacovigilance, forums de santé, relations causales</mots_cles>
			<title>Drugs that cure, drugs that make you sick : study of causal links to identify drug side effects</title>
			<abstract>In this paper, we study the textual manifestations of the relation between drugs and side effects in online health forums. Our goal is to find relevant linguistic cues in order to improve the automatic identification of side effects by leveraging the ambiguity between actual side effects and indications (the reason for drug use). We find that the use of discourse markers can be relevant for the identification of indications – a third of indication mentions follow markers like ‘pour' (‘for') or ‘dans le but de' (‘with the aim of') – while temporal informations are not as discriminating.</abstract>
			<keywords>Pharmacovigilance, Health Forums, Causal Links</keywords>
		</article>
		<article id="taln-2015-court-041" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Gabriel</prenom>
					<nom>Bernier-Colborne</nom>
					<email>g.b.colborne@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">OLST, Université de Montréal, CP 6128, succ. Centre-Ville, Montréal (QC) Canada, H3C 3J7</affiliation>
			</affiliations>
			<titre>Exploration de modèles distributionnels au moyen de graphes 1-PPV</titre>
			<type>court</type>
			<pages>593-599</pages>
			<resume>Dans cet article, nous montrons qu'un graphe à 1 plus proche voisin (graphe 1-PPV) offre différents moyens d'explorer les voisinages sémantiques captés par un modèle distributionnel. Nous vérifions si les composantes connexes de ce graphe, qui représentent des ensembles de mots apparaissant dans des contextes similaires, permettent d'identifier des ensembles d'unités lexicales qui évoquent un même cadre sémantique. Nous illustrons également différentes façons d'exploiter le graphe 1-PPV afin d'explorer un modèle ou de comparer différents modèles.</resume>
			<mots_cles>Sémantique distributionnelle, sémantique lexicale, graphe, terminologie, sémantique des cadres</mots_cles>
			<title>Exploring distributional semantic models using a 1-NN graph</title>
			<abstract>We show how a 1-NN graph can be used to explore the semantic neighbourhoods modeled by distributional models of semantics. We check whether the connected components of the graph, which represent sets of words that occur in similar contexts, can be used to identify sets of lexical units that evoke the same semantic frame. We then illustrate different ways in which the 1-NN graph can be used to explore a model or compare different models.</abstract>
			<keywords>Distributional semantics, lexical semantics, graph, terminology, frame semantics</keywords>
		</article>
		<article id="taln-2015-court-042" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Killian</prenom>
					<nom>Janod</nom>
					<email>kjanod@orkis.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Morchid</nom>
					<email>mohamed.morchid@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Dufour</nom>
					<email>richard.dufour@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Georges</prenom>
					<nom>Linares</nom>
					<email>georges.linares@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA - University of Avignon (France)</affiliation>
				<affiliation affiliationId="2">ORKIS - Aix en Provence (France)</affiliation>
			</affiliations>
			<titre>Apport de l'information temporelle des contextes pour la représentation vectorielle continue des mots</titre>
			<type>court</type>
			<pages>600-606</pages>
			<resume>Les représentations vectorielles continues des mots sont en plein essor et ont déjà été appliquées avec succès à de nombreuses tâches en traitement automatique de la langue (TAL). Dans cet article, nous proposons d'intégrer l'information temporelle issue du contexte des mots au sein des architectures fondées sur les sacs-de-mots continus (continuous bag-of-words ou CBOW) ou sur les Skip-Grams. Ces approches sont manipulées au travers d'un réseau de neurones, l'architecture CBOW cherchant alors à prédire un mot sachant son contexte, alors que l'architecture Skip-Gram prédit un contexte sachant un mot. Cependant, ces modèles, au travers du réseau de neurones, s'appuient sur des représentations en sac-de-mots et ne tiennent pas compte, explicitement, de l'ordre des mots. En conséquence, chaque mot a potentiellement la même inﬂuence dans le réseau de neurones. Nous proposons alors une méthode originale qui intègre l'information temporelle des contextes des mots en utilisant leur position relative. Cette méthode s'inspire des modèles contextuels continus. L'information temporelle est traitée comme coefficient de pondération, en entrée du réseau de neurones par le CBOW et dans la couche de sortie par le Skip-Gram. Les premières expériences ont été réalisées en utilisant un corpus de test mesurant la qualité de la relation sémantique-syntactique des mots. Les résultats préliminaires obtenus montrent l'apport du contexte des mots, avec des gains de 7 et 7,7 points respectivement avec l'architecture Skip-Gram et l'architecture CBOW.</resume>
			<mots_cles>Réseau de neurones, Représentation vectorielle continue, Information contextuelle, Word2vec , Modèle de langue</mots_cles>
			<title>Contribution of temporal context information to a continuous vector representation of words</title>
			<abstract>Word embedding representations are gaining a lot of attention from researchers and have been successfully applied to various Natural Language Processing (NLP) tasks. In this paper, we propose to integrate temporal context information of words into the continuous bag-of-words (CBOW) and Skip-gram architectures for computing word-vector representations. Those architectures are shallow neural-networks. The CBOW architecture predicts a word given its context while the Skip-gram architecture predicts a context given a word. However, in those neural-networks, context windows are represented as bag-of-words. According to this representation, every word in the context is treated equally : the word order is not taken into account explicitly. As a result, each word will have the same inﬂuence on the network. We then propose an original method that integrates temporal information of word contexts using their relative position. This method is inspired from Continuous Context Models. The temporal information is treated as weights, in input by the CBOW and in the output layer by the Skip-Gram. The quality of the obtained models has been measured using a Semantic-Syntactic Word Relationship test set. Results showed that the incorporation of temporal information allows a substantial quality gain of 5 and 0.9 points respectively in comparison to the classical use of the CBOW and Skip-gram architectures.</abstract>
			<keywords>Neural network, Continuous vectorial representation, Contextual information, Word2vec, language model</keywords>
		</article>
		<article id="taln-2015-court-043" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Tian</prenom>
					<nom>Tian</nom>
					<email>tian.tian@live.cn</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dinarelli</prenom>
					<nom>Marco</nom>
					<email>marco.dinarelli@ens.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Tellier</prenom>
					<nom>Isabelle</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cardoso</prenom>
					<nom>Pedro</nom>
					<email>pedro@synthesio.com</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice, UMR 8094, 1 rue Maurice Arnoux, 92120 Montrouge</affiliation>
				<affiliation affiliationId="2">Synthesio, 8-10 rue Villedo, 75001 Paris</affiliation>
			</affiliations>
			<titre>Etiquetage morpho-syntaxique de tweets avec des CRF</titre>
			<type>court</type>
			<pages>607-613</pages>
			<resume>Nous nous intéressons dans cet article à l'apprentissage automatique d'un étiqueteur mopho-syntaxique pour les tweets en anglais. Nous proposons tout d'abord un jeu d'étiquettes réduit avec 17 étiquettes différentes, qui permet d'obtenir de meilleures performances en exactitude par rapport au jeu d'étiquettes traditionnel qui contient 45 étiquettes. Comme nous disposons de peu de tweets étiquetés, nous essayons ensuite de compenser ce handicap en ajoutant dans l'ensemble d'apprentissage des données issues de textes bien formés. Les modèles mixtes obtenus permettent d'améliorer les résultats par rapport aux modèles appris avec un seul corpus, qu'il soit issu de Twitter ou de textes journalistiques.</resume>
			<mots_cles>tweets, CRF, étiquettage morpho-syntaxique</mots_cles>
			<title>Part-of-speech Tagging for Tweets with CRFs</title>
			<abstract>We are insterested in this paper in training a part-of-speach tagger for tweets in English. We first propose a reduced tagset with 17 different tags, which allows better results in accuracy than traditional tagsets which contain 45 tags. Since we have few annoted tweets, we then try and overcome this difficulty by adding data from other more standard texts into the training set. The obtained models reach better results compared to models trained with only one corpus, whether coming of Twitter or of journalistic texts.</abstract>
			<keywords>tweets, CRFs, part-of-speech tagging</keywords>
		</article>
		<article id="taln-2015-court-044" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Amalia</prenom>
					<nom>Todirascu</nom>
					<email>todiras@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Beatriz</prenom>
					<nom>Sanchez Cardenas</nom>
					<email>bsc@ugr.es</email>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LiLPa, Université de Strasbourg, 22, Rue René Descartes, BP 80010, 67084 STRASBOURG cedex, France</affiliation>
				<affiliation affiliationId="2">LexiCon, Universidad de Grananda, Calle Buensuceso, 11 18002 Granada, Espagne</affiliation>
			</affiliations>
			<titre>Caractériser les discours académiques et de vulgarisation : quelles propriétés ?</titre>
			<type>court</type>
			<pages>614-620</pages>
			<resume> L'article présente une étude des propriétés linguistiques (lexicales, morpho-syntaxiques, syntaxiques) permettant la classification automatique de documents selon leur genre (articles scientifiques et articles de vulgarisation), dans deux domaines différentes (médecine et informatique). Notre analyse, effectuée sur des corpus comparables en genre et en thèmes disponibles en français, permet de valider certaines propriétés identifiées dans la littérature comme caractéristiques des discours académiques ou de vulgarisation scientifique. Les premières expériences de classification évaluent l'influence de ces propriétés pour l'identification automatique du genre pour le cas spécifique des textes scientifiques ou de vulgarisation.</resume>
			<mots_cles>analyse linguistique, discours scientifique et de vulgarisation, corpus comparables, classification selon le genre</mots_cles>
			<title>Characterizing scientific genre for academia and general public: which properties should be taken into account?</title>
			<abstract>The article focuses on the study of a set of morpho-syntactic properties for audience-based classification. The linguistic analysis of academic discourse and of popular science discourse reveals that both discourse types are characterized by specific linguistic and textual properties. This research used two French comparable corpora in regards to genre and subject matter. The corpora was composed of scientific articles and popular science texts in the domains of medicine and computer science. The experiments performed as part of our study evaluated the influence of discourse-specific morpho-syntactic properties on genre-based classification, for scientific and popular science texts.</abstract>
			<keywords>linguistic analysis, academic and popular science discourse, comparable corpora, genre-based classification</keywords>
		</article>
		<article id="taln-2015-court-045" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Suzanne</prenom>
					<nom>Mpouli</nom>
					<email>suzanne.mpouli@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Gabriel</prenom>
					<nom>Ganascia</nom>
					<email>jean-gabrie.ganascia@lip6.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Sorbonne Universités, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu 75005 Paris</affiliation>
				<affiliation affiliationId="2">Labex OBVIL, Université Paris-Sorbonne, 1 rue Victor Cousin 75005 Paris</affiliation>
			</affiliations>
			<titre>Extraction et analyse automatique des comparaisons et des pseudo-comparaisons pour la détection des comparaisons figuratives</titre>
			<type>court</type>
			<pages>621-627</pages>
			<resume>Le présent article s'intéresse à la détection et à la désambiguïsation des comparaisons figuratives. Il décrit un algorithme qui utilise un analyseur syntaxique de surface (chunker) et des règles manuelles afin d'extraire et d'analyser les (pseudo-)comparaisons présentes dans un texte. Cet algorithme, évalué sur un corpus de textes littéraires, donne de meilleurs résultats qu'un système reposant sur une analyse syntaxique profonde.</resume>
			<mots_cles>comparaisons figuratives, comparé, comparant, analyse syntaxique de surface, règles manuelles, analyse syntaxique profonde</mots_cles>
			<title>Extraction and automatic analysis of comparative and pseudo-comparative structures for simile detection</title>
			<abstract>This article is focused on automatic simile detection and disambiguation. It describes an algorithm which uses syntactic chunks and handcrafted rules to extract and analyse similes in a given text. This algorithm, which was evaluated on a corpus of literary texts, performs better than a system based on dependency parsing.</abstract>
			<keywords>simile, tenor, vehicle, chunking, handcrafted rules, dependency parsing</keywords>
		</article>
		<article id="taln-2015-court-046" session="Session 2">
			<auteurs>
				<auteur>
					<prenom>Johan</prenom>
					<nom>Ferguth</nom>
					<email>jferguth@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Jouannet</nom>
					<email>jouannet.aurelie@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Asma</prenom>
					<nom>Zamiti</nom>
					<email>zamiti.asma@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Damien</prenom>
					<nom>Nouvel</nom>
					<email>damien.nouvel@inalco.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Valette</nom>
					<email>mvalette@inalco.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yunhe</prenom>
					<nom>Wu</nom>
					<email>yunhe.wu9@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERTIM, INALCO - 2 rue de Lille, 75007 Paris</affiliation>
			</affiliations>
			<titre>Proposition méthodologique pour la détection automatique de Community Manager. Étude multilingue sur un corpus relatif à la Junk Food</titre>
			<type>court</type>
			<pages>628-634</pages>
			<resume>Dans cet article, nous présentons une méthodologie pour l'identification de messages suspectés d'être produits par des Community Managers à des fins commerciales déguisées dans des documents du Web 2.0. Le champ d'application est la malbouffe (junkfood) et le corpus est multilingue (anglais, chinois, français). Nous exposons dans un premier temps la stratégie de constitution et d'annotation de nos corpus, en explicitant notamment notre guide d'annotation, puis nous développons la méthode adoptée, basée sur la combinaison d'une analyse textométrique et d'un apprentissage supervisé.</resume>
			<mots_cles>Community Management, Textométrie, Multilinguisme, Fouille de texte</mots_cles>
			<title>Methodological Proposal for Automatic Detection of Community Manager. Multilingual Study based on a Junk Food corpus</title>
			<abstract>This article describes the methodology for identifying a certain kind of speech in internet forums. The detection of the speech of a Community Manager combines recent issues in the domain of Natural Language Processing, including opinion mining and sentiment analysis, with another more abstract problem. Going beyond detecting the polarity of a message, this project targets the underlying intentions and identity of the author of the message on the forum.</abstract>
			<keywords>Community Management, Textometry, Multilingualism, Data Mining</keywords>
		</article>
		<article id="taln-2015-demo-001" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Zied</prenom>
					<nom>Sellami</nom>
					<email>zied.sellami@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Gabriel</prenom>
					<nom>Ganascia</nom>
					<email>jean-gabriel@ganascia.name</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed Amine</prenom>
					<nom>Boukhaled</nom>
					<email>mohamed.boukhaled@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d'Informatique de Paris 6 (LIP6) 4 Place Jussieu, 75005 Paris</affiliation>
			</affiliations>
			<titre>MEDITE : logiciel d'alignement de textes pour l'étude de la génétique textuelle</titre>
			<type>démonstration</type>
			<pages>635-636</pages>
			<resume>MEDITE est un logiciel d'alignement de textes permettant l'identification de transformations entre une version et une autre d'un même texte. Dans ce papier nous présentons les aspects théoriques et techniques de MEDITE.</resume>
			<mots_cles>Alignement de textes, Génétique textuelle, Détection d'homologies dans les séquences textuelles</mots_cles>
			<title>MEDITE: text alignment software for the study of textual genetics</title>
			<abstract>MEDITE is an alignment software able to identifying transformations between two versions of a same text. In this paper we show the theoretical and technical aspects of this tool.</abstract>
			<keywords>Text alignment, Textual genetics, Homology detection in text sequences</keywords>
		</article>
		<article id="taln-2015-demo-002" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Mohamed Amine</prenom>
					<nom>Boukhaled</nom>
					<email>mohamed.boukhaled@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Zied</prenom>
					<nom>Sellami</nom>
					<email>Zied.Sellami@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Gabriel</prenom>
					<nom>Ganascia</nom>
					<email>Jean-Gabriel.Ganascia@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIP6 (Laboratoire d'Informatique de Paris 6), Université Pierre et Marie Curie and CNRS (UMR7606)</affiliation>	
			</affiliations>
			<titre>Phœbus : un Logiciel d'Extraction de Réutilisations dans des Textes Littéraires</titre>
			<type>démonstration</type>
			<pages>637-639</pages>
			<resume>Phœbus est un logiciel d'extraction de réutilisations dans des textes littéraires. Il a été développé comme un outil d'analyse littéraire assistée par ordinateur. Dans ce contexte, ce logiciel détecte automatiquement et explore des réseaux de réutilisation textuelle dans la littérature classique.</resume>
			<mots_cles>Extraction de réutilisations, empreintes digitales textuelles, analyse littéraire assistée par ordinateur</mots_cles>
			<title>Phoebus: a Reuse Extraction Software for Literary Text</title>
			<abstract>Phoebus is a reuse extraction software for literary text. It was developed as a computer-assisted literary analysis tool. In this context, the software automatically detects and explores textual reuse networks in classical literature.</abstract>
			<keywords>Reuse Extraction, textual fingerprintng, computer-assisted literary analysis</keywords>
		</article>
		<article id="taln-2015-demo-003" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Jérôme</prenom>
					<nom>Lehuen</nom>
					<email>Jerome.Lehuen@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Carole</prenom>
					<nom>Lailler</nom>
					<email>Carole.Lailler@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Stenzhorn</nom>
					<email>Julien.Stenzhorn@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">	</affiliation>						
				<affiliation affiliationId="2">	</affiliation>
				<affiliation affiliationId="3">	</affiliation>
			</affiliations>
			<titre>YADTK : Une plateforme open-source à base de règles pour développer des systèmes de dialogue oral</titre>
			<type>démonstration</type>
			<pages>640-641</pages>
			<resume>YADTK est une plateforme open-source pour développer des systèmes de dialogue oral. De part son caractère déclaratif et unifié, le modèle de représentation des connaissances permet un développement rapide et facilité.</resume>
			<mots_cles></mots_cles>
			<title>YADTK: An open-source, rule-based platform for speech dialogue system development</title>
			<abstract>YADTK is an open-source, rule-based framework to build spoken dialogue systems. The declarative and unified nature of the model of knowledge representation allows a rapid and easier development process.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2015-demo-004" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Annie</prenom>
					<nom>Foret</nom>
					<email>foret@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA, Université Rennes 1, France</affiliation>						
			</affiliations>
			<titre>TermLis : un contexte d'information logique pour des ressources terminologiques</titre>
			<type>démonstration</type>
			<pages>642-643</pages>
			<resume>Nous présentons TermLis un contexte d'information logique construit à partir de ressources terminologiques disponibles en xml (FranceTerme), pour une utilisation ﬂexible avec un logiciel de contexte logique (CAMELIS). Une vue en contexte logique permet d'explorer des informations de manière ﬂexible, sans rédaction de requête a priori, et d'obtenir aussi des indications sur la qualité des données. Un tel contexte peut être enrichi par d'autres informations (de natures diverses), mais aussi en le reliant à d'autres applications (par des actions associées selon des arguments fournis par le contexte). Nous montrons comment utiliser TermLis et nous illustrons, à travers cette réalisation concrète sur des données de FranceTerme, les avantages d'une telle approche pour des données terminologiques.</resume>
			<mots_cles>Applications multilingues, Classification, Extraction d'information, Fouilles de données textuelles, Recherche d'information, Ressources du langage, Données Ouvertes, Qualité des données, Données légales</mots_cles>
			<title>TermLis : a logical information context for terminological resources</title>
			<abstract>We present TermLis a logical information context constructed from terminological resources available in XML (FranceTerme), for a ﬂexible use with a logical context system (CAMELIS). A logical view of a context allows to explore information in a ﬂexible way, without writing explicit queries, it may also provide insights on the quality of the data. Such a context can be enriched by other information (of diverse natures), it can also be linked with other applications (according to arguments supplied by the context). We show how to use TermLis and we illustrate, through this concrete realization from FranceTerme data, the advantages of such an approach with terminological data.</abstract>
			<keywords>Multilingual applications, Classification, Information extraction, Textual data mining, Information retrieval, Linguistic resources, Open Data, Information Quality, Legal Information</keywords>
		</article>
		<article id="taln-2015-demo-005" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Leila</prenom>
					<nom>Khouas</nom>
					<email>lkh@amisw.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Brun</nom>
					<email>caroline.brun@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Peradotto</nom>
					<email>anne.peradotto@edf.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Valère</prenom>
					<nom>Cossu</nom>
					<email>jean-valere.cossu@univ-avignon.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Boyadjian</nom>
					<email>julien.boyadjian@hotmail.fr</email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Velcin</nom>
					<email>julien.velcin@univ-lyon2.fr</email>
					<affiliationId>6</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">AMI Software R&amp;D, 1475 av. A. Einstein 34000 Montpellier, France</affiliation>				
				<affiliation affiliationId="2">Xerox Research Centre Europe, 6 chemin de Maupertuis, 38240 Meylan, France</affiliation>
				<affiliation affiliationId="3">EDF R&amp;D, ICAME, 1 av. du Général de Gaulle, 92140 Clamart, France</affiliation>
				<affiliation affiliationId="4">LIA CERI, 339 chemin des Meinajariès, 84911 Avignon, France</affiliation>
				<affiliation affiliationId="5">Centre d'Études Politiques de l'Europe Latine, 39 rue de l'Université, 34060 Montpellier, France</affiliation>
				<affiliation affiliationId="6">ERIC, Université de Lyon 2, 5 av. P. Mendès-France, 69676 Bron, France</affiliation>
			</affiliations>
			<titre>Etude de l'image de marque d'entités dans le cadre d'une plateforme de veille sur le Web social</titre>
			<type>démonstration</type>
			<pages>644-645</pages>
			<resume>Ce travail concerne l'intégration à une plateforme de veille sur internet d'outils permettant l'analyse des opinions émises par les internautes à propos d'une entité, ainsi que la manière dont elles évoluent dans le temps. Les entités considérées peuvent être des personnes, des entreprises, des marques, etc. Les outils implémentés sont le produit d'une collaboration impliquant plusieurs partenaires industriels et académiques dans le cadre du projet ANR ImagiWeb.</resume>
			<mots_cles>Plateforme de veille sur internet, médias sociaux, analyse d'opinion, fouille de données</mots_cles>
			<title>Study the brand image of entities as part of a social media-monitoring platform</title>
			<abstract>The work presented here is about a Web monitoring software providing powerful tools for the analysis of opinions expressed in social media about an entity, such as a celebrity, a company or a brand. The implemented tools result from the research ANR project ImagiWeb involving several industrial and academic partners.</abstract>
			<keywords>Web monitoring software, social media, opinion analysis, data mining</keywords>
		</article>
		<article id="taln-2015-demo-006" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Ngoc Tan</prenom>
					<nom>Le</nom>
					<email>letan.dhcn@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fatiha</prenom>
					<nom>Sadat</nom>
					<email>sadat.fatiha@uqam.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département Informatique, Université du Québec à Montréal, 201 avenue Président Kennedy, H2X 3Y7 Montréal, Québec, Canada</affiliation>
			</affiliations>
			<titre>Building a Bilingual Vietnamese-French Named Entity Annotated Corpus through Cross-Linguistic Projection</titre>
			<type>démonstration</type>
			<pages>646-647</pages>
			<resume>La création de ressources linguistiques de bonne qualité annotées en entités nommées est très coûteuse en temps et en main d'œuvre. La plupart des corpus standards sont disponibles pour l'anglais mais pas pour les langues peu dotées, comme le vietnamien. Pour les langues asiatiques, cette tâche reste très difficile. Le présent article concerne la création automatique de corpus annotés en entités nommées pour le vietnamien-français, une paire de langues peu dotée. L'application d'une méthode basée sur la projection cross-lingue en utilisant des corpus parallèles. Les évaluations ont montré une bonne performance (F-score de 94.90%) lors de la reconnaissance des paires d'entités nommées dans les corpus parallèles et ainsi la construction d'un corpus bilingue annoté en entités nommées.</resume>
			<mots_cles>Entité nommée, corpus parallèle, projection cross-lingue</mots_cles>
			<title>Building a Bilingual Vietnamese-French Named Entity Annotated Corpus through Cross-Linguistic Projection</title>
			<abstract>The creation of high-quality named entity annotated resources is time-consuming and an expensive process. Most of the gold standard corpora are available for English but not for less-resourced languages such as Vietnamese. In Asian languages, this task is remained problematic. This paper focuses on an automatic construction of named entity annotated corpora for Vietnamese-French, a less-resourced pair of languages. We incrementally apply different cross-projection methods using parallel corpora, such as perfect string matching and edit distance similarity. Evaluations on Vietnamese –French pair of languages show a good accuracy (F-score of 94.90%) when identifying named entities pairs and building a named entity annotated parallel corpus.</abstract>
			<keywords>Named entity, parallel corpus, cross-projection</keywords>
		</article>
		<article id="taln-2015-demo-007" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email>Bruno.Guillaume@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA, Inria Nancy Grand-Est</affiliation>						
			</affiliations>
			<titre>Recherche de motifs de graphe en ligne</titre>
			<type>démonstration</type>
			<pages>648-649</pages>
			<resume>Nous présentons un outil en ligne de recherche de graphes dans des corpus annotés en syntaxe.</resume>
			<mots_cles>Syntaxe de dépendances, Corpus, Graphes</mots_cles>
			<title>Online Graph Matching</title>
			<abstract>We present an online tool for graph pattern matching in syntactically annotated corpora.</abstract>
			<keywords>Dependency Syntax, Corpus, Graph matching</keywords>
		</article>
		<article id="taln-2015-demo-008" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Leonardo</prenom>
					<nom>Campillos</nom>
					<email>leonardo.campillos@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dhouha</prenom>
					<nom>Bouamor</nom>
					<email>dhouha.bouamor@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Bilinski</nom>
					<email>eric.bilnski@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Ligozat</nom>
					<email>annlor@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email>pz@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Rosset</nom>
					<email>rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI - CNRS, Orsay</affiliation>
			</affiliations>
			<titre>Un patient virtuel dialogant</titre>
			<type>démonstration</type>
			<pages>650-651</pages>
			<resume>Le démonstrateur que nous décrivons ici est un prototype de système de dialogue dont l'objectif est de simuler un patient. Nous décrivons son fonctionnement général en insistant sur les aspects concernant la langue et surtout le rapport entre langue médicale de spécialité et langue générale.</resume>
			<mots_cles>Patient virtuel, système de dialogue, langage spécialisé, langage grand public</mots_cles>
			<title>An Interactive Virtual Patient</title>
			<abstract>This paper describes the work-in-progress prototype of a dialog system that simulates a virtual patient consultation. We describe the general architecture and specifically the mapping between technical and lay terms in the medical domain.</abstract>
			<keywords>Virtual patient, dialog system, specialised language, lay language</keywords>
		</article>
		<article id="taln-2015-demo-009" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Achille</prenom>
					<nom>Falaise</nom>
					<email>achillefalaise@hotmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ICAR, ENS de Lyon, 15 parvis René Descartes, 69342 LYON cedex 7</affiliation>	
			</affiliations>
			<titre>Intégration du corpus des actes de TALN à la plateforme ScienQuest</titre>
			<type>démonstration</type>
			<pages>652-653</pages>
			<resume>Cette démonstration présente l'intégration du corpus arboré des Actes de TALN à la plateforme ScienQuest. Cette plateforme fut initialement créée pour l'étude du corpus de textes scientifiques Scientext. Cette intégration tient compte des méta­données propres au corpus TALN, et a été effectuée en s'efforçant de rapprocher les jeux d'étiquettes de ces deux corpus, et en convertissant pour le corpus TALN les requêtes prédéfinies conçues pour le corpus Scientext, de manière à permettre d'effectuer facilement des recherches similaires sur les deux corpus.</resume>
			<mots_cles>corpus, corpus arborés, environnement d'étude de corpus</mots_cles>
			<title>Integration of the TALN proceedings treebank to the ScienQuest platform</title>
			<abstract>This demonstration shows the integration of the TALN proceedings Treebank to the ScienQuest platform. This platform was initially created for the study of the Scientext scientific texts corpus. This integration takes into account the metadata to the TALN corpus, and was done in an effort to reconcile these two corpora's sets of labels, and to convert for the TALN corpus the predefined queries designed for the Scientext corpus, in order to easily perform similar queries on the two corpora.</abstract>
			<keywords>corpora, treebanks, corpus study environment</keywords>
		</article>
		<article id="taln-2015-demo-010" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Merlo</nom>
					<email>aurelie.merlo@yahoo.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>												
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Savoirs, Textes, Langage (STL), rue du Barreau, 59653 Villeneuve d'Ascq Cedex</affiliation>	
				<affiliation affiliationId="2">Ergonotics SAS, 165 avenue de Bretagne, 59000 Lille, France</affiliation>
			</affiliations>
			<titre>Une aide à la communication par pictogrammes avec prédiction sémantique</titre>
			<type>démonstration</type>
			<pages>654-656</pages>
			<resume>Cette démonstration présente une application mobile (pour tablette et smartphone) pour des personnes souffrant de troubles du langage et/ou de la parole permettant de générer des phrases à partir de la combinaison de pictogrammes puis de verbaliser le texte généré en Text-To-Speech (TTS). La principale critique adressée par les patients utilisant les solutions existantes est le temps de composition trop long d'une phrase. Cette limite ne permet pas ou très difficilement d'utiliser les solutions actuelles en condition dialogique. Pour pallier cela, nous avons développé un moteur de génération de texte avec prédiction sémantique ne proposant à l'utilisateur que les pictogrammes pertinents au regard de la saisie en cours (e.g. après le pictogramme [manger], l'application propose les pictogrammes [pomme] ou encore [viande] correspondant à des concepts comestibles). Nous avons ainsi multiplié de 5 à 10 la vitesse de composition d'une phrase par rapport aux solutions existantes.</resume>
			<mots_cles>génération de texte, prédiction sémantique, frame semantics, handicap, aide à la communication</mots_cles>
			<title>An Augmentative and Alternative Communication with semantic prediction</title>
			<abstract>This demo shows a mobile app (for smartphone and tablet) for people with language and/or speech disorders for generating sentences from the combination of icons and verbalize text generated by Text-To-Speech (TTS). The main criticism expressed by patients using existing solutions is the time too long required to compose a sentence. Hence, existing solutions are hard to use in dialogic conditions. To allieviate this problem, we have developed a text generation engine with semantic prediction. This engine only proposes to user the relevant pictograms in view of the current entry (e.g. after the pictogram [eat], the application offers pictograms [apple] or [meat] corresponding to edible concepts). We have multiplied from 5 to 10 the speed of composing a sentence compared to existing solutions.</abstract>
			<keywords>natural language generation, semantic prediction, frame semantics, disability, AAC</keywords>
		</article>
		<article id="taln-2015-demo-011" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Cédric</prenom>
					<nom>Lopez</nom>
					<email>cedric.lopez@viseo.com</email>
					<affiliationId>1</affiliationId>
				</auteur>												
				<auteur>
					<prenom>Aleksandra</prenom>
					<nom>Ponomareva</nom>
					<email>aleksandra.ponomareva@viseo.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cécile</prenom>
					<nom>Robin</nom>
					<email>robin@ho2s.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>André</prenom>
					<nom>Bittar</nom>
					<email>bittar@ho2s.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xabier</prenom>
					<nom>Larrucea</nom>
					<email>xabier.larrucea@tecnalia.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédérique</prenom>
					<nom>Segond</nom>
					<email>frederique.segond@viseo.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie-Hélène</prenom>
					<nom>Metzger</nom>
					<email>marie-helene.metzger@chu-lyon.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Viseo Technologies, 4 avenue Doyen Louis Weil, Grenoble (France)</affiliation>
				<affiliation affiliationId="2">Holmes Semantic Solutions, 12-14, rue Claude Genin, Grenoble (France)</affiliation>
				<affiliation affiliationId="3">Tecnalia, Parque tecnologico de Bizkaia, Edif. 202, Zamudio (Espagne)</affiliation>
				<affiliation affiliationId="4">Université Lyon 1, CNRS, UMR5558, Laboratoire de Biométrie et Biologie Evolutive, Villeurbanne (France)</affiliation>
			</affiliations>
			<titre>Un système expert fondé sur une analyse sémantique pour l'identification de menaces d'ordre biologique</titre>
			<type>démonstration</type>
			<pages>657-658</pages>
			<resume>Le projet européen TIER (Integrated strategy for CBRN – Chemical, Biological, Radiological and Nuclear – Threat Identification and Emergency Response) vise à intégrer une stratégie complète et intégrée pour la réponse d'urgence dans un contexte de dangers biologiques, chimiques, radiologiques, nucléaires, ou liés aux explosifs, basée sur l'identification des menaces et d'évaluation des risques. Dans cet article, nous nous focalisons sur les risques biologiques. Nous présentons notre système expert fondé sur une analyse sémantique, permettant l'extraction de données structurées à partir de données non structurées dans le but de raisonner.</resume>
			<mots_cles>TIER, SGRM, Système de Gestion de Règles Métier, analyse sémantique</mots_cles>
			<title>An Expert System Based on a Semantic Analysis for Identifying Biological Threats</title>
			<abstract>The European project TIER (Integrated strategy for CBRN – Chemical, Biological, Radiological and Nuclear - Threat Identification and Emergency Response) aims at developing a comprehensive and integrated strategy for emergency response in case of chemical, biological, radiological and nuclear danger, as well as explosives use, based on threat identification and risk assessment. In this article, we focus on the biological risks. We introduce our business rules management system based on a semantic analysis, that enables the extraction of structured data from unstructured data with the aim to make reasoning.</abstract>
			<keywords>TIER, BRMS, Business Rules Management System, semantic analysis</keywords>
		</article>
		<article id="taln-2015-demo-012" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>George</prenom>
					<nom>Christodoulides</nom>
					<email>george@mycontent.gr</email>
					<affiliationId>1</affiliationId>
				</auteur>												
				<auteur>
					<prenom>Giulia</prenom>
					<nom>Barreca</nom>
					<email>giulia.barreca@gmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>				
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Avanzi</nom>
					<email>mathieu.avanzi@gmail.com</email>
					<affiliationId>3</affiliationId>
				</auteur>				
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre Valibel, Institut Langue &amp; Communication, Université de LouvainPlace Blaise Pascal 1, B-1348 Louvain-la-Neuve, Belgique</affiliation>	
				<affiliation affiliationId="2">Laboratoire MoDyCo, CNRS, Université Paris Ouest Nanterre La Défense 200, Avenue de la République, FR-92001 Nanterre, France Université Catholique de Milan 1, Largo A. Gemelli, 20123 Milan, Italie</affiliation>
				<affiliation affiliationId="3">DTAL, Faculty of Modern &amp; Medieval Languages, University of Cambridge Sidgwick Avenue, CB3 9DA Cambridge, Royaume-Uni</affiliation>
			</affiliations>
			<titre>DisMo : un annotateur multi-niveaux pour les corpus oraux</titre>
			<type>démonstration</type>
			<pages>659-661</pages>
			<resume>Dans cette démonstration, nous présentons l'annotateur multi-niveaux DisMo, un outil conçu pour faire face aux spécificités des corpus oraux. Il fournit une annotation morphosyntaxique, une lemmatisation, une détection des unités poly-lexicales, une détection des phénomènes de disfluence et des marqueurs de discours.</resume>
			<mots_cles>annotation morphosyntaxique, corpus oraux, disfluences, unités poly-lexicales</mots_cles>
			<title>DisMo: a multi-level annotator for spoken language</title>
			<abstract>In this demonstration we present the multi-level automatic annotator DisMo which is specifically designed for the challenges posed by spoken language corpora. Its output comprises of part-of-speech tagging, lemmatization, multi-word unit detection, detection of disfluency phenomena and discourse markers.</abstract>
			<keywords>part-of-speech tagging, spoken corpora, disfluencies, multi-word expressions</keywords>
		</article>
	</articles>
</conference>