<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>RECITAL'2011</acronyme>
		<titre>13e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Montpellier</ville>
		<pays>France</pays>
		<dateDebut>2011-06-27</dateDebut>
		<dateFin>2011-07-01</dateFin>
		<presidents>
			<president>
				<prenom>Cédric</prenom>
				<nom>Lopez</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
		</typeArticles>
		<siteWeb>http://www.lirmm.fr/~lopez/TALN2011/</siteWeb>
	</edition>
	<articles>
		<article id="recital-2011-long-001" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<prenom>Fanny</prenom>
					<nom>Lalleman</nom>
					<email>fanny.lalleman@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE &amp; CNRS, 5, allées Antonio Machado 31058 Toulouse Cedex 9</affiliation>
				<affiliation affiliationId="2">Orange Labs, 2, Avenue Pierre Marzin 22307 Lannion Cedex</affiliation>
			</affiliations>
			<auteurs></auteurs>
			<titre>Analyse de l’ambiguïté des requêtes utilisateurs par catégorisation thématique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous cherchons à identifier la nature de l’ambiguïté des requêtes utilisateurs issues d’un moteur de recherche dédié à l’actualité, 2424actu.fr, en utilisant une tâche de catégorisation. Dans un premier temps, nous verrons les différentes formes de l’ambiguïté des requêtes déjà décrites dans les travaux de TAL. Nous confrontons la vision lexicographique de l’ambiguïté à celle décrite par les techniques de classification appliquées à la recherche d’information. Dans un deuxième temps, nous appliquons une méthode de catégorisation thématique afin d’explorer l’ambiguïté des requêtes, celle-ci nous permet de conduire une analyse sémantique de ces requêtes, en intégrant la dimension temporelle propre au contexte des news. Nous proposons une typologie des phénomènes d’ambiguïté basée sur notre analyse sémantique. Enfin, nous comparons l’exploration par catégorisation à une ressource comme Wikipédia, montrant concrètement les divergences des deux approches.</resume>
			<mots_cles>recherche d’information, ambiguïté, classification de requêtes</mots_cles>
			<title></title>
			<abstract>In this paper, we try to identify the nature of ambiguity of user queries from a search engine dedicated to news, 2424actu.fr, using a categorization task. At first, we see different forms of ambiguity queries already described in the works of NLP. We confront lexicographical vision of the ambiguity to that described by classification techniques applied to information retrieval. In a second step, we apply a method of categorizing themes to explore the ambiguity of queries, it allow us to conduct a semantic analysis of these applications by integrating temporal context-specific news. We propose a typology of phenomena of ambiguity based on our semantic analysis. Finally, we compare the exploration by categorization with a resource as Wikipedia, showing concretely the differences between these two approaches.</abstract>
			<keywords>Information retrieval, ambiguity, classification queries</keywords>
		</article>
		<article id="recital-2011-long-002" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<prenom>Boutheina</prenom>
					<nom>Smine</nom>
					<email>Boutheina.Smine@etudiants.univ-paris4.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Rim</prenom>
					<nom>Faiz</nom>
					<email>Rim.Faiz@ihec.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Pierre</prenom>
					<nom>Desclés</nom>
					<email>Jean-pierre.Descles@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaLIC, Université Paris-Sorbonne, 28 rue Serpente, 75006 Paris, France</affiliation>
				<affiliation affiliationId="2">LaRODEC, IHEC de Carthage, 2016 Carthage Présidence, Tunisie</affiliation>
			</affiliations>
			<titre>Extraction Automatique d'Informations Pédagogiques Pertinentes à partir de Documents Textuels</titre>
			<type>long</type>
			<pages></pages>
			<resume>Plusieurs utilisateurs ont souvent besoin d'informations pédagogiques pour les intégrer dans leurs ressources pédagogiques, ou pour les utiliser dans un processus d'apprentissage. Une indexation de ces informations s'avère donc utile en vue d'une extraction des informations pédagogiques pertinentes en réponse à une requête utilisateur. La plupart des systèmes d'extraction d'informations pédagogiques existants proposent une indexation basée sur une annotation manuelle ou semi-automatique des informations pédagogiques, tâche qui n'est pas préférée par les utilisateurs. Dans cet article, nous proposons une approche d'indexation d'objets pédagogiques (Définition, Exemple, Exercice, etc.) basée sur une annotation sémantique par Exploration Contextuelle des documents. L'index généré servira à une extraction des objets pertinents répondant à une requête utilisateur sémantique. Nous procédons, ensuite, à un classement des objets extraits selon leur pertinence en utilisant l'algorithme Rocchio. Notre objectif est de mettre en valeur une indexation à partir de contextes sémantiques et non pas à partir de seuls termes linguistiques.</resume>
			<mots_cles>extraction d’informations, objets pédagogiques, carte sémantique, exploration contextuelle, algorithme Rocchio</mots_cles>
			<title></title>
			<abstract>Different users need pedagogical information in order to use them in their resources or in a learning process. Indexing this information is therefore useful for extracting relevant pedagogical information in response to a user request. Several searching systems of pedagogical information propose manual or semi-automatic annotations to index documents, which is a complex task for users. In this article, we propose an approach to index pedagogical objects (Definition, Exercise, Example, etc.) based on automatic annotation of documents using Contextual Exploration. Then, we use the index to extract relevant pedagogical objects as response to the user's requests. We proceed to sort the extracted objects according to their relevance. Our objective is to reach the relevant objects using a contextual semantic analysis of the text.</abstract>
			<keywords>Information retrieval, pedagogical objects, semantic map, Contextual Exploration, Rocchio algorithm</keywords>
		</article>
		<article id="recital-2011-long-003" session="Extraction d’informations/de relations">
			<auteurs>
				<auteur>
					<prenom>Nikola</prenom>
					<nom>Tulechki</nom>
					<email>nikola.tulechki@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS, Université de Toulouse-Le Mirail, CNRS</affiliation>
			</affiliations>
			<titre>Des outils de TAL en support aux experts de sûreté industrielle pour l’exploitation de bases de données de retour d’expérience</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente des applications d’outils et méthodes du traitement automatique des langues (TAL) à la maîtrise du risque industriel grâce à l’analyse de données textuelles issues de volumineuses bases de retour d’expérience (REX). Il explicite d’abord le domaine de la gestion de la sûreté, ses aspects politiques et sociaux ainsi que l’activité des experts en sûreté et les besoins qu’ils expriment. Dans un deuxième temps il présente une série de techniques, comme la classification automatique de documents, le repérage de subjectivité, et le clustering, adaptées aux données REX visant à répondre à ces besoins présents et à venir, sous forme d’outils, en support à l’activité des experts.</resume>
			<mots_cles>REX, rapport d’incident, risque, sûreté industrielle, signaux faibles, classification automatique, clustering, recherche d’information, similarité, subjectivité</mots_cles>
			<title></title>
			<abstract>This article presents a series of natural language processing (NLP) techniques, applied to the domain of industrial risk management and the analysis of large collections of textual feedback data. First we describe the socio-political aspects of the risk mangement domain, the activity of the investigators working with this data. We then present present applications of NLP techniques like automatic text classification, clustering and opinion extraction, responding to different needs stated by the investigators.</abstract>
			<keywords>risk management, incident report, industrial safety, weak signals, automatic classification, information retrieval, similarity, clustering, subjectivity</keywords>
		</article>
		<article id="recital-2011-long-004" session="Discours">
			<auteurs>
				<auteur>
					<prenom>Charlotte</prenom>
					<nom>Roze</nom>
					<email>charlotteroze@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7</affiliation>
			</affiliations>
			<titre>Vers une algèbre des relations de discours pour la comparaison de structures discursives</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous proposons une méthodologie pour la construction de règles de déduction de relations de discours, destinées à être intégrées dans une algèbre de ces relations. La construction de ces règles a comme principal objectif de pouvoir calculer la fermeture discursive d’une structure de discours, c’est-à-dire de déduire toutes les relations que la structure contient implicitement. Calculer la fermeture des structures discursives peut permettre d’améliorer leur comparaison, notamment dans le cadre de l’évaluation de systèmes d’analyse automatique du discours. Nous présentons la méthodologie adoptée, que nous illustrons par l’étude d’une règle de déduction.</resume>
			<mots_cles>Relation de discours, fermeture discursive, évaluation, déduction</mots_cles>
			<title></title>
			<abstract>We propose a methodology for the construction of discourse relations inference rules, to be integrated into an algebra of these relations. The construction of these rules has as main objective to allow for the calculation of the discourse closure of a structure, i.e. deduce all the relations implicitly contained in the structure. Calculating the closure of discourse structures improves their comparison, in particular within the evaluation of discourse parsing systems. We present the adopted methodology, which we illustrate by the study of a rule.</abstract>
			<keywords>Discourse relation, discourse closure, evaluation, inference</keywords>
		</article>
		<article id="recital-2011-long-005" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<prenom>Prajol</prenom>
					<nom>Shrestha</nom>
					<email>Prajol.Shrestha@etu.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, Université de Nantes, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume>Les corpus comparables monolingues, alignés non pas au niveau des documents mais au niveau d’unités textuelles plus fines (paragraphe, phrases, etc.), sont utilisés dans diverses applications de traitement automatique des langues comme par exemple en détection de plagiat. Mais ces types de corpus ne sont pratiquement pas disponibles et les chercheurs sont donc obligés de les construire et de les annoter manuellement, ce qui est un travail très fastidieux et coûteux en temps. Dans cet article, nous présentons une méthode, composée de deux étapes, qui permet de réduire ce travail d’annotation de segments de texte. Cette méthode est évaluée lors de l’alignement de paragraphes provenant de dépêches en langue anglaise issues de diverses sources. Les résultats obtenus montrent un apport considérable de la méthode en terme de réduction de temps d’annotation. Nous présentons aussi des premiers résultats obtenus à l’aide de simples traitements automatiques (recouvrement de mots, de racines, mesure cosinus) pour tenter de diminuer encore la charge de travail humaine.</resume>
			<mots_cles>corpus comparable monolingue, alignement, similarité</mots_cles>
			<title>Alignment of Monolingual Corpus by Reduction of the Search Space</title>
			<abstract>Monolingual comparable corpora annotated with alignments between text segments (paragraphs, sentences, etc.) based on similarity are used in a wide range of natural language processing applications like plagiarism detection, information retrieval, summarization and so on. The drawback wanting to use them is that there aren’t many standard corpora which are aligned. Due to this drawback, the corpus is manually created, which is a time consuming and costly task. In this paper, we propose a method to significantly reduce the search space for manual alignment of the monolingual comparable corpus which in turn makes the alignment process faster and easier. This method can be used in making alignments on different levels of text segments. Using this method we create our own gold corpus aligned on the level of paragraph, which will be used for testing and building our algorithms for automatic alignment. We also present some experiments for the reduction of search space on the basis of stem overlap, word overlap, and cosine similarity measure which help us automatize the process to some extent and reduce human effort for alignment.</abstract>
			<keywords>monolingual comparable corpus, alignment, similarity</keywords>
		</article>
		<article id="recital-2011-court-001" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Prajol</prenom>
					<nom>Shrestha</nom>
					<email>prajol.shrestha@etu.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA-UFR Sciences, 44322 Nantes Cedex 3</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article concerne la détermination de la similarité entre des textes courts (phrases, paragraphes, ...). Ce problème est souvent abordé dans la littérature à l’aide de méthodes supervisées ou de ressources externes comme le thesaurus Wordnet ou le British National Corpus. Les méthodes que nous proposons sont non supervisées et n’utilisent pas de connaissances à priori. La première méthode que nous présentons est basée sur le modèle vectoriel de Salton auquel nous avons apporté des modifications pour prendre en compte le contexte, le sens et la relation entre les mots des textes. Dans un deuxième temps, nous testons les mesures de Dice et de ressemblance pour résoudre ce problème ainsi que l’utilisation de la racinisation. Enfin, ces différentes méthodes sont évaluées et comparées aux résultats obtenus dans la littérature.</resume>
			<mots_cles>Similarité, Modèle Vectoriel, Mesure de Similarité</mots_cles>
			<title>Corpus-Based methods for Short Text Similarity</title>
			<abstract>This paper presents corpus-based methods to find similarity between short text (sentences, paragraphs, ...) which has many applications in the field of NLP. Previous works on this problem have been based on supervised methods or have used external resources such as WordNet, British National Corpus etc. Our methods are focused on unsupervised corpus-based methods. We present a new method, based on Vector Space Model, to capture the contextual behavior, senses and correlation, of terms and show that this method performs better than the baseline method that uses vector based cosine similarity measure. The performance of existing document similarity measures, Dice and Resemblance, are also evaluated which in our knowledge have not been used for short text similarity. We also show that the performance of the vector-based baseline method is improved when using stems instead of words and using the candidate sentences for computing the parameters rather than some external resource.</abstract>
			<keywords>Similarity, Vector Space Model, Similarity metric</keywords>
		</article>
		<article id="recital-2011-court-002" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Inga</prenom>
					<nom>Gheorghita</nom>
					<email>inga.gheorghita@atilf.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF-CNRS, Nancy-Université (UMR 7118), France</affiliation>
				<affiliation affiliationId="2">XILOPIX, 37 rue de la Plaine, 75020 Paris, France</affiliation>
			</affiliations>
			<titre>Ressources lexicales au service de recherche et d’indexation des images</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente une méthodologie d’utilisation du Trésor de la Langue Française informatisée (TLFi) pour l’indexation et la recherche des images fondée sur l’annotation textuelle. Nous utilisons les définitions du TLFi pour la création automatique et l’enrichissement d’un thésaurus à partir des mots-clés de la requête de recherche et des mots-clés attribués à l’image lors de l’indexation. Plus précisement il s’agit d’associer, de façon automatisé, à chaque mot-clé de l’image une liste des mots extraits de ses définitions TLFi pour un domaine donné, en construisant ainsi un arbre hiérarchique. L’approche proposée permet une catégorisation très précise des images, selon les domaines, une indexation de grandes quantités d’images et une recherche rapide.</resume>
			<mots_cles>TLFi, indexation, recherche, images, thésaurus</mots_cles>
			<title></title>
			<abstract>This article presents a methodology for using the “Trésor de la Langue Française informatisée” (TLFi) for indexing and searching images based on textual annotation. We use the definitions of TLFi for automatic creation and enrichment of a thesaurus based on keywords from the search query and the keywords assigned to the image during indexing. More specifically it is automatically to associate, to each keyword of the image a list of words from their TLFi’s definitions for a given area, thus building a hierarchical tree. The proposed approach allows a very accurate categorization of images, depending on the fields, a indexing of large amounts of images and a quick search.</abstract>
			<keywords>TLFi, indexing, search, images, thesaurus</keywords>
		</article>
		<article id="recital-2011-court-003" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Mathias</prenom>
					<nom>Lambert</nom>
					<email>Mathias.Lambert@paris-sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris IV-Sorbonne, Laboratoire STIH (LaLIC) - 28 rue Serpente, 75006 Paris</affiliation>
			</affiliations>
			<titre>Repérer les phrases évaluatives dans les articles de presse à partir d’indices et de stéréotypes d’écriture</titre>
			<type>court</type>
			<pages></pages>
			<resume>Ce papier présente une méthode de recherche des phrases évaluatives dans les articles de presse économique et financière à partir de marques et d’indices stéréotypés, propres au style journalistique, apparaissant de manière concomitante à l’expression d’évaluation(s) dans les phrases. Ces marques et indices ont été dégagés par le biais d’une annotation manuelle. Ils ont ensuite été implémentés, en vue d’une phase-test d’annotation automatique, sous forme de grammaires DCG/GULP permettant, par filtrage, de matcher les phrases les contenant. Les résultats de notre première tentative d’annotation automatique sont présentés dans cet article. Enfin les perspectives offertes par cette méthode relativement peu coûteuse en ressources (à base d’indices non intrinsèquement évaluatifs) font l’objet d’une discussion.</resume>
			<mots_cles>Opinion, évaluation, repérage de phrases évaluatives, presse économique et financière, style journalistique, indices/marques/stéréotypes d’écriture</mots_cles>
			<title></title>
			<abstract>This paper presents a method to locate evaluative sentences in financial and economic newspapers, relying on marks and stereotyped signs. Peculiar to journalese, these are present concomitantly with the expression of evaluation(s) in sentences. These marks or signs have been found by means of a manual annotation. Then, in preparation for an automatic annotation phase, they have been implemented in the form of DCG/GULP grammars which, by filtering, allows to locate the sentences containing them. The results of our first automatic annotation attempt are shown in this article. Furthermore, the prospects offered by this method, which relies on nonintrinsically evaluative marks and therefore does not require long lists of lexical resources, are discussed.</abstract>
			<keywords>Opinion, appraisal, detection of evaluative sentences, financial and economic newspapers, journalese, writing signs/marks/stereotypes</keywords>
		</article>
		<article id="recital-2011-court-004" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Adrien</prenom>
					<nom>Barbaresi</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ICAR, ENS LYON</affiliation>
			</affiliations>
			<titre>La complexité linguistique Méthode d’analyse</titre>
			<type>court</type>
			<pages></pages>
			<resume>La complexité linguistique regroupe différents phénomènes dont il s’agit de modéliser le rapport. Le travail en cours que je décris ici propose une réflexion sur les approches linguistiques et techniques de cette notion et la mise en application d’un balayage des textes qui s’efforce de contribuer à leur enrichissement. Ce traitement en surface effectué suivant une liste de critères qui représentent parfois des approximations de logiques plus élaborées tente de fournir une image ``raisonnable'' de la complexité.</resume>
			<mots_cles>Complexité, lisibilité, allemand, analyse de surface</mots_cles>
			<title></title>
			<abstract>Linguistic complexity includes various linguistic phenomena which interaction is to be modeled. The ongoing work described here tackles linguistic and technical approaches of this idea as well as an implementation of a parsing method which is part of text enrichment techniques. This chunk parsing is performed according to a list of criteria that may consist in logical approximations of more sophisticated processes in order to provide a ``reasonable'' image of complexity.</abstract>
			<keywords>Complexity, lisibility, German, chunk parsing</keywords>
		</article>
	</articles>
</conference>
