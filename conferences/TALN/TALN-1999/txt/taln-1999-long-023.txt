ConfÃ©rence TALN 1999, CargÃ¨se, 12-17 juillet 1999
Apprentissage et Evaluation de ModÃ¨les de Langage par des
Techniques de Correction dâ€™Erreurs

Laurent Miclet et Jacques Chodorowski

ENSSAT - IRISA, miclet@enssat.fr, chodorow@enssat.fr
RÃ©sumÃ©
Cet article a pour but de dÃ©crire la mise au point et lâ€™expÃ©rimentation de mÃ©thodes dâ€™appren-
tissage de syntaxe Ã  partir dâ€™exemples positifs, en particulier pour des applications de Recon-
naissance de la Parole et de Dialogue Oral. Les modÃ¨les syntaxiques, destinÃ©s Ã  Ãª tre intÃ©grÃ©s
dans une chaÃ®ne de traitement de la parole, sont extraits des donnÃ©es par des mÃ©thodes dâ€™infÃ©-
rence grammaticale symbolique et stochastique. Ils sont fondÃ©s sur des techniques de correction
dâ€™erreurs dans les sÃ©quences. Lâ€™ensemble de ce travail a Ã©tÃ© rÃ©alisÃ© dans le cadre du contrat 97-
1B-004 avec France-Telecom (Centre National dâ€™Etudes des TÃ©lÃ©communications).
Dans la premiÃ¨re partie de cet article, nous rappellons les distances entre sÃ©quences basÃ©es
sur des opÃ©rations Ã©lÃ©mentaires de correction dâ€™erreur. Nous dÃ©crivons ensuite un algorithme
classique dâ€™infÃ©rence grammaticale fondÃ© sur cette notion, et nous en proposons une amÃ©liora-
tion. Nous abordons Ã  cet endroit le problÃ¨me de lâ€™Ã©valuation dâ€™un concept appris seulement Ã 
partir dâ€™exemples positifs, sans contre-exemples.
Par la suite, le modÃ¨le syntaxique est Ã©tendu en attribuant des probabilitÃ©s (apprises Ã  partir
des donnÃ©es) aux rÃ¨gles de la grammaire. On dispose dans ce cadre dâ€™un outil dâ€™Ã©valuation de
la qualitÃ© de lâ€™apprentissage : la perplexitÃ© ; cependant pour obtenir des rÃ©sultats significatifs,
il faut Ãª tre capable de probabiliser lâ€™espace entier des sÃ©quences, ce qui implique de lisser
la grammaire stochastique apprise. Une technique de lissage est proposÃ©e, qui permet alors
dâ€™Ã©valuer lâ€™apprentissage sur le corpus de donnÃ©es issues de lâ€™expÃ©rimentation en dialogue oral.
M OTS CL Ã©S : InfÃ©rence grammaticale rÃ©guliÃ¨re, analyse corrective, Ã©valuation du modÃ¨le
de language
1. Introduction
1.1. Apprentissage Automatique et Traitement Automatique de la Langue Naturelle

Les techniques dâ€™Apprentissage Automatique sont actuellement de plus en plus appliquÃ©es
aux diffÃ©rents aspects du Traitement du Langage Naturel. Il y a en effet une convergence entre

ces deux domaines qui sâ€™explique en particulier par les raisons suivantes :
â€“ Le besoin de traitement automatique du langage Ã©crit et oral a considÃ©rablement aug-
mentÃ© : recherche automatique sur le web Ã  partir du contenu des pages, reconnaissance
de la parole, traduction automatique, etc. Pour ces applications, et beaucoup dâ€™autres,
les corpus disponibles sont dÃ©sormais de taille suffisante pour que des algorithmes dâ€™ap-
prentissage puissent en extraire des modÃ¨les rÃ©alistes. Notons que ces corpus sont le plus
souvent composÃ©s uniquement dâ€™exemples positifs, ce qui pose un problÃ¨me particulier
aux algorithmes dâ€™apprentissage (comment limiter la gÃ©nÃ©ralisation?).
â€“ Les algorithmes dâ€™Apprentissage Automatique se sont dÃ©veloppÃ©s selon dâ€™une part un
axe principalement symbolique, dâ€™autre part selon un axe numÃ©rique. Actuellement la
synthÃ¨se se fait grË†ace Ã  des mÃ©thodes mixtes, symboliques-numÃ©riques (par exemple
lâ€™apprentissage de rÃ¨gles par arbres de dÃ©cision, ou lâ€™apprentissage de grammaires sto-
chastiques). Ces mÃ©thodes permettent en particulier dâ€™envisager lâ€™extraction de concepts
structurÃ©s Ã  partir de gros ensembles de donnÃ©es bruitÃ©es. Câ€™est une des tË†aches du traite-
ment de langue naturelle.
Compte tenu de la variÃ©tÃ© et de la complexitÃ© des problÃ¨mes posÃ©s, un bon nombre de mÃ©tho-
dologies diffÃ©rentes dâ€™apprentissage automatique ont dÃ©jÃ  Ã©tÃ© testÃ©es pour le traitement de la
langue naturelle. On peut citer entre autres :
â€“   Les mÃ©thodes dâ€™apprentissage Ã  base de cas (voir (DvdBW97) pour une revue),
â€“   Les mÃ©thodes bayÃ©siennes (par exemple (NMTM98)),
â€“   Lâ€™ILP (par exemple (MC95)),
â€“   Les arbres de dÃ©cision (par exemple (OW94)),
â€“   Lâ€™infÃ©rence grammaticale algÃ©brique et stochastique (cf. (ICG98)) etc.
Divers colloques et congrÃ¨s ont dÃ©jÃ  Ã©tÃ© consacrÃ©s Ã  lâ€™apprentissage automatique appliquÃ© au
traitement de la langue naturelle. On pourra se rÃ©fÃ©rer aux Actes de (ICM97), (AAA98).
La section qui suit introduit les techniques de correction dâ€™erreurs utilisÃ©es par les algo-
rithmes dâ€™apprentissage prÃ©sentÃ©s dans la deuxiÃ¨me partie de lâ€™article.
2. ModÃ¨les de correction dâ€™erreurs
Nous rappelons ici les principales notions concernant les opÃ©rations de correction dâ€™erreurs
(ou opÃ©rations dâ€™Ã©dition) entre sÃ©quences. Nous rappelons Ã©galement leur application au calcul
de la

distance

dâ€™une sÃ©quence Ã  une grammaire rÃ©guliÃ¨re, vue comme un automate fini.

2.1. DÃ©finitions et notations

Pour dÃ©terminer les diffÃ©rences entre deux sÃ©quences A1 et A2 , composÃ©es des lettres ap-
partenant au mÃªme alphabet , il est naturel de penser Ã  modifier lâ€™ordre et/ou la nature des
lettres de A1 afin dâ€™obtenir A2 . On connaÃ®t un ensemble de telles opÃ©rations de correction dâ€™er-
reurs, constituÃ© de la substitution, de lâ€™insertion, et de la suppression dâ€™Ã©lÃ©ments de , suffisant
pour transformer toute chaÃ®ne A1 en chaÃ®ne A2 . Si est la chaÃ®ne vide par, on note
a; b
la
substitution de la lettre a par la lettre b,
; a
lâ€™insertion de a, et
a;
la suppression de a.

2.2. Distance de Levenshtein

On appelle alors distance de Levenshtein le nombre minimal dâ€™opÃ©rations (prises dans cet
ensemble) nÃ©cessaires pour tranformer A1 en A2 . On appelle dÃ©rivation corrective optimale la
suite dâ€™opÃ©rations dâ€™Ã©ditions utilisÃ©es pour calculer la distance de Levenshtein :
D
A1 ; A2
= e1 ; e2; ::; en avec ek =
xi; xj
; 1  k  n; 8xi ; xj 2 f g
Un algorithme de programmation dynamique (WF74) permet de calculer la D
A1 ; A2
en un
temps de lâ€™ordre de O
jA1 j:jA2 j
, avec jA1 j (resp. jA2 j) la largeur de A1 (resp. de A2 ).
On peut aussi attribuer Ã  ces opÃ©rations des cÃ´uts unitaires de la faÌ§con suivante :
w
xi; xj
= 10 si xi 6= xj
si xi = xj 8xi ; xj 2 f          g
Dans cette optique, la distance de Levenshtein est Ã©galement le cÃ´ut minimal de la transforma-
tion de A1 en A2 selon des opÃ©rations avec cÃ´uts unitaires.

2.3. Distance dâ€™Ã©dition

On peut vouloir gÃ©nÃ©raliser le calcul prÃ©cÃ©dent en partant de la remarque suivante : il est par-
fois utile dâ€™attribuer une plus grande importance Ã  une opÃ©ration dâ€™Ã©dition quâ€™Ã  une autre. Dans
ce cas les w
xi ; xj
ne seront plus des cÃ´uts unitaires. Il faudra cependant leur imposer dâ€™Ãªtre
positfs ou nuls pour pouvoir traiter le problÃ¨me par programmation dynamique. Ces valeurs
dÃ©finissent alors une matrice de cÃ´uts sur     f g        f g,f gf g
On a un rÃ©sultat classique (KS83) pour cette extension : en mesurant le cÃ´ut de passage de A1
Ã  A2 pour une certaine dÃ©rivation corrective comme la somme des cÃ´uts des opÃ©rations Ã©lÃ©men-
taires employÃ©es, on sait encore calculer par programmation dynamique la distance dâ€™Ã©dition
(ou distance de Levenshtein pondÃ©rÃ©e) entre ces deux sÃ©quences : câ€™est le minimum sur tous les
cÃ´uts de passage entre ces deux sÃ©quences.
De mÃªme, on sait calculer la dÃ©rivation corrective optimale selon ces opÃ©rations pondÃ©rÃ©es.
Une propriÃ©tÃ© complÃ©mentaire est que, si la matrice des cÃ´uts w
xi ; xj
a les propriÃ©tÃ©s dâ€™une
distance, alors la distance dâ€™Ã©dition est aussi une distance au sens rigoureux du terme (WF74).
Dans la suite, nous garderons le terme de

distance

mÃªme si la propriÃ©tÃ© nâ€™est pas assurÃ©e.

2.4. Distance corrective entre une sÃ©quence et un automate

A
Une autre extension est la suivante : Ã©tant donnÃ©s un automate fini sur un alphabet , une
sÃ©quence B et une matrice de cÃ´uts unitaires, notons Z la phrase du langage acceptÃ© par dont A
la distance de Levenshtein Ã  B est la plus faible.
Il est Ã©galement possible de calculer la dÃ©rivation corrective entre B et Z par une extension
de lâ€™algorithme prÃ©cÃ©dent (KS83). Cette dÃ©rivation peut, par extension, Ãª tre appelÃ©e dÃ©rivation
corrective optimale entre un automate et une phrase.
Plus rÃ©cemment, on a dÃ©montrÃ© (AV98) que cet algorithme peut Ã©galement Ãª tre adaptÃ© pour
calculer la dÃ©rivation corrective optimale entre un automate fini et une phrase, mÃªme quand la
matrice de cÃ´uts nâ€™est pas unitaire. Ce rÃ©sultat Ã©tait connu dans le cas particulier des automates
sans cycle (Wag74), mais son extension au cas gÃ©nÃ©ral est un rÃ©sultat Ã  notre avis remarquable.

3. Apprentissage dâ€™un modÃ¨le syntaxique symbolique
Dans la suite de cet article nous nous focalisons donc sur le domaine particulier de lâ€™ap-
prentissage de lâ€™infÃ©rence des grammaires reguliÃ¨res Ã  partir dâ€™exemples positifs. Cette section
introduit un algorithme dâ€™apprentissage des grammaires non stochastiques qui utilise la dÃ©riva-
tion corrective optimale, propose un calcul de pondÃ©ration des opÃ©rations dâ€™Ã©dition puis dÃ©crit
un protocole dâ€™Ã©valuation de la qualitÃ© du modÃ¨le appris.

3.1. Algorithme dâ€™apprentissage par correction dâ€™erreurs : ECGIA

ProposÃ©e par Rulot et Vidal (RV88) (RPV89), cette mÃ©thode 1 construit de faÌ§con incrÃ©men-
tale une grammaire reguliÃ¨re G =
N; ; R; S
, 2 Ã©quivalente Ã  un automate non dÃ©terministe et
sans cycles. Le modÃ¨le initial est construit Ã  partir de la premiÃ¨re sÃ©quence presentÃ©e, puis il est
amÃ©liorÃ© par lâ€™ajout de nouvelles rÃ¨gles rÃ©sultant de lâ€™analyse corrective optimale des sÃ©quences
suivantes de lâ€™ensemble dâ€™apprentissage.
Cet algorithme utilise une matrice de cÃ´uts unitaires. Nous proposons une extension simple
de cet algorithme, en utilisant une matrice de cÃ´uts non unitaires. Lâ€™extension du modÃ¨le ne-
cessite quâ€™aucun cycle ne soit introduit dans lâ€™automate rÃ©sultant. Cette propriÃ©tÃ© permet de
calculer la dÃ©rivation corrective optimale par programmation dynamique classique comme nous
lâ€™avons vu au paragraphe 2.4.

3.2. Evaluation du poids des opÃ©rations dâ€™Ã©dition pendant lâ€™apprentissage

Il est logique de vouloir Ã©tendre cet algorithme Ã  lâ€™infÃ©rence par dÃ©rivation corrective opti-
male avec une matrice de cÃ´uts variables afin de lier le cÃ´ut dâ€™Ã©dition Ã  la frÃ©quence de lâ€™opÃ©-
ration dâ€™Ã©dition associÃ©e. Ceci est en particulier adaptÃ© Ã  la langue naturelle o`u certaines opÃ©ra-
tions nâ€™ont pas le mÃªme poids que les autres. Prenons par exemple la phrase :

euh, je voudrais
obtenir le numÃ©ro des pompiers

â€“ remplacer le mot pompiers par urgences est plus frÃ©quent
(autrement dit :

cÃ´ute moins

) que par hirondelles. De la mÃªme faÌ§con il sera moins cÃ´uteux de
supprimer le mot euh quâ€™un autre.
La matrice des cÃ´uts constitue alors un paramÃ¨tre dâ€™entrÃ©e de cet algorithme. Nous avons
proposÃ© dans (CM98) comment estimer cette matrice Ã  partir du corpus dâ€™apprentissage, en
comptant les frÃ©quences dâ€™opÃ©rations dâ€™Ã©dition utilisÃ©s lors de lâ€™infÃ©rence de la grammaire.
f
Nous utilisons pour cela une matrice MECGI dÃ©finie par : MECGI = f
xi ; xj
; xi ; xj            2            2
             g
; 1 i n; 1 j n avec f
xi ; xj
: nombre de fois que lâ€™opÃ©ration
xi; xj
est utilisÃ©e
j j
pendant lâ€™apprentissage et n =  . Lâ€™opÃ©ration
nâ€™est pas prise en compte : f
xi ; xj
= 0.
En raisonnant en termes de cÃ´uts, il est logique de considÃ©rer quâ€™une opÃ©ration qui arrive peu
frÃ©quemment doit Ãª tre considÃ©rÃ©e comme plus cÃ´uteuse quâ€™une autre dont le nombre dâ€™occu-
rences est Ã©levÃ©. Cette remarque nous permet donc de transformer MECGI en matrice de cÃ´uts.
La mÃ©thode que nous avons utilisÃ©e consiste Ã  normaliser sÃ©parement les insertions, les sup-
pressions et les substitutions selon les relations :
,                  f         g8 6
â€“ insertions : w
; xi
= 1 f
; xi
=max f
; xi
xi =
,                  f
â€“ suppressions : w
xi ;
= 1 f
xi ;
=max f
xi ;
xi =  g8 6
,                     f      g 8 6
â€“ substitutions : w
xi ; xj
= 1 f
xi ; xj
=max f
xi ; xj
; xi = et xj =      8 6
1. ECGIA pour : Error Correcting Grammar Inference Algorithm
N                                              R
2. avec : lâ€™ensemble de non-terminaux,  : lâ€™alphabet, : lâ€™ensemble de rÃ¨gles et   S : non-terminal initial

3.3. RÃ©apprentissage avec les nouveaux couts

Nous pouvons alors rÃ©apprendre la grammaire G avec les nouvelles valeurs attribuÃ©s aux
opÃ©rations correctives, en utilisant cette fois lâ€™extension de lâ€™algorithme ECGIA proposÃ©e ci-
dessus, puisque la matrice des cÃ´uts nâ€™est plus unitaire.
Ce processus dâ€™apprentissage, dâ€™estimation de la matrice des cÃ´uts, puis de rÃ©apprentissage
peut Ãª tre rÃ©iterÃ©. Nous avons constatÃ© experimentalement (CM98) sur un corpus non trivial,
issu du langage L0 de Feldman (FLSW90) une convergence du processus au bout dâ€™une dizaine
dâ€™itÃ©rations. Elle sâ€™accompagne de la rÃ©duction en taille de la grammaire apprise (nombre de
rÃ©gles et de non-terminaux en dÃ©croissance).
La similitude de principe entre cette mÃ©thode et lâ€™algorithme dâ€™

Estimation-Maximisation
(EM) (DLR77) nous a amenÃ© Ã  lâ€™appeller

ECGI EM-like

bien que sa convergence nâ€™a pas Ã©tÃ© dÃ©montrÃ©e Ã  ce jour. En effet, la convergence de lâ€™algorihme

EM

, par exemple pour les
HMM (ChaÃ®nes de Markov cachÃ©es) sâ€™appuye sur le fait que lâ€™on rÃ©estime les probabilitÃ©s sur
une structure qui ne change pas lors des itÃ©rations, ce qui nâ€™est pas le cas ici.

3.4. RÃ©sultats

Lâ€™algorithme

ECGI EM-like

nâ€™a pas pu etre appliquÃ© sur les donnÃ©es rÃ©elles fournies par
France Telecom (AGS : corpus des requÃªtes vocales, dÃ©crit dans : (SFC+ 96)), le lexique extrait
de celui-ci Ã©tant trop important (env. 1000 entrÃ©es) par rapport au nombre des phrases fournies.
Pour valider expÃ©rimentalement cette mÃ©thode nous avons eu recours Ã  un corpus gÃ©nÃ©rÃ© par
la grammaire du langage L0 (25 terminaux) mentionnÃ©e plus haut. Il a Ã©tÃ© divisÃ© en 3 parties :
corpus dâ€™apprentissage de 1000 phrases, corpus de test non bruitÃ© de 1000 phrases et corpus de
test de 1000 phrases avec 10% de bruit 3 introduit artificiellement. Nous nous sommes servis
de la grammaire cible pour eÌvaluer les rÃ©sutats obtenus avec les deux versions dâ€™ECGIA. Cette
mÃ©thode est faible dans lâ€™absolu, elle est cependant suffisante pour comparer deux algorithmes.
Les rÃ©sultats de lâ€™expÃ©rimentation sont resumÃ©es dans le tableau ci-dessous :
#non-terminaux          #rÃ¨gles                 corpus         sans     corpus         avec
bruit                   bruit
G
L0
58                      189                      100%                    59.0%
ECGI classique           572                     1102                     82.2%                   51.3%
ECGI EM-like             534                     1008                     88.3%                   53.1%
Sur les 531 phrases (53.1%) acceptÃ©es par la grammaire produite par ECGIA

EM-like

une
seule a Ã©tÃ© rejetÃ©e par G
L0
, contre deux sur les 513 phrases acceptÃ©es par la grammaire inferÃ©e
par ECGIA classique. On voit donc que la nouvelle mÃ©thode apprend un modÃ¨le plus compact
et plus proche du concept cible (meilleur taux dâ€™acceptation sans sur-gÃ©nÃ©ralisation).
4. Apprentissage dâ€™un modÃ¨le syntaxique stochastique
Dans cette section nous allons utiliser le modÃ¨le vu prÃ©cÃ©demment, enrichi dâ€™une distribution
de probabilitÃ©s. Nous prÃ©senterons une mÃ©thode fondÃ©e sur les techniques correctives permet-
tant de lâ€™Ã©valuer. Lâ€™illustration de cette eÌvaluation sera effectuÃ©e sur un corpus rÃ©el.
3. Par

10% de bruit

nous entendons quâ€™en moyenne un mot sur 10 du corpus de test a Ã©tÃ© altereÃ© : soit suprimÃ©,
soit remplacÃ© par un autre, soit il a vu un nouveau mot tirÃ© au hasard Ãª tre introduit avant lui.

4.1. ECGIA stochastique

Lâ€™algorithme prÃ©sentÃ© dans la section 3 possÃ¨de une extension stochastique, proposÃ©e par
les mÃªmes auteurs (RV88). Lâ€™estimation de probabilitÃ©s de chaque rÃ¨gle (ou transition) est faite
pendant le processus dâ€™apprentissage.
Rappelons briÃ¨vement quâ€™un automate fini peut voir ses transitions affectÃ©es de valeurs
rÃ©elles positives, avec la somme des valeurs sur les transitions partant dâ€™un Ã©tat soit Ã©gale Ã  1.
Lâ€™automate est alors dit probabilisÃ©, ou stochastique. On peut alors calculer pour toute phrase
j
x la probabilitÃ© p
x G
dâ€™avoir Ã©tÃ© engendrÃ©e par cet automate.
P     On dÃ©montre que ce modÃ¨le est
j
consistant : il munit  dâ€™une distribution de probabilitÃ©s, x2 p
x G
= 1
1

4.2. Evaluation de la qualitÃ© de lâ€™apprentissage

Un moyen dâ€™Ã©valuation des modÃ¨les stochastiques : la perplexitÃ©

La perplexitÃ© est un des critÃ¨res qui permettent dâ€™Ã©valuer la qualitÃ© dâ€™un modÃ¨le syntaxique
pourvu dâ€™une distribution de probabilitÃ©s (Dup96) (BJM83). Elle peut Ãª tre comprise comme
le pouvoir de prÃ©diction de ce modÃ¨le. Ce pouvoir est dâ€™autant plus grand que le nombre de
symboles proposÃ©s par le modÃ¨le pour correspondre au symbole suivant dâ€™une sÃ©quence est
petit. Si le modÃ¨le ne peut proposer aucun symbole (dans notre cas : si la probabilitÃ© dâ€™une rÃ¨gle
est nulle ou si elle nâ€™existe pas) alors son pouvoir prÃ©hdicitif est nul et sa perplexitÌ
i       e est maximale.
P
, kS1 k jiS=1j log2 P
xi

Formellement, la perplexitÃ© est dÃ©finie par : PP = 2                                    j j
avec S : le nombre de
k k
sÃ©quences de lâ€™Ã©chantillon de test, S : la somme des longueurs des sÃ©quences de lâ€™Ã©chantillon
de test, et P
xi
: la probabilitÃ© que la i-Ã¨me sÃ©quence xi ait Ã©tÃ© gÃ©nÃ©rÃ©e par le modÃ¨le.
Ce calcul est conditionnÃ© dâ€™une part par la consistance du modÃ¨le (ce dont nous sommes
assurÃ©s), dâ€™autre part par lâ€™attribution dâ€™une probabilitÃ© non nulle Ã  toute sÃ©quence composÃ©e
sur . Dans le cas gÃ©nÃ©ral, Ã©tant donnÃ©es une grammaire stochastique G et une sÃ©quence x,
il est possible de calculer la probabilitÃ© pour que x soit gÃ©nÃ©rÃ©e par G. Cette probabilitÃ© peut
valoir zÃ©ro, mais seulement si la syntaxe de la sÃ©quence nâ€™est pas reconnue par la grammaire.
Cependant, pour pallier le manque de donnÃ©es dâ€™apprentissage, qui conduit Ã  une mauvaise
estimation statistique du modÃ¨le, on doit lisser ses paramÃ¨tres. Ce lissage permet en outre dâ€™en-
gendrer toute chaÃ®ne avec une probabilitÃ© non nulle La technique de lissage proposÃ©e dans les
paragraphes suivants consiste Ã  crÃ©er les transitions manquantes et Ã  leur redistribuer une partie
des probabilitÃ©s des rÃ¨gles existantes.

Grammaire augmentÃ©e G
0
Soit G une grammaire apprise par lâ€™algorithme ECGI Ã©tendu au cas stochastique. Soit G =
0
N; ; R0 ; S
la grammaire G augmentÃ©e par les rÃ¨gles de correction dâ€™erreurs dÃ©finies ainsi :
Insertion de a :             A ! aA, 8
A ! bB
Substitution de b par a :    A ! aB , 8
A ! bB
Suppression de b :           A ! B , 8
A ! bB
P         Q
La probabilitÃ© pour que x 2  soit gÃ©nÃ©rÃ©e par G est : pG0
x
= 8D0
x

8r 2D0
x
p
ri
0
G          i   G
o`u DG
x
est une dÃ©rivation corrective (quelconque) de G pour engendrer x.
0

Calculs des probabilitÃ©s des rÃ¨gles de G
0
En supposant que lâ€™on connaÃ®t les probabilitÃ©s des opÃ©rations correctives : p
xi ; xj
8xi ; xj 2
f        g
 , nous pouvons Ã©crire :
8 A ! xj B 2 R p
A ! xj B
= p
A ! xiB 2 R
 p
xi ; xj

2

0
Pour que le modÃ¨le reste consistant (i.e. pour quâ€™il vÃ©rifie
1
), il est nÃ©cÃ©ssaire que la somme
P
des probabilitÃ©s des rÃ¨gles dont la partie gauche est constituÃ©e dâ€™un mÃªme non-terminal soit Ã©gale Ã  1 : 8x2f g;8B2N j9A!xiB2R0 p
A            !                8 2
xiB
= 1 A N . Nous satisfaisons cette
condition en ajoutant la contrainte :
P
8xj 2f g p
xi ; xj
= 1; 8xi   2 f g
3

Le problÃ¨me revient alors Ã  calculer les probabilitÃ©s des opÃ©rations dâ€™Ã©dition en respectant
3

Estimation des probabilitÃ©s des opÃ©rations correctives

Nous pouvons dÃ©duire les probabilitÃ©s des opÃ©rations dâ€™Ã©dition Ã  partir de la matrice MECGI
introduite dans 3.1 en la normalisant ligne par ligne (ce qui satisfait
3
). Deux problÃ¨mes se
posent alors :
1. ProbabilitÃ©s nulles des opÃ©rations
xi ; xj

Nous obtenons les probabilitÃ©s nulles pour les opÃ©rations dâ€™Ã©dition qui nâ€™ont pas Ã©tÃ© vues pen-
dant lâ€™apprentissage. Pour y remÃ©dier nous utilisons la technique de lissage dâ€™Absolute Dis-
counting (NE93) utilisÃ©es pour lisser des modÃ¨les de type N-grams que nous avons adaptÃ©es au
lissage des probabilitÃ©s dâ€™Ã©dition de la maniÃ¨re suivante :
xi ;xj
,
P            MECGI
xi ;xj
si M
xi ; xj
0
8 xj 2f g
P       
jj,n0
 
xi
si M
; xj
= 0
MECGI
xi ;xj

p
xi; xj
=       8 xj 2f g
P      
jj,n0 +1
 
xi
si M
xi 6= ; xj
= 0
8xj 2f g MECGI
xi ;xj

:   
xi
siP8x 2 MECGI
xi ; xj
= 0
j
Avec n0 : nombre de valeurs nulles sur une ligne de MECGI et 
xi
: pourcentage du nombre
dâ€™opÃ©rations
xj ; xi
8xi 6= xj calculÃ©e pour les valeurs nulles de MECGI
xi ; xj
pour un xi
donnÃ© (on lâ€™interprÃ¨te comme lâ€™importance des insertions des xi Ã  la place dâ€™autres lettres). La
contrainte (3) reste satisfaite.
2. ProbabilitÃ©s nulles des rÃ¨gles dâ€™insertion
Dans la grammaire apprise G, il nâ€™y a pas de rÃ¨gles correspondant aux opÃ©rations dâ€™insertion

A xiA
. Par consÃ©quence la rÃ©lation (2) leur attribue une probabilitÃ© nulle. La solution
consiste Ã  enlever une fraction de probabilitÃ©s Ã  chacune des rÃ¨gles A         !
xi B R0 et     2
lâ€™attribuer Ã  p
A!         2
xi A R
par :
0

!        2
p
A xiA R
= p
A xi B R
p
; xj
B = A.
0
 !                2                 8 6
Chaque rÃ¨gle de R se voit attribuer alors une probablitÃ© calculÃ©e selon :
0

p
A ! xi B 2 R00
 p
A ! xi B 2 R
 p
xi ; xj
8x 2 ; 8x 2 f                   g
p
A ! xi A 2 R
=  p
A ! xi B 2 R
 p
; xj
i       j

Nous obtenons ainsi un systÃ¨me complet dâ€™infÃ©rence et dâ€™Ã©valuation des grammaires sto-
chastiques par utilisation des techniques correctives. Remarquons que cette mÃ©thode sâ€™applique Ã©galement avec lâ€™algorithme

ECGI EM-like

, puisque le calcul des p
xi ; xj
intervient unique-
ment aprÃ¨s lâ€™itÃ©ration finale.

4.3. RÃ©sultats

Avec cette mÃ©thode le problÃ¨me posÃ© par le langage L0 est devenu

trop facile

(la per-
plexitÃ© calculÃ©e sur le corpus de test est de lâ€™ordre de 4). De plus lâ€™importance du lissage est
pratiquement nÃ©gligeable (une grande majoritÃ© des phrases de test est acceptÃ©e par lâ€™automate
appris). Nous avons donc effectuÃ© des tests sur les donnÃ©es de France Telecom : le corpus AGS
mentionnÃ© dans 3.4. Un bref resumÃ© de ses caractÃ©ristiques se trouve dans le tableau ci-dessous :
Corpus original                  Nombre                           Taille du lexique          Nombre de mots
dâ€™exemples                                                  par phrase
Apprentissage                    9850                             866                        5.04
Test                             724                              373                        4.95
Sur la figure 1 nous avons representÃ© la double variation des paramÃ¨tres : chaque courbe
correspond Ã  une valeur de , une courbe reprÃ©sente la perplexitÃ© en fonction de .
60
"beta = 0.01"
"beta = 0.05"
"beta = 0.1"
55
50
45
40
35
0   0.1    0.2       0.3       0.4   0.5      0.6   0.7        0.8   0.9
F IG . 1 â€“ PerplexitÃ© de la grammaire G lissÃ©e par les techniques correctives sur le corpus AGS
0

en version originale. La valeur de apparaÃ®t dans lâ€™etiquette des graphes, en haut Ã  droite de
la ligne
La valeur de perplexitÃ© la plus faible ( 40) correspond au couple : = 0:05,  = 0:6. La
valeur de illustre la faible importance de lâ€™opÃ©ration dâ€™insertion (cependant nÃ©cessaire car la
perplexitÃ© augmente pour les valeurs beta 0:05). La valeur obtenue pour  est typique pour
le lissage par Absolute Discounting.
Les donnÃ©es dâ€™apprentissage et de test, dans la version originale fournie par le CNET, pro-
viennent de locuteurs diffÃ©rents. Pour se placer dans des conditions dâ€™apprentissage statisti-
quement plus satisfaisantes, nous avons mÃ©langÃ© ces deux corpus, puis tirÃ© au hasard le mÃªme
nombre de phrases pour lâ€™apprentissage et pour le test. Les rÃ©sultats sur cette

version 2

du
corpus sont visualisÃ©s sur la figure 2.
Il est possible de comparer ces rÃ©sultats avec ceux obtenus par un modÃ¨le de N-grams lissÃ©s
avec

back-off

et

non-shadowing

, modÃ¨le dÃ©crit dans (DR97) : au premier abord on obtient
une perplexitÃ© dâ€™environ 18 pour des modÃ¨les bigrammes et dâ€™environ 14 pour des modÃ¨les tri-
grammes. Il faut cependant rester prudent car lâ€™Ã©quivalence totale de lâ€™ensemble des conditions
expÃ©rimentales entre nos mesures et les leurs reste Ã  vÃ©rifier. Rappelons que la

syntaxe

dâ€™un
modÃ¨le n-gram est markovienne : la probabilitÃ© dâ€™apparition dâ€™un mot nâ€™est conditionnÃ©e que

17
"beta = 0.001"
"beta = 0.01"
"beta = 0.05"
16
15
14
13
12
0   0.1   0.2    0.3   0.4       0.5   0.6   0.7        0.8   0.9
F IG . 2 â€“ PerplexitÃ© de la grammaire G lissÃ©e sur le corpus AGS en version 2
0
par les n 1 mots prÃ©cÃ©dents. Les

rÃ¨gles de grammaires

sous-jacentes ne rÃ©vÃ¨lent donc pas de
structure syntaxique au sens habituel, contrairement Ã  ce que lâ€™on espÃ¨re dâ€™un automate appris
par infÃ©rence grammaticale.
5. Conclusion
Nous avons proposÃ© dans cet article un systÃ¨me homogÃ¨ne dâ€™apprentissage et dâ€™Ã©valuation
de modÃ¨les de langages Ã  partir dâ€™un corpus. En particulier le lissage dâ€™automates stochastiques,
obtenus par infÃ©rence grammaticale, est un problÃ¨me nouveau dont lâ€™importance a Ã©tÃ© montrÃ©e
entre autres dans (TH98). La nouvelle mÃ©thode proposÃ©e ici possÃ¨de la caractÃ©ristique dâ€™Ãªtre
homogÃ¨ne avec la mÃ©thode dâ€™apprentissage. Les rÃ©sultats obtenus sur le corpus AGS, composÃ©
de phrases rÃ©elles prononcÃ©es en situation de dialogue oral homme-machine, montrent que cette
modÃ©lisation syntaxique est pertinente. Comme elle est destinÃ©e Ã  fournir lâ€™Ã©tage grammatical
du systÃ¨me de reconnaissance vocale qui se trouve en entrÃ©e du systÃ¨me de dialogue, il est intÃ©-
ressant de comparer ses rÃ©sultats avec le modÃ¨le syntaxique courant, un modÃ¨le de bigrammes.
Nous envisageons de poursuivre le travail principalement dans ces deux directions :
â€“ Lâ€™Ã©valuation dâ€™un modÃ¨le syntaxique non stochastique nÃ©cessite un modÃ¨le du langage
cible, ce qui est une contrainte difficile Ã  admettre. Une mÃ©thode dâ€™Ã©valuation de la qualitÃ©
de gÃ©nÃ©ralisation libÃ©rÃ©e de cette contrainte est en cours de constitution.
â€“ Il sera intÃ©ressant dâ€™introduire des connaissances linguistiques dans ce travail, en particu-
lier en travaillant sur des catÃ©gories grammaticales regroupant les mots. Le corpus AGS
devrait alors avoir un comportement meilleur vis Ã  vis de lâ€™algorithme non stochastique
itÃ©ratif (EM-like).
En conclusion, il nous semble que traiter par un concept commun (la correction dâ€™erreur) lâ€™ap-
prentissage de lâ€™automate, la rÃ©estimation des poids, lâ€™affectation de probabilitÃ© aus rÃ¨gles ap-
prises, puis le lissage de lâ€™automate appris est non seulement satisfaisant dâ€™un point de vue de
principe, mais donne des rÃ©sultats intÃ©ressants sur un corpus rÃ©el de langue naturelle. 4
4. Outre les aspects liÃ©s Ã  lâ€™infÃ©rence proprement dite, nous avons prÃ©sentÃ© le lissage dâ€™automates produits par
lâ€™algorithme ECGI qui ont la particularitÃ© dâ€™Ãªtre sans cycles. Un travail simultanÃ© et indÃ©pendant aborde une gÃ©-

RÃ©fÃ©rences
Applying Machine Learning to Discourse Processing. â€“ 1998. AAAI Press, Technical Report SS-98-01.
Amengual (J.-C.) et Vidal (E.). â€“ Efficient error-correcting viterbi parsing. IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. PAMI-20, nÌŠ 10, October 1998.
Bahl (L. R.), Jelinek (F.) et Mercer (R. L.). â€“ A maximum likelihood approach to continious speech
recognition. IEEE trans. on Pattern Analysis and Machine Intelligence, vol. 5, 1983, pp. 179â€“190.
Chodorowski (J.) et Miclet (L.). â€“ Applying grammar inference in learning a language model for oral
dialogue. In : Proceedings of International Colloquium on Grammatical Inference, pp. 102â€“113. â€“ 1998.
Springer-Verlag.
Dempster (A.P.), Laird (N.M.) et Rubin (D.B.). â€“ Maximum likehood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society, vol. 39, nÌŠ ser. B, 1977, pp. 1â€“38.
Dupont (P.) et Rosenfeld (R.). â€“ Lattice based language models. â€“ Rapport technique nÌŠ CMU-CS-97-
173, Carnegie Mellon University, 1997.
Dupont (P.). â€“ Utilisation et apprentissage de modÃ¨les de langages pour la reconnaissance de la parole
continue. â€“ ThÃ¨se de PhD, ENST, 1996.
Daelemans (W.), van den Bosch (A.) et Weijters (T.). â€“ Empirical learning of natural language proces-
sing tasks. In : Proceedings of 9th European Conference on Machine Learning, Workshop on Empirical
Learning of Natural Language Processing Tasks. â€“ 1997.
Feldman (J. A.), Lakoff (G.), Stolcke (A.) et Weber (S. Hollbach). â€“ Miniature Language Acquisition :
A touchstone for cognitive science. â€“ Rapport technique, ICSI, 1990. TR-90-009.
Grammatical Inference. â€“ 1998. Lecture Notes in Artificial Intelligence 1433, Springer-Verlag.
Empirical Learning of Natural Language Processing Tasks. â€“ 1997.
Kruskal (J. B.) et Sankoff (D.). â€“ Time Warps, String Edits, and Macromolecules : the Theory and
Practice of Sequence Comparaison. â€“ Addison-Wesley, 1983.
Mooney (R. J.) et Califf (M. E.). â€“ Induction of first-order decision lists : Results on learning the past
tense of english verbs. J.A.I.R., vol. 3, 1995, pp. 1â€“24.
Ney (H.) et Essen (U.). â€“ Estimating small probabilities by leaving-one-out. In : European Conference
on Speech Communication and Technology (Eurospeechâ€™93), pp. 2239â€“2242. â€“ Berlin, Germany, 1993.
Nigam (K.), McCallum (A.), Thrun (S.) et Mitchell (T.). â€“ Learning to classify text from labeled and
unlabeled documents. 5th National Conference on Artificial Intelligence (AAAI-98), 1998.
Ostendorf (M.) et Wightman (C. W.). â€“ Automatic labelling of prosodic patterns. IEEE Transactions on
Speech and Audio Processing, vol. 2, nÌŠ 4, 1994, pp. 469â€“481.
Rulot (H.), Prieto (N.) et Vidal (E.). â€“ Learning accurate finite-state structural models of words : the ecgi
algorithm. In : ICASSPâ€™89, pp. 643â€“646. â€“ 1989.
Rulot (H.) et Vidal (E.). â€“ An efficient algorithm for the inference of circuit-free automata. Syntactic and
Structural Pattern Recognition, 1988, pp. 173â€“184.
Sadek (D.), Ferrieux (A.), Cozannet (A.), Bretier (P.), Panaget (F.) et Simonin (J.). â€“ Effective human-
computer cooperative spoken dialogue : the ags demonstrator. In : Proceedings of ICSLP, pp. 542â€“545.
â€“ 1996.
Thollard (F.) et Higuera (de la) (C.). â€“ The importance of smoothing in learning deterministic stochastic
finite automata. â€“ ECMLâ€™98, Poster session, 1998.
Wagner (Robert A.). â€“ Order-n correction for regular languages. Communication of the ACM, vol. 17,
nÌŠ 5, May 1974.
Wagner (Robert A.) et Ficher (Michael J.). â€“ The string-to-string correction problem. Journal of the
Association for Computing Machinery, vol. 21, nÌŠ 1, January 1974, pp. 168â€“173.

nÃ©ralisation de ce problÃ¨me de lissage au cas o`u les automates comportent des cycles (P. Dupont, J.-C. Amengual :
Smoothing probabilistic automata: an error-correcting approach, Rapport technique EURISE 9806, UniversitÌ    e Jean
Monnet, Saint-Etienne, 1998)
