
Language Processing with Weighted Transducers

Mehryar Mohri
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932-0971, USA
mohri@research.att.com
Résumé - Abstract
Les automates et transducteurs pondérés sont utilisés dans un éventail d’applications allant de
la reconnaissance et synthèse automatiques de la langue à la biologie informatique. Ils four-
nissent un cadre commun pour la représentation des composants d’un système complexe, ce qui
rend possible l’application d’algorithmes d’optimisation généraux tels que la déterminisation,
l’élimination des mots vides, et la minimisation des transducteurs pondérés.
Nous donnerons un bref aperçu des progrès récents dans le traitement de la langue à l’aide
d’automates et transducteurs pondérés, y compris une vue d’ensemble de la reconnaissance
de la parole avec des transducteurs pondérés et des résultats algorithmiques récents dans ce
domaine. Nous présenterons également de nouveaux résultats liés à l’approximation des gram-
maires context-free pondérées et à la reconnaissance à l’aide d’automates pondérés.
Weighted automata and transducers are used in a variety of applications ranging from automatic
speech recognition and synthesis to computational biology. They give a unifying framework for
the representation of the components of complex systems. This provides opportunities for the
application of general optimization algorithms such as determinization, -removal and mini-
mization of weighted transducers.
We give a brief survey of recent advances in language processing with weighted automata and
transducers, including an overview of speech recognition with weighted transducers and recent
algorithmic results in that field. We also present new results related to the approximation of
weighted context-free grammars and language recognition with weighted automata.
Keywords: automatic speech recognition, weighted finite-state transducers, weighted automata,
context-free grammars, regular approximation of CFGs, rational power series.
1 Introduction
It is a common observation that massive quantities of digitized data are widely available for
various information sources such as text, speech, biological sequences, images, and handwritten
Mehryar Mohri
characters or patterns. But much needs to be done to fully exploit these resources since they
are all highly variable or noisy sources of information. Natural language texts are extremely
ambiguous, speech and handwritten texts highly variable and hard to detect in presence of
noise, biological sequences often altered.
To cope with this variability, sophisticated machine learning techniques have been used to de-
sign statistical models for these information sources (Vapnik1995). The theory of weighted
finite-state transducers combining the classical automata theory and statistical models provides
a general framework for processing such sources of information.
Weighted transducers are in fact used in a variety of applications ranging from automatic
speech recognition and synthesis to computational biology (Baldi and Brunak1998; Culik II
and Kari1997; Mohri1997).
They give a common representation for the components of complex language processing sys-

tems. This provides opportunities for the application of general optimization algorithms such
as determinization, -removal, and minimization of weighted transducers.
In what follows, we give a brief survey of some recent advances in language processing with
weighted automata and transducers, including an overview of speech recognition with weighted
transducers and recent algorithmic results in that field. We also present new results related to
the approximation of weighted context-free grammars and language recognition with weighted
automata.
2 Speech recognition with weighted transducers

2.1 Preliminaries
✁ ✂✆☎ ✝✟✞ ✠✡✞ ☛☞✞ ✌✍✞ ✎✏✞ ✑✒✞ ✓✔✞ ✕✗✖
✄                                                                                    ✘
☛                        ✝                      ✠
A weighted finite-state transducer                                     over a weight set       (Bers-
✑✢✙✛☛ ✌✚✙✛☛✢✜✣☎ ✝✥✤✧✓ ✦   ★ ✖✩✜✣☎ ✠✥✤✪✦   ★ ✖✫✜ ✘ ✜✧☛ ✕                   ✬✩✭ ☛
tel1979; Eilenberg1974) is a generalization of the classical definition of a finite automaton and
is given by a finite set of states , an input alphabet , an output alphabet , a finite set of
transitions                                            , an initial state      , a set of final states
, an initial weight and a final weight function . Figure 1 gives an example of weighted
transducer.
✘ ✂✯✮✢✤✰✦✲✱ ★
For numerical stability, the weights used in speech recognition systems are often log proba-

✁ ✵✳ ✴ ✶✸✷                         ☎ ✺✸✞ ✻✼✖ ✺ ✭ ✹✩✴✝✒✶✔✽✷ ✻ ✭ ✠✒✽
bilities, thus               . The weight of a path is obtained by adding the weights of its

constituent transitions. We denote by       the original and by
path . The weight associated by to a pair of strings        ,      ,
the destination state of a
, is then given by:

✴ ✁ ✷ ☎ ✺✸✞ ✻✼✖✫✂✄✿✔✾ ❀ ❁ ❂ ❃ ❄ ✓✍❅ ❆ ✴ ✶✔✷ ❅ ✕✏☎ ✹✩✴ ✶✸✷ ✖
❇ ☎ ✺✸✞ ✻✼✖                                 ✁                             ✺                 ✻
where the sum runs over
❉                  , the set of paths in             with input label             and output label and
❊ ✞ ❋ ✭ ✮●✤✥✦✲✱ ★ ✞ ❊ ❈ ❋✟✂■❍✥❏ ❑▼▲✼☎ ◆ ✺ ✳ ☎ ❍ ❊ ✖✵❖✰◆ ✺ ✳ ☎ ❍✡❋ ✖ ✖
where is defined by:
When a Viterbi approximation is assumed,                     ❈   is replaced by            ✍◗ ❘
in these definitions.
Language Processing with Weighted Transducers
a:e/1

f: ε/9
b:e/2
1                      e: ε/8
c:e/5
e: ε/10

3/0
a: ε/3                   e:e/11
0
b: ε/4

f:e/12
c: ε/7        2
f:e/13
d:e/8

e:d/9
Figure 1: Example of a weighted transducer. The initial state is represented by a bold circle,
final states by double circles. Inside each circle, the first number indicates the state number, the

✁✄✂✆☎✞✝ ✟               ✁                                   ☎                                          ✟
second, at final states only, the value of the final weight function at that state. Each transition

is labeled with            where is the input label, the output label and its weight. The
symbol denotes the empty string.
ε/3.961
ε/2.306
3         ε/1.882
data/10.89                                                                                                        5/0
ε/3.173
data/6.603                       data/9.268
ε/2.374                                                                  ε/1.913
good/11.11                                 good/9.394                                       day/11.51
0                         1                                                                     4
ε/3.537                                                       ε/1.102          2
day/13.97
Figure 2: Toy bigram model represented by a weighted automaton.
2.2 Speech recognition components

Weighted finite-state transducers provide a common and natural representation for all the com-
ponents used in the search stage of speech recognition systems: HMM models, context-depen-
dency, pronunciation dictionaries, and language models (Mohri et al.1998). We briefly illustrate
this in the following.
Most statistical grammars used in speech recognition can be represented by weighted automata.

Figure 2 shows a toy grammar corresponding to an -gram model restricted to a few words.

☛ ✡✌☞✎✍✑✏
In particular, -gram models can be represented very naturally by weighted automata. The
construction is based on associating one state to each possible          -gram sequence. For
example, in the bigram model of figure 2, state 1 corresponds to the word “good”, state 2 to the
word “day” and state 3 to the word “data”. More general weighted grammars such as weighted
context-free grammars can also be approximated with weighted regular grammars and thus be
represented by weighted automata as shown in the next section.
Pronunciation dictionaries can also be compactly represented by weighted transducers mapping
Mehryar Mohri
d: e/0                       ey:e/0.92                  dx: e/0.22                       ax:data/0
0                       1                                2                                3                         4/0
ae: e/0.51                     t: e/1.61

Figure 3: Sample pronunciation dictionary transducer encoding four different pronunciations
of the word “data”.
d/e_e:d
d/d_e:d

d/t_d:d
d/d_d:d
d/t_t:d
d/d_t:d                              t/d_d:t                              d/t_e:d
d/e_d:d       d,d                    d,t                                              t,d                 d,e
t/d_t:t                t/t_d:t
d/e_t:d                                        t/t_t:t
t/e_t:t                                                               t/t_e:t
e,e                                                                  t,t                                                        t,e
t/d_e:t
t/e_t:t
t/e_e:t
Figure 4: A triphonic context-dependent model for the phones and ✁ represented by a finite-
state transducer. The symbol ✂☎✄ ✆ ✝ represents the context-dependent phone ✂ with the left
context ✆ and right context ✝ .
phonemic transcriptions to word sequences. Figure 5 illustrates this in the particular case of
the pronunciation of the word “data” in English corresponding to the four cases of flapped d or
t. The weights can be used to encode the probability of each pronunciation typically based on
data collected from a large number of speakers.
Figure 4 illustrates the construction of a simple triphonic context-dependency transducer ✞

mapping context-dependent phones to phones with only two phones and ✁ (Pereira and Ri-
ley1997). Each state ✟ ✂☎✠ ✆☛✡ encodes the most recent pair of phones read. ☞ represents the
start or end of a phone sequence. More general context-dependent models corresponding to
weighted rewrite rules can also be compiled into weighted finite-state transducers (Mohri and
Sproat1996).
Figure 5 shows a three-state HMM transducer mapping sequences of distribution indices to
context-dependent phones. Such models can clearly be represented by weighted transducers.
The global HMM transducer for speech recognition is obtained by taking the closure of the
union of all HMMs used in acoustic modeling.
2.3 New algorithmic results

Weighted transducers map input sequences to output sequences with some weights. They can
be composed like other mappings to create more complex mappings. The composition of two
Language Processing with Weighted Transducers
d1:e                      d2:e                                d3:e

d1:p                         d2:e                         d3:e
0                          1                                  2                     3
Figure 5: Three-state HMM transducer mapping sequences of distribution indices to context-
dependent phones.

H                               C                                        L                     G
distributions            context−dependent                        phones                          words                word
phones                                                                                 sequences

Figure 6: Recognition cascade.

✂✁          ☎✄
transducers         and          is defined by:
✆✞✝☎✟ ✠☛✡  ✂✁✂☞✌ ☎✄ ✍ ✎ ✝☎✟ ✠✑✏✓✒✕✔✞✖✗✡  ☎✁ ✍ ✎ ✝☎✟ ✘✙✏✛✚✜✡  ✢✄ ✍ ✎ ✘✑✟ ✠✑✏

There exists a natural and efficient composition algorithm for combining weighted transducers
(Mohri, Pereira, and Riley1996). The algorithm is an extension to the weighted case of the
classical composition algorithm for unweighted transducers (Berstel1979). It is based on a
filter represented by a transducer that eliminates the redundancy of ✣ -paths.
The composition of the components just presented: ✤ , the transducer mapping sequences of
distribution indices to context-dependent phone, ✥ the context-dependent model, ✦ the lexicon
or pronunciation dictionary, and ✧ the grammar, gives a mapping from sequences of distribution
names to word sequences:
☞       ☞       ☞
✤        ✥       ✦       ✧

Figure 6 illustrates that recognition cascade. Recent work in very large-vocabulary speech
recognition has shown that it is in fact possible to build off-line the result of that cascade
of compositions, even for very large tasks, using general optimization algorithms such as ✣ -
removal (Mohri2000a), determinization (Mohri1997) and minimization of weighted finite-state
transducers (Mohri2000b).
The result is thus a single transducer that integrates all the speech recognition components,
directly mapping from HMM states to words (Mohri and Riley1999). Experiments with a
463,331-word vocabulary North American Business News (NAB) Task show that this also leads
to a substantial improvement of the recognition speed (Mohri and Riley1999). The size of the
integrated context-dependent networks constructed can be further dramatically reduced using a
factoring algorithm. With that construction, the integrated NAB recognition transducer contains
only about ★✪✩ ✫ times as many transitions as the language model ✧ it is constructed from (Mohri
and Riley1999).
The weights of the integrated recognition transducer can be distributed in many equivalent ways
along its paths. For speech recognition, the weight distribution is crucial since pruning is typ-
ically based on the combined weight from the acoustic, duration, pronunciation, and language
model components accumulated so far along an explored path: the integrated recognition trans-
Mehryar Mohri
b/0.5
b/0.3
a/1.428                   a/0.5     2       a/0.6
0                        1
3/0
c/0.1

(a)
b/0.5
b/2.328
a/0                    a/2.528       2       a/0
0                 1
3/0
c/1.528

(b)
Figure 7: Weight pushing algorithm in the log semiring. The resulting automaton (b) is equiv-
alent to (a) and is stochastic: at each state, the probability weights of outgoing transitions sum
to 1.

ducer is searched with a simple Viterbi decoder combined with a beam pruning to find the best
path and to output the transcription corresponding to the input speech utterance.
It is possible to modify the weights so that the sum of the probabilities for all transitions leaving

a state is . This makes the transducer stochastic and results in an equivalent transducer whose
weight distribution is more suitable for pruning and speech recognition and leads to substantial
improvements in the recognition speed as demonstrated in several tasks. As an example, with
this technique, we can obtain a ✁✂✁ ✄✆☎ speed-up at ✝✞✝✆☎ word accuracy in rescoring NAB word
lattices with more accurate 2nd-pass models.
Figures 7 (a)-(b) illustrate the application of the algorithm in the case of a simple weighted
automaton. There exists a generalization of the algorithm of Floyd-Warshall that can be used to
effectively compute the equivalent stochastic transducer (Mohri1998). However, that algorithm
has a quadratic time complexity and a cubic space complexity which makes its application to
the large transducers used in speech recognition impossible in practice – such transducers may
have several million transitions. A new algorithm devised recently has been shown to be prac-
tical for the computation of the resulting stochastic transducer even for such large transducers
(Mohri1998). The algorithm has been shown to be practical for transducers of more than 5M
transitions such as a factored integrated recognition transducer used for the 463,331-word vo-
cabulary NAB task. It is also applied to word lattices to speed-up rescoring with more accurate
2nd-pass models.
3 Regular approximation of context-free grammars

General context-free grammars are computationally too demanding for real-time applications
such as speech recognition. The grammars used in those applications often represent regular
languages either by construction or as as a result of a regular approximation of a more general
context-free grammar (Pereira and Wright1997; Grimley Evans1997; Johnson1998).
Language Processing with Weighted Transducers

✂  ✂✁☎
✁☎          ✄ ✝
✞✄ ✟    ✆               ✆☞✏✑✁✍
✌             ✄ ✎        ✆            ✓✟✌      ✏✗✁✍        ✘
✟         ✏   ✆
✕  ✁
☎     ✄ ✝  ✆
✟✠
✟✠         ✁
☎ ✁
☎            ✄
✡                     ✟☛✌✏✔✁✒ ✁✍
✁ ✒       ✞     ✓✟ ✏
✓  ✏          ✟✘✟✘✏✔✏✔✁✍   ✁✍
✁ ✍
✞✟ ✟ ✙✆✙✆ ✏✚✏✚✁☎
✟ ✘     ✏      ✁
☎     ✆✆☞✏
✟✠
✆✠           ✁☎
☎           ✞✞ ✟☛✟             ✟✕
✟☛✟✕✏✔✁✍   ✁✍        ✄✎✖ ✟☛✏                ✆✌
✆✙✆✌✏✑✁✍    ✁✍         ✞✖ ✆☞✏
✆✠
✆✠✁☎          ✁☎✞✄✝ ✆☞✆                          ✏ ✁✍   ✁✍✄✝✖                                  ✏ ✁✍   ✁✍✞✆   ✏
✛✙✣
(a)
Figure 8: Regular approximation by transformation. (a) Context-free grammar
mar     obtained from    by transformation..      ✛✜
(b)
✛✢✜   . (b) Gram-

a
b
a
1               b
a

0                         b                             3
b                                  a

2
Figure 9: Finite automaton realizing the approximated grammar
the compilation algorithm presented in (Mohri and Pereira1998).
✛✣    shown in figure 8 (b), using
The effect of such approximations are often complex and it is difficult for the grammar writer
to modify the resulting grammar or to adapt it to a specific application. Furthermore, many of
these approximations do not scale. They blow up for grammars of several hundred or thousand
rules (Nederhof2000).
A new approximation algorithm has been devised more recently that applies to any context-free
grammar and that guarantees that the result can be compiled into a finite automaton (Mohri
and Nederhof2001). The resulting grammar contains at most one new nonterminal for any
nonterminal symbol of the input grammar, and new rules are formed out of rules from the
input grammar by means of a straightforward decomposition. The result thus remains readable
and if necessary modifiable. The algorithm also extends to the case of weighted context-free
✟✟✦✁★✏ ✧✓✩✘✆ ✧ ✆ ✛☞✧ ✣ ✟✆☛✬✭✧✮✬ ✛ ✜
✘                                                                                                               ☛✟ ✏✤✆ ✁✥✖ ✆☛✬
grammars.
✜ ✜ ✣ ✣✫✪ ✪ ✪
An approximate grammar      is obtained from     by introducing at most one new non-terminal
symbol    for each non-terminal and by introducing the rule
✯✱✰✚✲         . Each rule of the
✟✗✁✦✜ ✳ ✧✵✴ ✩✙✴ ✴ ✆ ✳ ✜ ✆✙✏✜ ✁✦✧ ✜ ✆ ✣
✆✙✣✏ ✁✶✧ ✣ ✆✸✷ ✴ ✴ ✴ ✆☞✬✺✏ ✹ ✜ ✁✱✧✮✬✺✹ ✜ ✆☛✬✻✆✙✬✏ ✁✶✧✮✬✻✟☛✏
form                                       where          and where             are mutually
dependent non-terminals is split into the following set of rules:             ,
✛✤✣
Figures 8 (a)-(b) illustrate this approximation. The resulting grammar        is strongly regular
and can be compiled efficiently into the finite automaton of figure 9 (Mohri and Pereira1998).
This approximation algorithm, as well as three other variants, have been fully implemented
and incorporated in the GRM library (Mohri2001). We used that approximation algorithm and
implementation to approximate a weighted grammar of about
at AT&T. The transformed grammar had about                                         ✿✎❀✝✳ ✲✎✲✾✲
rules used for translation
rules. The whole approximation process
✼✾✽✝✳ ✲✾✲✎✲
including the creation of a finite automaton accepting that grammar took about one minute using
Mehryar Mohri
(/-1
*/0            */0
)/-1            )/1

a/0     e/0            e/0
b/0            b/0
a/0            a/0        +/0
(/1                                            (/-1
b/0                               a/0

e/0             )/1               b/0
0             1/0            3/0                2
+/0                               e/0

(/1                               (/-1

(/-1
Figure 10: A -state weighted automaton over the tropical semiring recognizing regular expres-
sions over the alphabet ✁ ✂☎✄ ✆ ✝ . The symbol ✞ corresponds to the empty string used in regular
expressions. For simplicity, the empty set regular expression ✟ has been omitted.
an SGI Origin 2000.
4 Context-free recognition with weighted automata

Weighted automata can be used to recognize more complex languages than just regular lan-
guages (Cortes and Mohri2000). The definition of recognition with weighted automata is a
natural generalization of that of recognition with unweighted automata. Let ✠ be a subset of the
weight set ✡ over which the automaton has been defined. A string ☛ is said to be ✠ -accepted by
the automaton ☞ when the sum 1 of the weights of all paths in ☞ labeled with ☛ is an element of
✠ . ✠ is often chosen to be a singleton which makes it possible to test in constant time if a weight
is in ✠ .
An example of a non-regular language that can be recognized by a weighted automaton is the
language of regular expressions. The description of that language in formal language theory
courses is often confusing since it is more powerful (it is context-free) than the set of objects
it is meant to describe (regular languages). This forces the introduction of the more general
concepts of context-free grammars and parsing to give a full description of the conceptually
simpler regular expressions.
Figure 10 shows a simple automaton that ✌ -recognizes the language of regular expressions over
the alphabet ✁ a,b ✝ . The semiring considered here is ✍ ✎✑✏✒✁✔✓✕✝✖✄ ✗✙✘ ✚☎✄ ✛✜✄ ✓✕✄ ✌✣✢ , the tropical
semiring. Thus, a string ☛ is accepted by that automaton iff the minimum weight of a path
labeled with ☛ is ✌ . This membership test can be performed in linear time.

1
The sum here corresponds to the first operation of the semiring ✤ .
Language Processing with Weighted Transducers
5 Conclusion
We gave a brief survey of recent algorithmic and theoretical results related to the use of weighted
finite-state transducers in language processing. Weighted transducers provide compact repre-
sentations for the components or models of language processing systems. Efficient algorithms
such as composition can be used to combine these models. General optimization algorithms
help reducing their size or increasing their efficiency of use. The theoretical foundation for
weighted finite-state transducers, the theory of rational power series, combines the theory of
probabilistic modeling and classical automata theory.
Acknowledgements
The material presented in this paper is in large parts the result of collaboration with Corinna
Cortes, Mark-Jan Nederhof, Fernando Pereira, and Michael Riley.
References
Baldi, Pierre and Soren Brunak. 1998. Bioinformatics: The Machine Learning Approach
(Adaptive Computation and Machine Learning). MIT Press.

Berstel, Jean. 1979. Transductions and Context-Free Languages. Teubner Studienbucher:
Stuttgart.

Cortes, Corinna and Mehryar Mohri. 2000. Context-Free Recognition with Weighted Au-
tomata. Grammars, 3(2-3).

Culik II, Karel and Jarkko Kari. 1997. Digital images and formal languages. In Grzegorz
Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages. Springer, pages 599–
616.

Eilenberg, Samuel. 1974. Automata, Languages and Machines, volume A. Academic Press.

Grimley Evans, E. 1997. Approximating context-free grammars with a finite-state calculus.
In 35th Annual Meeting of the ACL, pages 452–459.

Johnson, M. 1998. Finite-state approximation of constraint-based grammars using left-corner
grammar transforms. In 36th Annual Meeting of the ACL and 17th International Conference
on Computational Linguistics, volume 1, pages 619–623.

Mohri, Mehryar. 1997. Finite-State Transducers in Language and Speech Processing. Com-
putational Linguistics, 23:2.

Mohri, Mehryar. 1998. General Algebraic Frameworks and Algorithms for Shortest-Distance
Problems. Technical Memorandum 981210-10TM, AT&T Labs - Research, 62 pages.

Mohri, Mehryar. 2000a. Generic Epsilon-Removal Algorithm for Weighted Automata. In
Proceedings of the Fifth International Conference on Implementation and Application of Au-
tomata (CIAA’2000), London, Ontario, Canada, July.
Mehryar Mohri
Mohri, Mehryar. 2000b. Minimization algorithms for sequential transducers. Theoretical
Computer Science, 234:177–201, March.

Mohri, Mehryar. 2001. Weighted Grammar Tools: the GRM Library. In Jean claude Junqua
and Gertjan van Noord, editors, Robustness in Language and Speech Technology. Kluwer
Academic Publishers, The Netherlands, pages 165–186.

Mohri, Mehryar and Mark-Jan Nederhof. 2001. Regular Approximation of Context-Free
Grammars through Transformation. In Jean claude Junqua and Gertjan van Noord, editors,
Robustness in Language and Speech Technology. Kluwer Academic Publishers, The Nether-
lands, pages 153–163.

Mohri, Mehryar and Fernando C. N. Pereira. 1998. Dynamic Compilation of Weighted
✂✁
Context-Free Grammars. In th Meeting of the Association for Computational Linguistics
(ACL ’98), Proceedings of the Conference, Montréal, Québec, Canada. ACL.

Mohri, Mehryar, Fernando C. N. Pereira, and Michael Riley. 1996. Weighted Automata
in Text and Speech Processing. In Proceedings of the 12th biennial European Conference
on Artificial Intelligence (ECAI-96), Workshop on Extended finite state models of language,
Budapest, Hungary. ECAI.

Mohri, Mehryar and Michael Riley. 1999. Integrated Context-Dependent Networks in Very
Large Vocabulary Speech Recognition. In Proceedings of the 6th European Conference on
Speech Communication and Technology (Eurospeech ’99), Budapest, Hungary.

Mohri, Mehryar, Michael Riley, Don Hindle, Andrej Ljolje, and Fernando C. N. Pereira. 1998.
Full Expansion of Context-Dependent Networks in Large Vocabulary Speech Recognition. In
Proceedings of the International Conference on Acoustics, Speech, and Signal Processing
(ICASSP ’98), Seattle, Washington.

Mohri, Mehryar and Richard Sproat. 1996. An Efficient Compiler for Weighted Rewrite Rules.
✂✄
In th Meeting of the Association for Computational Linguistics (ACL ’96), Proceedings of
the Conference, Santa Cruz, California. ACL.

Nederhof, M.-J. 2000. Practical experiments with regular approximation of context-free lan-
guages. Computational Linguistics, 26(1).

Pereira, F.C.N. and R.N. Wright. 1997. Finite-state approximation of phrase-structure gram-
mars. In E. Roche and Y. Schabes, editors, Finite-State Language Processing. MIT Press,
pages 149–173.

Pereira, Fernando C. N. and Michael Riley, 1997. Finite State Language Processing, chapter
Weighted Rational Transductions and their Application to Human Language Processing. The
MIT Press.

Vapnik, Vladimir. 1995. The Nature of Statistical Learning Theory. Springer-Verlag: Berlin-
New York.
