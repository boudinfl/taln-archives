
Apprentissage discriminant pour les Grammaires à
Substitution d’Arbres

Antoine Rozenknop, Jean-Cédric Chappelier, Martin Rajman
Laboratoire d’Intelligence Artificielle - Ecole Polytechnique Fédérale de
Lausanne
CH-1015 Lausanne, Switzerland
{Antoine.Rozenknop,Jean-Cedric.Chappelier,Martin.Rajman}@epfl.ch
Mots-clefs – Keywords

STSG, Gibbs-Markov, Maximum d’Entropie, Vraisemblance Conditionnelle
STSG, Gibbs-Markov,Maximum Entropy, Conditionnal likelihood
Résumé - Abstract

Les grammaires stochastiques standards utilisent des modèles probabilistes de nature générati-
ve, fondés sur des probabilités de récriture conditionnées par le symbole récrit. Les expériences
montrent qu’elles tendent ainsi par nature à pénaliser les dérivations les plus longues pour
une même entrée, ce qui n’est pas forcément un comportement souhaitable, ni en analyse syn-
taxique, ni en reconnaissance de la parole. Dans cet article, nous proposons une approche proba-
biliste non-générative du modèle STSG (grammaire stochastique à substitution d’arbres), selon
laquelle les probabilités sont conditionnées par les feuilles des arbres syntaxiques plutôt que par
leur racine, et qui par nature fait appel à un apprentissage discriminant. Plusieurs expériences
sur ce modèle sont présentées.
Standard stochastic grammars use generative probabilistic models, focussing on rewriting pro-
babilities conditioned by the rewritten symbol. Such grammars therefore tend to give penalty to
longer derivations of the same input, which could be a drawback when they are used for analysis
(e.g. speech recognition). In this contribution, we propose a novel non-generative probabilistic
model of STSGs (Stochastic Tree Substitution Grammars), where probabilities are conditio-
ned by the leaves of the syntactic trees (i.e. the input symbols) rather than by the root. Several
experiments of this new model are presented.
A. Rozenknop, J.-C. Chappelier, M. Rajman
1 Motivations
Les grammaires stochastiques standards sont des modèles probabilistes génératifs dans les-
quels les probabilités sont conditionnées par le symbole à récrire. Par exemple, les probabilités
des grammaires hors-contextes stochastiques (SCFG) sont les probabilités de récrire la partie
gauche de la règle connaissant sa partie droite. Les grammaires à substitution d’arbres stochas-
tiques (STSG), utilisées dans le cadre DOP (Data-Oriented Parsing) (Bod, 1998; Chappelier &
Rajman, 2001), sont quant à elles des grammaires dont les règles sont des arbres syntaxiques,
appelés “arbres élémentaires”. Ces arbres élémentaires sont combinés à l’aide de l’opérateur de
substitution pour donner des dérivations d’arbres d’analyse complets. De plus, à chaque arbre élémentaire τ est associée la probabilité p(τ ) d’utiliser l’arbre élémentaire pour récrire son
symbole racine dans une dérivation. Les modèles SCFG et STSG sont équivalents d’un point
de vue structurel1 , mais clairement différents d’un point de vue probabiliste. De fait, les STSG
peuvent capturer plus de dépendances probabilistes que les SCFG, qui sont restreintes à des
règles hors-contextes équivalentes à des arbres élémentaires de profondeur strictement égale à
1.
Utilisées en reconnaissance de la parole ou en analyse syntaxique, ces grammaires présentent
différents inconvénients : les mécanismes d’apprentissage standards des SCFG mènent à des
modèles affectant des estimations biaisées aux probabilités des arbres syntaxiques (Johnson,
1998) ; pour sa part, le modèle DOP est connu pour sur-estimer les probabilités des arbres de
grande profondeur et critiqué pour le manque de justification de sa procédure d’apprentissage 2
(Bonnema et al., 1999).
Le but de cet article est de présenter des modèles log-linéaires discriminants de grammaires
à substitution d’arbres stochastiques qui évitent ces inconvénients : les modèles Gibbsiens de
Grammaires à Substitution d’arbres (GTSG). Nous en donnons d’abord une définition formelle.
Puis nous présentons les éléments-clefs pour l’apprentissage des paramètres de ces modèles à
partir d’un corpus, avant de conclure par une comparaison des GTSG aux STSG standards dans
une tˆache d’analyse syntaxique.
2 Le modèle GTSG
Le point de départ du modèle est de considérer une probabilisation des TSG qui ne se base pas
sur la probabilité d’un arbre conditionnellement à sa racine, mais plutôt conditionnellement à
ses feuilles, ce qui semble plus naturellement correspondre à une tˆache d’analyse syntaxique.
M. Collins traite le problème comme un problème de classification en proposant une approche
à base de “Voted Perceptron” (Collins & Duffy, 2002).
Nous utilisons ici une approche de type Gibbs-Markov (Lafferty, 1996), décrite dans (Rozenk-
nop, 2002) pour les grammaires hors-contextes, qui permet de choisir les traits pris en compte
pour le calcul des probabilités conditionnelles. Dans le cas de l’analyse, le modèle repose sur des
probabilités d’analyse conditionnellement aux phrases, les traits étant les arbres élémentaires
apparaissant dans les dérivations des analyses.
Cette approche implique que nous imposons a priori que la probabilité conditionnelle d’une
dérivation d connaissant la phrase w suit une distribution de Gibbs :
1
Le même langage est reconnu et les mêmes arbres sont engendrés pour une même phrase.
2
i.e. la façon de calculer ses paramètres à partir d’un corpus d’exemples.
Apprentissage discriminant pour les Grammaires à Substitution d’Arbres
1                                 1
p(d|w) =            e   τ ∈R   λτ fτ (d)
=          eλ·f (d)            (1)
Zλ (w)                            Zλ (w)

o`u Zλ (w) est le facteur de normalisation3 d⇒w e τ ∈R λτ fτ (d) , fτ (d) est le nombre d’occur-
rences de l’arbre élémentaire τ dans la dérivation d, R est l’ensemble des arbres élémentai-
res de la grammaire, λ = (λτ1 , . . . , λτn ) est le vecteur des paramètres associés aux arbres élémentaires, et f (d) = (fτ1 (d), . . . , fτn (d)).
De plus, comme pour une STSG standard, la probabilité conditionnelle d’un arbre t connais-
sant la phrase w est définie comme la somme des probabilités conditionnelles de toutes ses
dérivations4 :
pλ (t|w) =   pλ (d|w)                                  (2)
d⇒t

Nous appelons Gibbsian Tree Substitution Grammar (GTSG) une STSG ainsi probabilisée.
Notons que la différence principale entre GTSG et STSG tient dans la probabilisation du modèle
et dans l’algorithme d’apprentissage associé. Cependant, des algorithmes identiques peuvent
ê tre utilisés pour effectuer une analyse syntaxique, i.e. déterminer l’analyse la plus probable
(MPP5 ) d’une phrase.
Exemple jouet
L’exemple suivant illustre les différences entre STSG et GTSG. Considérons une TSG contenant
les arbres élémentaires de la figure 1.6
Là o`u la STSG associe des probabilités p1 , . . . , p5 aux arbres élémentaires, la GTSG utilise
des valeurs réelles non contraintes λ1 , . . . , λ5 , appelées potentiels et intégrées dans le modèle
probabiliste par l’équation (1).
Supposons alors que nous voulions utiliser ces grammaires pour analyser la phrase “Regarde la
femme avec un chapeau”, qui peut ê tre associée aux analyses (A) et (B) de la figure 2.
Avec la STSG, les probabilités des dérivations sont respectivement : 7

pST SG (d1 (A)) = p1 · p3 · p24 · p5            et         pST SG (d1 (B)) = p2 · p24 · p5

Avec la GTSG, l’équation (2) impose que le potentiel associé à une dérivation soit la somme
des potentiels de ses arbres élémentaires, d’o`u :

λ(d1 (A)) = λ1 + λ3 + 2λ4 + λ5                  et         λ(d1 (B)) = λ2 + 2λ4 + λ5

Les probabilités des dérivations d1 (A) et d1 (B) conditionnellement aux feuilles w sont alors :

eλ(d1 (A))                                                           eλ(d1 (B))
pGT SG (d1 (A)|w) =                                     et          pGT SG (d1 (B)|w) =
eλ(d1 (A)) + eλ(d1 (B))                                              eλ(d1 (A)) + eλ(d1 (B))
3
d ⇒ w représente l’ensemble des dérivations menant à w.
4
Avec une STSG, un arbre d’analyse peut avoir plusieurs dérivations différentes, contrairement au cas des CFG.
5
MPP = Most Probable Parse.
6
Notez que ces arbres sont de profondeur 1, et que la grammaire peut alors aussi ê tre considérée comme une
CFG ; ce n’est pas le cas en général, mais cela simplifie ici notre illustration.
7
Avec cet exemple élémentaire, chaque analyse possède une seule dérivation.
A. Rozenknop, J.-C. Chappelier, M. Rajman
Arbres                   STSG             GTSG élémentaires                proba.          potentiels
VP
τ1 =                                   p1            λ1
V         NP
VP
τ2 =                                       p2            λ2
V        NP         PP
NP
τ3 =                                     p3            λ3
NP        PP
NP
τ4 =                                   p4            λ4
Det  N
PP
τ5 =                                   p5            λ5
P         NP
p1 + p 2 = 1
Contraintes :             p3 + p 4 = 1     Zλ (w) = ...
p5 = 1

F IG . 1 – Exemple STSG/GTSG.
VP                                                              VP

(A)                          NP                            (B)                               PP
PP                                  NP                     NP
NP                               NP             V    Det        N       P   Det        N
V    Det        N          P          Det        N

F IG . 2 – Deux analyses : (A) correspond par exemple à “Regarde la femme avec le chapeau”,
et (B) à “Regarde la femme avec tes lunettes”.
Illustrons sur cet exemple quelques limitations des STSG pour l’analyse. Considérons par
exemple le cas o`u aucune autre information sur le langage utilisé n’est disponible. Dans ce
cas, il peut ê tre naturel de chercher à imposer que la grammaire affecte la même probabilité à
chaque analyse. Avec une STSG, l’affectation la plus “impartiale” des probabilités élémentaires
semble correspondre à p1 = p2 = p3 = p4 = 21 ; p5 = 1 : tous les arbres élémentaires de même
racine sont équiprobables, et la somme de leurs probabilités est 1.
En réalité, cependant, la grammaire obtenue est très peu impartiale : les contraintes stochas-
tiques choisies favorisent ici les arbres les plus petits, i.e. ceux qui sont produits avec le moins
d’étapes de dérivation. Par exemple, pST SG (d1 (A)) et pST SG (d1 (B)) sont respectivement ( 21 )4
et ( 12 )3 : (B) est donc deux fois plus probable que (A) !
Avec une GTSG au contraire l’affectation impartiale λ1 = λ2 = λ3 = λ4 = λ5 = 0 mène à
des probabilités conditionnelles de d1 (A) et d1 (B) égales à 1/2, ce qui apparaît plus conforme
à l’intuition.
Notez que cet exemple a pour seul but d’illustrer la différence entre GTSG et STSG, non de
démontrer la supériorité de l’une sur l’autre. Toutefois, les expérimentations pratiques confir-
ment que des inconvénients flagrants des STSG, soulignés dans (Johnson, 1998) et (Bonnema
Apprentissage discriminant pour les Grammaires à Substitution d’Arbres
et al., 1999), sont effectivement évités avec les GTSG associées à la technique d’apprentissage
décrite ci-après.
3 Apprentissage des paramètres à partir d’un corpus
Les paramètres du modèle sont appris à partir d’un corpus C selon le principe de Maximum de
Vraisemblance (Dempster et al., 1977).
Nous cherchons pour cela les paramètres λ∗ de la GTSG qui maximisent la vraisemblance
conditionnelle du corpus d’apprentissage Lp̃(pλ ), o`u p̃(w, t) est la fréquence relative dans C de
l’arbre t de feuilles w :
λ∗ = Argmax        p̃(w, t) log pλ (t|w)
λ           w,t
3.1 Algorithme “Improved Iterative Scaling” (IIS)
Pour résoudre ce problème non-trivial de maximisation, nous appliquons l’algorithme IIS géné-
ralisé (Lafferty, 1996) aux STSG8 : plutôt que de maximiser Lp̃(pλ ) directement, cet algorithme
recherche le maximum en améliorant itérativement le modèle λ, à partir d’un modèle λ 0 initial.
Dans notre cas, cette méthode conduit à chercher pour chaque arbre élémentaire τ la solution xˆ
de l’équation suivante, o`u f # (τ ) = τ ∈Grammaire fτ (d) :

# (d)
p̃(w, t)         pλ (d|w)fτ (d)xf                    =          p̃(w, t)         pλ (d|t)fτ (d)           (3)
w,t              d⇒w                                            w,t              d⇒t
Les modèles successifs sont alors obtenus en remplaçant λτ par λτ + log xˆ.
Comme ses coefficients sont positifs, l’équation (3) peut facilement se résoudre par la méthode
de Newton. La difficulté résiduelle est de calculer les coefficients en un temps “raisonnable”,
de façon à ce que l’apprentissage puisse ê tre effectivement réalisé. La prise en compte de cette
notion d’efficacité est l’objet de la section suivante.

3.2 Algorithme Inside-Outside
Membre de gauche Le membre de gauche de la formule de réestimation (3) repose sur une
double somme : l’une sur les exemples de la base, l’autre sur les dérivations possibles pour une
phrase donnée.
Le calcul de cette dernière doit ê tre factorisé pour ê tre effectué en pratique 9 .
De fait, le terme le plus problématique de la partie gauche de (3) peut se récrire :
# (d)         1                           λτ fτ (d)              # (d)
pλ (d|w)fτ (d)xf                   =              e           τ ∈R               fτ (d)xf
d⇒w
Zλ (w) d⇒w
1                        # (d)                  fτ (d)            1
=                  fτ (d)xf                    e λτ            =                        fτ (d)          v(τ )fτ (d)
Zλ (w)                                                            Zλ (w)
d⇒w                       τ ∈d                                    d⇒w               τ ∈d
8
les variables cachées de l’article mentionné correspondent aux dérivations des arbres, qui ne sont pas explicites
dans la base d’apprentissage.
9
Le nombre de dérivations d’une analyse peut ê tre exponentiel en sa taille, et, de plus, une phrase peut avoir un
grand nombre d’analyses possibles.
A. Rozenknop, J.-C. Chappelier, M. Rajman
o`u v(τ ) = eλτ x.
On peut alors calculer d⇒w fτ (d) τ ∈d v(τ )fτ (d) en utilisant l’algorithme Inside-Outside dé-
crit dans (Goodman, 1998) sur le semi-anneau des polynômes IP [v(τ )].
fτ (d)
De plus, nous avons par définition : Zλ (w) =        d⇒w    τ ∈d e
λτ
. Il apparaît ainsi que
Zλ (w) est la somme des coefficients associés au τ tel que fτ (d) = 1, qui est déjà calculée dans
l’algorithme Inside-Outside. Le membre de gauche est donc complètement déterminé.
Membre de droite           De façon similaire, le membre de droite de (3) peut se récrire :
1
pλ (d|t)fτ (d) =                  fτ (d)          v (τ )fτ (d)
d⇒t
Zλ (t)   d⇒t            τ ∈d
o`u v (τ ) = eλτ . Là encore, d⇒t fτ (d) τ ∈d v (τ )fτ (d) peut ê tre calculé par l’algorithme Inside-
Outside, en “épurant” préalablement la table utilisée pour factoriser les calculs intermédiaires,
de façon à ce que seules les dérivations menant à l’analyse t y figurent.

3.3 Apprentissage par profondeur croissante
L’algorithme IIS présente un inconvénient similaire à celui décrit dans (Bonnema & Scha, 2002)
pour DOP : dans le cas o`u l’ensemble des arbres élémentaires R contient les analyses complètes
du corpus d’apprentissage, le modèle sur-estime les paramètres de telle sorte que les analyses
non présentes dans le corpus reçoivent une probabilité arbitrairement petite.
Pour éviter ce comportement, on peut décider de ne pas mettre les arbres complets du corpus
dans R. Une autre possibilité est de changer légèrement la méthode d’apprentissage. Plutôt
que de considérer la vraisemblace du corpus avec la grammaire complète, on peut considérer,
de façon itérative, les vraisemblances du corpus en introduisant progressivement des arbres élémentaires dans la grammaire. Nous avons choisi d’introduire les arbres élémentaires par
ordre de profondeur croissante. Notez que ce type de mécanisme n’est rien de plus qu’une
méthode de lissage de l’apprentissage.
Nous considérons donc l’ensemble des TSG suivantes : Gp est la grammaire associée à l’en-
semble Rp des sous-arbres du corpus de profondeur maximale p (trivialement R p ⊂ Rp+1 ), et
à l’ensemble λRp des paramètres correspondants. La méthode d’apprentissage par profondeur
croissante (IDL)10 consiste alors à calculer les paramètres λR1 de G1 par l’algorithme Improved
Iterative Scaling (IIS), et pour chaque profondeur p, à calculer les paramètres λ Rp \λRp−1 de
Gp ,11 en gardant λRp−1 constant. Les paramètres λRp \λRp−1 sont déterminés en maximisant
la probabilité conditionnelle Lp̃(pλ ) du corpus d’apprentissage selon le modèle Gp , sachant
Rp
λRp−1 .
En pratique, IDL ne nécessite qu’une adaptation minime des algorithmes précédents. Seules
les étapes d’initialisation et de mise-à-jour sont modifiées ; les calculs proprement dits restent
inchangés :
– Initialisation : les paramètres λRpmax \λRp valent −∞ ; les paramètres λRp \λRp−1 valent
0 ; et les paramètres λRp−1 gardent la valeur prise à l’itération précédente.
10
IDL = Increasing Depth Learning.
11
λRp \λRp−1 represente les paramètres de λRp qui ne sont pas dans λRp−1 .
Apprentissage discriminant pour les Grammaires à Substitution d’Arbres
– Mise-à-jour : seuls les paramètres λRp sont modifiés.
IIS est ainsi répété pmax fois, pmax étant la profondeur maximale des arbres du corpus, ce qui
allonge évidemment le temps d’apprentissage12 . Cependant, la convergence IIS étant d’autant
plus rapide que les arbres considérés sont de grande profondeur, on peut encore optimiser IDL
en adaptant le nombre de passes IIS à la profondeur des arbres dont on apprend les paramètres.
4 Expériences
Le principe d’apprentissage exposé repose sur la probabilité d’un corpus d’analyses condition-
nellement aux phrases. Pour rester cohérent avec ce principe, l’analyse syntaxique doit repo-
ser sur cette même probabilité. Dans cette optique, le critère de choix parmi les analyses syn-
taxiques possibles sera donc de sélectionner l’analyse la plus probable (MPP) parmi toutes les
analyses possibles13 .

4.1 STSG polynomiales
Les STSG posent une grande difficulté : avec elles, la recherche du MPP est en effet un problème
NP-difficile dans le cas général (Sima’an, 1996). Des algorithmes approximant cette recherche
ont déjà été développés (Bod, 1992; Goodman, 1996; Chappelier & Rajman, 2000). Une alter-
native, introduite dans (Chappelier & Rajman, 2001), est d’utiliser des Grammaires à Substi-
tution d’Arbres Polynomiales (pSTSG14 ) qui, en n’utilisant comme arbres élémentaires qu’un
sous-ensemble bien choisi des sous-arbres du corpus, rendent polynomiale la complexité de la
recherche du MPP. C’est le cadre choisi pour nos expériences.
Min-Max pSTSG Les pSTSG “Min-Max” sont obtenues en sélectionnant du corpus deux
types d’arbres élémentaires : les sous-arbres minimaux, de profondeur 1, et les sous-arbres
maximaux, i.e. dont toutes les feuilles sont des terminaux. La polynomialité des pSTSG Min-
Max a été prouvée dans (Chappelier & Rajman, 2001).
Head-driven pSTSG Les “Head-driven pSTSG” sont obtenues de façon similaire : tous les
sous-arbres de profondeur 1 sont sélectionnés du corpus, les autres arbres élémentaires étant
ceux dont la “branche de tête” est étendue autant que possible. Une “branche de tête” est une
branche dont tous les nœuds, associés à des catégories syntaxiques, sont étendus seulement s’ils
correspondent à la tête lexicale du nœud qui les domine. Les têtes lexicales sont définies comme
dans (Collins, 1999).

4.2 Protocole
La version de Bod du corpus ATIS a été utilisée pour cette évaluation. Elle consiste en 750
arbres syntaxiques dans lesquels les feuilles lexicales ont été supprimées. La Min-Max pSTSG
extraite compte 2 434 arbres élémentaires : 381 sont de profondeur 1, les autres étant des arbres
12
Un ou deux jours sur une Sun Sparc 10, pour un corpus d’apprentissage de 3000 arbres, pmax = 16, et 200
passes IIS par profondeur.
13
D’autres approches sélectionnent la dérivation la plus probable, ou l’analyse ayant le maximum de constituants
probablement justes – voir (Goodman, 1996).
14
pSTSG = Polynomial Stochastic Tree Substitution Grammars.
A. Rozenknop, J.-C. Chappelier, M. Rajman
maximaux. La Head-driven pSTSG compte seulement 930 arbres élémentaires, dont 381 de
profondeur 1, et 549 “branches de têtes”.
La profondeur maximale des arbres du corpus est de 11. C’est donc également la profondeur
maximale des sous-arbres du modèle Min-Max. En revanche, la profondeur maximale est de 5
pour le modèle Head-driven, certaines règles n’ayant pas de tête lexicale, et les “branches de
têtes” n’étant pas forcément les plus longues dans un arbre.
Deux types d’expériences ont été réalisés. Dans “test 10%”, nous entraînons les modèles sur
90% du corpus, et nous les testons sur les 10% restant. Les résultats correspondent aux moyen-
nes des valeurs obtenues pour dix partitionnements aléatoires. Dans “Autotest”, apprentissage
et test sont réalisés sur le corpus complet ; ces secondes expériences ne servent donc que de
repère sur le gain maximum que l’on peut espérer par l’utilisation des modèles gibbsiens.
Dans chaque expérience, le modèle GTSG est comparé à une STSG obtenue par la méthode
d’apprentissage standard, o`u les arbres élémentaires reçoivent des probabilités proportionnelles
à leur fréquence dans le corpus.
Pour les modèles Head-driven GCFG, deux types d’apprentissage on été testés sur la partie
“test 10%” : le premier utilise la procédure IDL, la seconde non. De fait, cette procédure peut
ê tre évitée pour les Head-driven GTSG, car les arbres complets du corpus ne font pas partie de
l’ensemble de leurs arbres élémentaires. De même, IDL est inutile dans la partie “Autotest”, car
on ne cherche pas alors à observer les capacités de généralisation des grammaires.
Pour évaluer les résultats, nous avons utilisé les scores standards définis dans PARSEVAL (Man-
ning & Scḧutze, 1999) :
– “Couverture” : nombre de phrase recevant au moins une analyse (correcte ou pas) ;
– “Correcte” : taux de phrases correctement analysées parmi les phrases couvertes.
– “Croisé” : taux de phrases recevant un parenthésage incorrect parmi les phrases couvertes. Un
parenthésage est incorrect lorsqu’un des groupes syntaxiques d’une analyse a une intersection
avec un groupe de l’analyse de référence, sans que l’un des deux groupes soit inclus dans
l’autre.
– Précision “P” et Rappel “R” sont obtenus en considérant la séquence d’arbres τ̃ produits par
l’analyseur comme un ensemble E(̃        τ ) de triplets < N, p, d >, o`u N est un label syntaxique
attaché à un nœud d’un arbre, et p et d sont les index dans le corpus des premier et dernier
mots dominés par N . Par comparaison avec la séquence des arbres de référence τ̃ , les taux
sont calculés comme :
|E(̃
τ ) ∩ E(̃ τ )|          |E(̃
τ ) ∩ E(̃ τ )|
P (̃
τ) =                     , R(̃
τ) =
|E(̃τ )|                  |E(̃ τ )|
– le f-score “F” est la moyenne harmonique de la précision et du rappel :
1
F −1 = (P −1 + R−1 )
2
4.3 Résultats
Les résultats obtenus pour les pSTSG Min-Max et Head-driven sont résumés dans la table 1.
Avec les corpus mentionnés, le processus d’apprentissage prend environ 4 heures pour chaque
modèle sur une Sun Blade 60. Pour la procédure IDL, 20 itérations de IIS ont été réalisées
pour chaque profondeur d’arbre élémentaire. Sans IDL, 200 itérations IIS ont été utilisées pour
l’ensemble complet des arbres élémentaires.
Apprentissage discriminant pour les Grammaires à Substitution d’Arbres
Min-max                                                      Head-Driven
Autotest                                                        Autotest
Couv.    Corr.   Croisé    P      R      F                    Couv.    Correct   Croisé    P      R      F
STSG             750     664         0                          STSG            750       412        57
Taux (en %)         88.5       0.0    99.7   99.2   99.45     Taux (en %)            54.9       7.6    96.9   95.2   96.0
GTSG             750     691         0                          GTSG            750       479        43
Taux (en %)         92.1       0.0    99.6   99.5             Taux (en %)            63.8       5.7    97.6   97.0   97.3
Gain (%)                 3.6              –0.1    0.2    0.05   Gain (%)                   8.9     –1.9     0.7    1.8   1.25

Test 10%                                                        Test 10%
STSG             73.9   35.6      11.6                          STSG            73.9     30.4      10.4
Taux (en %)         48.2      15.7    93.6   94.4    94.0     Taux (en %)            41.1      14.0    93.7   93.6   93.6
GTSG-IDL         73.9   36.4       9.7                          GTSG-IIS        73.9     35.7      10.0
Taux (en %)         49.3      13.1    93.5   95.6    94.5     Taux (en %)            48.3      13.5    94.2   95.1   94.6
Gain (%)                 1.1      –2.6    –0.1    1.2     0.5   Gain (%)                  7.2       0.5     0.5    1.5      1
GTSG-IDL        73.9     35.2      10.2
Taux (en %)            47.6      13.8    94.2   94.8   94.4
Gain (%)                  6.5       0.2     0.5    1.2    0.8
TAB . 1 – Performances en analyse : GTSG comparées aux STSG.
4.4 Discussion
Les GTSG obtiennent le score maximum en autotest pour les grammaires Min-Max 15 . C’est
une illustration du fait que les faiblesses théoriques soulignées dans (Bonnema et al., 1999)
affectent les STSG et non les GTSG. La suppression de ces faiblesses n’a presque aucun effet
en “test-10%”. Nous pensons que ces effets sont masqués par les autres facteurs d’erreur qui
agissent dans cette situation, comme la grande variabilité du corpus utilisé.
L’avantage des Head-driven GTSG sur les Head-driven STSG est plus évident. Le taux d’ana-
lyses fausses est réduit de 12% sur les expériences “test-10%”. Les taux de parenthésage croisé,
de précision et de rappel en label sont également meilleurs.
Comme prévu par la théorie, la procédure IDL est inutile avec les GTSG Head-driven : les
modèles obtenus avec un IIS standard mènent à de meilleurs résultats.
5 Conclusion
Pour conclure, les modèles log-linéaires associés à un apprentissage discriminant développés
dans ce document semblent d’autant plus intéressants que les grammaires comptent moins de
paramètres : les grammaires Head-driven en bénéficient plus que les modèles Min-max.
Les résultats obtenus en testant les modèles sur le corpus d’apprentissage semblent confirmer
que les “faiblesses” théoriques connues des STSG sont supprimées dans les GTSG. Cet avantage
est dˆu principalement à la fonction d’apprentissage discriminante et conditionnée par les feuilles
que l’on utilise dans le modèle GTSG, qui est mieux adaptée à la tˆache d’analyse syntaxique à
laquelle on destine la grammaire. L’amélioration est cependant moins sensible en généralisation,
du fait de l’insuffisance des données d’apprentissage par rapport à la taille de la grammaire.
Les tests menés ne montrent cependant pas l’avantage que l’on pourrait tirer d’une autre ca-
ractéristique des GTSG, à savoir l’absence de normalisation de leurs paramètres. Nous pres-
sentons en effet que cette absence pourrait ê tre bénéfique dans des applications telles que la
15
Le score maximal n’est pas 100%, du fait que le corpus compte 555 arbres différents sur ses 750, pour seule-
ment 512 phrases différentes, du fait de la délexicalisation appliquée.
A. Rozenknop, J.-C. Chappelier, M. Rajman
reconnaissance de la parole, o`u la nature probabiliste des paramètres pose des problèmes lors
du mélange de modèles (modèle acoustique et modèle de langage). Il serait très intéressant
d’instancier les GTSG, avec une procédure d’apprentissage adaptée, dans un tel contexte.
Références
B OD R. (1992). Applying Monte Carlo techniques to Data Oriented Parsing. In Proceedings Computa-
tional Linguistics in the Netherlands, Tilburg (The Netherlands).
B OD R. (1998). Beyond Grammar, An Experience-Based Theory of Language. Number 88 in CSLI
Lecture Notes. Standford (CA) : CSLI Publications.
B ONNEMA R., B UYING P. & S CHA R. (1999). A new probability model for data oriented parsing.
In P.D EKKER & G.K ERDILES, Eds., Proceedings of the 12th Amsterdam Colloquium, Amsterdam :
Institute for Logic, Language and Computation.
B ONNEMA R. & S CHA R. (2002). Reconsidering the probability model of data-oriented parsing. In R.
B OD , R. S CHA & K. S IMA’ AN, Eds., Data-Oriented Parsing, chapter I.3, p. 25–41. CSLI Publications.
C HAPPELIER J.-C. & R AJMAN M. (2000). Monte-Carlo sampling for NP-hard maximization problems
in the framework of weighted parsing. In D. C HRISTODOULAKIS, Ed., Natural Language Processing –
NLP 2000, number 1835 in Lecture Notes in Artificial Intelligence, p. 106–117. Springer.
C HAPPELIER J.-C. & R AJMAN M. (2001). Grammaire à substitution d’arbre de complexité polyno-
miale : un cadre efficace pour DOP. In TALN’2001, volume 1, p. 133–142.
C OLLINS M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, Uni-
versity of Pennsylvania.
C OLLINS M. & D UFFY N. (2002). New ranking algorithms for parsing and tagging : Kernels over
discrete structures, and the voted perceptron. In ACL2002, p. 263–270.
D EMPSTER M. M., L AIRD N. M. & JAIN D. B. (1977). Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistics Society, 39, 1–38.
G OODMAN J. (1996). Efficient algorithms for parsing the DOP model. In Proc. of the Conf. on Empiri-
cal Methods in Natural Language Processing, p. 143–152.
G OODMAN J. (1998). Parsing Inside-Out. PhD thesis, Harvard University. cmp-lg/9805007.
J OHNSON M. (1998). PCFG Models of Linguistic Tree Representations. Computational Linguistics,
24(4), 613–632.
L AFFERTY J. (1996). Gibbs-Markov models. In Computing Science and Statistics, volume 27, p. 370–
377.
M ANNING C. & S CH UTZE H. (1999). Foundations of Statistical Natural Language Processing. Cam-
bridge : The MIT Press.
ROZENKNOP A. (2002). Une grammaire hors-contexte valuée pour l’analyse syntaxique. In 9 è me
Conférence Annuelle sur le Traitement Automatique des Langues Naturelles, volume 1, p. 95–104,
Nancy : Association pour le Traitement Automatique des Langues (ATALA).
S IMA’ AN K. (1996). Computational complexity of probabilistic disambiguation by means of tree gram-
mars. In Proceedings of COLING’96, Copenhagen (Denmark). cmp-lg/9606019.
