TALN 2004, Fès, 19–21 avril 2004
Couplage d’un étiqueteur morpho-syntaxique et d’un
analyseur partiel représentés sous la forme d’automates finis
pondérés
Alexis Nasr, Alexandra Volanschi
LATTICE-CNRS (UMR 8094)
Université Paris 7
{alexis.nasr, alexandra.volanschi}@linguist.jussieu.fr
Résumé - Abstract

Cet article présente une manière d’intégrer un étiqueteur morpho-syntaxique et un analyseur
partiel. Cette integration permet de corriger des erreurs effectuées par l’étiqueteur seul. L’étique-
teur et l’analyseur ont été réalisés sous la forme d’automates pondérés. Des résultats sur un
corpus du français ont montré une dimintion du taux d’erreur de l’ordre de          .
✁   ✂   ☎
This paper presents a method of integrating a part-of-speech tagger and a chunker. This in-
tegration lead to the correction of a number of errors made by the tagger when used alone.
Both tagger and chunker are implemented as weighted finite state machines. Experiments on a
French corpus showed a decrease of the word error rate of about    .
✁   ✂   ☎
Mots-clefs – Keywords

Analyse morpho-syntaxique, analyse syntaxique partielle, automates finis pondérés
Part-of-speech tagging, chunking, weighted finite state machines
1 Introduction

L’étiquetage morpho-syntaxique constitue souvent une étape préliminaire à un certain nombre
de traitements linguistiques plus poussés tels que l’analyse syntaxique totale ou partielle. Les
processus d’étiquetage morpho-syntaxique reposent généralement sur l’hypothèse que la caté-
gorie d’un mot dépend d’un contexte local, qui est réduit à la catégorie du mot ou des deux mots
précédents, dans le cas d’étiqueteurs probabilistes fondés sur les modèles de Markov cachés
(MMC). Cette hypothèse est généralement correcte et a permis la réalisation d’étiqueteurs ef-
ficaces et précis (de l’ordre de     de mots correctement étiquetés) dont les paramètres sont
✞   ✟
estimés à partir d’un corpus annoté. Il demeure que cette hypothèse n’est pas toujours vérifiée
Ce travail a été partiellement financé par le projet WATSON dans le cadre de l’action Technolangue.
Nasr, Volanschi
et est à l’origine d’une partie des erreurs d’étiquetage. Ces dernières mènent généralement à
des erreurs dans les traitements suivants, voire à leur échec, en particulier pour l’analyse syn-
taxique. Cette situation est particulièrement frustrante dans la mesure où les traitements syntax-
iques possèdent souvent les connaissances qui auraient pu éviter les erreurs d’étiquetage. Le but
de cet article est de pallier partiellement ce problème en couplant les deux étapes d’étiquetage
et d’analyse partielle. Dans un tel couplage, le choix de la catégorie d’un mot est effectué
en tenant compte des connaissances propres à l’étiqueteur, mais aussi de celles provenant de
l’analyseur partiel.
Le type d’erreur que l’on vise à corriger peut être illustré par la phrase suivante : La recapi-
talisation n’est pas indispensable. Lors de l’étiquetage morpho-syntaxique de cette phrase, le
choix de la catégorie correcte pour l’adjectif indispensable (adjectif qualificatif féminin sin-
gulier) est délicat du fait que ce dernier peut être féminin ou masculin et que le nom avec lequel
il s’accorde (recapitalisation) est relativement éloigné de l’adjectif, du moins pour un étiqueteur
probabiliste fondé sur un MMC. Dans un tel cas, un analyseur partiel regroupera respectivement
les suites la recapitalisation, n’est pas et indispensable au sein d’unités appelées chunks. Le
résultat de ce regroupement est le rapprochement des deux unités (la recapitalisation et indis-
pensable) entre lesquelles s’effectue l’accord et la possibilité de le modéliser dans un MMC.
Le modèle de couplage proposé ici se pose en alternative à un modèle séquentiel où l’analyseur
partiel prend en entrée la meilleure solution de l’étiqueteur. Il n’est alors plus possible de revenir
sur les choix effectués par ce dernier.
Cet article vise un autre objectif qui est de montrer l’avantage de réaliser ces traitements à l’aide
d’automates finis pondérés et d’opérations sur ces derniers. Dans ce cadre, toutes les données
(phrase à analyser, lexique, grammaire, n-grams) sont représentées sous la forme d’automates
et (quasiment) tous les traitements sont réalisés par des opérations standard de manipulation
d’automates. Cette homogénéité possède plusieurs avantages dont le premier est la facilité
de combiner différents modules entre eux grâce aux opérations de combinaison d’automates,
combinaisons plus difficiles à réaliser lorsque les différents modules reposent sur des modèles
formels différents. Un autre avantage de l’homogénéité de ce cadre est la facilité de mise
en œuvre : plus de formats spécifiques à concevoir pour différents types de données, plus
d’algorithmes à adapter, à programmer et à optimiser. La réalisation de tels traitements dépend
de manière cruciale de l’existence de bibliothèques logicielles de manipulation d’automates.
Dans le cadre de ce travail, nous avons utilisé les outils FSM et GRM de ATT (8). Notre tra-
vail se situe dans la mouvance du traitement probabiliste de la langue à l’aide d’automates
pondérés, dont on trouvera un apperçu dans (12). Il se distingue dans son esprit d’autres ap-
proches fondées sur les automates finis non probabilistes, telles qu’INTEX (7), dans lesquelles
des règles sont construites manuellement pour être ensuite utilisées dans le cadre de traitements
automatiques.
L’organisation de l’article est la suivante : dans la partie 2, on reprend quelques définitions con-
cernant les automates pondérés et on introduit quelques notations. Les sections 3 et 4 décrivent
respectivement les principes d’un étiqueteur probabiliste et d’un analyseur partiel et leur im-
plémentation sous la forme d’automates pondérés. Dans la section 5, l’integration des deux
modules est décrite. Enfin, des experiences sont présentées dans la partie 6 et des travaux futurs
sont annoncés dans la partie 7. La revue de la littérature n’a pas été regroupée dans une section,
nous avons préféré établir des comparaisons avec d’autres travaux dans le cours de l’article.
Couplage d’un étiqueteur morpho-syntaxique et d’un analyseur partiel
2 Définitions et notations
Dans la suite de cet article, nous manipulerons deux types d’automates finis, des reconnaisseurs,
qui permettent de reconnaître des mots construits sur un alphabet (                    ) et des trans-                                                                                                                          ☛                                                                                                               ☞                       ☛                   ✌                           ☞                   ✏
ducteurs, qui permettent de reconnaître des couples de mots           construits sur deux alphabets                                                                                                                                                                         ✒   ☛       ✔           ✖           ✘
et
☞      (✚       ☞   ✜   ✒  ). En plus des opérations régulières standard (union, concaténation
☛   ✔   ✖   ✘       ✌               ☞
✚           ✥
et itération) définies sur les deux types de machines, certaines opérations sont spécifiques aux
transducteurs, en particulier l’opération de composition, qui joue un rôle fondamental dans le
reste de cet article. Etant donnés deux transducteurs et reconnaissant respectivement les                                                                                                                                                               ✦               ✧
couples de mots          et        , la composition de et (notée
✒       ☛       ✔          ) est un transducteur qui
✖               ✘                           ✒       ✖                   ✔   ✩           ✘                                                                           ✦       ✧                                                       ✦           ✫       ✧
reconnaît le couple         .                                           ✒           ☛               ✔       ✩       ✘
On définit de plus la notion de semi-anneau qui est un quintuplet                tel que est
✸                               ✁
✒           ✯       ✔           ✱   ✔                   ✴       ✔                   ✷       ✔               ✷           ✘                                           ✯
un ensemble de scalaires muni de deux opérations généralement apellées addition (notée )                                                                                                                                                                                                                                                                                                                                                                                                                ✱
et multiplication (notée ) ayant chacune un élément neutre noté respectivement et . En
✸                       ✁
✷               ✷
associant à chaque transition d’un reconnaisseur un poids prenant sa valeur dans un ensemble
✯ , on obtient un reconnaisseur pondéré construit sur un semi-anneau sur l’ensemble . Un                                                                                                                                                                                                                                                                                                                                                                                ✯
reconnaisseur pondéré, en conjonction avec un semi-anneau génère une fonction partielle                                                                                                                                                                                                     ✯
qui associe aux mots du langage reconnu par le reconnaisseur des valeurs de . Etant donné                                                                                                                                                                                                                                                                                                                                       ✯
un reconnaisseur     et un mot , la valeur associée à par , notée
✺                 , est le produit                                                                                  ☛                                                                               ☛                               ✺                                                               ✻   ✻       ✺       ✽           ✽   ✒           ☛           ✘
( ) des poids des transitions du chemin de correspondant à . Si plusieurs chemins de
✴                                                                                                                                                                                                                                   ✺                                                               ☛                                                                                                                                                                                               ✺
permettent de reconnaître , alors          est égale à la somme ( ) des poids des différents                                                    ☛                                                   ✻   ✻   ✺   ✽   ✽   ✒   ☛       ✘                                                                                       ✱
chemins correspondant à . Etant donné un reconnaisseur pondéré , on définit l’opérateur                                         ☛                                                                                                                                                                                                                   ✺
n-meilleurs chemins, noté            qui retourne le reconnaisseur constitué de l’union des                                                         ❄           ❅   ✒       ✺           ✔   ❉   ✘                                                                                                                                                                                                                                                                                                                   ❉
chemins les plus probables dans . Toutes ces notions sont étendues aux transducteurs.                                                                                               ✺
Dans les expériences décrites dans ce papier on a associé aux transitions des transducteurs
l’opposé de logarithmes de probabilités1 ; on a utilisé le semi-anneau tropical sur   . Dans ce                                                                                                                                                                                                                                                                                                                                                     ❋   ●
dernier, l’opération correspond à l’addition usuelle (pour connaître le poids d’un chemin on
additionne les poids des transitions) alors que l’opération est le minimum (le poids associé par                                                                                                                                                                    ✱
un transducteur a un mot reconnu est le minimum des poids de tous les chemins du transducteur
reconnaissant le mot, c’est-à-dire le chemin ayant la meilleure probabilité).
3 Etiquetage morpho-syntaxique
Le processus d’étiquetage morpho-syntaxique utilisé dans le cadre de ce travail reprend les
principes de l’étiquetage morpho-syntaxique fondé sur les chaînes de Markov cachées, introduit
dans (5). Les états du MMC correspondent aux catégories morpho-syntaxiques et les observ-
ables aux mots du lexique. Ces derniers constituent l’alphabet      et les étiquettes des catégories                                                                                                                                                                                ☞                   ❍
morpho-syntaxiques constituent l’alphabet         . Le processus d’étiquetage, dans un tel modèle,                                                                                                                                      ☞       ❏
consiste à retrouver la suite d’états la plus probable étant donné une suite d’observables.
Les paramètres d’un MMC se divisent en probabilités d’émission et en probabilités de transi-
tion. Une probabilité d’émission est la probabilité d’un mot étant donné une catégorie (   )                                                                                                                                                                                                                                                                                                                                                                        ❑       ✒           ❄       ▼   ❅           ✘
1
On préfère les logarithmes de probabilités aux probabilités pour des questions de stabilité numérique (les
probabilités pouvant être des réels très petits, on risque de ne pas pouvoir les représenter en machine). L’utilisation
de l’opposé du logarithme permet d’obtenir les chemins de probabilité maximale lors de l’utilisation de l’opérateur
n-meilleurs chemins.
Nasr, Volanschi
tandis qu’une probabilité de transition est la probabilité qu’une catégorie suive directement                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ❖
une catégorie (          ). Ces deux ensembles de paramètres permettent de calculer la prob-                                                                                                                                                                                                                                                                                                     P                                                   ❑                                       ✒                                   ❖                                           ▼       P                                   ✘
abilité jointe d’une suite de catégories       (une suite d’états du modèle) et d’une suite de                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ❅                       ✚           ❘               ❙
mots        (une suite d’observables) en utilisant les probabilités d’émission et de transition :                                                                        ❄                                                               ✚                                   ❘                   ❙
❴                                           ❴                                                                       ❴                                                   ❴       ❜
❙❴                   ❵
❑                                                       ✒                   ❅                       ✚                           ❘                    ❙                                           ✔                       ❄                                                                                       ✚               ❘           ❙                   ✘               ❲                                       ❑                                           ✒                           ❅                                       ✚                       ✘                       ❑                                                           ✒                   ❄                                               ✚                                   ▼                   ❅                       ✚                               ✘                           ❪                                                                                                                                ❑                                                ✒                       ❄                                                           ▼   ❅                               ✘       ❑                       ✒               ❅                               ▼       ❅                                       ✚                   ✘
Un tel modèle, appelé modèle bigramme, repose sur l’hypothèse markovienne qu’une catégorie
ne dépend que de la catégorie précédente. Cette hypothèse, fort contraignante, peut être assou-
plie sans changer de cadre théorique en faisant dépendre une catégorie non plus de la catégorie
précédente mais des deux catégories précédentes pour aboutir à un modèle trigramme qui est
le modèle généralement utilisé pour une telle tâche. Dans un modèle trigramme, un état corre-
spond non plus à une catégorie, mais à un couple de catégories.
❞                               ❡               ✉
Un tel MMC peut être représenté par deux
❣                       ❤                       ✐               ❦                            ❧                           ♥                           ♣               ❞                       q       ✉                                   s
t
transducteurs pondérés. Le premier, que nous
✉                   ❣                           ❤                   ✐                        ❦                           ❧                   ♥                               ♣
t
q                   ✉                           s
appellerons , et dont un exemple apparaît dans                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ⑤
la partie gauche de la figure 1 (dans cet exem-                                                                                                                                                                                                                                                                                                                                                                                      ✉                                   ❣                           ❤                               ✐       ❦                   ❧                   ♥                           ♣           ✉                       s                                                                                                                                                                                                                                                    ❢                   ❣                   ❤                   ✐       ❦                ❧                   ♥                       ♣   ❢                   s
ple                 et                ) permet de                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ❢                               ❣                               ❤                               ✐           ❦               ❧               ♥               ♣       ❢                           q        ✉                                   s
☞                           ❍                                       ❲                           ⑦                   ⑨           ✔               ⑩               ❶                           ☞           ❏               ❲                                       ⑦               ✦           ✔               ✧                   ❶
représenter les probabilités d’émission. Son al-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ①                                                                                                                                                                                                                                                                                                                                                                                    ②
phabet d’entrée est       et son alphabet de sortie
t                                                                                                                                                                                                                            t
❡               ❢                       ❣                       ❤                   ✐                            ❦                           ❧                   ♥                               ♣                               q                       ❢                               s
☞               ❍
✉                           ❣               ❤                                   ✐                       ❦           ❧                   ♥                           ♣       ✉                               q       ✉                   s                                                                                                                                                                                                                                                                                                                        ❢           ❣                       ❤                        ✐           ❦           ❧                   ♥                   ♣           ❢           q       ❢               s
❞                               ❡               ❢
. Ce transducteur est doté d’un seul état, et
❣                   ❤                       ✐               ❦                                    ❧                           ♥                           ♣           ❞                   q               ❢                                   s                                                                                                                                                                                                                                                                                                                                                                                                       ✉                       ❣                                   ❤                       ✐               ❦               ❧                       ♥               ♣       ✉               q       ❢                                    s
☞                   ❏
possède autant de transitions (de l’unique état
Figure1 : Les transducteurs E et T
vers lui même) qu’il y a de couples          où                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ✒       ❄                   ✔       ❅   ✘                           ❄
est un mot du lexique (              ) et une catégorie (        ) tels que la probabilité d’émission                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ❄                                                                                   ✌                                                               ☞                                               ❍                                                                                                                                    ❅                                                                                                                                                                                                                                                                                                            ❅                               ✌                           ☞               ❏
❑    soit non nulle. L’opposé du logarithme de cette probabilité (
✒               ❄              ) constitue                                               ▼                           ❅                           ✘                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ❿       ➀               ➂           ➄                   ❑                   ✒       ❄                   ▼       ❅           ✘
le poids de la transition étiquetée           . Dans la figure 1, une telle transition est étiquetée                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ✒                           ❄                                           ✔                   ❅                        ✘
❄                          . Le second transducteur, ayant pour alphabet d’entrée et de sortie
➆                                                ❅                                       ➇                                                           ❿                                                                                       ➀               ➂                   ➄               ❑                           ✒                       ❄                                                       ▼               ❅                                       ✘                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ☞           ❏
(partie droite de la figure 1), appelé , permet de représenter les probabilités de transition. Il                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ➉
reprend la structure du MMC : autant d’états que de catégories et des transitions entre tout cou-
ple d’états        (orienté de vers ) tel que             est non nulle. Le poids de la transition est                                                                                                                                                                                                                                               ✒       ❖                   ✔                   P               ✘                                                                                                                                                                                                                                                                                                                                       ❖                                                                                                                                                                            P                                                                                                                                                                                                                        ❑                           ✒       P                   ▼   ❖                       ✘
2
égal à                   . Dans le cas d’un modèle trigramme, la structure de l’automate est plus                                                                                                        ❿                                                                       ➀                   ➂                                       ➄                           ❑                               ✒       P                   ▼                   ❖                       ✘                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ➉
complexe : un état correspond à une séquence de deux catégories et les poids des transitions
sont de la forme                    .                                                                                                                                                                                                                                                                                                                                                                                                    ❿                                           ➀                               ➂                                       ➄                                           ❑                                                   ✒                           ➌                       ▼               ❖                                           P                                   ✘
La composition de et de (              ) permet de combiner probabilités d’émission et de tran-                                                                                                                                                                                                                                                                                                                                                                                                              ⑤                                                                                                                                                                                                                           ➉                                                                                                       ⑤                                                   ✫                                                            ➉
sition pour aboutir à un transducteur dont l’alphabet d’entrée est       et l’alphabet de sortie est                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ☞           ❍
. Un tel transducteur permet d’associer au couple
☞                                               ❏           le poids                                                                                                                                                                                                                                                                                                                                                                 ❴                                                   ❴                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ❴                                       ❴
❄                           ✚               ❘   ❙       ✔           ❅               ✚   ❘           ❙               ✘                                                                       ✻       ✻   ⑤                           ✫           ➉               ✽   ✽       ✒       ❅           ✚               ❘       ❙           ✔       ❄           ✚   ❘   ❙   ✘       ❲
qui n’est autre que l’opposé du loga-
❙❴                           ❵                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ❙❴       ❵
❿                                                       ➒                                                                                                                                                                                ➀                                       ➂                               ➄                                   ❑                       ✒                   ❄                                                   ▼   ❅                                                               ✘                                           ❿                                                               ➀               ➂                               ➄                                   ❑                                                               ✒                           ❅                           ✚               ✘                               ❿                                                                                ➒                                                                                                                ➀                       ➂                   ➄                           ❑                   ✒   ❅                                   ▼           ❅                                               ✚       ✘
✚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ✜
rithme de la probabilité              , telle que définie ci-dessus.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ❑                                                       ✒               ❅                                   ✚                       ❘                   ❙                               ✔                           ❄                                       ✚                   ❘                ❙                                       ✘
L’étiquetage d’une suite de mots particulière       est réalisé en représentant la suite   sous la                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ➓                                                                                                                                                                                                                                                                                                                                                                                                                                               ➓
forme d’un reconnaisseur de structure linéaire (une transition pour chaque mot de ), appelé                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ➓
lui-même       puis en effectuant la composition de       avec       . La recherche de la suite de                                                                                                                                                                                                                               ➓                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ➓                                                                                                                   ⑤                                       ✫                       ➉
catégories la plus probable étant donné est alors réalisée par la recherche du meilleur chemin                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ➓
dans le transducteur             . L’étiqueteur s’écrit donc :
➓                                                                               ✫                                                   ⑤                                                               ✫                                           ➉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ❄                       ❅               ✒       ➓                   ✫               ⑤               ✫           ➉                           ✔                   ✘
Les probabilités des trigrammes représentées dans l’automate                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ➉                   ne sont généralement pas

2
Strictement parlant, l’automate décrit est un reconnaisseur, mais il peut être vu comme un transducteur dont
l’alphabet de sortie est égal à l’alphabet d’entrée et dont chaque transition possède le même symbole en entrée
et en sortie. Un tel transducteur représente par conséquent la relation identité réduite au langage reconnu par le
reconnaisseur.
Couplage d’un étiqueteur morpho-syntaxique et d’un analyseur partiel
estimées par simple maximum de vraisemblance sur un corpus d’apprentissage, car des tri-
grammes apparaissant dans les textes à étiqueter peuvent n’avoir jamais été observés dans le
corpus d’apprentissage. C’est la raison pour laquelle on a recours à des méthodes de lissage des
probabilités, telles que les méthodes de repli (10) qui consistent à se replier sur la probabilité
du bigramme b c lorsque le trigramme a b c n’a pas été observé dans le corpus et, lorsque
le bigramme b c n’a pas été observé, à se replier sur l’unigramme c. Un modèle de repli peut
être directement représenté sous la forme d’un automate comportant des transitions par défaut
comme décrit dans (4). Etant donné un symbole , une transition par défaut émanant d’un état
est empruntée lorsqu’il n’existe pas de transition émanant de étiquetée par . Dans le cas du
modèle de repli, une transition par défaut est empruntée lorsqu’un trigramme ou un bigramme
n’a jamais été observé. Il ne nous est pas possible ici de décrire plus en détail la structure de
tels automates. Pour plus de détails, le lecteur est invité à se référer à l’article cité ci-desssus.
Plusieurs approches dans la littérature (14; 11; 9) utilisent les automates finis pondérés afin de
simuler le fonctionnement d’un MMC. Dans les trois cas, les n-grammes sont représentés sous
la forme d’automates, de manière proche de la notre. Cependant, ces travaux se distinguent du
notre en ne modélisant pas directement les probablilités d’émission (        ❑   ) estimées sur un
✒       ❄   ▼   ❅   ✘
corpus d’apprentissage, mais en recourant à des classes d’ambiguïtés, qui sont des ensembles
de catégories associées à un mot.
4 Analyse syntaxique partielle

L’analyse syntaxique partielle désigne un ensemble de techniques dont le but est de mettre au
jour une partie de la structure syntaxique d’une phrase, plus précisément, la structure asso-
ciée aux fragments qui n’ont qu’une analyse possible. Par exemple, même si une suite comme
‘maison des sciences de l’homme’ constitue dans une grammaire traditionnelle un groupe nom-
inal ayant une structure complexe avec plusieurs niveaux intermédiaires, dans une analyse par-
tielle elle sera segmentée en trois unités appelées chunks : [maison]      [des sciences] [de
❏       ↔                               ❏   ↕
l’homme] car le rattachement des syntagmes prépostionnels est potentiellement ambigu. Ap-
❏   ↕
pelée aussi chunking, l’analyse partielle a été introduite par (3) comme réponse aux difficultés
d’analyse soulevées par le traitement robuste des textes tout-venants.
Plusieurs approches dont (2) ont abordé l’analyse partielle à l’aide des automates finis, plus
précisément à l’aide des cascades de transducteurs finis. Une cascade de transducteurs est
une succession de transducteurs où chacun permet de reconnaître un type de chunk. L’entrée
de chaque transducteur est constituée par la sortie du transducteur précédent. Notre solution
consiste en l’application simultanée, plutôt que séquentielle, de tous les automates des chunks
qui sont intégrés au sein d’un MMC.
Les chunks, du fait de leur caractère non récursif, peuvent être représentés sous la forme
d’automates finis construits sur l’alphabet  ☞  . A chaque type de chunk (par exemple chunk
❏                                           ➙
nominal, prépositionnel, . . . ) correspond un automate appelé aussi , qui reconnaît toute
séquence de catégories qui constitue un chunk bien formé de type . De plus, au chunk de
type sont associés deux symboles, un symbole de début de chunk, noté <K>, et un symbole
de fin de chunk, noté </K>. L’ensemble des symboles de début et de fin de chunk constituent
un nouvel alphabet appelé    ☞   . Les différents automates associés aux chunks sont regroupés
entre eux au sein d’un transducteur, appelé , qui constitue l’analyseur et dont la structure est
représentée dans la figure 2.
Nasr, Volanschi
➛               ➧
L’alphabet d’entrée de     est      et son al-           ✦                                       ☞                   ❏
➜   ➝       ➞           ➟   ➠                       ➡                                                                   ➜       ➝       ➞   ➝   ➟       ➠       ➡
phabet de sortie est           . Il accepte en
☞           ❏               ➫           ☞   ➛
➛           ➩
entrée des séquences de catégories et produit
des séquences mélant catégories et symboles
de début et de fin de chunk. Etant donné une
I                                                                                                                                                                               F
➜       ➝   ➞           ➟       ➢                       ➡                                                   ➜           ➝   ➞       ➝   ➟   ➢           ➡
➛       ➥
➜                                                                               ➜
séquence de catégories en entrée, produira       ➭                                                                               ✦
c1
en sortie la même séquence dans laquelle toute
c2
➜                                                                                   ➜
occurrence d’un chunk de type sera encadrée                                              ➙
cm
➜                                                                                   ➜
des deux symboles <K> et </K>.        est com-                                                                               ✦
posé de deux parties, une partie supérieure qui
est elle même composée des différents auto-                          ❴
Figure 2 : Structure de l’analyseur partiel                                                                                                                                     ✦
mates de chunks, notés    mis en parallèle.          ➙
Les transitions reliant l’état initial de aux états initaux des différents automates  permettent                                                                            ✦                                                                                                                ➙
d’introduire les symboles de début de chunk et les transititions reliant les états d’acceptation
des automates       à l’état F introduisent des symboles de fin de chunk. La partie inférieure
de est composée d’autant de transitions qu’il y a de catégories morpho-syntaxiques. Enfin
une transition reliant F à I permet de réaliser une boucle et de reconnaître ainsi plusieurs
occurrences de chunks dans une séquence de catégories.
L’automate reconnaît n’importe quel mot construit sur               . L’analyse de est réalisée
✦                                                                                                               ➭                ☞   ❏                                                                                       ➭
en représentant sous la forme d’un automate linéaire (une transition pour chaque catégorie
constituant ) appelé lui aussi et en effectuant la composition  ➭         . On pourra remarquer                                     ➭                                                                        ➭       ✫           ✦
que le produit de cette composition est ambigu, car pour chaque sous-mot de correspondant
➳                   ➭
à un chunk , deux résultats seront produits : la reconnaissance de en tant que chunk (passage
à travers l’automate ) et la reconnaissance de comme une suite de catégories ne constituant     ➙                                                                                       ➳
pas un chunk (passage dans les transitions de la partie inférieure de ). Parmi ces différents                                                                                                                                ✦
résultats, un seul nous intéresse, celui dans lequel toute occurrence de chunk a été marquée par
l’introduction de balises de début et de fin de chunk. Il est facile de limiter le produit de la
composition à ce seul résultat en associant à chaque transition intra chunk un poids de et aux
transitions extra chunk un poids de et en ne gardant des résultats produits que le chemin de
poids minimal. Le processus d’analyse peut être représenté par l’expression :
❄           ❅               ✒   ➭                   ✫   ✦       ✔       ✘
5 Couplage de l’étiquetage et de l’analyse partielle

Les modèles d’étiquetage morpho-syntaxique à l’aide des transducteurs pondérés cités dans la
section 3, intégrent aussi (ou prévoient la possibilité d’intégrer) des contraintes syntaxiques
dans le processus d’étiquetage. Kempe (11) prévoit la possibilité de composer la sortie du tag-
ger avec des transducteurs encodant des règles de correction des erreurs les plus fréquentes,
Tzoukerman (14) utilise des contraintes négatives afin de diminuer de façon drastique la proba-
bilité des chemins comportant des suites improbables d’étiquettes (par exemple un déterminant
suivi d’un verbe). D’un point de vue général, notre travail se distingue des autres par le fait
qu’il intègre deux modules complets (un module d’étiquetage et un module d’analyse partielle)
au sein d’un seul, réalisant l’étiquetage et l’analyse partielle. Il ne s’agit pas d’intégrer dans
un étiqueteur des grammaires locales conçues pour éliminer certaines structures agrammati-
cales, mais d’intégrer véritablement l’information statistique avec les connaissances linguis-
tiques modélisées par l’analyseur partiel dans le but d’améliorer la qualité de l’étiquetage.
Couplage d’un étiqueteur morpho-syntaxique et d’un analyseur partiel
Le couplage de l’étiquetage morpho-syntaxique et du découpage en chunks peut être réalisé par
simple composition des deux modèles que nous avons décrit :                                    .
✁                                           ✁
❄           ❅       ✒   ❄           ❅           ✒       ➓       ✫   ⑤   ✫   ➉       ✔               ✘       ✫   ✦               ✔           ✘
Ce modèle est une instance de l’architecture séquentielle que nous avons introduite et critiquée
dans la section 1 : la sélection d’une étiquette morpho-syntaxique est réalisée indépendamment
de la tâche d’analyse syntaxique (ici réalisée par un simple découpage en chunks) et ne peut
être remise en cause par cette dernière.
Il est possible de fournir à l’analyseur non plus le meilleur étiquetage possible mais l’ensemble
de toutes les solutions de l’étiqueteur représentées sous la forme d’un automate :                                                                                                                                                              ❄       ❅       ✒       ➓           ✫       ⑤                       ✫
. Ceci montre la souplesse du traitement par automates finis. Mais un tel modèle
➉                   ✫               ✦           ✔           ✘
n’offre pas beaucoup d’intérêt dans la mesure où l’analyseur n’a quasiment aucun pouvoir dis-
criminant permettant de favoriser certaines des sorties de l’étiqueteur. En effet, contrairement
à un analyseur fondé sur une grammaire hors-contexte, par exemple, qui n’associe une struc-
ture qu’aux phrases appartenant au langage reconnu par la grammaire, notre analyseur accepte
toutes les suites de catégories, son rôle se borne à reconnaître certaines sous-suites de cette
dernière comme formant des chunks. C’est la raison pour laquelle nous allons introduire une
version probabiliste de l’analyseur partiel. Ce dernier effectue un découpage en chunks d’une
suite de catégories et lui associe de plus une probabilité d’après un modèle dont les paramètres
ont été estimés sur un corpus. Un tel modèle n’a pas pour objectif de favoriser un découpage
en chunks d’une même suite de catégories plutôt qu’un autre (l’analyse en chunks est unique !).
Son objectif est de fournir un moyen de comparer entre elles différentes séquences de caté-
gories possibles pour une même phrase. Pour cela, l’analyseur partiel associe à toute séquence
de catégories une probabilité qui est d’autant plus élevée que la séquence de catégories corre-
spond à des séquences de chunks bien formés, agencés dans un ordre linéaire observé sur un
corpus d’apprentissage. Cette approche partage plusieurs points communs avec les travaux de
(6) qui utilisent eux aussi des transducteurs pondérés pour réaliser un analyseur partiel proba-
biliste. Cependant, dans leur cas, plusieurs découpages de la phrase en chunks sont possibles
et l’objectif de l’analyseur est de fournir le découpage le plus probable. De plus, leur analyseur
prend en entrée une séquence unique de catégories.
La probabilité d’une suite de catégories découpée en chunks est calculée à partir de deux types
de probabilités : des probabilités intra chunk et des probabilités inter chunks. Une probabilité                                                                                                                                                                                                                ❴
intra chunk est la probabilité qu’une suite de catégories        constitue un chunk d’un type .                   ❴
❅   ✚   ❘   ➸                                                                                                                                       ➙
Cette probabilité est notée             . Les probabilités inter chunk sont les probabilités condi-
❑   ➺   ✒   ❅   ✚   ❘   ➸   ▼   ➙       ✘
tionnelles d’occurrence d’un chunk d’un type donné, étant donnés les          chunks ou catégories
❉           ❿
précédents (s’agissant d’une analyse partielle, certaines catégories de la suite analysée ne seront
pas intégrées dans des chunks). La probabilité associée par l’analyseur à une suite de catégories
est le produit des probabilités internes des chunks qui le composent et des probabilités externes
de la séquence des chunks reconnus.
Etant donné la suite <s> D N V D N P D A N </s> 3. Le découpage proposé par l’analyseur
est : C = <s> <CN> D N </CN> V <CN> D N </CN> <CP> P D A N </CP> </s>
La probabilité associée à cette séquence est le produit de la suite des chunks reconnus (notée
❑), et des probabilités internes de chacun des chunks :
➻           ✒       ➼       ✘
➾           ➚                                       ➾           ➚                                                                 ➾       ➚                                                 ➾                       ➚
C                                               <s> <CN> V <CN> <CP> </s>                                                 D N|<CN>                                                                      P D A N| <CP>
➻                                                                         ➺                                                                 ➺
➪               ➶                                                                                 ➪   ➹                                     ➪           ➹                                                                                                       ➪
Les probabilités internes sont estimées par maximum de vraisemblance sur un corpus d’apprentis-
3
Où D, N, V, P et A sont les étiquettes correspondant respectivement aux catégories déterminant, nom, verbe,
préposition et adjectif.
Nasr, Volanschi
sage, comme nous le verrons en 5.1. La probabilité d’une suite d’étiquettes de chunks et
d’étiquettes morpho-syntaxique est calculée à l’aide d’un modèle -gram, appelé modèle ex-                                                                                                                                                                                                 ❉
terne, appris lui aussi sur un corpus, qui modélise la probabilité d’un chunk étant donné les
❉       ❿
chunks ou catégories précédentes. Dans le cas d’un modèle externe bigramme, la probabilité
externe de est calculée de la manière suivante :                ➭
➾               ➚                               ➾                       ➚                                                                        ➾                   ➚                                                             ➾       ➚                     ➾           ➚                                        ➾        ➚
C   ➪           ➶
<CN>|<s>                                 ➪               ➹
V|<CN>                                            ➪   ➹
<CN>|V    ➪   ➹
<CP>|<CN>                ➪       ➹
</s>|<CP>                        ➪
5.1 Construction du modèle et estimation de ses paramètres

L’estimation des paramètres du modèle externe et des modèles internes s’effectue en deux
étapes à partir d’un corpus étiqueté. Lors d’une première étape, le corpus est analysé par
l’analyseur partiel A. Le résultat de cette analyse est un nouveau corpus dans lequel des sym-
boles de début et de fin de chunks ont été introduits. Deux objets sont produits à partir de ce                                                                                                                                                                                                                                                         ❴
corpus. D’une part toutes les suites de catégories correspondant à chaque type de chunk         et                                                                                                                                                                                                                                      ➙
d’autre part un corpus hybride dans lequel toute occurrence de chunk a été remplacée par un
seul symbole, matérialisant le chunk (ce symbole n’est autre que la marque de début de chunk).
Le corpus hybride se présente donc sous la forme d’une séquence de catégories et de symboles
de chunk, trace du chunk qui a été détecté à cet endroit. Le premier va servir à estimer les
probabilités intra chunks et le second les probabilités inter chunks. Les différentes étapes de ce
traitement sont représentées dans la figure 3.
L’estimation des probabilités inter                         corpus étiqueté
chunk à partir du corpus hybride est
analyse partielle                        identique à l’estimation des proba-
bilités -gram décrite en 3 et sur                                                                                                                                                                            ❉
corpus analysé
laquelle nous ne reviendrons pas.
extraction des chunks             remplacement des chunks     Ces probabilités sont représentées
dans un transducteur (appelé mod-
différentes réalisations de chaque chunk          corpus hybride
èle externe) reprenant la structure
estimation des probabilités intra chunks apprentissage du n-gram       de       dans la figure 1 et dont                                                                                                                                                                        ➉
les transitions sont étiquetées par
modèles intra chunks                  modèle inter chunks     des catégories ou des symboles de
opération de remplacement                     chunks. L’estimation des probabil-
ités intra chunk est une simple es-
modèle final                          timation par maximum de vraisem-
blance. Etant donné un chunk       et                                                                                                                                                                                                                                  ➭           ➸
Figure 3 : Les étapes de la construction du modèle            suites différentes d’étiquettes                                                                                                                                                              ❉
( , , . . . ) représentant toutes les réalisations de ce chunk dans un corpus d’apprentissage,
➳       ✚       ➳       ✜
➳       ❙
❴                                                        ❴
on note        le nombre d’occurrences de la suite . La probabilité de
❉                                              n’est autre que sa            ❴
❅                                                    ➳
fréquence relative :                            . Cette probabilité est la probabilité du chemin correspon-
❙           ➷
➬               ➮ ➱       ✃
❑   ✒       ➳               ✘       ❲
❴                                                                                                                                                          ❴
dant à dans le reconnaisseur .  ➳                                                                                                                                        ➙
Les modèles intra chunks et le modèle externe sont combinés pour former un unique trans-
ducteur à l’aide de l’opération de remplacement, intoduite dans (13). Cette dernière permet de                                                                                                                                                                                                    ❴
remplacer dans le modèle externe une transition <Ki> par l’automate . Le transducteur résul-                                                                                                                                                                                                  ➙
tant est appelé     (pour analyseur probabiliste). Le modèle conjoint d’étiquetage et d’analyse
✦       ❑
est maintenant                      .
❄       ❅   ✒   ➓           ✫               ⑤               ✫       ✦           ❑                         ✔                       ✘
Couplage d’un étiqueteur morpho-syntaxique et d’un analyseur partiel
6 Expériences

Les expériences ont été menées sur le corpus étiqueté Paris 7 (1). Le corpus est constitué de
mots étiquetés avec un jeu de          étiquettes indiquant la catégorie et les traits mor-
✸   ✸                                                                                                                                                                                                                                                                           ✂               ✂                   ✂
✞                       ➙
phologiques des mots. On a réservé une partie du corpus de                mots pour l’apprentissage
❐   ❒           ➙
(App). Les tests ont été réalisés sur un fragment de             mots (Test). Le taux d’erreur du                                                                                                                                                                                                                                                                                                                                                           ❒           ❒           ➙
4
modèle trigramme (noté         ), tel qu’il est décrit dans la partie 3 sur Test est de         . Ce
✂               ✁   Ï           ☎
❮                               ✚                                                                                                                                                                                                                                                                                                                                                                                                                                                               ✔
chiffre constitue notre point de référence. grammaires de chunks différents ont été construites
✂                   Ï
manuellement. Ces grammaires appartiennent à une sous-classe des grammaires hors-contexte
qui représentent des langages réguliers et qui peuvent être compilées sous forme d’automates
afin d’effectuer l’analyse partielle du corpus. Les probabilités intra chunks et les probabilités
externes ont été estimées sur App. Les expériences ont été réalisées grâce aux librairies FSM et
GRM de AT&T
Les performances du modèle                             (noté      ) sont quasiment identiques à celle
❄                   ❅           ✒               ➓                                           ✫                           ⑤                               ✫                               ✦                       ❑               ✔                       ✘                                               ❮                           ✜
de       . Cependant, les deux modèles n’effectuent pas les mêmes erreurs. En effet,
❮               ✚                                                            corrige                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ❮                       ✜
des erreurs effectuées par        mais effectue quasiment autant d’erreurs en plus. Ces
Ò   ✸       ☎
❮                                       ✚
nouvelles erreurs ont différentes causes dont certaines proviennent de l’hypothèse du modèle
❮     que la forme d’un chunk (la suite de catégories qui constituent le chunk) est indépendante
du contexte d’occurrence de ce dernier. Cette hypothèse n’est pas toujours valide, comme
l’illustre la phrase ’la discussion a été ouverte par l’article ...’. Dans cet exemple, ’ouverte’ a
correctement été étiqueté participe passé par        , alors que       l’a étiqueté adjectif. La raison                                                                                                                                                                                                                                                                             ❮                           ✚                                                                                   ❮               ✜
de cette erreur provient du fait que     a reconnu ’a été ouverte’ comme chunk verbal et a choisi                                                                                                               ❮                                               ✜
la catégorie de ’ouverte’ indépendamment du contexte du chunk.                 de son côté a tiré parti                                                                                                                                                                                                                                                                                                                                                                                                                             ❮           ✚
du fait que ’ouverte’ était suivi d’une préposition pour lui assigner la catégorie participe passé.
Afin de pallier partiellement ce problème, nous avons combiné les modèles                et      au sein                                                                                                                                                                                                                                                                                                                                                                                                                                                    ❮                           ✚               ❮               ✜
du modèle suivant :                                              , noté      . Ce dernier ne conserve
❄       ❅               ✒               ✒           ➓                                   ✫           ⑤                                   ✫               ✦                                   ❑                                   ✘               Õ                                       ✒       ➓                                   ✫                           ⑤               ✫       ➉       ✘           ✔               ✘                                   ❮       ×
que les solutions communes à          et      auxquelles il associe la somme des poids attibuées                                                                        ❮                                           ✚                                                           ❮                                                   ✜
5
par        et     (             ❮       ✚   ❮   ✜   ✻   ✻   ). Cette combinaison permet d’attenuer
❮       ×       ✽       ✽       ✒       ❖                       ✘                   ❲                           ✻       ✻   ❮                                   ✚               ✽   ✽                   ✒       ❖                       ✘                           Ù                                   ✻   ✻       ❮                           ✜           ✽           ✽   ✒   ❖       ✘
l’hypothèse d’indépendance. En effet, la dépendance entre la forme d’un chunk et son contexte
d’occurrence est partiellement modélisé par        . Le taux d’erreur de         sur Test est de
✁           ✂   ☎
❮                                   ✚                                                                                                                                           ❮       ×                                                                                   ✔   ✞
soit une diminution de            par rapport à notre modèle de référence :             . Une analyse
✁               ✁                                   ☎
✔       ✞                                                                                                                                                                                                                                                                                                                                                                                                               ❮                           ✚
d’erreurs a montré que        corrige         des erreurs de        mais effectue          de nouvelles
✁                                                                           ☎                                                                                                                                                                                                                                                                               ☎
❮                                       ×                                                                                                                           ✟                   ✔                   ✟                                                                                                                                                                                   ❮                       ✚                                                       ❐       ✔   ✞
erreurs. Les raisons des erreurs effectuées par        sont diverses, certaines proviennent toujours                                                                                                                                                                                                                                                                                    ❮                                   ×
de l’hypothèse d’indépendance citée ci-dessus, d’autres sont dues à l’estimation des probabilités
intra chunk (la probabilité qu’une séquence de catégories donnée constitue un chunk d’une
nature donnée). Ces dernières sont en effet estimées par simple maximum de vraisemblance
et attribuent par conséquent une probabilité nulle à une réalisation de chunk qui n’a jamais
été observée dans App. Une forme de lissage de ces probabilités semble nécessaire. D’autres
erreurs proviennent des limites théoriques du modèle et nécessiteraient pour être corrigées une
analyse syntaxique complète.
4
Ce résultat est supérieur au résultat de (14) (      de taux d’erreur) sur le même corpus et avec le même jeu                                                                                                                                                                                                                     Û       Ü
d’étiquettes. Cette différence provient, au moins en partie, du fait que nous avons travaillé sans mots inconnus :
tous les mots de Test apparaissent dans le dictionnaire. Nous avons effectué cette hypothèse car l’objet de notre
travail est d’étudier l’apport de l’analyseur partiel sur les performances d’un étiqueteur fondé sur les MMC et nous
estimons que l’influence des mots inconnus sera quasiment la même sur les différents modèles que nous avons
testé.
5
Contrairement aux modèles          et     les poids associés à une séquence de mots par        ne correspondent               Ý                           Þ                                   Ý                               ß                                                                                                                                                                                                                                                                                                                               Ý           à
pas à des probabilités.
Nasr, Volanschi
7 Conclusion
Le travail présenté dans cet article a montré que la prise en compte de connaissances syn-
taxiques, sous la forme d’une analyse partielle, permet d’améliorer le résultat d’un étique-
teur morpho-syntaxique. Il a aussi montré que les différentes étapes pouvaient être réalisées à
l’aide d’automates pondérés. De nombreuses améliorations pourraient être apportées au modèle
décrit, telles qu’une meilleure méthode d’estimation des probabilités intra chunk ainsi qu’une
meilleure modélisation de l’influence du contexte sur la réalisation d’un chunk.
Références
1. Anne Abeillé and Lionel Clément. A tagged reference corpus for french. In Proceedings LINC-EACL,
Bergen, 1999.
2. S. Abney. Partial parsing via finite-state cascades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information, Prague, Czech Republic, pages 8–15., 1996.
3. Steven P. Abney. Parsing by chunks. In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors,
Principle-Based Parsing: Computation and Psycholinguistics, pages 257–278. Kluwer, Dordrecht, 1991.
4. Cyril Allauzen, Mehryar Mohri, and Brian Roark. Generalized algorithms for constructing statisti-
cal language models. In 41st Meeting of the Association for Computational Linguistics, pages 40–47,
Sapporo, Japon, 2003.
5. L. R. Bahl and R. L. Mercer. Part of speech assignment by a statistical decision algorithm. In Pro-
ceedings IEEE International Symposium on Information Theory, pages 88–89, 1976.
6. Kuang-Hua Chen and Hsin-Hsi Chen. Extracting noun phrases from large-scale texts: A hybrid ap-
proach and its automatic evaluation. In Meeting of the Association for Computational Linguistics, pages
234–241, 1994.
7. http://www.nyu.edu/pages/linguistics/intex/.
8. http://www.research.att.com/sw/tools/{fsm,grm}.
9. Bryan Jurish. A hybrid approach to part-of-speech tagging. Technical report, Berlin-Brandenburgishe
Akademie der Wissenschaften, 2003.
10. Slava M. Katz. Estimation of probabilities from sparse data for the language model component of
a speech recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400–401,
1987.
11. André Kempe. Finite state transducers approximating hidden markov models. In ACL’97, pages
460–467, Madrid, Spain, 1997.
12. Mehryar Mohri. Finite-state transducers in language and speech processing. Computational Linguis-
tics, 23(2), 1997.
13. Mehryar Mohri. Robustness in Language and Speech Technology, chapter Weighted Grammars Tools:
the GRM Library, pages 19–40. Jean-Claude Junqua and Gertjan Van Noord (eds) Kluwer Academic
Publishers, 2000.
14. Evelyne Tzoukermann and Dragomir R. Radev. Use of weighted finite state trasducers in part of
speech tagging. Natural Language Engineering, 1997.
