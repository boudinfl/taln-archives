TALN 2004, Fez, 19-22 April 2004

TALN 2004

L’outil de traitement de corpus LIKES

Francois ROUS SELOT
LIIA-INSA
24, Boulevard de la Victoire
67 084 Strasbourg-CEDEX
rousselot(cD,liia.insa-strasbourg.fr
tel 03 88 14 47 53

Résumé — Abstract

LIKES (LInguistic and Knowledge Engineering Station) est une station d’ingénierie
linguistique destinée a traiter des corpus, elle fonctionne pour l’instant sur la plupart des
langues européennes et slaves en utilisant des ressources minimales pour chaque langue. Les
corpus sont constitués d’un ou plusieurs textes en ASCII ou en HTML, l’interface donne la
possibilité de constituer son corpus et d’y exécuter un certain nombre de taches allant de
simples taches de découpage en mot, de tri ou de recherche de motifs £1 des taches plus
complexes d’aide 51 la synthése de grammaire, d’aide au repérage de relations, d’aide a la
construction d’une terrninologie. Nous décrivons ici les principales fonctionnalités de LIKES
en rapport avec le traitement des corpus et ce qui fait sa spéciﬁcité par rapport a d’autres
environnements comparables : l’utilisation minimale de ressources linguistiques.

LIKES (Llnguistic and Knowledge Engineering Station) is a linguistic engineering
environment, build for corpora processing. Its provides different modules able to process most
european and slavian languages. Corpora in Likes must be constituted by Texts in TXT format
or in HTML texts of one particular. Tasks available are elementary likes classical basic
corpora processing tasks (making list of forms, segmenting, sorting) and also more
sophisticated as term extraction, help in relation extraction, pattern search, aimed at helping
terminology building and ontology building. Main ﬁmctionalities usefull for corpora
processing are presented here.

Keywords — Mots Clés

Traitement de corpus, segments répétés, recherches de relations, automates, transducteurs
Corpus processing, repeated segments, search of semantic relations, automata, transducers

F rangois Rousselot

1 Introduction

LIKES (Llnguistic and Knowledge Engineering Station) est une station d’ingénierie
linguistique destinée a traiter des corpus, elle fonctionne pour l’instant sur la plupart des
langues européennes : l’anglais et le francais, le portugais, l’espagnol, l’allemand, le roumain
et quelques langues slaves: le tcheque, le bulgare, le slovaque en utilisant des ressources
minimales pour chaque langue. Les corpus sont constitués d’un ou plusieurs textes en ASCII
ou en HTML, l’interface donne la possibilité de constituer son corpus et d’y exécuter un
certain nombre de taches allant de simples taches de découpage en mot, de tri ou de recherche
de motifs a des taches plus complexes d’aide a la synthese de grammaire, d’aide au repérage
de relation, d’aide a la construction d’une terminologie. L’interface donne également la
possibilité de constituer des ﬁltres grammaticaux spéciﬁques a chaque utilisateur et/ou a
chaque corpus qui sont utilisés dans les divers traitements de la station: découpage en
phrases, lemmatisation et calcul de segments répétés.

Nous décrivons ici les principales fonctionnalités de LIKES et ce qui fait sa spécificité par
rapport a d’autres environnements comparables. Likes possede un certain nombre de
fonctionnalités supplémentaires utiles dans le domaine de la fouille de texte et la cre'ation
d’ontologies que nous ne présenterons pas ici.

Likes bien que proche d’INTEX (Silberztein 1992) par certains aspects, adoptel une
philosophie totalement différente vis a vis des ressources linguistiquesz. En effet, LIKES vise
a minimiser celles-ci et en consequence n’utilise aucun dictionnaire. La parenté est indéniable
en ce qui conceme les outils de recherches qui sont bases sur des automates et des
transducteurs avec une syntaxe et un mode de fonctionnement tres proche de celui d’INTEX.
Likes s’adresse a des utilisateurs linguistes ou terminologues aussi bien qu’infor1naticiens,
étudiants aussi bien que chercheurs. Outre les taches usuelles visées par le traitement de
corpus : recherche de motifs, étude de distributions, Likes procure une aide a la construction
de terminologie. Pour faciliter le travail du linguiste un principe permanent est adopté: le
retour au contexte, toute entité de Likes, tout segment de texte est toujours visualisable dans
son contexte. On peut élargir(zoom) le contexte et visualiser la phrase, puis le paragraphe,
puis le texte o1‘1 se trouve l’élément.

Pour faciliter le travail de l’informaticien et d’éventuels développements sur Likes, toutes les
taches sont exécutables a partir d’une console o1‘1 on peut donner des commandes en ligne. Les
exemples que nous foumirons ici seront plutét tirés de corpus spécialisés comme le corpus
MENELAS, car nous travaillons davantage sur les langues de spécialité que la langue

1 L’auteur qui a utilise INTEX pour l’enseigner a des linguistes a pu constater l’intérét de l’approche utilisée
par INTEX pour les recherches de motifs: automates et transducteurs et de la syntaxe choisie. La partie
recherche de la station a été concue a un moment o1‘1 INTEX n’était plus diffusée gratuitement et également a
un moment ou la librairie Java des expressions régulieres n’existait pas encore.

2 Et également vis a vis de sa diffusion: c‘est un programme écrit en Java qui tourne sur n‘importe quel
systeme, contrairement a INTEX (bientét réécrit en C# et rebaptisé NOOJ) qui ne fonctionne que sur le
systeme Windows. Likes est téléchargeable a 1’adresse suivante : http://www-insa.u-strasbg.fr/liia/download
avec une documentation en ﬁancais (bientét en anglais). Les sources et les API sont disponibles sur
demande.Likes est utilise depuis maintenant 3 ans par des chercheurs aussi bien linguistes qu’informaticiens
qui travaillent sur différentes langues dont l’anglais, le ﬁancais, le bulgare et le tcheque, il est également
utilise depuis 3 ans pour l’enseignement du TAL en sciences du langage et de la terminologie et des
ontologies en Langues Etrangeres Appliquées (LEA).

L ’outil de traitement de corpus LIKES

générale, toutefois, les outils présentés ici, peuvent évidemment étre utilisés sur la langue
générale.

2 Likes et le traitement de corpus

Les traitements possibles dans Likes vont des taches élémentaires concemant les mots, les
familles de mots, les mots composés, a celles plus complexes qui traitent des distributions et
des relations.

2.1 Les tﬁches de base concernant les mots simples

Elles sont au nombre de quatre : constitution du corpus, découpage en mots et en phrases, tri
alphabétique, tri par fréquence.

L’interface donne la possibilité de constituer un corpus. Dans un corpus donné, les textes
doivent étre de meme nature, soit tous en texte (.txt), soit tous en XML, appartenant tous a
une méme langue et utilisant le meme encodage3. L’interface demande a l’utilisateur la langue
du corpus : cette information est ensuite utilisée par le programme pour déterminer l’encodage
des textes.

Ces textes sont ensuite découpés. Les tokens sont séparés par les séparateurs habituels :
signes de ponctuation faibles ou forts ou typographiques non alphanumériques. La langue du
corpus détermine les ponctuations non coupantes : par exemple le tiret en francais. Elle
détermine également l’algorithme utilise’ pour la segmentation en phrases du texte. Cet
algorithme prend en compte les acronymes et un certain nombre de cas spéciﬁques au francais
(ces cas sont fournis dans un ﬁchier du dossier des parametres pour le francais) : Pr. Dr. M.
Mme J.C. J-C . etc.

Likes ne sépare pas les tokens des mots ainsi «I24» est un token au meme titre que «et», les
tokens sont des formes, ainsi»Plus» est—il un token différent de «plus».

Le tri alphabétique prend en compte l’ordre lexicographique utilise’ dans les dictionnaires :
Exemple «a<a<a<a <A<A<A< A» ou «e<é<e<é<é < E<E<e‘:<]3:<E»“

2.2 Les téiches de base concernant des groupes de mots

Deux taches de plus haut niveau concement des entités formées de plusieurs mots : la fusion
et le calcul des segments répétés.

2.2.1 La fusion

La fusion sert a ramener a une seule forme des familles de mots proches : l’usage spéciﬁque
est décidé par l’utilisateur. Elle peut servir a lemmatiser. L‘utilisateur peut par exemple
décider de regrouper les formes du pluriel et du singulier d’un meme mot, il décide alors du
représentant de cette nouvelle famille d’occurrences qui rassemblera bien évidemment les

3 ISO-Latinl, ISO-Latin2 etc.

4 Les données pour la fusion et le découpage en phrase seront bientot fournis par des ﬁchiers pour chaque
langue.

F rangois Rousselot

occurrences des deux formes dans le corpus. Exemple: «medecin» et «medecins» sera
represente par <<medecin_S».

On peut generaliser cette lemmatisation semi-automatique. ll sufﬁt de speciﬁer que les mots
qui existent avec un certain nombre de suffixes sont a fusionner. Ainsi «Xe et Xeront
+Xeait+Xeons» calculera tous les ensembles de mots pour lesquels il existe des occurrences
avec au moins deux de ces sufﬁxes. Par exemple l’ensemble: «mange»,»mangeons»,
«mangeait». Chaque ensemble valide sera fusionne et le representant de toutes ses
occurrences sera par exemple: mange_V. L’utilisateur peut egalement, s’il le desire,
rassembler deux synonymes, exemple «ECG» et «electrocardiogramme».

L’interface de fusion permet egalement de regler des problemes de variations
orthographiques. Par exemple, on peut collecter aisement tous les couples de mots qui ont
meme preﬁxe et meme sufﬁxe, mais ne varient que par la partie inteme par exemple «c» dans
l’un et «c» dans l’autre. Le programme fournit alors une liste de couples possibles que
l‘utilisateur peut valider pour fusion. Exemple «facon» ou «facon». Les fusions permettent
d’operer une reduction du nombre des formes, elles vont etre utilisees dans la suite,
notamment avant le calcul des segments repetes.

2.2.2 Le calcul des segments répétés

La technique des segments repetes a ete utilisee relativement frequemment dans des approches
orientees plutet statistique d’abord pour le depouillement d’enquetes ( Lebart, Salem, 1994),
puis pour de l’aide a l’extraction de termes (Justeson, Katz, 1995). Nous l’avons implementee
(F rath & al. 1995) en 1995 d’une facon tres particuliere, apres avoir remarque que la donnee de
deux listes ﬁltres distinctes permettait de decouper des segments repetes correspondant a des
termes composes ou plutet a des «candidats termes» a valider. Notons que LEXTER
(Bourigault, 1996) utilise des listes de mots semblables pour segmenter, mais sur un texte
prealablement etiquete.

ll s‘agit de reperer des sequences de mots repetees dans le corpus. L‘hypothese est la suivante:
si un polytermes du domaine est interessant, il sera utilise’ plusieurs fois dans le corpus, sous
exactement la meme forme. En effet, dans les langues de specialites, les termes sont tres
ﬁges.

Le programme selectionne des segments correspondant a des groupes nominaux qui sont
susceptibles d‘etre des termes. C‘est ainsi que la premiere liste ﬁltre dite «ﬁltre coupant» est
construite avec des verbes et des adverbes, des pronoms relatifs et des conjonctions. Dans un
souci d‘economie ne ﬁgurent au depart dans cette liste que les formes les plus frequentes de
«etre», «avoir», <<devoir» et «pouvoir» (3‘ém° personne), l‘utilisateur completera cette liste au vu
des premiers resultats de l‘algorithme. Cette phase iterative lui permettra egalement de reperer
les verbes les plus interessants du domaine et leur emploi. On adjoint a ce ﬁltre un certain
nombre de mots qui prennent en compte le statut particulier du ter1ne: il doit pouvoir
representer une denomination decontextualisee, c‘est ainsi qu‘il ne peut donc contenir, ni
adjectifs possessifs, ni demonstratifs, ni certains adjectifs de jugement: “mauvais”,”bon”... Un
second ﬁltre permet la mise en forme de fmition: il supprime des mots aux bomes des
sequences trouvees. Des resultats precedents, on supprime les articles en premiere place et de

5 L’algorithme presuppose des termes composes, c’est £1 dire composes de mots distincts, ainsi il donne de

nettement moins bons resultats pour les langues germaniques qui forment beaucoup de mots composes par
concatenation.

L ’outil de traitement de corpus LIKES

meme, en demiere place, les articles, les prepositions. Ainsi, le segment trouve « l’artere
gauche de » est ramene a la forme « artere gauche ».

Chaque langue est dotee au depart de deux ﬁltres initiaux minimaux, le traitement des
segments repetes s‘opere interactivement, les ﬁltres sont completes eventuellement, apres
chaque examen des resultats.

Ce choix a ete effectue afm que l‘utilisateur controle au maximum ce que fait le programme et
observe les caracteristiques du corpus traite. Par exemple, on ne peut y mettre la liste de tous
les verbes, ceux-ci peuvent etre ambigus, exemple “montre” verbe et “montre “nom.
L’adj onction des verbes doit etre toujours controlee.

Les resultats de ce traitement sont foumis sous forme d’arbre : une tete avec des extensions.
Un clic permet un acces au contexte de chaque occurrence.

Un exemple de resultat sous forme d’arbre tire du corpus MENELAS

taux 8
d’expression 2
de transcription 6
technique 14
de 10
Southern 6
Maxam et Gilbert 4
d'amplification PCR 4

Les «tetes» ( parties communes a plusieurs segments repetes comme ici <<technique»)
permettent de reperer des termes simples. Le systeme foumit donc comme resultat
supplementaire, la liste des tetes et donne acces a toutes leurs occurrences.

La qualite ﬁnale des resultats depend des competences de l’utilisateur dans le domaine et l’on
ne pourra en general s’abstenir de faire appel a un expert du domaine pour selectionner la liste
deﬁnitive des termes.

Efﬁcacité: L‘algorithme est lineaire en fonction de la taille du corpus, son fonctionnement est
tres voisin de l'algorithme SEQUITUR utilise pour compresser des textes (Witten 2000). La
methode, quelle que soit son implementation, est confrontee a un probleme inevitable de
reperage de redondances6 qui implique une deuxieme passe dans les resultats reduit son
efﬁcacite. Pour ﬁxer les idees, les implementations actuelles7 permettent de travailler
confortablement sur des corpus relativement moyens d’au maximum 5 M octets.(la gestion
des contextes est en effet consommatrice en espace memoire). Les corpus plus importants
devront etre segmentes manuellement et traites par parties.

Evaluation des résultats : L'evaluation de la qualite d'un extracteur de termes est difﬁcile,
car pour un humain decider ce qu‘est un terme est une decision difﬁcile et parfois subjective.
Une precedente evaluation (F rath & al. 2000) montre que le bruit peut etre tres reduit : de 15%
environ. Nous designons par bruit le pourcentage de candidats termes rejetes par la validation
humaine d’un expert (apres la selection faite par l’utilisateur) qui sont a rapprocher des taux
de bruits d‘autres methodes (Bourrigault l999)(cf. LEXTER qui oscille e11tre 60 et 80%).
Notons aussi que les termes produits par Likes ne sont pas structures : les dependances qui
lient les elements d’unter1ne compose sont ignorees.

Le silence, lui aussi difﬁcile a mesurer, n‘a malheureusement pas ete evalue. Par son principe
meme la methode induit du silence, elle ne repere pas tous les termes simples, ni les

6 Si on trouve 4 occurrences de <<revascularisation de l’artere femorale » et 4 occurrences de «artere

femorale », il s’agit forcement des memes occurrences et1’entree artere femoral est alors redondante.

7 PC 512K de RAM Pentium 4 2,4 GHz avec la machine virtuelle Java 1.4.

F rangois Rousselot

polyterrnes non répétés, ni les variantes.8 La qualité des résultats dépend évidemment de la
qualité nécessaire a l’application visée, ainsi en indexation les résultats rapidement obtenus
sont tres satisfaisants, dans une application terrninologique, ils doivent etre complétés. Si on
travaille dans un corpus incrémental ou les situations de productions des nouveaux textes sont
les memes, au bout d’un moment, les ﬁltres se stabilisent et le programme peut fonctionner de
maniere totalement automatique.

Implémentation d’un nouvelle langue : Celle—ci est aisée, elle conceme la mise en place des
deux ﬁltres initiaux a partir des mots les plus frequents de la langue (Catach, 1995), elle ne
nécessite pas une tres grande compétence linguistique.

2.3 Les tﬁches de recherche.

2.3.1 Recherches simples

Elles sont réalisées grace a un outillage expressions régulieres equivalent a des automates9 (et
transducteurs) récursifs comparables a ceux qu’on trouve dans la station INTEX et utilisant
une syntaxe proche. Les entités des expressions sont des séquences de lettres et non forcément
des mots1° et on peut utiliser également des métacaracteres comme <L> pour lettre“. On peut
ainsi manipuler des schémas morpho-syntaxiques aisément.

Exemple: l'expression “ <L><L>*(ion+ment) “ recherchera dans le corpus tous les mots
composes d‘une lettre suivie de zéro ou plusieurs lettres suivies de «ion» ou de «ment».

2.3.2 Recherche de collocations

Un module spéciﬁque permet de rechercher des cooccurrences et d’exprimer des contraintes
de distances entre les mots choisis : on déﬁnit une recherche concemant deux mots distants
d’au plus n mots, ou présents dans la meme phrase, ou dans le meme paragraphe etc.

2.4 Les téiches évoluées concernant les distributions

2.4.1 La synthese de relations

Un certain nombre de programmes traitent de la synthese de schémas morpho-syntaxiques
exprimant des relations comme par exemple (Séguéla, Aussenac, 1999) ou (Morin 1999).
Elles sont en général orientées vers le repérage de relations génériques comme l’hypo-
hyperonymie et utilisent souvent des techniques statistiques. Nous avons choisi ici une autre

8 Des améliorations sont en cours d‘implémentations, pour la recuperation de terrnes recherchés a partir des
tétes de segments répétés et le repérage des Variantes.

9 Les expressions régulieres sont traduites en automates qui sont déterminisés puis minimalisés.

10 INTEX a deux modes de fonctionnement distincts : soit les étiquettes d’arcs sont des lettres, soit ce sont des
mots, mais on ne peut facilement écrire de schémas morpho syntaxiques.

11 Nous avons essayé d‘étre dans la mesure du possibles compatibles avec INTEX.

L ’outil de traitement de corpus LIKES

approche, orientée vers le repérage de relations spéciﬁques au domaine, et utilisant une
technique basique. Celle-ci implique un controle et une réédition de l’utilisateur.

Le principe en est relativement simple : on repere deux mots donnés pour laquelle la relation
hypothétique tient souvent. Comme par exemple dans MENELAS, on a remarqué la présence
tres fréquente de «coronarographie montre lesion», on donne <<coronarographie» et «lesion» en
entrée au module de synthese. L’idée est de condenser toutes les facons d’exprimer la relation
«montre». Le module de synthese collecte tous les contextes” repérés entre deux occurrences
de <<coronarographie» et de «lesion», ces contextes sont ensuite comparés deux a deux.
Chaque comparaison donne une chaine de mots communs éventuellement vide, on sélectionne
parmi toutes les chaines obtenues celles qui ne sont pas constituées uniquement de mots du
ﬁltre non coupant. On élimine ainsi les chaines constituées uniquement de mots
grammaticaux.

L’interface du programme donne alors a l’utilisateur la possibilité d’éditer les schémas
morpho-syntaxiques obtenus afm d’éli1niner certains mots parasites : négations, adverbes, etc.
et de construire ainsi un schéma synthétisant toutes les facons d’exprimer la relation.

Ce schéma est alors lancé sur le corpus tout entier pour rechercher les classes
distributionnelles auxquelles appartiennent respectivement les deux mots.

sur l’exemple coronaire lesion, le programme rends
met en évidence des
montre des
objective des
a montré des
réalisée immédiatement montrait des

etapréSédﬁkH1(met en évidence+montre+objective+a montré+montrait)
2.4.2 La recherche des classes distributionnelles

La recherche effectuée au moyen du motif précédent permet de récupérer un certain nombre
de contextes ou le sujet n’est pas coronarographie: comme «examen coronarographique»,
<<échographie», «ECG», «électrocardiogramme», «épreuve d’effort» etc. et également ou
l’objet n’est plus lesion : «sténose», «resténose» etc. Cette distribution permet de formuler une
hypothese de classe sémantique: on peut en effet inférer l’existence de la classe des «examens
médicaux». On en déduit que la relation examinée est bien la relation sémantique «montre»
qui tient e11tre des «examens médicaux» et des «signes».

2.5 Les tﬁches d’analyses

Dans le traitement de corpus, le besoin de travailler sur des textes étiquetés totalement ou
partiellement désambiguisés se fait assez vite sentir. Aussi avons nous tenté d’apporter une
réponse a ce besoin par différents modules. Nous avons donc concu, d’une part, un module
traditionnel de resolution d’ambigu'1'tés lexicales locales, construit dans un but pédagogique,
capable d’étiqueter partiellement un texte, d’autre part, un segmenteur appelé encore chunker
(Voutilainen, Tapanainen P., 1993) (Vergne, Giguet 1999). Ce demier, reprends les idées
défendues par Vergne sur l’inutilité d’un dictionnaire pour segmenter et sur l’intérét de
segmenter avant tout étiquetage voire avant toute analyse syntaxique. L’étiqueteur qui devrait
lui faire logiquement suite est en cours de réalisation.

12 La taille maximum des contextes considérés est paramétrable, par défaut elle est de 8 mots.

F rangois Rousselot

2.5.1 Application de dictionnaire et levée des ambiguités lexicales

Le premier qui renie quelque peu le principe de ressources minimales, est un module qui,
d’une part, applique un dictionnaire au texte et qui, d’autre part, permet d’appliquer sur ce
texte des transducteurs de désambigu'1'sations locales (encore appelées «grammaires locales»
dans INTEX).

L’application d’un dictionnaire sur un texte doit en effet étre possible, s’il y en a un de
disponible. Les dictionnaires de Likessont gérés dans une base de données mySQL, un autre
environnement appelé DICOMANAGER permet de gérer la création de dictionnaires et les
ﬂexions.

L’expressivité des transducteurs de levée d’ambigu'1'té est supérieure a celle des transducteurs
d’INTEX, car il est possible en Likes d’exprimer par une étiquette des ensembles en intension
complexes. On peut par exemple exprimer que un mot attendu doit étre une entité du
dictionnaire ambigue entre Adjectif et Nom (A/N), la ou INTEX oblige a énumérer une liste
de mots en extension. Par contre la visualisation des résultats et uniquement textuelle, on ne
peut visualiser les graphes résultats, ni les automates.

2.5.2 Le segmenteur (ou chunker) dfaibles ressources

En attendant l’i1nplémentation d’un étiqueteur, nous avons réalisé un segmenteur qui découpe
une phrase en un ensemble de ﬁagments élémentaires destiné a faciliter les traitements
ultérieurs. Celui-ci se base, d’une part sur une liste ferrnée de mots grammaticaux pour
déterminer le début des segments a isoler et d’autre part devine la catégorie de certains mots
suivant leur morphologie pour détecter les ﬁns de segments. Les unités segrnentées sont: les
SNR( syntagmes nominaux non récursifs) syntagmes nominaux sans syntagme prépositionnel
ni relative, les SV, syntagmes verbaux contenant un verbe et éventuellement un ou des
adverbes et des négations, les Pr pour propositions etc.

L’i1nplémentation actuelle utilise les transducteurs de Likes, les items foumis en entrée des
transducteurs qui sont appliqués en cascade ( Roche, Shabes,l997 ) , inserent dans le texte des
balises qui sont utilisées par les transducteurs suivants. Les expressions des transducteurs qui
segmentent le francais ont été construites tres rapidement en trois semaines et donnent des
résultats tres corrects sur MENELAS par exemple. Ils ont été également essayés sur des textes
plus difﬁciles a traiter comme des poemes, par exemple le Jaseroque 13 de Levis Carol voir
l’extrait qui suit.

Le jaseroque

<pn>I1 brilgue y: |<Ck plur>1es toves <Adj>lubrici11eux |<V plur>Se gyrent
|<Ck>en <V ant>vri11ant |<Ck>dans <e>1e guave y, <N>Enmimés |<V p1ur>sont

y|<Ck plur>1es <Adj>gougebosqueux y, |<Pr>Et |<Ck sing>1e momerade
horsgrave.

<Adj> signale que le mot suivant est un adj ectif, <pn> que le mot suivant est un pronom, <V
plur > que le mot suivant est un verbe au pluriel, <Ck> signal le début d’un segment au
pluriel, il s’agit du début d’un SNR au pluriel, <Ck> sans nombre indique le début d’un
syntagme prépositionnel. 11 et | indiquent une fin de segment.

L’i1nplémentation de ce module s’avere extrémement intéressante, car elle permet de bien voir
l’intégralité des regles du segmenteur et éventuellement des adaptions a un usage particulier.
Cette technique pose toutefois deux problemes : d’une part, celui de la lisibilité des regles, par

13 Traduction ﬁancaise du Jaberwock poeme de Levis Caroll dans Alice au pays des merveilles.

L ’outil de traitement de corpus LIKES

des informaticiens et a fortiori par des linguistes, d’autre part celui de leur validité. C’est
pourquoi nous travaillons a l’élaboration d’une méthode pour construire ces regles de facon
objective a partir d’un dictionnaire et non plus par introspection. Nous menons également une
réﬂexion sur une formulation declarative et donc plus lisible des regles.

2.6 La construction d’une terminologie

Nous nous placons ici dans une perspective ou la construction d’une terminologie est vue
comme un probleme linguistique et non comme un probleme de compilation de termes
étiquettes de concept comme dans une école terminologique connue (Kleiber 2003). Les outils
présentés ici peuvent étre utilisés soit pour faire une terminologie monolingue, dans un
domaine specialise, en prélude éventuel a la construction d’une ontologie, soit pour construire
une terminologie multilingue.

Nous ne détaillerons pas ici les fonctionnalités d’aide a la construction terminologique,
nous en resterons aux principes. Notre implementation fait une separation claire entre le
linguistique et le conceptuel. En effet, nous proposons a l’utilisateur une gestion de ﬁches
terminologiques permettant de prendre en compte les différentes étapes de construction de la
terminologie. On part d’une liste de candidats termes éventuellement fournis par les modules
de Likes, pour les étudier et leur donner éventuellement le statut de terme.

La création d’une ﬁche terminologique s’effectue apres sélection par l’utilisateur d’une
famille de mots formée de formes différentes et de synonymes d’un meme terme, par exemple
«ECG» et «électroencéphalogramme». La ﬁche correspondante permet d’accéder a tous les
contextes des occurrences d’un terme (et de ses synonymes) dans le corpus : un éditeur permet
de ne sélectionner que les occurrences représentatives d’un terme si désiré. Chaque terme peut
étre attache’ a un concept, grace a une ﬁche qui permet de le déﬁnir et de l’implanter dans la
hiérarchie conceptuelle, par la donnée de ses freres et de son pere, cette ﬁche donne acces aux
termes qui le désignent dans plusieurs langues éventuellement. Dans les applications
multilingues, les langues sont séparées (a chaque langue correspond un corpus et un ensemble
de termes) et ne sont liées que grace aux concepts.

3 Conclusion

Nous avons présenté ici un atelier de développement d’outils de traitement de corpus qui
permet de réaliser un certain nombre de taches utiles avec peu de ressources linguistiques. Les
situations sont nombreuses ou les ressources linguistiques adaptées ne sont pas disponibles et
sont trop coﬁteuses a construire. Ces situations concement certaines langues pour lesquelles
les ressources sont faibles et bien évidemment certains langages de spécialité. Likes permet
rapidement de donner des résultats qui bien qu’imparfaits sont signiﬁcatifs et utiles. Les outils
de Likes sont tres utiles tant pour les traitements de corpus que pour la formation a certains
concepts de TAL eta l’extraction d’informations a partir de textes.

Pour l’instant Likes ne permet de ne traiter que des textes non formatés ou en HTML. Des
développements sont en cours pour permettre le traitement des textes en RTF et en PDF.

Likes n’est pas voué a une théorie particuliere et procure des outils qui peuvent étre adaptés a
des taches diverses. On peut par exemple utiliser tres facilement les transducteurs pour oter les
balises des ﬁchiers ( en XML ou en HTML). On peut bien sﬁr y utiliser des ressources
linguistiques si on le désire.

Francois Rousselot

Nous continuons actuellement de développer des modules dans le meme esprit, chaque
module doit pouvoir étre utilise’ pour des buts éventuellement différents, ses fonctionnalités
doivent étre simples a comprendre et etre utiles en TAL.

Mentionnons a titre d’exemple, le module d’annotation que nous n’avons pas décrit ici faute
de place qui permet d’annoter manuellement des textes (avec des balises XML) et ensuite
d’analyser les segments de textes annotés de la meme maniere pour en synthétiser des
automates. En effet, une de nos applications consiste a annoter des textes de biologie, des
articles scientiﬁques sur la structure des protéines pour faciliter l’acces a leur contenu par les
chercheurs en biologie. Nous avons donc commence’ par leur foumir un ban d’annotation
manuelle. Le module de synthese de contexte existant sera affine pour permettre l’aide a la
synthese d’expressions régulieres exprimant les régularités de ces contextes.

Références

Bourigault D. (1996), LEXTER a natural language processing tool for terminology extraction.
In Proceedings of the 7”’ E URALEX International Congress, Goteborg .

Catach N. (1995), L ’orthographefrancaise. Nathan.

Frath P., Oueslati F., Rousselot F. (1995), Identiﬁcation de relations sémantiques par repérage
et analyse de cooccurrences de signes linguistiques. Actes de JAVA 1995 (Grenoble ) réédité
(2000) dans Ingénierie des Connaissances, évolutions récentes et nouveaux déﬁs. Eyrolles.
Justeson J ., Katz S. (1995), Technical terminology: some linguistic properties and W. an
algorithm for identiﬁcation in text. Natural Language Engineering, Vol 1(1), pp.9-28.

Morin E. (1999), Acquisition de patrons lexico-syntaxiques caractéristiques d’une relation
sémantique. TAL (TraitementAutomatique des Langues) Vol.(1) pp 143-166, Paris .

Kleiber G. Expose’ introductif au Colloque TIA 2003 Strasbourg.

Roche E., Schabes Y., (1997), Finite state language Processing, MIT Press.

Rousselot F., Frath P. , Oueslati R. (1996) Extracting concepts and relations from corpora .
Proceedings of Workshop on Corpus—oriented Semantic Analysis European Conference on
Artiﬁcial Intelligence ECAI 96, Budapest.

Séguéla P., Aussenac_Gilles N. (1999), Extraction de relations sémantiques entre termes et
enrichissement de modeles du domaine. Actes de IC’99 (Ingénierie des Connaissances), 79-
88, Paris.

Silberztein M. (1993) Dictionnaires électroniques et analyse automatique des textes. Paris.
Masson.

Vergne J. et Giguet E. (1998), Regards Théoriques sur le "Tagging". Actes de TALN98.
Nancy.

Voutilainen A ., Tapanainen P.(1993) Ambiguity resolution in a reductionistic parser.
Proceedings of the Sixth Conference of the European Chapter of the ACL, Utrecht.

Witten I.H. (2000), Adaptative Text Mining: Infering Structure ﬁom Sequences. Journal of
Discrete Algorithms Vol 0 (0), pp. 1-23.
