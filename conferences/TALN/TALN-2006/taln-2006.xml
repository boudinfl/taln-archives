<?xml version="1.0" encoding="UTF-8"?>
<!--
	Fichier construit à partir des fichiers pdfs de la conférence
-->
<conference>
	<edition>
		<acronyme>TALN'2006</acronyme>
		<titre>13ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Leuven</ville>
		<pays>Belgique</pays>
		<dateDebut>2006-04-10</dateDebut>
		<dateFin>2006-04-13</dateFin>
		<presidents>
			<president>
				<prenom>Piet</prenom>
				<nom>Mertens</nom>
			</president>
			<president>
				<prenom>Cédrick</prenom>
				<nom>Fairon</nom>
			</president>
			<president>
				<prenom>Anne</prenom>
				<nom>Dister</nom>
			</president>
			<president>
				<prenom>Patrick</prenom>
				<nom>Watrin</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="invite">Conférenciers invités</type>
			<type id="long">Communications orales</type>
			<type id="poster">Posters</type>
			<type id="tutoriel">Tutoriels</type>
		</typeArticles>
		<siteWeb>http://cental.fltr.ucl.ac.be/~taln2006/</siteWeb>
	</edition>
	<articles>
		<article id="taln-2006-invite-001" session="Conférence invitée">
			<auteurs>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Fontenelle</nom>
					<email>thierryf@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Microsoft Speech &amp; Natural Language Group, Redmond</affiliation>
			</affiliations>
			<titre>Les nouveaux outils de correction linguistique de Microsoft</titre>
			<type>invite</type>
			<pages>3-19</pages>
			<resume>De nouveaux outils de correction linguistique sont disponibles pour le français depuis quelques mois. Mis à la disposition des utilisateurs de Microsoft Office 2003, un nouveau correcteur orthographique et un nouveau correcteur grammatical permettent d’améliorer le processus de rédaction de documents. En partant d’évaluations externes effectuées récemment, nous présentons les diverses facettes de ces améliorations et de ces outils, en abordant la question de l’évaluation des outils de correction linguistique (qu’évaluer ? quels critères appliquer ? pourquoi développer une nouvelle version ?). La réforme de l’orthographe, la féminisation des noms de métier, l’évolution de la langue figurent parmi les thèmes abordés dans cet article.</resume>
			<mots_cles>correcteur orthographique, correcteur grammatical, français, outils de correction linguistique, Microsoft, réforme de l’orthographe, féminisation des noms de métier</mots_cles>
			<title></title>
			<abstract>New French proofing tools were recently made available to Microsoft Office 2003 users. A new spell-checker and a new grammar checker make it possible to improve the document creation process. Based on recent external evaluations, we present specific aspects of these improved tools and discuss the more fundamental issue of how to evaluate proofing tools (What do we need to evaluate? Which criteria should be applied? Why develop a new version?). Current language changes including the spelling reform, and such innovations as feminine job titles are among the themes we discuss in this paper.</abstract>
			<keywords>spell-checker, grammar checker, French, proofing tools, Microsoft, spelling reform, feminine job titles</keywords>
		</article>
		<article id="taln-2006-invite-002" session="Conférence invitée">
			<auteurs>
				<auteur>
					<prenom>Gertjan</prenom>
					<nom>van Noord</nom>
					<email>vannoord@let.rug.nl</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Groningen</affiliation>
			</affiliations>
			<titre></titre>
			<type>invite</type>
			<pages>20-42</pages>
			<resume>Les systèmes de traitement du langage naturel qui combinent des méthodes basées sur les règles (connaissances explicitées) et celles basées sur les corpus deviennent suffisamment précis pour permettre leur utilisation dans des applications variées. Nous décrivons un système d’analyse syntaxique de ce type pour le néerlandais, appelé Alpino, et nous montrons la nécessité d’utiliser des méthodes basées sur l’utilsation des corpus en vue d’obtenir des analyseurs par règles fiables. Nous décrivons plus particulièrement un ensemble de cas où les résultats de l’analyseur sont exploités pour améliorer l’analyseur lui-même.</resume>
			<mots_cles>analyse syntaxique, méthodes à base de connaissances, méthodes basées sur corpus, Stochastic Attribute Value Grammar, question/réponses, error mining</mots_cles>
			<title>At Last Parsing Is Now Operational</title>
			<abstract>Natural language analysis systems which combine knowledge-based and corpus-based methods are now becoming accurate enough to be used in various applications. We describe one such parsing system for Dutch, known as Alpino, and we show how corpus-based methods are essential to obtain accurate knowledge-based parsers. In particular we show a variety of cases where large amounts of parser output are used to improve the parser.</abstract>
			<keywords>parsing, knowledge-based methods, corpus-based methods, Stochastic Attribute Value Grammar, question answering, error mining</keywords>
		</article>
		<article id="taln-2006-long-001" session="">
			<auteurs>
				<auteur>
					<prenom>Atelach Alemu</prenom>
					<nom>Argaw</nom>
					<email>atelach@dsv.su.se</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lars</prenom>
					<nom>Asker</nom>
					<email>asker@dsv.su.se</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Stockholm University, KTH Department of Computer and Systems Sciences</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>43-52</pages>
			<resume>Nous prouvons que l’information mutuelle entre des paires de mots peut être employée avec succès pour distinguer entre différents usages des mots dans la traduction des requêtes pour la recherche d’information translinguistique. Les expérmentations sont entreprises dans le contexte de la recherche d’information translinguistique amhariquefrancais. Des expérmentations sont entreprises qui comparent la performance de la collection des termes des requêtes désambiguïsés et non désambiguïsés contre une collection de documents ordonnés. Les résultats montrent une amélioration de performance pour les requêtes désambiguïsées en comparison avec l’approche alternative qui emploie la collection de termes entièrement expansés.</resume>
			<mots_cles>recherche d’information, désambiguïsation des mots, information mutuelle, amharique</mots_cles>
			<title>Increased Retrieval Performance using Word Sense Discrimination</title>
			<abstract>We show that Mutual Information between word pairs can be successfully used to discriminate between word senses in the query translation step of Cross Language Information Retrieval. The experiment is conducted in the context of Amharic to French Cross Language Information Retrieval. We have performed a number of retrieval experiments in which we compare the performance of the sense discriminated and non-discriminated set of query terms against a ranked document collection. The results show an increased performance for the discriminated queries compared to the alternative approach, which uses the fully expanded set of terms.</abstract>
			<keywords>information retrieval, word sense discrimination, mutual information, amharic</keywords>
		</article>
		<article id="taln-2006-long-002" session="">
			<auteurs>
				<auteur>
					<prenom>Marianna</prenom>
					<nom>Apidianaki</nom>
					<email>marianna.apidianaki@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 7, Denis Diderot – Lattice – CNRS / ENS</affiliation>
			</affiliations>
			<titre>Traitement de la polysémie lexicale dans un but de traduction</titre>
			<type>long</type>
			<pages>53-62</pages>
			<resume>La désambiguïsation lexicale a une place centrale dans les applications de Traitement Automatique des Langues relatives à la traduction. Le travail présenté ici fait partie d’une étude sur les recouvrements et les divergences entre les espaces sémantiques occupés par des unités polysémiques de deux langues. Les correspondances entre ces unités sont rarement biunivoques et l’étude de ces correspondances aide à tirer des conclusions sur les possibilités et les limites d’utilisation d’une autre langue pour la désambiguïsation des unités d’une langue source. Le but de ce travail est l’établissement de correspondances d’une granularité optimale entre les unités de deux langues entretenant des relations de traduction. Ces correspondances seraient utilisables pour la prédiction des équivalents de traduction les plus adéquats de nouvelles occurrences des éléments polysémiques.</resume>
			<mots_cles>polysémie, cooccurrences, correspondances de traduction, prédiction de traduction</mots_cles>
			<title></title>
			<abstract>Word Sense Disambiguation has a central role in NLP applications relevant to translation. The work presented in this article is a part of a study on the overlaps and divergences between the semantic spaces occupied by the polysemous items of two languages. Correspondences between these items are rarely biunivocal and their study provides insights into the possibilities and limits of using a second language for the disambiguation of polysemous items of a source language. The aim of this work is to establish correspondences of optimal granularity between the items of two languages in translation. Such correspondences could be used for the prediction of the most adequate translation equivalents for new occurrences of the polysemous source language items.</abstract>
			<keywords>polysemy, cooccurrences, translation correspondences, translation prediction</keywords>
		</article>
		<article id="taln-2006-long-003" session="">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Barthélemy</nom>
					<email>barthe@cnam.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNAM, Laboratoire Cédric INRIA, Projet Atoll</affiliation>
			</affiliations>
			<titre>Un analyseur morphologique multi-niveaux utilisant la jointure</titre>
			<type>long</type>
			<pages>63-72</pages>
			<resume>Dans cet article nous présentons un analyseur morphologique pour le verbe akkadien. Cette langue est de la famille des langues sémitiques. Les flexions du verbe font intervenir des changements internes à la racine. L’analyseur présenté ici illustre l’utilisation d’un formalisme multi-niveaux et d’opérateurs relationnels puissants, notamment la jointure. La multiplicité de niveaux intermédiaires entre les formes profondes et de surface, ainsi que les opérateurs de compositions permettent de diviser la description en contraintes relativement simples qui sont ensuite rassemblées pour s’exercer soit simultanément, soit en cascade, soit encore d’une façon mixte, c’est-à-dire simultanément pour certains des niveaux et en cascade pour d’autres. Ce mécanisme nous permet de décrire la vocalisation du radical comme un processus d’insertions successives de voyelles. Cela présente l’intérêt d’être plus simple que l’utilisation d’un schéma vocalique figé soumis à interdigitation. De plus, cela semble expliquer de façon plus économique les formes des verbes faibles.</resume>
			<mots_cles>analyseur morphologique, morphologie à deux niveaux</mots_cles>
			<title></title>
			<abstract>In this paper, we present a morphological analyzer for the Akkadian verb. This language belongs to the semitic family. Verb inflection involves modifications within the root. The analyzer is an example of the use of a multilevel formalism having powerful relational operators, notably the join. The morphology is described using relatively simple constraints which are composed together in three possible ways : they apply simultaneously or sequentially, or even in a mixed way, where the constraints apply simultaneously on some levels and sequentially on others. This mechanism is used to describe the vocalization of verbal roots as successive vowels insertions. This process is simpler than the use of fixed vocalic patterns which implies interdigitation. Moreover, it conveniently explains the forms of weak verbs.</abstract>
			<keywords>morphological analyzer, two-level morphology</keywords>
		</article>
		<article id="taln-2006-long-004" session="">
			<auteurs>
				<auteur>
					<prenom>Ann</prenom>
					<nom>Bertels</nom>
					<email>ann.bertels@ilt.kuleuven.be</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dirk</prenom>
					<nom>Speelman</nom>
					<email>dirk.speelman@arts.kuleuven.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dirk</prenom>
					<nom>Geeraerts</nom>
					<email>dirk.geeraerts@arts.kuleuven.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Katholieke Universiteit Leuven, ILT</affiliation>
				<affiliation affiliationId="2">Katholieke Universiteit Leuven, QLVL</affiliation>
			</affiliations>
			<titre>Analyse quantitative et statistique de la sémantique dans un corpus technique</titre>
			<type>long</type>
			<pages>73-82</pages>
			<resume>Cet article présente la méthodologie et les résultats d’une analyse sémantique quantitative d’environ 5000 spécificités dans le domaine technique des machines-outils pour l’usinage des métaux. Les spécificités seront identifiées avec la méthode des mots-clés (KeyWords Method). Ensuite, elles seront soumises à une analyse sémantique quantitative, à partir du recouvrement des cooccurrences des cooccurrences, permettant de déterminer le degré de monosémie des spécificités. Finalement, les données quantitatives de spécificité et de monosémie feront l’objet d’analyses de régression. Nous avançons l’hypothèse que les mots (les plus) spécifiques du corpus technique ne sont pas (les plus) monosémiques. Nous présenterons ici les résultats statistiques, ainsi qu’une interprétation linguistique. Le but de cette étude est donc de vérifier si et dans quelle mesure les spécificités du corpus technique sont monosémiques ou polysémiques et quels sont les facteurs déterminants.</resume>
			<mots_cles>sémantique lexicale, sémantique quantitative, spécificités, polysémie, cooccurrences, analyse de régression</mots_cles>
			<title></title>
			<abstract>This article discusses the methodology and results of a quantitative semantic analysis of about 5000 keywords (pivotal terms) in the domain of French machining terminology. The KeyWords Method is used in order to identify the most typical words. Next, a quantitative semantic analysis of the keywords determines their degree of monosemy, which is implemented in terms of degree of overlap between co-occurrents of co-occurrents of keywords. Finally, the quantitative data is submitted to various regression analyses, in order to check the hypothesis that the most typical terms are not always the most monosemous terms. This article presents the statistical results of this semantic analysis and provides linguistic interpretation. Building on corpus data, the investigation attempts to establish in how far keywords are polysemous and which factors are most predictive.</abstract>
			<keywords>lexical semantics, quantitative semantics, keywords, polysemy, co-occurrences, regression analysis</keywords>
		</article>
		<article id="taln-2006-long-005" session="">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Blanc</nom>
					<email>olivier.blanc@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Matthieu</prenom>
					<nom>Constant</nom>
					<email>matthieu.constant@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Laporte</nom>
					<email>eric.laporte@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Marne-la-Vallée, IGM</affiliation>
			</affiliations>
			<titre>Outilex, plate-forme logicielle de traitement de textes écrits</titre>
			<type>long</type>
			<pages>83-92</pages>
			<resume>La plate-forme logicielle Outilex, qui sera mise à la disposition de la recherche, du développement et de l’industrie, comporte des composants logiciels qui effectuent toutes les opérations fondamentales du traitement automatique du texte écrit : traitements sans lexiques, exploitation de lexiques et de grammaires, gestion de ressources linguistiques. Les données manipulées sont structurées dans des formats XML, et également dans d’autres formats plus compacts, soit lisibles soit binaires, lorsque cela est nécessaire ; les convertisseurs de formats nécessaires sont inclus dans la plate-forme ; les formats de grammaires permettent de combiner des méthodes statistiques avec des méthodes fondées sur des ressources linguistiques. Enfin, des lexiques du français et de l’anglais issus du LADL, construits manuellement et d’une couverture substantielle seront distribués avec la plate-forme sous licence LGPL-LR.</resume>
			<mots_cles>analyse syntaxique, motifs lexico-syntaxiques, analyse lexicale, ressources linguistiques, formats d’échange, automates finis, réseaux de transitions récursifs</mots_cles>
			<title></title>
			<abstract>The Outilex software platform, soon available to research, development and industry, comprises software components implementing all the fundamental operations of written text processing, including processing without lexicons, exploitation of lexicons and grammars, and language resource management. All data are structured in XML formats, and more compact readable or binary formats, if required. The required format converters are included in the platform ; the grammar formats allow for statistical approaches to be combined with resource-based approaches. Manually constructed lexicons for French and English, originating from the LADL, with substantial coverage, will be distributed with the platform under LGPL-LR license.</abstract>
			<keywords>syntactic parsing, lexico-syntactic patterns, lexical analysis, language resources, exchange formats, finite-state automata, recursive transition networks</keywords>
		</article>
		<article id="taln-2006-long-006" session="">
			<auteurs>
				<auteur>
					<prenom>Pierrette</prenom>
					<nom>Bouillon</nom>
					<email>pierrette.bouillon@issco.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Manny</prenom>
					<nom>Rayner</nom>
					<email>mrayner@riacs.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruna</prenom>
					<nom>Novellas</nom>
					<email>novella2@etu.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yukie</prenom>
					<nom>Nakao</nom>
					<email>yukie-n@khn.nict.go.jp</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marianne</prenom>
					<nom>Santaholma</nom>
					<email>marianne.santaholma@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marianne</prenom>
					<nom>Starlander</nom>
					<email>marianne.starlander@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nikos</prenom>
					<nom>Chatzichrisafis</nom>
					<email>nikos.chatzichrisafis@vozzup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève</affiliation>
				<affiliation affiliationId="2">National Institute for Communications Technology</affiliation>
			</affiliations>
			<titre>Une grammaire multilingue partagée pour la traduction automatique de la parole</titre>
			<type>long</type>
			<pages>93-102</pages>
			<resume>Aujourd’hui, l’approche la plus courante en traitement de la parole consiste à combiner un reconnaisseur statistique avec un analyseur robuste. Pour beaucoup d’applications cependant, les reconnaisseurs linguistiques basés sur les grammaires offrent de nombreux avantages. Dans cet article, nous présentons une méthodologie et un ensemble de logiciels libres (appelé Regulus) pour dériver rapidement des reconnaisseurs linguistiquement motivés à partir d’une grammaire générale partagée pour le catalan et le français.</resume>
			<mots_cles>traduction automatique de la parole, modélisation du langage, grammaire d’unification, reconnaissance linguistique, grammaires multilingues</mots_cles>
			<title></title>
			<abstract>Today, the most common architecture for speech understanding consists of a combination of statistical recognition and robust semantic analysis. For many applications, however, grammar-based recognisers can offer significant advantages. In this paper, we present a methodology and an Open Source platform (Regulus), which together permit rapid derivation of linguistically motivated recognisers, in either language, from a bilingual grammar of Catalan and French.</abstract>
			<keywords>speech to speech translation, language modelling, unification grammar, grammar-based recognition, multilingual grammars</keywords>
		</article>
		<article id="taln-2006-long-007" session="">
			<auteurs>
				<auteur>
					<prenom>Rémi</prenom>
					<nom>Bove</nom>
					<email>remi.bove@up.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christine</prenom>
					<nom>Chardenon</nom>
					<email>christine.chardenon@rd.francetelecom.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean</prenom>
					<nom>Véronis</nom>
					<email>jean.veronis@up.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Provence – Équipe DELIC</affiliation>
				<affiliation affiliationId="2">France Télecom Division Recherche et Développement</affiliation>
			</affiliations>
			<titre>Prise en compte des disfluences dans un système d’analyse syntaxique automatique de l’oral</titre>
			<type>long</type>
			<pages>103-111</pages>
			<resume>Nous présentons dans cette étude un essai de prise en compte des disfluences dans un système d’analyse linguistique initialement prévu pour l’écrit, en vue de la réalisation d’un prototype de traduction parole-parole. À partir d’une étude approfondie sur corpus, nous montrons comment des modifications du lexique et de la grammaire ont permis de traiter les cas les plus simples (pauses remplies, répétitions de mots isolés, etc.). D’autres cas plus complexes comme répétitions et auto-corrections de syntagmes ont nécessité la mise au point d’un mécanisme de contrôle sémantique permettant de limiter la combinatoire. Cette étude a mis également en évidence la difficulté de traitement de phénomènes tels que les amorces (mots interrompus) et les constructions inachevées, qui pour l’instant restent sans solution satisfaisante.</resume>
			<mots_cles>disfluences, analyse syntaxique en dépendances, traitement automatique de l’oral</mots_cles>
			<title></title>
			<abstract>In this paper we describe an attempt to take speech disfluencies into account in a linguistic analysis system initially designed for written data. Using a detailed corpus analysis, we show how the lexicon and grammar can be modified to solve the simplest cases (such as filled pauses, single-word repeats, and so forth). More difficult cases such as phrasal repeats and self-repairs required the development of a semantic control mechanism in order to avoid combinatorial explosion. This study also reveals the difficulty of processing word fragments and aborted constructs, which receive no satisfactory solution in the current state of the art.</abstract>
			<keywords>disfluencies, parsing, automatic speech processing</keywords>
		</article>
		<article id="taln-2006-long-008" session="">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Collin</nom>
					<email>olivier.collin@francetelecom.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Émmanuelle</prenom>
					<nom>Pétrier</nom>
					<email>epe@teamlog.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Télécom R&amp;D – Lannion</affiliation>
				<affiliation affiliationId="2">Teamlog –Lannion</affiliation>
			</affiliations>
			<titre>Acquisition de concepts bilingues à partir du Web</titre>
			<type>long</type>
			<pages>112-120</pages>
			<resume>Nous montrons une utilisation du Web, corpus multilingue de grande taille, pour effectuer une acquisition supervisée de concepts bilingue français/anglais. Cette acquisition utilise comme point initial un verbe français. Nous apparions ensuite des phrases provenant des deux langues à partir de couples de noms propres possédant la même forme dans les deux langues. Cet appariement automatique mais sommaire ne garantit pas l’alignement des phrases. Nous montrons qu’il nous permet cependant d’extraire des termes français et anglais équivalents dans leur contexte d’utilisation. Ces termes constituent des ressources multilingues particulièrement adaptées au Web, notamment pour les applications question réponse « crosslingue ».</resume>
			<mots_cles>acquisition, concepts bilingue, alignement superficiel, Web</mots_cles>
			<title></title>
			<abstract>This article describes a way of using the Web as a huge multilingual corpus to perform supervised acquisition of bilingual French/English concepts. Such acquisition is initialized with a French verb. Sentences expressed in both languages are then matched by using couples of proper nouns which are unchanged in these two languages. Such matching is automatic, highly succinct, but does not prevent sentences from being wrongly aligned. However, we show that it makes it possible to extract French and English terms that are equivalent in their context of use. These terms constitute multilingual resources that are particularly adapted to the Web, especially for cross-lingual question answering.</abstract>
			<keywords>acquisition, bilingual concepts, shallow alignment, Web</keywords>
		</article>
		<article id="taln-2006-long-009" session="">
			<auteurs>
				<auteur>
					<prenom>Cécile</prenom>
					<nom>Fabre</nom>
					<email>cecile.fabre@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Bourigault</nom>
					<email>didier.bourigault@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Toulouse-Le Mirail – ERSS, CNRS</affiliation>
			</affiliations>
			<titre>Extraction de relations sémantiques entre noms et verbes au-delà des liens morphologiques</titre>
			<type>long</type>
			<pages>121-129</pages>
			<resume>Nous étudions les relations de proximité sémantique entre les noms et les verbes à partir de données calculées sur un corpus de 200 millions de mots par un programme d’analyse distributionnelle automatique. Nous exposons les résultats d’une méthode d’extraction de couples Nom/Verbe, qui combine un indice de proximité distributionnelle et un indice de cooccurrence : un couple est extrait si le nom et le verbe apparaissent avec les mêmes arguments sur l’ensemble du corpus, d’une part, et s’ils apparaissent au moins une fois dans un même paragraphe munis du même argument, d’autre part. L’article élabore une typologie des 1441 couples extraits et démontre l’intérêt de prendre en compte les couples non liés morphologiquement, qui constituent 70 % des données.</resume>
			<mots_cles>relations sémantiques, ressources lexicales, analyse distributionnelle</mots_cles>
			<title></title>
			<abstract>In this paper, we study the semantic relations that hold between nouns and verbs. We benefit from the data provided by Upery, a program that automatically extracts word associations from a 200 million words corpus by means of distributional analysis. We present the results of an experiment in which noun-verb associations are extracted by crossing two criteria : distributional proximity and cooccurrence. The 1441 couples share the same arguments in the corpus and appear at least once with the same argument within the same paragraph. We present a typology of these noun-verb couples, showing the necessity to take into account non-mophologically related couples which amounts to 70 % of the data.</abstract>
			<keywords>semantic relations, lexical resources, distributional analysis</keywords>
		</article>
		<article id="taln-2006-long-010" session="">
			<auteurs>
				<auteur>
					<prenom>Atefeh</prenom>
					<nom>Farzindar</nom>
					<email>farzindar@nlptechnologies.ca</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">NLP Technologies Inc.</affiliation>
				<affiliation affiliationId="2">Université de Montréal, RALI</affiliation>
			</affiliations>
			<titre>Résumé multidocuments orienté par une requête complexe</titre>
			<type>long</type>
			<pages>130-138</pages>
			<resume>Nous présentons un système de synthèse d’information pour la production de résumés multidocuments orientés par une requête complexe. Après une analyse du profil de l’utilisateur exprimé par des questions complexes, nous comparons la similarité entre les documents à résumer avec les questions à deux niveaux : global et détaillé. Cette étude démontre l’importance d’étudier pour une requête la pertinence d’une phrase à l’intérieur de la structure thématique du document. Cette méthodologie a été appliquée lors de notre participation à la campagne d’évaluation DUC 2005 où notre système a été classé parmi les meilleurs.</resume>
			<mots_cles>synthèse d’information, résumés multidocuments, évaluation du résumé</mots_cles>
			<title></title>
			<abstract>We present an information synthesis system for the production of multidocument summaries tailored by a complex query.We first analyze the user profile expressed by complex questions and we then compare the similarity between the documents and the questions at two levels : global and detailed. This research shows the importance of the study of the relevance of a sentence in the thematic structure of the document. This methodology has been applied for our participation in the DUC 2005 evaluation conference where our system was judged to be among of the best ones.</abstract>
			<keywords>information synthesis, multi-document summarization, evaluation of the summary</keywords>
		</article>
		<article id="taln-2006-long-011" session="">
			<auteurs>
				<auteur>
					<prenom>Claire</prenom>
					<nom>Gardent</nom>
					<email>claire.gardent@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email>bruno.guillaume@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Perrier</nom>
					<email>guy.perrier@loria.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ingrid</prenom>
					<nom>Falk</nom>
					<email>ingrid.falk@loria.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS, LORIA</affiliation>
				<affiliation affiliationId="2">INRIA, LORIA</affiliation>
				<affiliation affiliationId="3">Université Nancy 2, LORIA</affiliation>
				<affiliation affiliationId="4">CNRS, ATILF</affiliation>
			</affiliations>
			<titre>Extraction d’information de sous-catégorisation à partir des tables du LADL</titre>
			<type>long</type>
			<pages>139-148</pages>
			<resume>Les tables du LADL (Laboratoire d’Automatique Documentaire et Linguistique) contiennent des données électroniques extensives sur les propriétés morphosyntaxiques et syntaxiques des foncteurs syntaxiques du français (verbes, noms, adjectifs). Ces données, dont on sait qu’elles sont nécessaires pour le bon fonctionnement des systèmes de traitement automatique des langues, ne sont cependant que peu utilisées par les systèmes actuels. Dans cet article, nous identifions les raisons de cette lacune et nous proposons une méthode de conversion des tables vers un format mieux approprié au traitement automatique des langues.</resume>
			<mots_cles>lexique-grammaire, M. Gross, sous-catégorisation</mots_cles>
			<title></title>
			<abstract>Maurice Gross’ grammar lexicon contains rich and exhaustive information about the morphosyntactic and syntactic properties of French syntactic functors (verbs, adjectives, nouns). Yet its use within natural language processing systems is hampered both by its non standard encoding and by a structure that is partly implicit and partly underspecified. In this paper, we present a method for translating this information into a format more amenable for use by NLP systems, we discuss the results obtained so far, we compare our approach with related work and we identify the possible further uses that can be made of the reformatted information.</abstract>
			<keywords>Grammar Lexicon, M. Gross, subcategorisation</keywords>
		</article>
		<article id="taln-2006-long-012" session="">
			<auteurs>
				<auteur>
					<prenom>Claire</prenom>
					<nom>Gardent</nom>
					<email>claire.gardent@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS/LORIA</affiliation>
			</affiliations>
			<titre>Intégration d’une dimension sémantique dans les grammaires d’arbres adjoints</titre>
			<type>long</type>
			<pages>149-158</pages>
			<resume>Dans cet article, nous considérons un formalisme linguistique pour lequel l’intégration d’information sémantique dans une grammaire à large couverture n’a pas encore été réalisée à savoir, les grammaires d’arbres adjoints (Tree Adjoining Grammar ou TAG). Nous proposons une méthode permettant cette intégration et décrivons sa mise en oeuvre dans une grammaire noyau pour le français. Nous montrons en particulier que le formalisme de spécification utilisé, XMG, (Duchier et al., 2004) permet une factorisation importante des données sémantiques facilitant ainsi le développement, la maintenance et le déboggage de la grammaire.</resume>
			<mots_cles>grammaire d’arbres adjoints, calcul sémantique</mots_cles>
			<title></title>
			<abstract>In this paper, we consider a linguistic formalism for which the integration of semantic information within a large scale grammar has not yet been realised, namely, Tree Adjoining Grammar (TAG). We propose a method for integrating this information and describe its implementation within a core TAG for French called FRAG. Furthermore, we show that the formalism used to specify the grammar, XMG, (Duchier et al., 2004) allows for a strong factorisation of the semantic data, thus giving better support for the development, maintenance and debugging of the grammar.</abstract>
			<keywords>Tree Adjoining Grammar, Semantic Construction</keywords>
		</article>
		<article id="taln-2006-long-013" session="">
			<auteurs>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Gillard</nom>
					<email>laurent.gillard@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marc</prenom>
					<nom>El-Bèze</nom>
					<email>marc.elbeze@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université d’Avignon et des Pays de Vaucluse Laboratoire d’Informatique d’Avignon (LIA)</affiliation>
			</affiliations>
			<titre>Questions Booléennes : Oui ou Non, des Questions et des Réponses</titre>
			<type>long</type>
			<pages>159-167</pages>
			<resume>Dans cet article, nous présentons une approche afin de traiter les questions booléennes, c’est-à-dire des questions dont la réponse peut être un Oui ou un Non, cela, dans le cadre d’un système de Questions-Réponses. En effet, la campagne Technolangue-EQueR, première campagne francophone de Questions-Réponses (QR) utilisant des questions et un corpus en français, a également été la première campagne QR à introduire une évaluation pour ce type de questions. Nous détaillons, parallèlement à notre approche, des pistes de réflexion sur les aspects sous-jacents à ces questions booléennes, notamment au travers d’une analyse des résultats obtenus par notre système dans un contexte similaire à celui de notre participation à la campagne officielle.</resume>
			<mots_cles>système de questions-réponses, questions booléennes</mots_cles>
			<title></title>
			<abstract>In this paper, we propose a method to answer Yes/No questions from a Questions-Answering System point of view. These questions were called “Boolean question” during the first Technolangue French question answering campaign called EQueR which used a French corpus and questions. It was also the first campaign to introduce an evaluation for this particular kind of question. We discuss some key points related to these Yes/No questions as well as report on our system used in the EQueR evaluation campaign.</abstract>
			<keywords>question answering, yes/no questions, boolean question</keywords>
		</article>
		<article id="taln-2006-long-014" session="">
			<auteurs>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Tribout</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Georgette</prenom>
					<nom>Dal</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bernard</prenom>
					<nom>Fradin</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nabil</prenom>
					<nom>Hathout</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphanie</prenom>
					<nom>Lignon</nom>
					<email></email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fiammetta</prenom>
					<nom>Namer</nom>
					<email></email>
					<affiliationId>6</affiliationId>
				</auteur>
				<auteur>
					<prenom>Clément</prenom>
					<nom>Plancq</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email></email>
					<affiliationId>7</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>8</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Descartes, Faculté de Médecine Inserm, U729, SPIM</affiliation>
				<affiliation affiliationId="2">Université Paris 7, CNRS, UMR 7110 LLF</affiliation>
				<affiliation affiliationId="3">Universités Lille 3 et Lille 1, CNRS, UMR 8163 STL</affiliation>
				<affiliation affiliationId="4">CNRS &amp; Université Toulouse 2, UMR 5610 ERSS</affiliation>
				<affiliation affiliationId="5">Université de Haute-Alsace</affiliation>
				<affiliation affiliationId="6">Université Nancy 2, CNRS, ATILF</affiliation>
				<affiliation affiliationId="7">GET/ENST, CNRS/LTCI</affiliation>
				<affiliation affiliationId="8">INaLCO/Paris, DSI, AP-HP</affiliation>
			</affiliations>
			<titre>Productivité quantitative des suffixations par -ité et -Able dans un corpus journalistique moderne</titre>
			<type>long</type>
			<pages>167-177</pages>
			<resume>Dans ce travail, nous étudions en corpus la productivité quantitative des suffixations par -Able et par -ité du français, d’abord indépendamment l’une de l’autre, puis lorsqu’elles s’enchaînent dérivationnellement (la suffixation en -ité s’applique à des bases en -Able dans environ 15 % des cas). Nous estimons la productivité de ces suffixations au moyen de mesures statistiques dont nous suivons l’évolution par rapport à la taille du corpus. Ces deux suffixations sont productives en français moderne : elles forment de nouveaux lexèmes tout au long des corpus étudiés sans qu’on n’observe de saturation, leurs indices de productivité montrent une évolution stable bien qu’étant dépendante des calculs qui leur sont appliqués. On note cependant que, de façon générale, de ces deux suffixations, c’est la suffixation par -ité qui est la plus fréquente en corpus journalistique, sauf précisément quand -ité s’applique à un adjectif en -Able. Étant entendu qu’un adjectif en -Able et le nom en -ité correspondant expriment la même propriété, ce résultat indique que la complexité de la base est un paramètre à prendre en considération dans la formation du lexique possible.</resume>
			<mots_cles>morphologie, corpus journalistique, -Able, -ité, productivité morphologique quantitative</mots_cles>
			<title></title>
			<abstract>In this paper, we study the quantitative productivity of French suffixes -Able and -ité in corpora. We first analyze them independently and then when they belong to the same derivational chain (the suffix -ité chooses bases with -Able in about 15 % of its formations). The productivity of these suffixations is statisticallly estimated in relation to the corpus size. Both these affixes are productive in modern French : they continue to form new lexemes throughout the corpora and are not saturated, their productivity indexes show a stable evolution although this is dependent on applied calculations. Nevertheless, -ité is more frequent in the journalistic corpora except when it is applied to adjectives suffixed with -Able. Knowing that both -Able adjective and its -ité noun convey the same property, this finding indicates that the base complexity has to be taken into account in the formation of possible lexicon.</abstract>
			<keywords>morphology, newspaper corpus, -Able, -ité, quantitative morphological productivity</keywords>
		</article>
		<article id="taln-2006-long-015" session="">
			<auteurs>
				<auteur>
					<prenom>Marie-Laure</prenom>
					<nom>Guénot</nom>
					<email>mlg@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Provence &amp; CNRS, Laboratoire Parole et Langage</affiliation>
			</affiliations>
			<titre>La coordination considérée comme un entassement paradigmatique : description, représentation et intégration</titre>
			<type>long</type>
			<pages>178-187</pages>
			<resume>Nous proposons de traiter la coordination comme un entassement paradigmatique, établissant une relation de parataxe entre ses constituants. Par cette considération et ses implications sur la description et l’analyse, on s’éloigne des assomptions les plus fréquentes en linguistique formelle sur le traitement de la coordination. Nous introduisons une description des caractéristiques syntaxiques de cette proposition, ainsi que sa représentation formelle et son intégration au sein d’une grammaire du français qui a pour objet d’être utilisée en traitement automatique. Cette description strictement syntaxique a vocation à être complétée par des informations provenant d’autres domaines, ce qui nous permet d’illustrer quelques spécificités notables de notre modèle.</resume>
			<mots_cles>syntaxe, coordination, développement de grammaire, grammaires de propriétés (GP), grammaires de construction (CxG)</mots_cles>
			<title></title>
			<abstract>We propose to treat coordination phenomena as syntagmatic accumulations, establishing a parataxis relation among their constituents. This consideration and its implications on description and analysis represent a departure from orthodox assumptions of formal linguistics about this question. We propose a description of the syntactic characteristics of coordination, then its formal representation and its integration into a grammar for french which is used for natural language processing. This strictly syntactic description is meant to be completed by information coming from other linguistic domains, which allows us to illustrate some notable specificities of our model.</abstract>
			<keywords>syntax, coordination, grammar development, property grammars (PG), construction grammars (CxG)</keywords>
		</article>
		<article id="taln-2006-long-016" session="">
			<auteurs>
				<auteur>
					<prenom>Christine</prenom>
					<nom>Jacquin</nom>
					<email>christine.jacquin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laura</prenom>
					<nom>Monceaux</nom>
					<email>laura.monceaux@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Desmontils</nom>
					<email>emmanuel.desmontils@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, Laboratoire Informatique Nantes Atlantique</affiliation>
			</affiliations>
			<titre>Systèmes question-réponse et EuroWordNet</titre>
			<type>long</type>
			<pages>188-197</pages>
			<resume>Pour améliorer l’efficacité des systèmes de recherche d’informations précises, l’utilisation de connaissances sémantiques est nécessaire. Cependant pour le français, les outils de connaissances sémantiques telles les thesaurus sur domaine ouvert ne sont d’une part pas très nombreux et d’autre part pas suffisamment complets. Dans cet article, nous expliquons premièrement, l’intérêt de l’utilisation de connaissances sémantiques pour un système de question réponse. Puis, nous présentons le thesaurus EuroWordNet, notamment ses limites et les améliorations que nous avons effectuées pour la base française dans un souci de le rendre plus satisfaisant pour notre application par l’ajout de relations inexistantes entre concepts et de définitions par le biais de l’encyclopédieWikipedia (2006).</resume>
			<mots_cles>thesaurus, système de question-réponse, similarité</mots_cles>
			<title></title>
			<abstract>In order to improve question answering systems, the use of semantic knowledge is essential. However for French, such knowledge is not easily available. Indeed, French thesauruses are scarse and under-developed. In this paper, we firstly explain the reason why semantic knowledge should be used in a question answering system. Then, we present the EuroWordnet thesaurus, focussing on its limitations and the improvements we have made. We add undefined relationships with the help of the English base by using Wikipedia (2006).</abstract>
			<keywords>thesaurus, question answering system, similarity</keywords>
		</article>
		<article id="taln-2006-long-017" session="">
			<auteurs>
				<auteur>
					<prenom>Tita</prenom>
					<nom>Kyriacopoulou</nom>
					<email>tita@frl.auth.gr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>Martineau</nom>
					<email>claude.martineau@univ-mlv.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anastasia</prenom>
					<nom>Yannacopoulou</nom>
					<email>anastasia.annacopoulou@univ-mlv.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Aristote de Thessaloniki, Faculté des Lettres Laboratoire de traduction et de traitement des langues</affiliation>
				<affiliation affiliationId="2">Université de Marne-la-Vallée Laboratoire d’Informatique linguistique de l’Institut Gaspard-Monge</affiliation>
			</affiliations>
			<titre>Reconnaissance automatique de formes dérivées dans les textes grecs</titre>
			<type>long</type>
			<pages>198-206</pages>
			<resume>Notre objectif est la reconnaissance automatique de certaines formes dérivées, i.e. des diminutifs et des augmentatifs des noms et des adjectifs simples, ainsi que des comparatifs et des superlatifs des adjectifs simples du grec moderne. Il s’agit de formes qui sont généralement produites par l’adjonction d’un suffixe à la forme standard correspondante. Nous justifions notre choix de les ajouter dans le dictionnaire électronique. Leur traitement a nécessité une nouvelle représentation du dictionnaire qui utilise désormais un système de règles permettant de générer aisément les formes fléchies dérivées, de les étiqueter en tant que telles, et de les mettre en relation avec leur forme de base. Il en résulte une meilleure structuration des ressources lexicales et une production de dictionnaires flexible.</resume>
			<mots_cles>dictionnaire électronique, flexion, formes dérivées</mots_cles>
			<title></title>
			<abstract>Our study concerns the automatic recognition of certain derived forms, i.e. the diminutives and the augmentatives of simple nouns and adjectives, as well as the comparatives and the superlatives of simple adjectives of Modern Greek. These forms are generally produced by the adjunction of a suffix to the corresponding standard form. We justify our choice to add them to the electronic dictionary of Modern Greek. Their processing implies an evolution of the representation of the dictionary which now uses a system of rules in order to easily generate the derived inflected forms, to label them as such, and to clearly put them in relation to their base form. We achieve better structuring of the lexical resources and flexible modular dictionary production.</abstract>
			<keywords>electronic dictionary, inflection, derived forms</keywords>
		</article>
		<article id="taln-2006-long-018" session="">
			<auteurs>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Landragin</nom>
					<email>frederic.landragin@thalesgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Thales Research &amp; Technology, Palaiseau</affiliation>
			</affiliations>
			<titre>Influence de la situation lors de la résolution des anaphores dans le dialogue</titre>
			<type>long</type>
			<pages>207-216</pages>
			<resume>La résolution des anaphores dans les systèmes de dialogue homme-machine s’inspire généralement des modèles et des algorithmes développés pour le texte. Or le dialogue met en jeu une situation, c’est-à-dire un environnement physique immédiat et des événements dont la perception est partagée par les interlocuteurs. Cette situation peut servir d’ancrage à des expressions référentielles dites « anaphores à antécédents non linguistiques ». L’attribution de référents à de telles expressions s’avère difficile pour deux raisons : premièrement les facteurs situationnels sont nombreux et peu explicites ; deuxièmement des ambiguïtés peuvent apparaître entre de possibles antécédents situationnels et de possibles antécédents linguistiques. Nous proposons ici un modèle clarifiant l’intervention des facteurs situationnels et permettant leur prise en compte lors de la compréhension des expressions référentielles potentiellement anaphoriques. En intégrant la notion de saillance valable à la fois pour les aspects situationnels et linguistiques, nous montrons comment utiliser des scores numériques pour gérer les interférences entre hypothèses situationnelles et linguistiques.</resume>
			<mots_cles>co-référence, anaphore, contexte, perception visuelle, saillance</mots_cles>
			<title></title>
			<abstract>Anaphora resolution in human-machine dialogue systems is often based on models and algorithms that were designed for text. But dialogue happens in a situation, i.e., an immediate physical environment and events that are perceived simultaneously by the participants. This situation can be at the origin of referential expressions called “anaphora without linguistic antecedents”. Attributing some referents to such expressions is difficult for two reasons. First, because situational factors are numerous and implicit. Second, because ambiguities can appear between a potential situational antecedent and a potential linguistic antecedent. In this paper we propose a model that clarifies how situational factors work and how they can be taken into account when interpreting anaphoric expressions. By integrating the notion of salience that is common to both situational and linguistic aspects, we show with numeric scores how the interference between the situation and the language can be solved.</abstract>
			<keywords>coreference, anaphora, context, visual perception, salience</keywords>
		</article>
		<article id="taln-2006-long-019" session="">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrizio</prenom>
					<nom>Gotti</nom>
					<email>gottif@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Patry</nom>
					<email>patryale@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Montréal, RALI/DIRO</affiliation>
			</affiliations>
			<titre>De la Chambre des communes à la chambre d’isolement : adaptabilité d’un système de traduction basé sur les segments de phrases</titre>
			<type>long</type>
			<pages>217-226</pages>
			<resume>Nous présentons notre participation à la deuxième campagne d’évaluation de CESTA, un projet EVALDA de l’action Technolangue. Le but de cette campagne consistait à tester l’aptitude des systèmes de traduction à s’adapter rapidement à une tâche spécifique. Nous analysons la fragilité d’un système de traduction probabiliste entraîné sur un corpus hors-domaine et dressons la liste des expériences que nous avons réalisées pour adapter notre système au domaine médical.</resume>
			<mots_cles>traduction probabiliste, adaptabilité, aspiration de bitextes, mémoire de traduction</mots_cles>
			<title></title>
			<abstract>We present our participation in the second evaluation campaign of CESTA, an EVALDA project within the framework of Technolangue. The goal of this task consisted in testing the adaptability of translation systems.We analyze the inadequacy of a statistical phrase-based system trained on legislative texts to translate medical corpora. We describe the experiments we conducted in order to adapt our engine to the new task.</abstract>
			<keywords>statistical translation, adaptation, Web crawling, memory-based translation</keywords>
		</article>
		<article id="taln-2006-long-020" session="">
			<auteurs>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Ligozat</nom>
					<email>anne-laure.ligozat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>brigitte.grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Robba</nom>
					<email>isabelle.robba@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>anne.vinat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
			</affiliations>
			<titre>L’extraction des réponses dans un système de question-réponse</titre>
			<type>long</type>
			<pages>227-236</pages>
			<resume>Les systèmes de question-réponse sont la plupart du temps composés de trois grands modules : l’analyse de la question, la sélection des documents et l’extraction de la réponse. Dans cet article, nous nous intéressons au troisième module, plus particulièrement dans le cas plus délicat où la réponse attendue n’est pas du type entitée nommée. Nous décrivons comment l’analyseur Cass est employé pour marquer la réponse dans les phrases candidates et nous évaluons les résultats de cette approche. Au préalable, nous décrivons et évaluons le module dédié à l’analyse de la question, car les informations qui en sont issues sont nécessaires à notre étape finale d’extraction.</resume>
			<mots_cles>systèmes de question-réponse</mots_cles>
			<title></title>
			<abstract>Question-answering systems are usually composed of three main modules, namely, question analysis, document selection and answer extraction. In this paper, we focus on the third module, more specifically when the expected answer is not a named entity because this kind of answer is more difficult to extract. We describe how the parser Cass is employed to tag the answer in the candidate sentence. Then, we evaluate the results of this approach. Beforehand, we describe and evaluate the question analysis module, as the information it produces is used during our final answer extraction step.</abstract>
			<keywords>question answering systems</keywords>
		</article>
		<article id="taln-2006-long-021" session="">
			<auteurs>
				<auteur>
					<prenom>Denis</prenom>
					<nom>Maurel</nom>
					<email>denis.maurel@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jan</prenom>
					<nom>Daciuk</nom>
					<email>jandac@eti.pg.gda.pl</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François-Rabelais de Tours – Laboratoire d’Informatique</affiliation>
				<affiliation affiliationId="2">Université Polytechnique de Gdańsk</affiliation>
			</affiliations>
			<titre>Les transducteurs à sorties variables</titre>
			<type>long</type>
			<pages>237-245</pages>
			<resume>Dans le traitement automatique du langage naturel, les dictionnaires électroniques associent à chaque mot de l’information. La représentation informatique la plus efficace de ces dictionnaires utilise des machines à nombre fini d’états (automates ou transducteurs). Dans cet article, nous nous inspirons des algorithmes de construction directe d’un automate déterministe minimal pour proposer une nouvelle forme de transducteur. Cette nouvelle forme permet un calcul rapide des sorties associées aux mots, tout en étant plus compacte quant au nombre de transitions et de sorties distinctes, comme le montrent nos expérimentations.</resume>
			<mots_cles>automates à nombre fini d’états, transducteurs, dictionnaires électroniques</mots_cles>
			<title></title>
			<abstract>In natural language processing, dictionaries usually associate additional information with lexical entries. The most effective representation of dictionaries makes use of finite-state machines – either automata (recognizers) or transducers. In this paper, we draw our inspiration from algorithms to directly build the minimal deterministic automaton and we propose a new form of a transducer. This new form outperforms existing transducers in terms of speed while computing outputs and in terms of size calculated on the basis of the number of transitions and different outputs, as shown in our experiments.</abstract>
			<keywords>finite state automata, transducers, electronic dictionaries</keywords>
		</article>
		<article id="taln-2006-long-022" session="">
			<auteurs>
				<auteur>
					<prenom>Farid</prenom>
					<nom>Nouioua</nom>
					<email>nouiouaf@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Daniel</prenom>
					<nom>Kayser</nom>
					<email>dk@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris-Nord – LIPN</affiliation>
			</affiliations>
			<titre>Une expérience de sémantique inférentielle</titre>
			<type>long</type>
			<pages>246-255</pages>
			<resume>Nous développons un système qui doit être capable d’effectuer les mêmes inférences que le lecteur humain d’un constat d’accident de la route, et plus particulièrement de déterminer les causes apparentes de l’accident. Nous décrivons les niveaux linguistiques et sémantiques de l’analyse, et les règles d’inférence utilisées par ce système.</resume>
			<mots_cles>sémantique inférentielle, raisonnement non monotone, causalité</mots_cles>
			<title></title>
			<abstract>We are developing a system that aims to perform the same inferences as a human reader, on car-crash reports. More precisely, we expect it to determine the causes of the accident as they appear from the text. We describe the linguistic and semantic levels of analysis, and the inference rules used by the system.</abstract>
			<keywords>inferential semantics, non monotonic reasoning, causation</keywords>
		</article>
		<article id="taln-2006-long-023" session="">
			<auteurs>
				<auteur>
					<prenom>Andrei</prenom>
					<nom>Popescu-Belis</nom>
					<email>andrei.popescu-belis@issco.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève, ISSCO/TIM/ETI</affiliation>
			</affiliations>
			<titre>Résolution des références aux documents dans un corpus de dialogues humains</titre>
			<type>long</type>
			<pages>256-265</pages>
			<resume>Cet article étudie la résolution des références à des entités lorsqu’une représentation informatique de ces entités est disponible. Nous nous intéressons à un corpus de dialogues entre humains, portant sur les grands titres de la presse francophone du jour, et proposons une méthode pour détecter et résoudre les références faites par les locuteurs aux articles des journaux. La détection des expressions nominales qui réfèrent à ces documents est réalisée grâce à une grammaire, alors que le problème de la détection des pronoms qui réfèrent aux documents est abordé par des moyens statistiques. La résolution de ces expressions, à savoir l’attribution des référents, fait quant à elle l’objet d’un algorithme inspiré de la résolution des coréférences. Ces propositions sont évaluées par le biais de mesures quantitatives spécifiques.</resume>
			<mots_cles>résolution des références, dialogue humain, évaluation quantitative</mots_cles>
			<title></title>
			<abstract>This article studies the resolution of references to entities in cases when a computational representation of the entities is available. Our data is a corpus of human dialogues about the front pages of one or more daily francophone newspapers. The main goal is to propose a method for the detection and resolution of references made by speakers to newspaper articles. A grammar is used to detect the nominal expressions referring to documents, while statistical methods are used to detect the pronouns referring to documents. An algorithm inspired from coreference resolution is used to solve all these expressions, i.e. to find the documents and articles they refer to. Specific quantitative metrics are applied to evaluate these proposals.</abstract>
			<keywords>reference resolution, human dialogue, quantitative evaluation</keywords>
		</article>
		<article id="taln-2006-long-024" session="">
			<auteurs>
				<auteur>
					<prenom>Mathias</prenom>
					<nom>Rossignol</nom>
					<email>mathias.rossignol@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascale</prenom>
					<nom>Sébillot</nom>
					<email>pascale.sebilot@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de Recherche International MICA UMI-2954 CNRS, HUT, INP Grenoble</affiliation>
				<affiliation affiliationId="2">IRISA</affiliation>
			</affiliations>
			<titre>Mise au jour semi-automatique de nuances sémantiques entre mots de sens proches</titre>
			<type>long</type>
			<pages>266-275</pages>
			<resume>L’acquisition automatique sur corpus d’informations lexicales sémantiques donne une place importante à la constitution de classes sémantiques rassemblant des mots de sens proches. Or, l’intérêt pratique de celles-ci reste limité en l’absence d’information sur les distinctions individualisant les sens des mots qu’elles rassemblent. Nous présentons dans cet article un premier système permettant de mettre au jour, de manière semi-automatique et à partir des seules données textuelles rassemblées dans un corpus, des éléments de distinction sémantique fine entre mots appartenant à une même classe, atteignant ainsi un degré de définition du sens encore inédit en acquisition automatique d’informations sémantiques lexicales. La technique mise au point regroupe, en s’appuyant sur l’étude de grands voisinages autour des occurrences des mots comparés, des paires de mots distingués par des nuances similaires. Cette approche présente la faiblesse de ne permettre qu’une représentation implicite des nuances découvertes : les listes de paires de mots rapprochées doivent être interprétées afin de « comprendre » l’élément de distinction commun. En revanche, elle permet une automatisation importante du processus de recherche de nuances, suffisante pour assurer que le travail humain de validation des résultats n’introduise dans ceux-ci de biais interprétatif trop important.</resume>
			<mots_cles>classes sémantiques, nuances de sens, acquisition sur corpus</mots_cles>
			<title></title>
			<abstract>The corpus-based acquisition of lexical semantic information has given rise to numerous studies on the automatic constitution of semantic classes, clustering words with similar meanings. However, the practical interest of these classes remains limited in the absence of knowledge about the nuances of meaning differentiating the words of a same class. We present a first system to make explicit such semantic nuances, in a semi-automatic way, using data from a text corpus, thus reaching a degree of word meaning definition, to our knowledge, never attained before by automatic means, This technique exploits large contexts around word occurrences to bring together pairs of words characterised by a similar meaning nuance. The limitation of this approach is that it only provides an implicit representation of the discovered distinctions : human interpretation is still required to name them. However, it enables an important level of automation, so that the human validation work can only introduce a limited bias in the results.</abstract>
			<keywords>semantic classes, nuances, corpus-based acquisition</keywords>
		</article>
		<article id="taln-2006-long-025" session="">
			<auteurs>
				<auteur>
					<prenom>Jean</prenom>
					<nom>Royauté</nom>
					<email>royaute@lidil.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Élisabeth</prenom>
					<nom>Godbert</nom>
					<email>godbert@lidil.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed Madhi</prenom>
					<nom>Malik</nom>
					<email>malik@lidil.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de la Méditerranée – LIF et CNRS</affiliation>
			</affiliations>
			<titre>Groupes Nominaux Prédicatifs : utilisation d’une grammaire de liens pour l’extraction d’information</titre>
			<type>long</type>
			<pages>276-286</pages>
			<resume>L’identification des structures prédicatives présente un grand intérêt quand on se situe dans une problématique d’extraction d’information. Si une littérature abondante existe à ce sujet, particulièrement dans le domaine de la génomique, la plupart des travaux portent sur les relations autour du verbe. Peu s’intéressent à la relation qui peut unir une nominalisation et ses actants dans un groupe nominal à tête prédicative (GNP). Nous montrons la complexité des différents types de GNP et des relations paraphrastiques qui les unissent avec les formes verbales, afin de donner une vue unifiée des structures prédicatives nomino-verbales. Nous montrons ensuite comment nous avons conçu une grammaire de liens permettant l’identification de chacun des actants dans les GNP. Nous en décrivons la mise en oeuvre avec le Link Parser, pour l’extraction d’information dans des articles scientifiques du domaine de la Biologie.</resume>
			<mots_cles>nominalisation, groupe nominal prédicatif, marqueurs prépositionnels, extraction d’information</mots_cles>
			<title></title>
			<abstract>The identification of predicative structures is of great interest in information extraction. Although there is abundant literature on this subject, particularly in the genomic field, the majority relates to the relations around the verb. Few are interested in the relation which can link a nominalization and its actants in a noun phrase with predicative head (NPP). Our work involves firstly showing the complexity of different types of NPPs and the paraphrastic relations which link them with the verbal forms, followed by a unified view of the nomino-verbal predicative structures. We further show how we designed a link grammar allowing the identification of each actant in the NNPs. We describe the implementation with Link Parser for information extraction in scientific articles in the field of Biology.</abstract>
			<keywords>nominalization, predicative noun phrase, prepositional markers, information extraction</keywords>
		</article>
		<article id="taln-2006-long-026" session="">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Villemonte De La Clergerie</nom>
					<email>eric.de_la_clergerie@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Projet ATOLL, INRIA</affiliation>
			</affiliations>
			<titre>Trouver le coupable : Fouille d’erreurs sur des sorties d’analyseurs syntaxiques</titre>
			<type>long</type>
			<pages>287-296</pages>
			<resume>Nous présentons une méthode de fouille d’erreurs pour détecter automatiquement des erreurs dans les ressources utilisées par les systèmes d’analyse syntaxique. Nous avons mis en oeuvre cette méthode sur le résultat de l’analyse de plusieurs millions de mots par deux systèmes d’analyse différents qui ont toutefois en commun le lexique syntaxique et la chaîne de traitement pré-syntaxique. Nous avons pu identifier ainsi des inexactitudes et des incomplétudes dans les ressources utilisées. En particulier, la comparaison des résultats obtenus sur les sorties des deux analyseurs sur un même corpus nous a permis d’isoler les problèmes issus des ressources partagées de ceux issus des grammaires.</resume>
			<mots_cles>analyse syntaxique, fouille d’erreurs</mots_cles>
			<title></title>
			<abstract>We introduce an error mining technique for automatically detecting errors in resources used in parsing systems.We applied this technique on parsing results produced on several millions of words by two distinct parsing systems, which share a common syntactic lexicon and pre-parsing processing chain.We were thus able to identify errors and missing elements in the resources. In particular, by comparing both systems’ results, we were able to differentiate between problems stemming from shared resources and those resulting from grammars.</abstract>
			<keywords>parsing, error mining</keywords>
		</article>
		<article id="taln-2006-long-027" session="">
			<auteurs>
				<auteur>
					<prenom>Susanne</prenom>
					<nom>Salmon-Alt</nom>
					<email>salt@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF-CNRS, Nancy</affiliation>
			</affiliations>
			<titre>V1Ω a=able ou Normaliser des lexiques syntaxiques est délectable</titre>
			<type>long</type>
			<pages>297-306</pages>
			<resume>Partant des lexiques TAL syntaxiques existants, cet article propose une représentation lexicale unifiée et normalisée, préalable et nécessaire à toute exploitation des lexiques syntaxiques hors de leur propre contexte de conception. Ce travail s’inscrit dans un cadre de modélisation privilégié − le Lexical Markup Framework − qui a été conçu dès le départ comme un modèle lexicographique intégrant les différents niveaux de description. Ce modèle permet d’articuler des descriptions extensionnelles et intensionnelles et fait référence à un jeu de descripteurs normalisés, garantissant la rigueur de la description des faits linguistiques et assurant, à terme, la compatibilité avec des formats de données utilisés pour l’annotation de corpus.</resume>
			<mots_cles>lexique, TAL, syntaxe, lexique-grammaire, sous-catégorisation, standardisation</mots_cles>
			<title></title>
			<abstract>Based on existing lexical resources for NLP, in particular inflected form and subcategorization lexica, this paper proposes a unified and normalized representation, required for any further use of the data out of their original context. As a starting point for our model, we chose the Lexical Markup Framework, for three reasons. Firstly, it covers various layers of linguistic description including morphology, syntax and semantics. Secondly, it allows for combining extensional (i.e. lists of forms or constructions) and intensional (i.e. reference to paradigms) lexical descriptions. Thirdly, it makes use of externally defined data categories, ensuring linguistic soundness and, ultimately, compatibility with standardized corpus annotation formats.</abstract>
			<keywords>lexicon, NLP, syntax, lexicon-grammar, normalization, subcategorization</keywords>
		</article>
		<article id="taln-2006-long-028" session="">
			<auteurs>
				<auteur>
					<prenom>Marina</prenom>
					<nom>Santini</nom>
					<email>marina.santini@itri.brighton.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Brighton</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>307-316</pages>
			<resume>Dans cet article nous présentons un modèle déductif-inductif pour l’identification des typologies textuelles et des genres dans les pages Web. Dans ce modèle, les typologies textuelles sont déduites en utilisant une forme modifiée du théorème de Bayes, tandis que les genres sont dérivés au moyen de simples règles « si-alors ». Étant donné que le système des genres sur le Web est complexe et que les pages Web sont plus imprévisibles et individualisées que les documents traditionnels, nous proposons cette approche déductive-inductive comme une alternative aux méthodes statistiques supervisées et non-supervisées. En effet, le modèle déductif-inductif permet une classification qui peut s’accommoder des genres non complètement standardisés. Il est aussi plus respectueux à l’égard de la vraie nature de la page Web, qui est en fait mixte et ne correspond presque jamais à un type idéal ou à un prototype précis, mais présente plutôt un mélange de genres, ou pas de genre du tout. L’évaluation de ce modèle reste un problème à résoudre.</resume>
			<mots_cles>genre, typologies textuelles, pages Web, modèle déductif-inductif, identification automatique, théorème de Bayes</mots_cles>
			<title>Identifying Genres of Web Pages</title>
			<abstract>In this paper, we present an inferential model for text type and genre identification of Web pages, where text types are inferred using a modified form of Bayes’ theorem, and genres are derived using a few simple if-then rules. As the genre system on the Web is a complex phenomenon, and Web pages are usually more unpredictable and individualized than paper documents, we propose this approach as an alternative to unsupervised and supervised techniques. The inferential model allows a classification that can accommodate genres that are not entirely standardized, and is more capable of reading a Web page, which is mixed, rarely corresponding to an ideal type and often showing a mixture of genres or no genre at all. A proper evaluation of such a model remains an open issue.</abstract>
			<keywords>genre, text types, Web pages, inferential model, automatic identification, Bayes’ theorem</keywords>
		</article>
		<article id="taln-2006-long-029" session="">
			<auteurs>
				<auteur>
					<prenom>Nasredine</prenom>
					<nom>Semmar</nom>
					<email>nasredine.semmar@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Meriama</prenom>
					<nom>Laib</nom>
					<email>meriama.laib@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Fluhr</nom>
					<email>christian.fluhr@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA – Laboratoire d’Ingénierie de la Connaissance Multimédia Multilingue</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>317-326</pages>
			<resume>La recherche d’information consiste à trouver les documents pertinents parmi un ensemble de documents en réponse à une requête de l’utilisateur. Ces documents sont triés par ordre de pertinence. Le but du traitement automatique du langage naturel dans la recherche d’information est de transformer les mots potentiellement ambigus de la requête et des documents en représentations internes non ambiguës sur lesquelles s’effectuera l’appariement. Cette transformation est généralement réalisée à l’aide de plusieurs niveaux d’analyse linguistique (morphologique, syntaxique, etc.). Cet article présente l’analyseur linguistique de l’arabe du moteur de recherche crosslingue du LIC2M. Nous allons nous concentrer sur l’analyseur morphologique et plus particulièrement sur le module de segmentation qui permet de découper les mots agglutinés en proclitiques, formes simples et enclitiques. Nous allons démontrer qu’une bonne segmentation améliore la précision et le rappel du moteur de recherche.</resume>
			<mots_cles>analyse morphologique, désambiguïseur morpho-syntaxique, analyse syntaxique, entités nommées, découpage, recherche d’information crosslingue</mots_cles>
			<title>Using Stemming in Morphological Analysis to Improve Arabic Information Retrieval</title>
			<abstract>Information retrieval (IR) consists in finding all relevant documents for a user query in a collection of documents. These documents are ordered by the probability of being relevant to the user’s query. The highest ranked document is considered to be the most likely relevant document. Natural Language Processing (NLP) for IR aims to transform the potentially ambiguous words of queries and documents into unambiguous internal representations on which matching and retrieval can take place. This transformation is generally achieved by several levels of linguistic analysis, morphological, syntactic and so forth. In this paper, we present the Arabic linguistic analyzer used in the LIC2M cross-lingual search engine. We focus on the morphological analyzer and particularly the clitic stemmer which segments the input words into proclitics, simple forms and enclitics. We demonstrate that stemming improves search engine recall and precision.</abstract>
			<keywords>morphological analysis, part-of-speech tagging, syntactic analysis, named entities, stemming, crosslingual information retrieval</keywords>
		</article>
		<article id="taln-2006-long-030" session="">
			<auteurs>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Servan</nom>
					<email>christophe.servan@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Béchet</nom>
					<email>frederic.bechet@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université d’Avignon, LIA</affiliation>
			</affiliations>
			<titre>Décodage conceptuel et apprentissage automatique : application au corpus de dialogue Homme-Machine MEDIA</titre>
			<type>long</type>
			<pages>327-336</pages>
			<resume>Cette étude présente les travaux du LIA effectués sur le corpus de dialogue homme-machine MEDIA et visant à proposer des méthodes d’analyse robuste permettant d’extraire d’un message audio une séquence de concepts élémentaires. Le modèle de décodage conceptuel présenté est basé sur une approche stochastique qui intègre directement le processus de compréhension au processus de Reconnaissance Automatique de la Parole (RAP). Cette approche permet de garder l’espace probabiliste des phrases produit en sortie du module de RAP et de le projeter vers un espace probabiliste de séquences de concepts. Les expériences menées sur le corpus MEDIA montrent que les performances atteintes par notre modèle sont au niveau des meilleurs systèmes ayant participé à l’évaluation sur des transcriptions manuelles de dialogues. En détaillant les performances du système en fonction de la taille du corpus d’apprentissage on peut mesurer le nombre minimal ainsi que le nombre optimal de dialogues nécessaires à l’apprentissage des modèles. Enfin nous montrons comment des connaissances a priori peuvent être intégrées dans nos modèles afin d’augmenter significativement leur couverture en diminuant, à performance égale, l’effort de constitution et d’annotation du corpus d’apprentissage.</resume>
			<mots_cles>dialogue homme-machine, reconnaissance automatique de la parole, apprentissage automatique à base de corpus</mots_cles>
			<title></title>
			<abstract>Within the framework of the French evaluation program MEDIA on spoken dialogue systems, this paper presents the methods developed at the LIA lab for the robust extraction of basic conceptual constituents or concepts from an audio message. The conceptual decoding model proposed follows a stochastic paradigm and is directly integrated into the Automatic Speech Recognition (ASR) process. This approach allows us to both keep the probabilistic search space on sequences of words produced by the ASR module and project it to a probabilistic search space of sequences of concepts. The experiments carried on on the MEDIA corpus show that the performance reached by our approach is state of the art on manual transcriptions of dialogues. By partitioning the training corpus according to different sizes, one can measure the impact of the training corpus on the decoding performance, therefore estimate both the minimal and optimal number of dialogue examples required. Finally we detail how a priori knowledge can be integrated in our models in order to increase their coverage and therefore lowering, for the same level of performance, the amount of training corpus required.</abstract>
			<keywords>spoken dialogue, automatic speech recognition, corpus-based methods</keywords>
		</article>
		<article id="taln-2006-long-031" session="">
			<auteurs>
				<auteur>
					<prenom>Laurianne</prenom>
					<nom>Sitbon</nom>
					<email>laurianne.sitbon@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jens</prenom>
					<nom>Grivolla</nom>
					<email>jens.grivolla@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Gillard</nom>
					<email>laurent.gillard@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique d’Avignon</affiliation>
				<affiliation affiliationId="2">Université de Provence – LPL-CNRS</affiliation>
			</affiliations>
			<titre>Vers une prédiction automatique de la difficulté d’une question en langue naturelle</titre>
			<type>long</type>
			<pages>337-346</pages>
			<resume>Nous proposons et testons deux méthodes de prédiction de la capacité d’un système à répondre à une question factuelle. Une telle prédiciton permet de déterminer si l’on doit initier un dialogue afin de préciser ou de reformuler la question posée par l’utilisateur. La première approche que nous proposons est une adaptation d’une méthode de prédiction dans le domaine de la recherche documentaire, basée soit sur des machines à vecteurs supports (SVM) soit sur des arbres de décision, avec des critères tels que le contenu des questions ou des documents, et des mesures de cohésion entre les documents ou passages de documents d’où sont extraits les réponses. L’autre approche vise à utiliser le type de réponse attendue pour décider de la capacité du système à répondre. Les deux approches ont été testées sur les données de la campagne Technolangue EQUER des systèmes de questions-réponses en français. L’approche à base de SVM est celle qui obtient les meilleurs résultats. Elle permet de distinguer au mieux les questions faciles, celles auxquelles notre système apporte une bonne réponse, des questions difficiles, celles restées sans réponses ou auxquelles le système a répondu de manière incorrecte. A l’opposé on montre que pour notre système, le type de réponse attendue (personnes, quantités, lieux...) n’est pas un facteur déterminant pour la difficulté d’une question.</resume>
			<mots_cles>questions-réponses, prédiction de la difficulté, SVM, arbres de décision</mots_cles>
			<title></title>
			<abstract>This paper presents two methods for automatically predicting the ability for a question answering system to automatically reply to a factoid question. The context of this prediction is the determination of the need to initiate a dialog with the user in order to focus or reformulate the question. The first method is an adaptation of a document retrieval prediction system based on SVM and decision trees. The features involved include question or document text, and cohesion measures between documents or extracts from which the answer is extracted. The second method uses only expected answer type to predict the answer validity. Both methods have been evaluated with data from the participation of our QA engine in the Technolangue EQUER campaign. On the one hand, the SVM based method leads to the best results. It correctly determines which are easy questions, namely, those to which our system gives the right answer, and which are hard questions, those to which our system gives bad or no answer. On the other hand, we show that for our system, the expected answer type (proper nouns, numbers, locations ...) is not a determining factor in defining question hardness.</abstract>
			<keywords>question-answering, difficulty prediction, SVM, decision trees</keywords>
		</article>
		<article id="taln-2006-long-032" session="">
			<auteurs>
				<auteur>
					<prenom>Annie</prenom>
					<nom>Tartier</nom>
					<email>annie.tartier@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA FRE CNRS 2729</affiliation>
			</affiliations>
			<titre>Variation terminologique et analyse diachronique</titre>
			<type>long</type>
			<pages>347-356</pages>
			<resume>Cet article présente un travail destiné à automatiser l’étude de l’évolution terminologique à partir de termes datés extraits de corpus diachroniques de textes scientifiques ou techniques. Les apparitions et disparitions d’attestations de termes au cours du temps constituent la manifestation la plus simple de l’évolution. Mais la prise en compte des formes variantes apporte une information de meilleure qualité sur le suivi des termes. Une distance entre termes complexes permet de rendre opérationnelle l’intégration de la variation terminologique à l’analyse diachronique. Des résultats montrant la prise en compte des variantes sont présentés et commentés à la fin de l’article.</resume>
			<mots_cles>évolution terminologique, corpus diachronique, terme complexe, variation terminologique, distance</mots_cles>
			<title></title>
			<abstract>This paper presents a work about terminological evolution of dated terms, extracted from diachronic corpora of scientific or technical texts. Disappearance and appearances of terms attestations are the most obvious evidence of change. Nonetheless taking into account the variant forms of terms helps describe their historic development. A distance between complex terms allows terminological variation to be integrated into diachronic analysis. Some results and comments are given at the end of the paper.</abstract>
			<keywords>terminological evolution, diachronic corpus, complex term, terminological variation, distance</keywords>
		</article>
		<article id="taln-2006-long-033" session="">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Valette</nom>
					<email>mathieu.valette@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexander</prenom>
					<nom>Estacio-Moreno</nom>
					<email>alexander.estacio-moreno@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Étienne</prenom>
					<nom>Petitjean</nom>
					<email>etienne.petitjean@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Évelyne</prenom>
					<nom>Jacquey</nom>
					<email>evelyne.jacquey@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF UMR 7118 CNRS-Nancy</affiliation>
			</affiliations>
			<titre>Éléments pour la génération de classes sémantiques à partir de définitions lexicographiques Pour une approche sémique du sens</titre>
			<type>long</type>
			<pages>357-366</pages>
			<resume>Ce papier expose une expérience de classification menée sur un corpus de définitions dictionnairiques. Le cadre général de cette recherche est la constitution d’une ressource lexico-sémantique fondée sur une conception structuraliste du sens (le contenu sémantique d’une unité lexicale est structuré en sèmes ; le sens d’un texte émerge de faisceaux de regroupements sémiques stabilisés). L’objectif de l’expérience rapportée est de découvrir des classes sémantiques à partir de définitions dictionnairiques avec la méthode CAH. Les classes sémantiques regroupent des unités lexicales en fonction de sèmes génériques (i.e. communs à toutes les unités lexicales de la classe) et s’organisent différentiellement en fonction de sèmes spécifiques. À partir d’une sélection d’entrées dictionnairiques partageant le sème générique /arbre/, nous étudions la distribution et l’organisation d’une hypothétique classe sémantique liée au domaine de la sylviculture.</resume>
			<mots_cles>ressources lexico-sémantiques, dictionnaire sémique, sémantique textuelle, classification automatique, CAH, Jaccard</mots_cles>
			<title></title>
			<abstract>This paper describes an experiment of classification, based on a corpus of dictionary definitions. Underlying this research is the building of a lexico-semantic resource based on a structuralist approach to meaning. The semantic content of a lexical item is made up of semes; the meaning of a text emerges from groupings of stabilised seme sets. The purpose of the experiment is to make up semantic classes (or clusters) with dictionary definitions by using the HCA method. Semantic classes are built from lexical items according to generic semes in that they are shared by all lexical items of the class. They are differentiated according to their specific semes. From a selection of dictionary entries sharing generic seme /arbre/ (“tree”), the distribution and the organisation of an assumed semantic class linked to the domain of Forestry will be studied.</abstract>
			<keywords>lexico-semantic resources, seme dictionary, text semantics, clustering, HCA, Jaccard, UPGM</keywords>
		</article>
		<article id="taln-2006-long-034" session="">
			<auteurs>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>awidloch@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Caen - Basse-Normandie, GREYC - CNRS UMR 6072</affiliation>
			</affiliations>
			<titre>Analyse par contraintes de l’organisation du discours</titre>
			<type>long</type>
			<pages>367-376</pages>
			<resume>Nous abordons ici la question de l’analyse de la structure du discours, du point de vue de sa description formelle et de son traitement automatique. Nous envisageons l’hypothèse selon laquelle une approche par contraintes pourrait permettre la prise en charge de structures discursives variées d’une part, et de différents types d’indices de leur manifestation d’autre part. Le formalisme CDML que nous introduisons vise précisément une telle approche.</resume>
			<mots_cles>analyse du discours, formalisation du discours, approche par contraintes</mots_cles>
			<title></title>
			<abstract>We focus on the problem of discourse structure analysis from the point of view of its formal description and automatic processing.We intend to test the hypothesis that constraint-based approaches could enable various structures and cues of their presence to be taken into account. The CDML formalism which is here introduced allows such an approach.</abstract>
			<keywords>discourse analysis, discourse formalisation, constraint-based approach</keywords>
		</article>
		<article id="taln-2006-poster-001" session="">
			<auteurs>
				<auteur>
					<prenom>Abdelkarim</prenom>
					<nom>Abdelkader</nom>
					<email>abdelkader.abdelkarim@laposte.net</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kais</prenom>
					<nom>Haddar</nom>
					<email>kais.haddar@fss.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Abdelmajid</prenom>
					<nom>Ben Hamadou</nom>
					<email>abdelmajid.benhamadou@isimsf.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Sfax – MIRACL, ISIMS</affiliation>
			</affiliations>
			<titre>Étude et analyse de la phrase nominale arabe en HPSG</titre>
			<type>poster</type>
			<pages>379-388</pages>
			<resume>Dans cet article, nous proposons une démarche d’analyse syntaxique pour les phrases nominales arabes à l’aide du formalisme des grammaires syntagmatiques guidées par les têtes HPSG. Pour ce faire, nous commençons par étudier la typologie de la phrase nominale arabe en précisant ses différentes formes. Puis, nous élaborons une grammaire HPSG traitant ce type de phrase et qui respecte la spécificité de la langue arabe. Ensuite, nous présentons une démarche d’analyse syntaxique se basant sur une approche ascendante et sur le mécanisme d’unification. Enfin, nous donnons une idée sur l’implémentation et l’expérimentation du prototype réalisé.</resume>
			<mots_cles>phrase nominale arabe, grammaire HPSG, schéma adapté, analyse syntaxique</mots_cles>
			<title></title>
			<abstract>In this paper, we propose a parsing approach for Arabic nominal sentences using the Head-Driven Phrase Grammar formalism HPSG. To elaborate this approach, we begin by studying the typology of the Arabic nominal sentence specifying its different forms. Then, we elaborate a HPSG grammar for this type of sentence. The elaborated grammar respects the Arabic language specificity. Next, we present a parsing approach based on a bottom-up unification process. Finally, we illustrate the implementation and experimentation of the realised prototype.</abstract>
			<keywords>arabic nominal sentence, HPSG grammar, parsing</keywords>
		</article>
		<article id="taln-2006-poster-002" session="">
			<auteurs>
				<auteur>
					<prenom>Sandra</prenom>
					<nom>Antunes</nom>
					<email>sandra.antunes@clul.ul.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maria Fernanda</prenom>
					<nom>Bacelar do Nascimento</nom>
					<email>fbacelar.nascimento@clul.ul.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>João Miguel</prenom>
					<nom>Casteleiro</nom>
					<email>miguel.casteleiro@zmail.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Amália</prenom>
					<nom>Mendes</nom>
					<email>amalia.mendes@clul.ul.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Luísa</prenom>
					<nom>Pereira</nom>
					<email>luisa.alice@clul.ul.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Tiago</prenom>
					<nom>Sá</nom>
					<email>ptsa@clul.ul.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Universidade de Lisboa – Centro de Linguística</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>389-397</pages>
			<resume>Cet article présente la méthodologie suivie et les résultats obtenus dans le cadre d’un projet qui a pour objectif la construction d’une large base de données d’expressions multi-mots de la langue portugaise. Ces expressions multi-mots ont été automatiquement extraites d’un corpus équilibré de 50 millions de mots, interprétées statistiquement à l’aide de mesures d’association lexicales et ont été ensuite manuellement vérifiées. La base de données lexicales recouvre différent types d’expressions multi-mots avec différents degrés de cohésion, qui vont de la quasi totale fixité jusqu’aux groupes de mots qui se réalisent préférentiellement ensemble, comme les collocations. Le large ensemble de données de cette ressource permettra une révision des typologies d’unités multi-mots en portugais et l’évaluation de différentes mesures d’associations lexicales.</resume>
			<mots_cles>expressions multi-mots, collocations, extraction d’information, base de données lexicales, mesures d’associations lexicales, typologies d’expressions multi-mots</mots_cles>
			<title>Corpus-based extraction and identification of Portuguese Multiword Expressions</title>
			<abstract>This presentation reports on an on-going project aimed at building a large lexical database of corpus-extracted multiword (MW) expressions for the Portuguese language. MW expressions were automatically extracted from a balanced 50 million word corpus compiled for this project, furthermore these were statistically interpreted using lexical association measures, followed by a manual validation process. The lexical database covers different types of MW expressions, from named entities to lexical associations with different degrees of cohesion, ranging from totally frozen idioms to favoured co-occurring forms, such as collocations. We aim to achieve two main objectives with this resource. Firstly to build on the large set of data of different types of MW expressions, thus revising existing typologies of collocations and integrating them in a larger theory of MW units. Secondly, to use the extensive hand-checked data as training data to evaluate existing statistical lexical association measures.</abstract>
			<keywords>multiword expressions, collocations, information extraction, lexical database, lexical association measures, typology of multiword expressions</keywords>
		</article>
		<article id="taln-2006-poster-003" session="">
			<auteurs>
				<auteur>
					<prenom>Gemma</prenom>
					<nom>Bel-Enguix</nom>
					<email>gemma.bel@urv.net</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maria Dolores</prenom>
					<nom>Jiménez-López</nom>
					<email>mariadolores.jimenez@urv.net</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Rovira i Virgili University Research Group on Mathematical Linguistics (GRLMC)</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>398-406</pages>
			<resume>L’interface homme-machine a besoin de modèles de structures de dialogue qui expliquent la variabilité et la spontanéité au dialogue. Le contexte sémantique et pragmatique évolue continuellement pendant le développement de la conversation, surtout par la distribution de turns qui ont un effet direct dans les échanges de dialogue. Dans cet article nous utilisons un paradigme de langue formel pour modéliser les conversations de système de multiagent. Notre modèle computationnel combine des unités minimales pragmatiques -les actes de parole- pour construire des dialogues. Dans ce cadre, nous montrons comment la distribution de turn-taking peut être ambiguë et proposer un algorithme pour la résoudre, considérant turn coherence, trajectories et le turn-pairing. Finalement, nous suggérons overlapping comme un des phénomènes possibles naissants d’un turn-taking non résolu.</resume>
			<mots_cles>dialogue, dialogue games, turn-taking</mots_cles>
			<title>Ambiguous Turn-Taking Games in Conversations</title>
			<abstract>Human-computer interfaces require models of dialogue structure that capture the variability and unpredictability within dialogue. Semantic and pragmatic context are continuously evolving during conversation, especially by the distribution of turns that have a direct effect in dialogue exchanges. In this paper we use a formal language paradigm for modelling multi-agent system conversations. Our computational model combines pragmatic minimal units –speech acts– for constructing dialogues. In this framework, we show how turn-taking distribution can be ambiguous and propose an algorithm for solving it, considering turn coherence, trajectories and turn pairing. Finally, we suggest overlapping as one of the possible phenomena emerging from an unresolved turn-taking.</abstract>
			<keywords>dialogue modelling, dialogue games, turn-taking</keywords>
		</article>
		<article id="taln-2006-poster-004" session="">
			<auteurs>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Bestgen</nom>
					<email>yves.bestgen@psp.ucl.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Piérard</nom>
					<email>sophie.pierard@psp.ucl.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Fonds national de la recherche scientifique, Université catholique de Louvain</affiliation>
			</affiliations>
			<titre>Comment évaluer les algorithmes de segmentation automatique ? Essai de construction d’un matériel de référence.</titre>
			<type>poster</type>
			<pages>407-414</pages>
			<resume>L’objectif de cette recherche est d’évaluer l’efficacité d’algorithmes lors de l’identification des ruptures thématiques dans des textes. Pour ce faire, 32 articles de journaux ont été segmentés par des groupes de 15 juges. L’analyse de leurs réponses indique que chaque juge, pris individuellement, est peu fiable contrairement à l’indice global de segmentation, qui peut être dérivé des réponses de l’ensemble des juges. Si les deux algorithmes testés sont capables de retrouver le début des articles lorsque ceux-ci sont concaténés, ils échouent dans la détection des changements de thème perçus par la majorité des juges. Il faut toutefois noter que les juges, pris individuellement, sont eux-mêmes inefficaces dans l’identification des changements de thème. Dans la conclusion, nous évaluons différentes explications du faible niveau de performance observé.</resume>
			<mots_cles>segmentation automatique, évaluation, accord interjuges</mots_cles>
			<title></title>
			<abstract>The objective of this research is to evaluate the efficacy of algorithms in identifying thematic breaks in texts. With this aim, 32 newspaper articles were segmented by groups of 15 judges. The analysis of their answers indicates that each judge, taken individually, is not very reliable, unlike the global index of segmentation derived from all the judges. If two algorithms tested are able to find the beginning of the articles when those are concatenated, they fail in the detection of the changes of topic perceived by the majority of the judges. It should however be noted that the judges, taken individually, are themselves largely ineffective in the identification of the changes of topic. In conclusion, we examine various explanations for the low level of performance observed.</abstract>
			<keywords>automatic segmentation, evaluation, interrater agreement</keywords>
		</article>
		<article id="taln-2006-poster-005" session="">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Rauzy</nom>
					<email>stephane.rauzy@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Provence &amp; CNRS, Laboratoire Parole et Langage</affiliation>
			</affiliations>
			<titre>Mécanismes de contrôle pour l’analyse en Grammaires de Propriétés</titre>
			<type>poster</type>
			<pages>415-424</pages>
			<resume>Les méthodes d’analyse syntaxiques hybrides, reposant à la fois sur des techniques statistiques et symboliques, restent peu exploitées. Dans la plupart des cas, les informations statistiques sont intégrées à un squelette contextfree et sont utilisées pour contrôler le choix des règles ou des structures. Nous proposons dans cet article une méthode permettant de calculer un indice de corrélation entre deux objets linguistiques (catégories, propriétés). Nous décrivons une utilisation de cette notion dans le cadre de l’analyse des Grammaires de Propriétés. L’indice de corrélation nous permet dans ce cas de contrôler à la fois la sélection des constituants d’une catégorie, mais également la satisfaction des propriétés qui la décrivent.</resume>
			<mots_cles>analyseur syntaxique, modèle des patrons, indice de corrélation, Grammaires de Propriétés, technique d’analyse hybride</mots_cles>
			<title></title>
			<abstract>Hybrid parsing techniques based both on statistical and symbolic methods remain rare. In general, they integrate statistical information into a context-free skeleton, in order to control the selection of rules and structures. We propose a statistical method which allows the evaluation of a correlation index between two linguistic objects, namely, category, and property. We describe how to integrate this statistical information into the framework of Property Grammars. The correlation index is used for controling both the selection process of category constituents and the evaluation of properties satisfaction.</abstract>
			<keywords>parsing, patterns model, correlation index, Property Grammars, hybrid parsing techniques</keywords>
		</article>
		<article id="taln-2006-poster-006" session="">
			<auteurs>
				<auteur>
					<prenom>Armelle</prenom>
					<nom>Brun</nom>
					<email>brun@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>David</prenom>
					<nom>Langlois</nom>
					<email>langlois@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kamel</prenom>
					<nom>Smaïli</nom>
					<email>smaili@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Nancy 2, LORIA</affiliation>
			</affiliations>
			<titre>Exploration et utilisation d’informations distantes dans les modèles de langage statistiques</titre>
			<type>poster</type>
			<pages>425-434</pages>
			<resume>Dans le cadre de la modélisation statistique du langage, nous montrons qu’il est possible d’utiliser un modèle n-grammes avec un historique qui n’est pas nécessairement celui avec lequel il a été appris. Par exemple, un adverbe présent dans l’historique peut ne pas avoir d’importance pour la prédiction, et devrait donc être ignoré en décalant l’historique utilisé pour la prédiction. Notre étude porte sur les modèles n-grammes classiques et les modèles n-grammes distants et est appliquée au cas des bigrammes. Nous présentons quatre cas d’utilisation pour deux modèles bigrammes : distants et non distants. Nous montrons que la combinaison linéaire dépendante de l’historique de ces quatre cas permet d’améliorer de 14 % la perplexité du modèle bigrammes classique. Par ailleurs, nous nous intéressons à quelques cas de combinaison qui permettent de mettre en valeur les historiques pour lesquels les modèles que nous proposons sont performants.</resume>
			<mots_cles>modélisation statistique du langage, modèles distants, combinaison linéaire</mots_cles>
			<title></title>
			<abstract>In the framework of statistical language modeling, we show that it is possible to use n-gram models with a history different to the one used during training. Our study deals with classical and distant n-gram models and is restricted to bigram models. We present four use cases for two bigram models : distant and non distant. By using the linear combination, we show an improvement of 14 % in terms of perplexity compared to the classic bigram model. Moreover, a study has been performed in order to emphasize the histories for which our models are efficient.</abstract>
			<keywords>statistical language modeling, distant models, linear combination</keywords>
		</article>
		<article id="taln-2006-poster-007" session="">
			<auteurs>
				<auteur>
					<prenom>Francis</prenom>
					<nom>Brunet-Manquat</nom>
					<email>francis.brunet-manquat@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gilles</prenom>
					<nom>Sérasset</nom>
					<email>gilles.serasset@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Joseph Fourier, Grenoble – Laboratoire CLIPS-IMAG</affiliation>
			</affiliations>
			<titre>Création d’une base terminologique juridique multilingue à l’aide de la plateforme générique Jibiki : le projet LexALP</titre>
			<type>poster</type>
			<pages>435-444</pages>
			<resume>Cet article présente l’utilisation de « Jibiki » (la plateforme de développement du serveur Web Papillon) dans le cadre du projet LexALP1. Le but de ce projet est d’harmoniser la terminologie des quatre langues (français, allemand, italien et slovène) de la Convention Alpine2 de sorte que les états membres puissent coopérer efficacement. Pour cela, le projet utilise la plateforme Jibiki afin de construire une banque terminologique permettant de comparer la terminologie spécialisée de sept systèmes légaux dans quatre langues, et de l’harmoniser, optimisant ainsi la compréhension entre les états alpins sur des questions environnementales au niveau supranational. Dans cet article, nous présentons comment peut être employée la plateforme générique Jibiki afin de gérer un dictionnaire particulier.</resume>
			<mots_cles>dictionnaire multilingue, banque terminologique juridique, édition de terme</mots_cles>
			<title></title>
			<abstract>This paper presents the particular use of « Jibiki » (Papillon’s Web server development platform) for the LexALP1 project. LexALP’s goal is to harmonize the terminology of the Alpine Convention’s2 four languages (French, German, Italian and Slovenian) so that member states are able to cooperate effectively. For this, the project uses the Jibiki platform in order to build a term bank used to compare the specialized terminology of seven different national legal systems in four different languages, and to harmonize it, optimizing the understanding between various alpine states in environmental matters at a supranational level. We describe how a generic platform like Jibiki is used to cope with a new kind of dictionary.</abstract>
			<keywords>multilingual dictionary, legal term bank, term editing</keywords>
		</article>
		<article id="taln-2006-poster-008" session="">
			<auteurs>
				<auteur>
					<prenom>Jean</prenom>
					<nom>Caelen</nom>
					<email>jean.caelen@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hoá</prenom>
					<nom>Nguyen</nom>
					<email>hoa.nguyen@vnu.edu.vn</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut d’Informatique et Mathématiques Appliques de Grenoble, CLIPS</affiliation>
				<affiliation affiliationId="2">Vietnam National University, Hanoi</affiliation>
			</affiliations>
			<titre>Traitement des incompréhensions et des malentendus en dialogue homme-machine</titre>
			<type>poster</type>
			<pages>445-454</pages>
			<resume>Traiter les erreurs en dialogue homme-machine est un problème difficile compte-tenu des multiples sources possibles depuis la reconnaissance de la parole jusqu’à la génération en passant par d’autres modules comme l’analyse sémantique, l’interprétation pragmatique ou la gestion du dialogue. Dans cet article, ce problème est envisagé dans le but d’apporter de la généricité et de la robustesse au système ; il est traité au niveau du contrôleur de dialogue. Les différents types d’erreurs sont d’abord identifiés et regroupés en deux catégories qui seules ont un sens vis-à-vis de l’utilisateur : les incompréhensions et les malentendus. Puis, ces deux catégories d’erreur sont traitées de manière spécifique pour que le système puisse générer une réponse convenable et intelligente à l’utilisateur, sans rupture de dialogue. L’expérimentation effectuée en appliquant cette approche au système de dialogue Mélina présente des résultats prometteurs pour traiter les erreurs en dialogue.</resume>
			<mots_cles>incompréhension, malentendu, acte de dialogue, but de dialogue, stratégie de dialogue, dialogue homme-machine</mots_cles>
			<title></title>
			<abstract>Miscommunication causes inconvenience within a spoken dialogue system. In this paper, we investigate this problem with regard to achieving genericity and robustness within dialogue management. Therefore, different error types occurring in a spoken dialogue system were first identified. Two markers were then proposed to detect and to trigger misunderstanding to the dialogue manager that should then choose an adequate strategy so as to generate an intelligent and suitable answer to the user. Moreover, several issues are investigated in order to process misinterpretations in the dialogue manager. An experiment performed using our approach to the spoken dialogue system Mélina shows promising results in processing miscommunication and proves the value of our approach.</abstract>
			<keywords>misunderstanding, misinterpretation, dialogue act, dialogue goal, dialogue strategy, humancomputer dialogue</keywords>
		</article>
		<article id="taln-2006-poster-009" session="">
			<auteurs>
				<auteur>
					<prenom>Frederik</prenom>
					<nom>Cailliau</nom>
					<email>cailliau@sinequa.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Sinequa Labs – Sinequa / LIPN – Université Paris 13</affiliation>
			</affiliations>
			<titre>Un modèle pour unifier la gestion de ressources linguistiques en contexte multilingue</titre>
			<type>poster</type>
			<pages>455-461</pages>
			<resume>Le bon fonctionnement d’Intuition, plate-forme de recherche d’information, repose sur le développement et l’intégration d’un grand nombre de ressources linguistiques. Dans un souci de cohérence et de meilleure gestion, l’unification de ressources contenant des connaissances hétérogènes s’impose. Comme Intuition est disponible dans la plupart des langues européennes, cette unification se heurte au facteur multilingue. Pour surmonter les problèmes causés par les différences structurelles entre les langues, une nouvelle architecture linguistique a été conçue et exprimée en UML. Ce méta-modèle est le point de départ pour la nouvelle base de données qui sera le noyau d’un nouvel environnement de travail centré sur son utilisateur, l’expert linguistique. Cet environnement centralisera la gestion de toutes les ressources linguistiques d’Intuition.</resume>
			<mots_cles>gestion de lexiques, base de données linguistique, ressources linguistiques, multilinguisme, architecture linguistique</mots_cles>
			<title></title>
			<abstract>The proper functioning of Intuition, Sinequa’s platform for information retrieval, depends on the development and integration of a large number of linguistic resources. To improve their coherence and management, these resources which contain very diverse knowledge, need to be unified. As Intuition is available in most European languages, the multilingual factor complicates this unification. To overcome the problems caused by the structural differences among languages, a new linguistic architecture has been designed and modelled using UML. This meta-model is the starting point for a new database that will be the core component of a new workbench, centred on its user, the linguistic expert. This workbench will provide a centralised management of all the linguistic resources within Intuition.</abstract>
			<keywords>lexicon management, linguistic database, linguistic resources, multilinguism, linguistic architecture</keywords>
		</article>
		<article id="taln-2006-poster-010" session="">
			<auteurs>
				<auteur>
					<prenom>Hafedh</prenom>
					<nom>El Ayech</nom>
					<email>hafedh.elayech@edunet.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Amine</prenom>
					<nom>Mahfouf</nom>
					<email>amine.mahfouf@cgi.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Adnane</prenom>
					<nom>Zribi</nom>
					<email>adn@gnet.tn</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut Supérieur de Gestion de Tunis – Département Informatique</affiliation>
			</affiliations>
			<titre>Reconnaissance de la métrique des poèmes arabes par les réseaux de neurones artificiels</titre>
			<type>poster</type>
			<pages>462-472</pages>
			<resume>Nous avons construit un système capable de reconnaître les modes de composition pour les poèmes arabes, nous décrivons dans cet article les différents modules du système. Le recours à une technique d’apprentissage artificiel pour classer une séquence phonétique de syllabes est justifiable par le fait que nous avons imité le processus d’apprentissage naturel humain suivi par les poètes pendant des siècles. Les réseaux de neurones artificiels de type Perceptron multicouches ont montré un pouvoir très puissant de classification.</resume>
			<mots_cles>réseau de neurones, perceptron multi couches, classification, poèmes arabes, syllabes, analyse phonétique</mots_cles>
			<title></title>
			<abstract>We have created a new system to recognize the different composition modes of Arab poetry. In this article we describe the different modules of this system. The use of an artificial learning technique for grouping a phonetic sequence of syllables is explained by the fact that we imitate the human process learning.</abstract>
			<keywords>artificial neural networks, multi layer perceptron, classification, arabic poetry, syllabes, phonetic analysis</keywords>
		</article>
		<article id="taln-2006-poster-011" session="">
			<auteurs>
				<auteur>
					<prenom>Jorge</prenom>
					<nom>García-Flores</nom>
					<email>jorge.gflores@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Elena</prenom>
					<nom>Ivanova</nom>
					<email>elena.ivanova@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Pierre</prenom>
					<nom>Desclés</nom>
					<email>jpdescles@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brahim</prenom>
					<nom>Djioua</nom>
					<email>bdjioua@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Paris-Sorbonne – LaLICC</affiliation>
			</affiliations>
			<titre>Annotation automatique de relations de contrôle dans des spécifications des besoins informatiques</titre>
			<type>poster</type>
			<pages>473-482</pages>
			<resume>La conception de logiciels est un processus technologique complexe, qui nécessite d’être assisté par des outils de traitement automatique des langues. Cet article présente une méthode pour l’annotation de relations discursives de contrôle dans des textes de spécification de besoins informatiques (SBI). La méthode vise à distinguer les actions contrôlées par le système de celles contrôlées par son environnement, ce qui permet d’établir de façon claire les limites et les responsabilités d’un système informatique. Notre méthode fait appel à la sémantique discursive pour analyser les moyens d’expression du contrôle dans un corpus de SBI industrielles ; l’expression du contrôle est identifiable par la présence, dans un certain contexte, de marqueurs linguistiques exprimés par des règles dites d’Exploration Contextuelle. La dernière partie montre le processus d’annotation automatique de la notion de contrôle par le système EXCOM et termine par la présentation d’un début d’évaluation de cette méthodologie.</resume>
			<mots_cles>relations de contrôle, spécifications des besoins, extraction d’information, contrôle, sémantique, annotation automatique</mots_cles>
			<title></title>
			<abstract>This paper presents a technique for the automatic extraction of control sentences from software requirements specifications (SRS). Our aim is to annotate action sentences from an SRS document, and to recognize if those actions are machine controlled or user controlled, in order to better understand the limits and responsibilities of a software system and to support the Requirements Engineering process. We present a linguistic analysis of control markers and a technique to automatically annotate control sentences by means of contextual exploration rules. These rules are implemented in the contextual exploration engine Excom and applied to a set of commercial SRS.</abstract>
			<keywords>semantic filtering, natural language requirements, requirements engineering, control</keywords>
		</article>
		<article id="taln-2006-poster-012" session="">
			<auteurs>
				<auteur>
					<prenom>Fabrizio</prenom>
					<nom>Gotti</nom>
					<email>gottif@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>Coulombe</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Montréal, RALI/DIRO</affiliation>
				<affiliation affiliationId="2">Lingua Technologies Inc.</affiliation>
			</affiliations>
			<titre>Vers l’intégration du contexte dans une mémoire de traduction sous-phrastique : détection du domaine de traduction</titre>
			<type>poster</type>
			<pages>483-492</pages>
			<resume>Nous présentons dans cet article une mémoire de traduction sous-phrastique sensible au domaine de traduction, une première étape vers l’intégration du contexte. Ce système est en mesure de recycler les traductions déjà « vues » par la mémoire, non seulement pour des phrases complètes, mais également pour des sous-séquences contiguës de ces phrases, via un aligneur de mots. Les séquences jugées intéressantes sont proposées au traducteur. Nous expliquons également la création d’un utilisateur artificiel, indispensable pour tester les performances du système en l’absence d’intervention humaine. Nous le testons lors de la traduction d’un ensemble disparate de corpus. Ces performances sont exprimées par un ensemble de métriques que nous définissons. Enfin, nous démontrons que la détection automatique du contexte de traduction peut s’avérer bénéfique et prometteuse pour améliorer le fonctionnement d’une telle mémoire, en agissant comme un filtre sur le matériel cible suggéré.</resume>
			<mots_cles>traduction assistée par ordinateur, mémoire de traduction sous-phrastique, récupération sensible au contexte, détection du domaine de traduction</mots_cles>
			<title></title>
			<abstract>In this article, we present a sub-sentential translation memory sensitive to the translation topic, a first step towards a full-fledged context-sensitive memory. This system is able to recycle previous translations indexed into the memory, not only for full sentences, but also for contiguous subsegments of these sentences, through word alignment information. Interesting segments are proposed to the translator. We also describe the creation of an artificial user (a simulator), necessary to test the system performances when no human intervention is possible, as is the case for these experiments. We test it when translating a set of disparate bilingual corpora. These performances are reflected in different metrics which we define. Finally, we show that a first attempt to automatically detect the translation context can be beneficial and promises to improve such a memory, by acting as a filter on the target material proposed to the user.</abstract>
			<keywords>computer assisted machine translation, sub-sentential translation memory, context-sensitive retrieval, translation topic detection</keywords>
		</article>
		<article id="taln-2006-poster-013" session="">
			<auteurs>
				<auteur>
					<prenom>Lamia</prenom>
					<nom>Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nouha</prenom>
					<nom>Chaâben</nom>
					<email>nouha.chaaben@laposte.net</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Faculté des Sciences Économiques et de Gestion de Sfax – Laboratoire LARIS</affiliation>
			</affiliations>
			<titre>Analyse et désambiguïsation morphologiques de textes arabes non voyellés</titre>
			<type>poster</type>
			<pages>493-501</pages>
			<resume>Dans ce papier nous proposons d’abord une méthode d’analyse et de désambiguïsation morphologiques de textes arabes non voyellés permettant de lever l’ambiguïté morphologique due à l’absence des marques de voyelles et aussi à l’irrégularité des formes dérivées de certains mots arabes (e.g. formes irrégulières du pluriel des noms et des adjectifs). Ensuite, nous présentons le système MORPH2, un analyseur morphologique de textes arabes non voyellés basé sur la méthode proposée. Ce système est évalué sur un livre scolaire et des articles de journaux. Les résultats obtenus son et très encourageants. En effet, les mesures de rappel et de précision globales sont respectivement de 69,77 % et 68,51 %.</resume>
			<mots_cles>analyse morphologique, désambiguïsation morphologique, TALN arabe</mots_cles>
			<title></title>
			<abstract>In this paper we first propose a morphological disambiguation and analysis method of non voweled Arabic texts. This method could handle morphological ambiguities, caused by both the absence of the vowel marks and the irregularity of certain words (e.g. irregularity of the plural form for nouns and adjectives). Then, we present the MORPH2 system, a morphological analyser of non voweled Arabic texts, based on the proposed method. This system is evaluated on a school book and newspaper articles. The obtained results are very encouraging. Indeed, the global measures of recall and precision are 69,77 % and 68,51 % respectively.</abstract>
			<keywords>morphological analysis, morphological disambiguation, arabic NLP</keywords>
		</article>
		<article id="taln-2006-poster-014" session="">
			<auteurs>
				<auteur>
					<prenom>Johannes</prenom>
					<nom>Heinecke</nom>
					<email>johannes.heinecke@francetelecom.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Télécom, Division R&amp;D, TECH/EASY/Langues Naturelles</affiliation>
			</affiliations>
			<titre>Génération automatique des représentations ontologiques</titre>
			<type>poster</type>
			<pages>502-511</pages>
			<resume>Depuis la conception du Web sémantique une tâche importante se pose au niveau de traitement automatique du langage : rendre accessible le contenu existant duWeb dit classique aux traitements et raisonnements ontologiques. Comme la plupart du contenu est composé de textes, on a besoin de générer des représentations ontologiques de ces informations textuelles. Dans notre article nous proposons une méthode afin d’automatiser cette traduction en utilisant des ontologies et une analyse syntaxico-sémantique profonde.</resume>
			<mots_cles>analyse syntaxico-sémantique, ontologies, représentations ontologiques,Web sémantique</mots_cles>
			<title></title>
			<abstract>Since the conception of the semantic Web an important challenge faces natural language processing: i.e. making the existing textual contents of the « classic »Web available for ontology based applications, reasoning and finally for the semantic Web. Since the majority of Web sites contain textual information, the creation of ontological representations of these will be useful if not necessary. In our article we propose a method to automate this task by using domain ontologies and a deep syntactic-semantic analysis.</abstract>
			<keywords>syntactic and semantic analysis, ontologies, ontological representations, semantic Web</keywords>
		</article>
		<article id="taln-2006-poster-015" session="">
			<auteurs>
				<auteur>
					<prenom>Alain</prenom>
					<nom>Joubert</nom>
					<email>alain.joubert@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Schwab</nom>
					<email>schwab@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique, Robotique et Microélectronique de Montpellier</affiliation>on>
			</affiliations>
			<titre>Approche évolutive des notions de base pour une représentation thématique des connaissances générales</titre>
			<type>poster</type>
			<pages>512-521</pages>
			<resume>Dans le domaine du Traitement Automatique du Langage Naturel, pour élaborer un système de représentation thématique des connaissances générales, des méthodes s’appuyant sur des thésaurus sont utilisées depuis une quinzaine d’années. Un thésaurus est constitué d’un ensemble de concepts qui définissent un système générateur d’un espace vectoriel modélisant les connaissances générales. Ces concepts, souvent organisés en une hiérarchie arborescente, constituent un instrument fondamental, mais totalement figé. Même si les notions évoluent (nous pensons par exemple aux domaines techniques), un thésaurus ne peut quant à lui être modifié que lors d’un processus particulièrement lourd, car nécessitant la collaboration d’experts humains. C’est à ce problème que nous nous attaquons ici. Après avoir détaillé les caractéristiques que doit posséder un système générateur de l’espace vectoriel de modélisation des connaissances, nous définissons les « notions de base ». Celles-ci, dont la construction s’appuie initialement sur les concepts d’un thésaurus, constituent un autre système générateur de cet espace vectoriel. Nous abordons la détermination des acceptions exprimant les notions de base, ce qui nous amène naturellement à nous poser la question de leur nombre. Enfin, nous explicitons comment, s’affranchissant des concepts du thésaurus, ces notions de base évoluent par un processus itératif au fur et à mesure de l’analyse de nouveaux textes.</resume>
			<mots_cles>thésaurus, vecteurs conceptuels, notions de base, évolutivité</mots_cles>
			<title></title>
			<abstract>In the field of Natural Language Processing, in order to arrive at a thematic representation system of general knowledge, methods leaning on thesaurus have been widely used for about fifteen years. A thesaurus consists of a set of concepts defining a system generating a vector space to model general knowledge. These concepts, often organized as a hierarchy, constitute a fundamental, but fixed tool. When concepts evolve (as in technical fields), a thesaurus can evolve, but only as a result of an arduous process, requiring the collaboration of human experts. After detailing the desired characteristics of knowledge modelling systems, we define the « basic notions ». Their construction is initially based on the concepts of a thesaurus. They constitute another generating system of this vector space. We discuss the establishment of these basic notions, which naturally leads us to the question of their number. Lastly, we clarify how, being freed from the concepts of the thesaurus, the basic notions evolve progressively in an iterative process as new texts are being analysed.</abstract>
			<keywords>thesaurus, conceptual vectors, basic notions, evolution</keywords>
		</article>
		<article id="taln-2006-poster-016" session="">
			<auteurs>
				<auteur>
					<prenom>Sonia</prenom>
					<nom>Krivine</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Masaru</prenom>
					<nom>Tomimitsu</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email></email>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Monique</prenom>
					<nom>Slodzian</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INaLCO, CRIM</affiliation>
				<affiliation affiliationId="2">Université de Nantes, LINA, FRE CNRS 2729</affiliation>
				<affiliation affiliationId="3">Université René Descartes - Paris 5, Faculté de Médecine</affiliation>
				<affiliation affiliationId="4">Inserm, U729</affiliation>
				<affiliation affiliationId="5">SPIM</affiliation>
			</affiliations>
			<titre>Relever des critères pour la distinction automatique entre les documents médicaux scientifiques et vulgarisés en russe et en japonais</titre>
			<type>poster</type>
			<pages>522-531</pages>
			<resume>Dans cet article, nous cherchons à affiner la notion de comparabilité des corpus. Nous étudions en particulier la distinction entre les documents scientifiques et vulgarisés dans le domaine médical. Nous supposons que cette distinction peut apporter des informations importantes, par exemple en recherche d’information. Nous supposons par là même que les documents, étant le reflet de leur contexte de production, fournissent des critères nécessaires à cette distinction. Nous étudions plusieurs critères linguistiques, typographiques, lexicaux et autres pour la caractérisation des documents médicaux scientifiques et vulgarisés. Les résultats présentés sont acquis sur les données en russe et en japonais. Certains des critères étudiés s’avèrent effectivement pertinents. Nous faisons également quelques réflexions et propositions quant à la distinction des catégories scientifique et vulgarisée et aux questionnements théoriques.</resume>
			<mots_cles>recherche d’information translangue, corpus comparables, typologie de documents, catégorisation, document scientifique, document vulgarisé</mots_cles>
			<title></title>
			<abstract>In this paper, we aim to ripen the notion of corpora comparability. We study especially the distinction between scientific and popularized documents in the medical domain. We suppose that this distinction can give important informations, for instance in information retrieval. In the same time, we suppose that documents reflect the context of their production and provide features necessary for this distinction. We study and present several features, linguistic, typographic, lexical and other, for the characterization of medical documents as scientific or popularized. The results presented are acquired on data in Russian and Japanese. Some of analyzed features turn out to be relevant.We give then some remarks and suggestions as for the distinction of scientific and popularized documents and their theoretical issues.</abstract>
			<keywords>translingual information retrieval, comparable corpora, document typology, categorisation, scientific document, popularized document</keywords>
		</article>
		<article id="taln-2006-poster-017" session="">
			<auteurs>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Laporte</nom>
					<email>eric.laporte@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sébastien</prenom>
					<nom>Paumier</nom>
					<email>sebastien.paumier@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Marne-la-Vallée – IGM</affiliation>
			</affiliations>
			<titre>Graphes paramétrés et outils de lexicalisation</titre>
			<type>poster</type>
			<pages>532-540</pages>
			<resume>La lexicalisation des grammaires réduit le nombre des erreurs d’analyse syntaxique et améliore les résultats des applications. Cependant, cette modification affecte un système d’analyse syntaxique dans tous ses aspects. Un de nos objectifs de recherche est de mettre au point un modèle réaliste pour la lexicalisation des grammaires. Nous avons réalisé des expériences en ce sens avec une grammaire très simple par son contenu et son formalisme, et un lexique syntaxique très informatif, le lexique-grammaire du français élaboré au LADL. La méthode de lexicalisation est celle des graphes paramétrés. Nos résultats tendent à montrer que la plupart des informations contenues dans le lexique-grammaire peuvent être transférées dans une grammaire et exploitées avec succès dans l’analyse syntaxique de phrases.</resume>
			<mots_cles>grammaires, syntaxe, lexicalisation, réseaux de transitions récursifs</mots_cles>
			<title></title>
			<abstract>Shifting to a lexicalized grammar reduces the number of parsing errors and improves application results. However, such an operation affects a syntactic parser in all its aspects. One of our research objectives is to design a realistic model for grammar lexicalization. We carried out experiments for which we used a grammar with very simple content and formalism, and a very informative syntactic lexicon, namely the lexicon-grammar of French elaborated by the LADL. Lexicalization was performed by applying the parameterized-graph approach. Our results suggest that most information in the lexicon-grammar can be transferred into a grammar and exploited successfully for the syntactic parsing of sentences.</abstract>
			<keywords>grammar, syntax, lexicalization, recursive transition networks</keywords>
		</article>
		<article id="taln-2006-poster-018" session="">
			<auteurs>
				<auteur>
					<prenom>Jorge Antonio</prenom>
					<nom>Leoni de León</nom>
					<email>jorge.leonideleon@lettres.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Athina</prenom>
					<nom>Michou</nom>
					<email>athina.michou@lettres.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève, Département de linguistique Laboratoire d’Analyse et de Technologie du Langage</affiliation>
			</affiliations>
			<titre>Traitement des clitiques dans un environnement multilingue</titre>
			<type>poster</type>
			<pages>541-550</pages>
			<resume>Cet article décrit le traitement automatique des pronoms clitiques en espagnol et en grec moderne, deux langues de familles distinctes, dans le cadre de l’analyseur syntaxique FIPS multilingue, développé au Laboratoire d’Analyse et de Technologie de Langage (LATL). Nous abordons la distribution des pronoms clitiques, leurs similarités ainsi que leurs particularités par rapport à leur usage général. Ensuite nous présentons la méthode appliquée pour leur traitement, commune aux deux langues. Nous montrons que l’algorithme proposé peut facilement s’étendre à d’autres langues traitées par Fips qui partagent le phénomène de la cliticisation.</resume>
			<mots_cles>analyseur syntaxique, clitiques, traitement multilingue</mots_cles>
			<title></title>
			<abstract>In this article we present an algorithm for the computational processing of clitic pronouns in Spanish and Modern Greek by the multilingual syntactic parser Fips that has been developed at the Laboratory For Language Analysis And Technology, LATL. We provide an overview of the distribution of clitic pronouns, their similarities and particular behaviour compared to general use. Then we present a common method of processing clitics for both languages. Finally, we show that the algorithm developed can be easily extended to other languages with clitics, when treated by Fips.</abstract>
			<keywords>syntactic parser, clitics, multilingual processing</keywords>
		</article>
		<article id="taln-2006-poster-019" session="">
			<auteurs>
				<auteur>
					<prenom>Yayoi</prenom>
					<nom>Nakamura-Delloye</nom>
					<email>yayoi@free.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Denis Diderot - Paris 7, Lattice</affiliation>
			</affiliations>
			<titre>Détection des propositions syntaxiques du français en vue de l’alignement des propositions de textes parallèles français-japonais</titre>
			<type>poster</type>
			<pages>551-560</pages>
			<resume>Nous présentons dans cet article SIGLé (Système d’Identification de propositions avec Grammaire Légère), un système réalisant la détection des propositions françaises. Ce système détecte les propositions – à partir de phrases en entrée ségmentées et étiquetées en chunk par un analyseur extérieur –, analyse leurs relations et leur attribue une étiquette indiquant leur nature syntaxique. Il est caractérisé d’une part par sa grammaire de type CFG proposant un ensemble d’étiquettes adaptées à notre analyse pour les mots dits en « qu- », et d’autre part par l’utilisation du formalisme DCG et du langage PROLOG.</resume>
			<mots_cles>analyse syntaxique partielle, proposition syntaxique, subordination, Prolog, CFG, DCG</mots_cles>
			<title></title>
			<abstract>In this paper we present SIGLé (Clauses Identification System with Light Grammar), which is a system recognizing French clauses. Sentences are divided into chunks and tagged by an external analyzer. The system then identifies the clauses, analyzes their relations and assigns them a tag indicating their syntactic nature. The system is characterized by its context-free grammar, proposing a set of tags for the "qu-" words adapted to our analysis, and by the use of the DCG formalism and the PROLOG language.</abstract>
			<keywords>partial syntactic analysis, syntactic clause, subordination, Prolog, CFG, DCG</keywords>
		</article>
		<article id="taln-2006-poster-020" session="">
			<auteurs>
				<auteur>
					<prenom>Hung</prenom>
					<nom>Nguyen Thanh</nom>
					<email>hung64@yahoo.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Khanh</prenom>
					<nom>Bui Doan</nom>
					<email>vhquan@yahoo.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">High School for the Gifted, VNU-HCM</affiliation>
				<affiliation affiliationId="2">University of Paris 6</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>561-570</pages>
			<resume>Ce papier présente une nouvelle approche de la segmentation du vietnamien pour la catégorisation de texte. Au lieu d’utiliser des corpus d’entraînement annotés ou des lexiques (qui font défaut pour le vietnamien) nous utilisons des informations statistiques extraites directement d’un moteur de recherche commercial et des algorithmes génétiques pour trouver les segmentations les plus probables. Les informations extraites incluent la fréquence des documents et l’information mutuelle des n-grams. Nos résultats expérimentaux obtenus sur la segmentation et la catégorisation de résumés de nouvelles montrent que notre approche est très prometteuse. Elle offre des résultats semblables à 80 % avec le jugement humain sur la segmentation et à 90 % en catégorisation. Le temps de traitement est inférieur à une seconde par document quand l’information statistique est maintenue en cache.</resume>
			<mots_cles>catégorisation de texte, segmentation de texte, algorithmes génétiques</mots_cles>
			<title>Word Segmentation for Vietnamese Text Categorization An Internet-based Statistic and Genetic Algorithm Approach</title>
			<abstract>This paper suggests a novel Vietnamese segmentation approach for text categorization. Instead of using an annotated training corpus or a lexicon which are still lacking in Vietnamese, we use both statistical information extracted directly from a commercial search engine and a genetic algorithm to find the optimal routes to segmentation. The extracted information includes document frequency and n-gram mutual information. Our experiment results obtained on the segmentation and categorization of online news abstracts are very promising. It matches near 80 % human judgment on segmentation and over 90 % micro-averaging F1 in categorization. The processing time is less than one second per document when statistical information is cached.</abstract>
			<keywords>text categorization, text segmentation, genetics algorithms</keywords>
		</article>
		<article id="taln-2006-poster-021" session="">
			<auteurs>
				<auteur>
					<prenom>Jungyeul</prenom>
					<nom>Park</nom>
					<email>jungyeul.park@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 7 – Denis Diderot – UFRL – Laboratoire de linguistique formelle</affiliation>
			</affiliations>
			<titre>Extraction de grammaires TAG lexicalisées avec traits à partir d’un corpus arboré pour le coréen</titre>
			<type>poster</type>
			<pages>571-579</pages>
			<resume>Nous présentons, ici, une implémentation d’un système qui n’extrait pas seulement une grammaire lexicalisée (LTAG), mais aussi une grammaire LTAG avec traits (FB-LTAG) à partir d’un corpus arboré. Nous montrons les expérimentations pratiques où nous extrayons les grammaires TAG à partir du Sejong Treebank pour le coréen. Avant tout, les 57 étiquettes syntaxiques et les analyses morphologiques dans le corpus SJTree nous permettent d’extraire les traits syntaxiques automatiquement. De plus, nous modifions le corpus pour l’extraction d’une grammaire lexicalisée et convertissons les grammaires lexicalisées en schémas d’arbre pour résoudre le problème de la couverture lexicale limitée des grammaires lexicalisées extraites.</resume>
			<mots_cles>grammaire d’arbre d’adjoint lexicalisée, LTAG, LTAG avec traits, FB-LTAG, structure des traits, corpus arboré, extraction automatique d’une grammaire, coréen</mots_cles>
			<title></title>
			<abstract>We present the implementation of a system which extracts not only lexicalized grammars but also feature-based lexicalized grammars from Sejong Treebank for Korean. We report on some practical experiments, in which we extract TAG grammars. Above all, full-scale syntactic tags and well-formed morphological analysis in Sejong Treebank allow us to extract syntactic features. In addition, we modify the Treebank to extract lexicalized grammars and convert them into tree schemata to resolve limited lexical coverage problems related to extracted lexicalized grammars.</abstract>
			<keywords>lexicalized tree adjoining grammar, LTAG, feature-based LTAG, feature structure, treebank, automatic grammar extraction, Korean</keywords>
		</article>
		<article id="taln-2006-poster-022" session="">
			<auteurs>
				<auteur>
					<prenom>Thibault</prenom>
					<nom>Roy</nom>
					<email>thibault.roy@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Ferrari</nom>
					<email>stephane.ferrari@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Beust</nom>
					<email>pierre.beust@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Caen / Basse-Normandie, Laboratoire GREYC UMR 6072 CNRS</affiliation>
			</affiliations>
			<titre>Étude de métaphores conceptuelles à l’aide de vues globales et temporelles sur un corpus</titre>
			<type>poster</type>
			<pages>580-589</pages>
			<resume>Cet article présente des expériences récentes menées dans le cadre d’un projet de recherche consacré à l’étude de métaphores conceptuelles. Ces expériences consistent à appréhender visuellement la répartition de trois domaines pouvant être à l’origine de métaphores conceptuelles dans un corpus d’articles boursiers. Les trois domaines étudiés sont la météorologie, la guerre et la santé, un grand nombre d’emplois métaphoriques du lexique de ces trois domaines ayant été observés dans le corpus d’étude. Afin de visualiser la répartition de ces domaines en corpus, nous exploitons la plate-forme ProxiDocs dédiée à la cartographie et à la catégorisation de corpus. Les cartes construites à partir du corpus et des domaines d’étude nous ont ainsi permis de localiser certaines métaphores conceptuelles dans des articles et des groupes d’articles du corpus. Des articles contenant des emplois non métaphoriques des domaines étudiés ont également été distingués sur les cartes. Des représentations cartographiques du corpus mettant dynamiquement en évidence l’évolution des trois domaines d’étude au fil du temps nous ont permis d’amorcer une étude sur le lien entre la présence de certaines métaphores conceptuelles et des faits d’actualité.</resume>
			<mots_cles>métaphores conceptuelles, visualisation de corpus, linguistique de corpus</mots_cles>
			<title></title>
			<abstract>This article presents the results of recent experiments achieved within the framework of a research project dedicated to the study of conceptual metaphors. These experiments comprise in a visual study of the distribution of three source domains of conceptual metaphors in a corpus of stock exchange articles. The three domains studied are meteorology, war and health, a large number of metaphorical uses of the lexicon of these three domains has been noted in the corpus. In order to visualize the distribution of these domains in the corpus, we use the ProxiDocs cartography tool and categorization. Maps of the corpus allow us to locate some conceptual metaphors in articles and sets of articles in the corpus. Articles containing non-metaphorical uses of studied domains are also differentiated on maps. Graphic representations of the corpus showing the temporal evolution of the three domains allow us to study the bond between some conceptual metaphors and the topic.</abstract>
			<keywords>conceptual metaphors, corpus visualization, corpus linguistics</keywords>
		</article>
		<article id="taln-2006-poster-023" session="">
			<auteurs>
				<auteur>
					<prenom>Fatiha</prenom>
					<nom>Sadat</nom>
					<email>fatiha.sadat@cnrc-nrc.gc.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>George</prenom>
					<nom>Foster</nom>
					<email>george.foster@cnrc-nrc.gc.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Roland</prenom>
					<nom>Kuhn</nom>
					<email>roland.kuhn@cnrc-nrc.gc.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut de Technologie de l’Information, Québec</affiliation>
			</affiliations>
			<titre>Système de traduction automatique statistique combinant différentes ressources</titre>
			<type>poster</type>
			<pages>590-599</pages>
			<resume>Cet article décrit une approche combinant différents modèles statistiques pour la traduction automatique basée sur les segments. Pour ce faire, différentes ressources sont utilisées, dont deux corpus parallèles aux caractéristiques différentes et un dictionnaire de terminologie bilingue et ce, afin d’améliorer la performance quantitative et qualitative du système de traduction. Nous évaluons notre approche sur la paire de langues français-anglais et montrons comment la combinaison des ressources proposées améliore de façon significative les résultats.</resume>
			<mots_cles>traduction automatique statistique basée sur les segments, corpus parallèle, dictionnaire de terminologie bilingue</mots_cles>
			<title></title>
			<abstract>This paper describes an approach combining different statistical models for phrase-based machine translation. Different knowledge resources are used, such as two parallel corpora with different characteristics and a bilingual dictionary of terminology, in order to improve the qualitative and quantitative performance of the translation system. We evaluate our approach on the French-English language pair and show how combining the proposed resources significantly improves results.</abstract>
			<keywords>statistical phrase-based machine translation, parallel corpora, bilingual dictionary of terminology</keywords>
		</article>
		<article id="taln-2006-poster-024" session="">
			<auteurs>
				<auteur>
					<prenom>Javier M.</prenom>
					<nom>Sastre Martínez</nom>
					<email>sastre@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Marne-la-Vallée – IGM</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>600-608</pages>
			<resume>Le lexique grammaire est une méthode systématique d’analyse et de représentation des structures de phrase élémentaire d’une langue naturelle ; son produit : des grandes collections de dictionnaires syntaxiques électroniques ou tables de lexique-grammaire (LGTs). Du travail collaboratif à très long terme est nécessaire pour achever la description d’une langue. Cependant, les outils informatiques de gestion de LGTs actuels ne remplissent pas les besoins suivant : intégration automatique de données multisource, controle de cohérence de données et de versions, filtrage et tri, formats d’échange, gestion couplée des données et de la documentation, interfaces graphiques (GUIs) dédiées et gestion d’utilisateurs et contrôle d’accès. Dans cet article nous proposons une solution basée sur PostgreSQL et/ou MySQL (systèmes de gestion de bases de données libres), Swing (une librairie pour la programmation de GUIs en Java), JDBC (API pour la connectivité de Java aux bases de données), et StAX (API pour l’analyse et la création des documents en XML).</resume>
			<mots_cles>table de lexique-grammaire, base de données, interface graphique, XML</mots_cles>
			<title>Computer Tools for the Management of Lexicon-Grammar Databases</title>
			<abstract>Lexicon grammar is a systematic method for the analysis and the representation of the elementary sentence structures of a natural language producing large collections of syntactic electronic dictionaries or lexicongrammar tables (LGTs). In order to describe a language, very long term collaborative work is required. However, the current computer tools for the management of LGTs do not fulfill key requirements including automatic integration of multisource data, data coherence and version control, filtering and sorting, exchange formats, coupled management of data and documentation, dedicated graphical interfaces (GUIs) and user management and access control. In this paper we propose a solution based on PostgreSQL and/or MySQL (open source database management systems), Swing (a GUI toolkit for Java), JDBC (the API for Java database connectivity) and StAX (an API for the analysis and generation of XML documents).</abstract>
			<keywords>Lexicon-grammar table, database, graphic interface, XML</keywords>
		</article>
		<article id="taln-2006-poster-025" session="">
			<auteurs>
				<auteur>
					<prenom>Djamé</prenom>
					<nom>Seddah</nom>
					<email>djame.seddah@computing.dcu.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Dublin City University, National Centre for Language Technology</affiliation>
				<affiliation affiliationId="2">Projet ATOLL - INRIA</affiliation>
			</affiliations>
			<titre>Modélisation et analyse des coordinations elliptiques par l’exploitation dynamique des forêts de dérivation</titre>
			<type>poster</type>
			<pages>609-618</pages>
			<resume>Nous présentons dans cet article une approche générale pour la modélisation et l’analyse syntaxique des coordinations elliptiques. Nous montrons que les lexèmes élidés peuvent être remplacés, au cours de l’analyse, par des informations qui proviennent de l’autre membre de la coordination, utilisé comme guide au niveau des dérivations. De plus, nous montrons comment cette approche peut être effectivement mise en oeuvre par une légère extension des Grammaires d’Arbres Adjoints Lexicalisées (LTAG) à travers une opération dite de fusion. Nous décrivons les algorithmes de dérivation nécessaires pour l’analyse de constructions coordonnées pouvant comporter un nombre quelconque d’ellipses.</resume>
			<mots_cles>analyse syntaxique, TAG, coordination, ellipses, forêt partagée, forêt de dérivation</mots_cles>
			<title></title>
			<abstract>In this paper, we introduce a generic approach to elliptic coordination modelization and parsing.We show that the erased lexical items can be replaced during parsing, by information gathered from the other member of the coordination, used as a guide at the derivation level. Moreover, we show how this approach can indeed be implemented as a light extension of the LTAG formalism through a fusion operation.We provide the derivation algorithms required to parse coordination constructions which can have an arbitrary number of elisions.</abstract>
			<keywords>parsing, TAG, coordination, ellipsis, shared forest, derivation forest</keywords>
		</article>
		<article id="taln-2006-poster-026" session="">
			<auteurs>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Vaillant</nom>
					<email>pvaillan@martinique.univ-ag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Nock</nom>
					<email>rnock@martinique.univ-ag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claudia</prenom>
					<nom>Henry</nom>
					<email>chenry@martinique.univ-ag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université des Antilles-Guyane, GRIMAAG</affiliation>
				<affiliation affiliationId="2">Université des Antilles-Guyane, GEREC-F</affiliation>
			</affiliations>
			<titre>Analyse spectrale des textes : détection automatique des frontières de langue et de discours</titre>
			<type>poster</type>
			<pages>619-629</pages>
			<resume>Nous proposons un cadre théorique qui permet, à partir de matrices construites sur la base des données statistiques d’un corpus, d’extraire par des procédés mathématiques simples des informations sur les mots du vocabulaire de ce corpus, et sur la syntaxe des langues qui l’ont engendré. À partir des mêmes données initiales, on peut construire une matrice de similarité syntagmatique (probabilités de transition d’un mot à un autre), ou une matrice de similarité paradigmatique (probabilité de partager des contextes identiques). Pour ce qui concerne la première de ces deux possibilités, les résultats obtenus sont interprétés dans le cadre d’une modélisation du processus génératif par chaînes de Markov. Nous montrons que les résultats d’une analyse spectrale de la matrice de transition peuvent être interprétés comme des probabilités d’appartenance de mots à des classes. Cette méthode nous permet d’obtenir une classification continue des mots du vocabulaire dans des sous-systèmes génératifs contribuant à la génération de textes composites. Une application pratique est la segmentation de textes hétérogènes en segments homogènes d’un point de vue linguistique, notamment dans le cas de langues proches par le degré de recouvrement de leurs vocabulaires.</resume>
			<mots_cles>classification spectrale continue, segmentation de textes, identification de langue</mots_cles>
			<title></title>
			<abstract>We propose a theoretical framework within which information on the vocabulary of a given corpus can be inferred on the basis of statistical information gathered on that corpus. Inferences can be made on the categories of the words in the vocabulary, and on their syntactic properties within particular languages. Based on the same statistical data, it is possible to build matrices of syntagmatic similarity (bigram transition matrices) or paradigmatic similarity (probability for any pair of words to share common contexts). When clustered with respect to their syntagmatic similarity, words tend to group into sublanguage vocabularies, and when clustered with respect to their paradigmatic similarity, into syntactic or semantic classes. Experiments have explored the first of these two possibilities. Their results are interpreted in the frame of a Markov chain modelling of the corpus’ generative processe(s).We show that the results of a spectral analysis of the transition matrix can be interpreted as probability distributions of words within clusters. This method yields a soft clustering of the vocabulary into sublanguages which contribute to the generation of heterogeneous corpora. As an application, we show how multilingual texts can be visually segmented into linguistically homogeneous segments. Our method is specifically useful in the case of related languages which happen to be mixed in corpora.</abstract>
			<keywords>soft spectral clustering, text segmentation, language identification</keywords>
		</article>
		<article id="taln-2006-poster-027" session="">
			<auteurs>
				<auteur>
					<prenom>Tonio</prenom>
					<nom>Wandmacher</nom>
					<email>tonio.wandmacher@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Yves</prenom>
					<nom>Antoine</nom>
					<email>jean-yves.antoine@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François-Rabelais de Tours – Laboratoire d’Informatique</affiliation>
			</affiliations>
			<titre>Adaptation de modèles de langage à l’utilisateur et au registre de langage : expérimentations dans le domaine de l’aide au handicap</titre>
			<type>poster</type>
			<pages>630-639</pages>
			<resume>Les modèles markoviens de langage sont très dépendants des données d’entraînement sur lesquels ils sont appris. Cette dépendance, qui rend difficile l’interprétation des performances, a surtout un fort impact sur l’adaptation à chaque utilisateur de ces modèles. Cette question a déjà été largement étudiée par le passé. En nous appuyant sur un domaine d’application spécifique (prédiction de texte pour l’aide à la communication pour personnes handicapées), nous voudrions l’étendre à la problématique de l’influence du registre de langage. En considérant des corpus relevant de cinq genres différents, nous avons étudié la réduction de cette influence par trois modèles adaptatifs différents : (a) un modèle cache classique favorisant les n derniers mots rencontrés, (b) l’intégration au modèle d’un dictionnaire dynamique de l’utilisateur et enfin (c) un modèle de langage interpolé combinant un modèle général et un modèle utilisateur mis à jour dynamiquement au fil des saisies. Cette évaluation porte un système de prédiction de texte basé sur un modèle trigramme.</resume>
			<mots_cles>modèles de langage, adaptation, utilisateur, thème, aide au handicap</mots_cles>
			<title></title>
			<abstract>Statistical language models (LM) are highly dependent on their training resources. This makes it not only difficult to interpret evaluation results, it also has a strong impact on users of a LM-based application. This question has already been studied by others. Focussing on a specific domain (text prediction in a communication aid for handicapped persons) we want to extend it to the influence of the language register. Considering corpora from five different registers, we discuss three methods to adapt a language model to its actual language resource hereby reducing the effect of training dependency: (a) a simple cache model augmenting the probability of the n last inserted words, (b) a dynamic user dictionary, keeping every unseen word and (c) an interpolated LM combining a base model with a currently updated user model. Our evaluation is based on the results obtained from a text prediction system working on a trigram LM.</abstract>
			<keywords>language model, dynamic adaptation, user, theme, AAC systems</keywords>
		</article>
		<article id="taln-2006-poster-028" session="">
			<auteurs>
				<auteur>
					<prenom>Anis</prenom>
					<nom>Zouaghi</nom>
					<email>anis.zouaghi@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mounir</prenom>
					<nom>Zrigui</nom>
					<email>mounir.zrigui@fsm.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Ben Ahmed</nom>
					<email>mohamed.benahmed@riadi.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Monastir – RIADI</affiliation>
				<affiliation affiliationId="2">Université de la Mannouba – RIADI</affiliation>
			</affiliations>
			<titre>L’influence du contexte sur la compréhension de la parole arabe spontanée</titre>
			<type>poster</type>
			<pages>640-648</pages>
			<resume>Notre travail s’intègre dans le cadre du projet intitulé « Oréodule » : un système de reconnaissance, de traduction et de synthèse de la langue arabe. L’objectif de cet article est d’essayer d’améliorer le modèle probabiliste sur lequel est basé notre décodeur sémantique de la parole arabe spontanée. Pour atteindre cet objectif, nous avons décidé de tester l’influence de l’utilisation du contexte pertinent, et de l’intégration de différents types de données contextuelles sur la performance du décodeur sémantique employé. Les résultats sont satisfaisants.</resume>
			<mots_cles>analyse sémantique, modèle probabiliste, extraction automatique, contexte pertinent, information mutuelle moyenne</mots_cles>
			<title></title>
			<abstract>This work is part of a larger research project entitled « Oréodule » aiming to develop tools for automatic speech recognition, translation, and synthesis for the Arabic language. The core of our interest in this work is in improving the probabilistic model on which our semantic decoder rests. To achieve this goal, we tested the influence of the pertinent context use, and of the contextual data integration of different types, on the effectiveness of the semantic decoder. The results are satisfactory.</abstract>
			<keywords>semantic analysis, probabilistic model, automatic extraction, pertinent context, overage mutual information</keywords>
		</article>
		<article id="taln-2006-tutoriel-001" session="">
			<auteurs>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Lepage</nom>
					<email>yves.lepage@atr.jp</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATR, Keihanna</affiliation>
			</affiliations>
			<titre>Analogie en traitement automatique des langues. Application à la traduction automatique</titre>
			<type>tutoriel</type>
			<pages>781-792</pages>
			<resume>On se place ici dans la tendance actuelle en traitement automatique des langues, celle à base de corpus et aussi dans une perspective que l’on peut qualifier d’approche à moindre effort : il s’agit d’examiner les limites des possibilités de traitement à partir de données textuelles brutes, c’est-à-dire non pré-traitées. L’interrogation théorique présente en arrière-plan est la suivante : quelles sont les opérations fondamentales en langue ? L’analogie proportionnelle a été mentionnée par de nombreux grammairiens et linguistes. On se propose de montrer l’efficacité d’une telle opération en la testant sur une tâche dure du traitement automatique des langues : la traduction automatique. On montrera aussi les bonnes conséquences de la formalisation d’une telle opération avec des résultats théoriques en théorie des langages en relation avec leur adéquation à la description des langues. De cette façon, une opération fondamentale en langue, l’analogie proportionnelle, se verra illustrée tant par ses aspects théoriques que par ses performances en pratique.</resume>
			<mots_cles>Analogie, proportion, chaîness de symboles, traduction automatique,divergences entre langues</mots_cles>
			<title></title>
			<abstract>We position ourselves in the current trend of Natural Language Processing, i.e., corpus-based approach and “least effort” approach.We shall inquire how far it is possible to go without any preprocessing of raw data. The theoretical question in the background is: which operations are fundamental in language? Proportional analogy has been mentionned by many grammarians and linguists. We shall inspect the efficiency of such an operation by testing it against a difficult task of NLP: machine translation. We shall also show some good properties brought by the formalisation of such an operation with theoretical results in formal language theory and the adequacy of analogy with the description of natural languages. To summarize, we shall illustrate an operation that is fundamental in language in its theoretical aspects as well as in its practical peformance.</abstract>
			<keywords>Analogy, proportion, strings of symbols, machine translation,divergences across languages</keywords>
		</article>
	</articles>
</conference>
