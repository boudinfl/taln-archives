
Vers l’évaluation de systèmes de dialogue homme-machine :
de l’oral au multimodal

Frédéric Landragin

CNRS – Laboratoire LaTTICe (UMR 8094)
1 rue Maurice Arnoux – 92120 Montrouge
frederic.landragin@linguist.jussieu.fr
Résumé        L’évaluation pour le dialogue homme-machine ne se caractérise pas par
l’efficacité, l’objectivité et le consensus que l’on observe dans d’autres domaines du
traitement automatique des langues. Les systèmes de dialogue oraux et multimodaux restent
cantonnés à des domaines applicatifs restreints, ce qui rend difficiles les évaluations
comparatives ou normées. De plus, les avancées technologiques constantes rendent vite
obsolètes les paradigmes d’évaluation et ont pour conséquence une multiplication de ceux-ci.
Des solutions restent ainsi à trouver pour améliorer les méthodes existantes et permettre des
diagnostics plus automatisés des systèmes. Cet article se veut un ensemble de réflexions
autour de l’évaluation de la multimodalité dans les systèmes à forte composante linguistique.
Des extensions des paradigmes existants sont proposées, en particulier DQR/DCR, sachant
que certains sont mieux adaptés que d’autres au dialogue multimodal. Des conclusions et
perspectives sont tirées sur l’avenir de l’évaluation pour le dialogue homme-machine.

Abstract      Evaluating human-machine dialogue systems is not so efficient, objective, and
consensual than evaluating other natural language processing systems. Oral and multimodal
dialogue systems are still working within reduced applicative domains. Comparative and
normative evaluations are then difficult. Moreover, the continuous technological progress
makes obsolete and numerous the evaluating paradigms. Some solutions are still to be
identified to improve existing methods and to allow a more automatic diagnosis of systems.
The aim of this paper is to provide a set of remarks dealing with the evaluation of multimodal
spoken language dialogue systems. Some extensions of existing paradigms are presented, in
particular DQR/DCR, considering that some paradigms fit better multimodal issues than
others. Some conclusions and perspectives are then drawn on the future of the evaluation of
human-machine dialogue systems.
Mots-clés :        Dialogue finalisé, multimodalité, évaluation pour le dialogue homme-
machine, paradigme d’évaluation, test utilisateur, diagnostic, paraphrase multimodale
Keywords: Task-driven dialogue, multimodality, evaluating human-machine dialogue,
evaluation paradigm, user test, diagnosis
Frédéric Landragin
1 Introduction
La communauté internationale voit paraître un nombre grandissant d’articles sur l’évaluation
des systèmes de dialogue oraux et multimodaux. Outre les livres de référence en dialogue
homme-machine qui incluent désormais systématiquement un chapitre sur l’évaluation
(Gibbon et al., 2000 ; puis López-Cózar, Araki, 2005), on trouve un grand nombre de
propositions ciblées sur le dialogue oral ou sur une modalité particulière d’un système
multimodal. Des paradigmes d’évaluation sont proposés, de plus en plus larges et complexes,
regroupant entre autres des ensembles de métriques, des tests utilisateurs et des méthodes
d’analyse de questionnaires remplis par des sujets après leur utilisation du système à évaluer.
Dans la communauté française, les propositions se cantonnent pour le moment à l’oral ou à
certains aspects du dialogue multimodal comme le comportement d’agents animés, et il n’y a
pas encore de chapitre sur l’évaluation de la multimodalité dans des ouvrages tels que
(Chaudiron, 2004) ou (Caelen, Xuereb, 2007). Ces efforts sont pertinents et louables, mais ne
doivent pas faire oublier plusieurs constats récurrents qui restent particulièrement valables.

Premier constat : contrairement aux systèmes de recherche d’information, de reconnaissance
de la parole ou d’analyse syntaxique, les systèmes de dialogue homme-machine restent
toujours au niveau de prototypes de recherche difficiles à réaliser et à faire fonctionner, ainsi
que très sensibles au comportement des utilisateurs. A part quelques exemples marginaux,
ludiques par exemple, il n’existe à l’heure actuelle aucun système fiable commercialisé et
utilisé de manière profitable par une population de taille importante. Autrement dit, le passage
à l’échelle reste un problème majeur en dialogue homme-machine et les évaluations réalisées
s’en tiennent à des prototypes de recherche ou à des systèmes professionnels tellement
finalisés (militaires par exemple) qu’ils ne s’adressent qu’à un nombre extrêmement réduit
d’utilisateurs. Ce qui est valable pour les systèmes oraux l’est encore plus pour les systèmes
multimodaux. L’évaluation pour le dialogue homme-machine se cantonne donc à un périmètre
limité qui, sans remettre en cause son utilité, nuance quelque peu sa portée.

Deuxième constat : il risque d’exister bientôt autant de méthodologies d’évaluation que de
systèmes proprement dits. Ce n’est pas un problème en soi, mais cela soulève des
interrogations. En particulier, on peut s’interroger sur le bien-fondé d’une méthode
d’évaluation proposée par les concepteurs d’un système dans le but d’évaluer ce seul système,
la méthode étant elle-même évaluée par son application au système en question. Cette
description peut sembler caricaturale, elle reflète pourtant une certaine réalité, ou en tout cas
elle s’en approche. Cette situation est tout à fait normale et inévitable, compte tenu du nombre
réduit de systèmes et, face aux avancées de chaque système, de la nécessité de prendre en
compte des aspects qui ne sont pas traités par les méthodologies d’évaluation existantes.
Ainsi, on en vient à proposer ou à étendre une méthodologie d’évaluation en vue de pouvoir
évaluer les avancées d’un nouveau système. Les avancées technologiques rapides ne font
qu’augmenter ce phénomène. L’évaluation pour le dialogue homme-machine semble ainsi
perpétuellement en retard par rapport à son objectif.

Troisième constat : l’évaluation sert non seulement à améliorer le développement d’un
système particulier (en passant par des mesures, des diagnostics et des questionnaires de
satisfaction), mais aussi à comparer des systèmes les uns par rapport aux autres. Plusieurs
campagnes ont été lancées, et ce qu’il en ressort finalement, c’est qu’il est très difficile de
comparer plusieurs systèmes de dialogue, même s’ils ont été réalisés pour des contextes
applicatifs comparables, par exemple le renseignement ferroviaire ou hôtelier pour ne citer
que ces deux applications largement exploitées. Pour cet aspect, le domaine du dialogue
Vers l’évaluation des systèmes de dialogue multimodaux

homme-machine semble poser un problème plus délicat que les autres domaines du TAL, et
contribue à l’image de fragilité attachée à son évaluation.

Face à ces constats, on peut s’interroger sur la faisabilité de l’évaluation pour le dialogue
homme-machine. Dans ce but, la section 2 passera en revue les principaux problèmes et
quelques méthodologies qui nous semblent prometteuses. La question de la faisabilité nous
semble constituer un problème de fond qui n’est pas assez discuté et pour lequel nous
essaierons d’apporter quelques pistes de réflexion. Les critiques que nous venons de porter
avec les constats précédents ne nous empêcheront pas, dans un second temps, de proposer des
pistes pour une meilleure prise en compte de la multimodalité dans des paradigmes existants.
La section 3 s’attachera ainsi à faire un point sur les possibilités d’extension à la
multimodalité des méthodologies prévues pour l’oral, en particulier les méthodes DQR et
DCR, et sur la prise en compte de phénomènes pour l’instant ignorés dans les méthodologies
déjà prévues pour la multimodalité. A défaut d’évaluer nos propositions sur des systèmes
existants ou sur de nouveaux systèmes (c’est l’une des perspectives de ce travail), nous les
expliciterons sur des exemples typiques tels que le classique « mets ça ici » (Bolt, 1980).
2 Méthodologies existantes
Dans le contexte du dialogue homme-machine oral, beaucoup de méthodologies ont été
proposées (Antoine, Caelen, 1999 ; Bonneau-Maynard et al., 2006 ; Devillers et al., 2004 ;
Dybkjær et al., 1998 ; Eckert et al., 1998 ; Litman, Pan, 1999 ; Möller et al., 2007). Elles
constituent une sorte de cadre de référence comprenant des recommandations pour mettre en
œuvre des tests d’interaction avec des utilisateurs, des méthodes pour analyser
automatiquement ou semi-automatiquement les traces d’interaction obtenues, des repères pour
déterminer des métriques d’évaluation, ou encore des principes pour constituer et analyser des
questionnaires remplis a posteriori par les utilisateurs. Chaque évaluateur peut ainsi piocher
dans ce stock pour déterminer la ou les méthodes qu’il va appliquer. En fait, un seul test
semble insuffisant et une véritable évaluation semble devoir grouper plusieurs types de test.
Les campagnes d’évaluation (EVALDA/MEDIA), les groupes de travail (groupe MADCOW,
groupe « compréhension de parole » du GdR I3) et les divers consortiums de projets
européens exploitent largement ce principe. Lorsque plusieurs systèmes sont en jeu et que
l’évaluation est comparative, des règles de fonctionnement peuvent être définies de manière à
mieux contrôler la qualité de l’évaluation. La campagne d’évaluation par défi avec sa gestion
croisée des rôles des concepteurs des systèmes en jeu (Antoine, 2003) en est un exemple.

Les principales propositions de méthodologie s’accompagnent chacune d’une idée originale
qui vient simplifier la mise en œuvre d’un type de test en lui apportant un moyen d’être
opérationnalisé dans un contexte déterminé. Le paradigme du groupe MADCOW (Hirschman,
1992) apporte ainsi la notion de gabarit qui caractérise les solutions minimales et maximales à
une requête et rend ainsi son évaluation plus rigoureuse. Le paradigme PARADISE –
PARAdigm for DIalogue System Evaluation (Walker et al., 2001) se focalise sur la
maximisation de la satisfaction de l’utilisateur et propose de prendre la satisfaction de la tâche
comme référence. Autre exemple d’idée originale, (López-Cózar et al., 2003) propose
d’évaluer un système en générant automatiquement des énoncés utilisateurs de test, c’est-à-
dire en modélisant le comportement de l’utilisateur, y compris ses erreurs. En France, la
méthodologie DQR – Donnée–Question–Réponse (Zeiliger et al., 1997) introduit le principe
de questionner le système sur le point à évaluer, avec l’avantage de déplacer ainsi l’objet de
l’évaluation de la donnée vers la question, et donc ni sur les réponses ou réactions du système
(méthode « boîte noire », qui ne nécessite pas d’explorer les structures internes au système,
mais qui manque de précision), ni sur les structures sémantiques du système (méthode « boîte
Frédéric Landragin

transparente », précise et conduisant facilement à un diagnostic, mais qui nécessite de disposer
de représentations sémantiques de référence). Encore faut-il que le système soit capable de
répondre aux questions Q de DQR. Le paradigme adapté DCR – Demande–Contrôle–
Réponse/Résultat/Référence (Antoine, Caelen, 1999) minimise ce problème en remplaçant la
question par un contrôle qui est une simplification ou une reformulation de la demande
utilisateur initiale. Pour sa part, le paradigme PEACE – Paradigme d’Evaluation Automatique
de ComprEhension (Devillers et al., 2002) apporte l’idée originale de modéliser l’historique
du dialogue par une paraphrase, ce qui permet de rester dans le mode « boîte noire » tout en
permettant une évaluation de la compréhension en contexte.

Dans le contexte du dialogue homme-machine multimodal, les propositions sont loin d’être
aussi pertinentes. Le paradigme PROMISE – PROcedure for Multimodal Interactive System
Evaluation (Beringer et al., 2002) est présenté comme une extension de PARADISE à la
multimodalité, avec des principes pour affecter des scores aux entrées et sorties multimodales.
La proposition reste en fait à un niveau très approximatif, bien en deçà de la variété des
phénomènes multimodaux. Les aspects intéressants de l’article concernent le dialogue oral,
avec des considérations sur le niveau de complétude de la tâche et le niveau de coopération de
l’utilisateur. Les travaux de Bernsen et Dybkjær, qui font pourtant référence dans le milieu du
dialogue multimodal, sont plutôt décevants en ce qui concerne l’évaluation. (Bernsen,
Dybkjær, 2004) présente ainsi une méthodologie prévue pour un système, avec une
focalisation sur la méthode du questionnaire rempli a posteriori par les utilisateurs (la raison
donnée est d’ailleurs que les autres méthodes ne sont pas encore bien établies).
Malheureusement, les questions du questionnaire restent à un niveau très superficiel pour ce
qui concerne la multimodalité : « avez-vous utilisé la souris ou avez-vous pointé sur
l’écran ? », « quelles étaient vos impressions en produisant un geste ? », et « auriez-vous aimé
en faire plus avec le geste ? si oui, pour faire quoi ? ». Les réponses qui ont été fournies par
les utilisateurs semblent également très pauvres, d’autant plus qu’une des conclusions des
auteurs est que les utilisateurs ont préféré parler plutôt qu’exploiter les possibilités
multimodales… Pour sa part, (Dybkjær et al., 2004) est plus une revue de méthodologies et de
projets qu’une proposition de nouvelle méthodologie pour la multimodalité : le propos reste
au niveau de recommandations très générales. Par ailleurs, une des remarques finales de
l’article rejoint notre point de vue : « The field is moving rapidly beyond the standard task-
oriented, speech-only SLDS [Spoken Language Dialogue System] towards multimodal
SLDSs, mobile systems, situation-aware systems, location-aware systems, internet access
systems, educational systems, entertainment systems, etc. In fact, technology development
may appear to speed further ahead of the knowledge we already have on evaluation, usability
and standards, increasing the proportion of what we do not know compared with what we do
know. ». Dans un autre registre, (Vuurpijl et al., 2004) présente un outil, appelé « µeval »,
pour la transcription des données multimodales et l’évaluation d’un système. Or l’évaluation
ne concerne que les tours de dialogue et ne traite pas les phénomènes multimodaux. Enfin,
(Walker et al., 2004), malgré son titre, se focalise sur les modèles utilisateur et les stratégies
de dialogue (oral) mais quasiment pas sur les aspects multimodaux. D’une manière générale
pour l’évaluation des systèmes multimodaux, on ne retrouve donc pas les principes appliqués
dans les campagnes d’évaluation des systèmes oraux. C’est ce que nous allons contribuer à
faire en nous focalisant sur les méthodologies qui nous semblent les plus prometteuses.
3 Extension au dialogue multimodal
(Zeiliger et al., 2000) ont retenu une méthodologie de type « boîte noire » permettant de faire
un diagnostic du système, méthodologie qui repose sur des tests génériques pour l’évaluation
de la compréhension d’un énoncé isolé. Les aspects contextuels ont été négligés (nous y
Vers l’évaluation des systèmes de dialogue multimodaux

reviendrons avec PEACE), mais c’était le prix à payer pour obtenir une méthodologie simple
et bien délimitée. Le principe est de procéder à des évaluations ponctuelles, chacune d’entre
elles étant centrée sur un phénomène particulier. Ainsi, dans la matérialisation DQR,
l’évaluation ponctuelle prend la forme d’une question Q adressée au système et permet de
vérifier sa bonne compréhension de la demande initiale D. Un des exemples donnés concerne
la résolution des anaphores, avec la demande, la question et la réponse suivantes :

•     D = « Vous prenez la rue à droite et vous la suivez sur 300 mètres » (énoncé initial,
tel qu’il a été adressé au système dans le but de faire avancer la tâche) ;

•     Q = « Suivre rue à droite ? » (question adressée au système juste après l’énoncé D et
destinée à évaluer la bonne compréhension de D) ;

•     R = « oui » (réponse du système montrant que l’anaphore a été bien comprise et
rendant l’évaluation positive).

Les auteurs spécifient sept niveaux caractérisant la portée des questions posées. Nous allons
reprendre ces niveaux en indiquant à chaque fois comment étendre le paradigme pour pouvoir
l’exploiter en dialogue multimodal.
3.1 DQR multimodal
Niveau 1 = « information explicite ». Il s’agit du repérage d’une information explicitée dans
l’énoncé, l’intérêt étant de tester la bonne compréhension de l’énoncé littéral compte tenu de
la grande variabilité du langage spontané. Les exemples donnés par les auteurs se contentent
de reprendre une partie de l’énoncé et de demander une confirmation de la compréhension de
cette partie : D = « vous prenez à droite après les bâtiments blancs aux volets bleus » puis
Q = « volets blancs ? » ou « volets bleus ? ». L’extension de ce principe à la multimodalité
consiste à poser des questions sur les éléments de l’énoncé multimodal. Avec D = « mets ça
ici » + geste en (x1, y1) + geste en (x2, y2), on peut tester les capacités de capture de la
multimodalité en posant les questions Q suivantes : « ça ? » + geste en (x1, y1) ; « mettre
ici ? » + geste en (x2, y2) ; « mettre ça ? » + geste en (x2, y2) ; « mettre ça ici » + geste en (x2,
y2) + geste en (x1, y1) ; etc. La procédure peut sembler naïve, mais elle permet de tester de
manière très simple le bon appariement des gestes avec les expressions référentielles, ce qui
constitue un processus non négligeable de la fusion multimodale. Une attention particulière
sera donnée à la synchronisation temporelle entre les mots prononcés et les gestes produits.
Ainsi, un décalage temporel entre « ça » et le geste dans la question Q pourra conduire selon
le système à une réponse positive reflétant sa robustesse pour l’appariement multimodal
même quand les conditions de production sont déviantes, ou au contraire à une réponse
négative reflétant l’incapacité du système à sortir d’un certain intervalle temporel.

Niveau 2 = « information implicite ». Ce niveau concerne la résolution des anaphores, des
ellipses, des incomplétudes et autres informations implicites mais récupérables aux niveaux
syntaxique et sémantique. Un exemple fait intervenir : D = « donnez-moi un billet pour Paris
et aussi pour Lyon » et Q = « billet pour Lyon ? ». La résolution de la référence étant l’un des
principaux aspects de la multimodalité spontanée, un DQR multimodal devra bien entendu en
rendre compte. Ainsi, en reprenant comme D la primitive universelle de la multimodalité,
« mets ça ici » avec deux gestes de désignation, les questions Q pourront introduire des
précisions sur les référents, en partant par exemple de la mention de leur catégorie et en allant
jusqu’à donner leur identifiant unique tel qu’il est géré par le système : « mettre cet objet ? » +
geste en (x1, y1) ; « mettre ce fichier ? » + geste en (x1, y1) ; « mettre ‘submis.tex’ ? » (sans
Frédéric Landragin

geste) ; « mettre obj4353 ? » (sans geste) ; etc. La procédure d’évaluation inclut donc la
paraphrase en langage naturel d’une référence multimodale. Ce qui reste simple pour le geste
déictique l’est beaucoup moins pour les autres types de gestes co-verbaux. Imaginons par
exemple que « mets ça ici », ou plutôt « déplace ça ici » pour ne pas trop compliquer
l’exemple1, s’accompagne d’un seul geste qui part de l’objet à déplacer et aboutit au lieu de
destination. Selon une première hypothèse, cette trajectoire gestuelle est considérée comme la
matérialisation de la nécessaire transition entre la désignation d’objet et la désignation de lieu.
Dans ce cas, seules les extrémités de la courbe sont utilisées lors des analyses sémantiques : le
point (x1, y1) puis l’objet présent en ce point ou dans un voisinage immédiat sont unifiés avec
« ça », et le point (x2, y2) est unifié avec « ici ». Autrement dit on revient au cas précédent.
Selon une seconde hypothèse, la trajectoire est considérée comme la combinaison de ces deux
désignations avec un geste co-verbal illustratif apportant une caractéristique de l’action de
déplacement, à savoir le chemin (ou points de passage) à suivre. La trajectoire est alors
analysée d’un point de vue temporel (courbe produite de manière régulière, sans point d’arrêt
significatif) et d’un point de vue structurel (arc de cercle), avant d’être unifiée à « déplace »,
c’est-à-dire d’être interprétée comme un chemin de déplacement. Si l’on veut tester cette
fonctionnalité du système multimodal, il suffit de poser une question Q supplémentaire :
« suivre cette trajectoire ? » ou « déplacer selon ces points de passage ? », en reprenant dans
un cas comme dans l’autre le geste complet. Le seul inconvénient reste celui qui est valable
pour l’ensemble de la méthodologie DQR, à savoir la nécessité pour le système de traiter de
telles questions.

Niveau 3 = « inférence ». Il s’agit ici de la construction du sens complet de l’énoncé, la
difficulté étant l’identification des sous-entendus, identification qui fait appel à des
raisonnements de sens commun et à des inférences pragmatiques. Avec D : « je voudrais un
aller-retour pour Paris », les auteurs proposent Q : « vouloir billet ? ». Cet aspect est
indépendant des modalités, et reste valable dans l’état pour le dialogue multimodal. Même si
des inférences peuvent être identifiées lors de l’utilisation consécutive de plusieurs gestes, il
est vrai que ce niveau concerne surtout la langue orale.

Niveau 4 = « interprétation du type d’acte illocutoire ». On entre ici dans les niveaux de
dialogue, avec un premier aspect concernant les actes de langage et la capacité du système à
identifier le bon type d’acte, même en cas d’acte de langage indirect. Avec D : « un billet pour
Paris », qui peut faire suite à une question ou qui peut correspondre à une demande initiale, la
question Q : « est-ce une demande ? » permet d’évaluer l’acte identifié par le système. Il est
nécessaire ici de distinguer deux types de dialogue multimodal. Dans le premier type, les
gestes et les autres modalités de communication restent co-verbaux, c’est-à-dire que
l’information qu’ils apportent s’ajoute à celle portée par l’énoncé en langage naturel, et l’acte
de langage de l’énoncé multimodal est celui de l’énoncé oral. C’est le cas des exemples
étudiés jusqu’à présent avec « mets ça ici » et « déplace ça ici ». Un autre type de dialogue
multimodal autorise des gestes quasi-linguistiques ou, d’une manière générale, un message
effectué avec une modalité autre que le langage naturel et portant en lui-même un acte de
communication (ou acte de dialogue), similaire à un acte de langage. C’est le cas par exemple
lorsque l’on étend une interface graphique et que l’on autorise des gestes ayant des formes
telles qu’une croix ou une flèche, chaque forme étant associée à un déclenchement d’action.
La croix est un équivalent du « supprimer » en langage naturel et prend comme argument

1
La différence relève de la résolution de la référence aux actions : « déplace » ne peut référer qu’à une action de
déplacement, alors que « mets » peut référer soit à une action de déplacement, soit à une action de création.
Les considérations suivantes concernent l’existence de plusieurs primitives pour « déplace » :
‘déplacer(objet,lieu)’ et ‘déplacer(objet,lieu,chemin)’.
Vers l’évaluation des systèmes de dialogue multimodaux

l’objet ciblé par le geste. Quant à la flèche, elle équivaut au « déplace ça ici » avec les aspects
dont nous avons parlé. Dans un tel type de système multimodal, l’analyse en termes d’actes de
langage et d’actes de dialogue met en jeu plusieurs processus : l’identification de l’acte de
l’énoncé oral et du prédicat associé, l’identification de l’acte du geste et du prédicat associé,
ainsi que l’analyse de la compatibilité ou de l’incompatibilité entre les diverses hypothèses de
manière à aboutir à un seul acte de dialogue qui sera à l’origine de la réaction du système. Par
exemple, un geste en forme de flèche effectué en même temps que l’énoncé oral « déplace ça
ici » ne posera pas de problème, alors qu’un geste en forme de croix effectué en même temps
que le même énoncé oral conduira à une incohérence. Selon le système, cette incohérence
pourra être interprétée soit comme une erreur soit comme l’exécution de deux tâches
parallèles. Tous ces aspects peuvent être évalués grâce aux questions Q suivantes : « ce geste
est-il une demande ? » + geste ; « ce geste accompagne-t-il la parole ? » + geste ; « l’énoncé
multimodal est-il une demande ? » ; etc. A ce stade, nous avons fait le tour des principaux
problèmes qui se posent pour le traitement des entrées en dialogue multimodal.

Niveau 5 = « reconnaissance des intentions ». Il s’agit ici de déterminer les intentions ou les
buts sous-jacents aux énoncés de l’utilisateur, donc à un niveau plus profond que le niveau 4.
Le principe est d’interroger explicitement les états intentionnels, avec des questions Q telles
que : « l’utilisateur sait-il/veut-il… ? ». De tels états intentionnels sont indépendants des
modalités, et l’extension de DQR à la multimodalité ne change rien à ce niveau.

Niveau 6 = « pertinence de la réponse ». L’objet de la question évaluative est ici assez large
puisqu’il s’agit de tester la pertinence des réponses du système. Les aspects couverts sont
donc a priori les capacités linguistiques (et donc multimodales) dont font preuve les réponses,
leur adéquation par rapport à l’énoncé initial de l’utilisateur, par rapport aux connaissances de
l’application, par rapport aux moyens de communication, par rapport au profil de l’utilisateur,
etc. Dans (Zeiliger et al., 2000), les exemples de questions Q sont les suivants : « cette
question est-elle agressive ? » ; « cette question est-elle nécessaire ? » ; « cette proposition est-
elle possible à cet instant ? ». Ces exemples interrogent à la fois la forme et le contenu de la
réponse. Réaliser un système de dialogue capable de répondre à de telles questions n’est pas
simple. Cela suppose que le système (chacun de ses modules) soit capable d’évaluer la
pertinence de ses propres décisions, un peu comme dans le modèle du carnet d’esquisses de
(Sabah, 1996). En dialogue multimodal, il faudrait ajouter tous les aspects liés à la
multimodalité en sortie, c’est-à-dire aux décisions que le système a prises lors de la
détermination du contenu et de la forme de la réponse multimodale. Ainsi, des exemples
possibles pour Q sont : « le choix de la ou des modalités de sortie est-il pertinent ? » ; « le
message est-il surchargé ? » ; « le message est-il redondant ? » ; « le message est-il bien
synchronisé ? » ; « les informations présentées sont-elles pertinentes ? » ; etc. Ces questions
font le tour des principaux problèmes qui se posent en sortie dans le dialogue multimodal.
Elles intègrent des aspects métalinguistiques qui ne sont généralement pas implantés dans le
modèle conceptuel et le lexique des systèmes. Ces aspects métalinguistiques, en plus des
aspects métacognitifs vus précédemment, constituent clairement une limite à la faisabilité de
ce sixième niveau, ainsi qu’à celle du niveau suivant.

Niveau 7 = « pertinence de la stratégie ». Ce dernier niveau teste la qualité de la stratégie de
dialogue, c’est-à-dire si elle a été efficacement menée et si elle est réussie. En fait, les
questions couvrent non seulement la stratégie de dialogue, mais également la stratégie de
gestion de la tâche : « le client est-il content ? » ; « y a-t-il trop de questions de confirmation
indirectes ? » ; etc. Peuvent également être interrogés la lenteur, le nombre d’incidences, les
raisons possibles d’une rupture, c’est-à-dire tous les aspects que le système de dialogue peut
(théoriquement) calculer. Ces aspects étant indépendants des modalités, nous les gardons et
nous obtenons un DQR multimodal complet.
Frédéric Landragin

3.2 DCR multimodal
Comme nous l’avons déjà évoqué, une autre matérialisation est le paradigme DCR (Antoine,
Caelen, 1999) qui, en remplaçant la question évaluative par un contrôle C, minimise le
problème de la capacité du système à répondre à cette question parfois métalinguistique. Le
contrôle consiste en une simplification ou une reformulation de la demande utilisateur initiale.
Ainsi, en reprenant quelques-uns des exemples précédents, on fera intervenir les contrôles
multimodaux C suivants : « mets ‘submis.tex’ en (x1, y1) » ; « déplace obj4353 de (x1, y1) à (x2,
y12 » ; « déplace obj4353 selon les points de passage (x3, y3), (x4, y4), (x5, y5)… ». Passer du
DQR multimodal à un DCR multimodal nécessite donc la paraphrase de manière simple et
non ambiguë des références multimodales, avec la description en langage naturel de
coordonnées spatiales. Les autres aspects ne posent pas de problème particulier, en tout cas
pas plus de problème que le passage de DQR à DCR.
3.3 Vers un PEACE pour le dialogue multimodal ?
Les principes de PEACE (Devillers et al., 2002) sont la reformulation de l’historique en une
phrase unique, l’utilisation de cette phrase pour une évaluation contextuelle de l’énoncé
courant, et l’exploitation de représentations sémantiques de référence. Nous avons déjà parlé
de la difficulté d’appliquer ce dernier principe au dialogue multimodal, et c’est donc l’idée de
reformulation de l’historique qu’il s’agit d’étudier ici.

La modélisation de l’historique du dialogue est un problème récurrent en dialogue homme-
machine, et s’avère particulièrement complexe en dialogue multimodal (Landragin, 2004). En
nous focalisant sur le problème de la référence aux objets, l’historique doit conserver à la fois
l’identifiant des référents (pour ressortir ceux-ci lors de l’interprétation d’une anaphore) et les
mentions utilisées pour y référer (non seulement pour interpréter les éventuelles références
mentionnelles ou métalinguistiques, mais aussi et surtout pour interpréter les ellipses, en
particulier les ellipses nominales). En multimodal, il en est de même et les formes
référentielles multimodales doivent donc être conservées. (Landragin, 2004) montre que l’état
de la scène visuelle doit également être sauvegardé à chaque étape, conduisant ainsi au moins
à un historique linguistique, un historique gestuel et un historique visuel. Une chaîne de
référence faisant appel aux modalités utilisées ou aux souvenirs de l’utilisateur est alors
interprétable, par exemple : « l’objet que je viens de désigner », « les deux objets groupés un
peu plus loin », « celui de gauche », « celui qui était à droite », « le dernier ». Ces expressions
référentielles montrent d’elles-mêmes que la paraphrase d’un historique multimodal est une
tâche impossible à réaliser, ou alors au prix de simplifications telles que le biais introduit
enlèvera toute plausibilité à l’évaluation. En effet, le seul processus de paraphrase
automatisable est l’utilisation systématique des identifiants des référents, or cette solution
semble plus destructrice en dialogue multimodal qu’en dialogue oral : elle met en effet de côté
l’ensemble des aspects multimodaux. Il nous apparaît donc difficile d’appliquer les principes
de PEACE au dialogue multimodal.
4 Conclusion et perspectives
L’évaluation des systèmes de dialogue multimodaux s’avère en fin de compte plus complexe
que celle des systèmes oraux (pourtant déjà bien délicate), surtout quand la multimodalité est
considérée comme l’association complémentaire du langage naturel et d’autres modalités de
communication sur lesquelles s’appuie le langage. Dans cet article nous avons proposé une
extension du paradigme DQR/DCR à la multimodalité. Nos illustrations ont porté sur des
Vers l’évaluation des systèmes de dialogue multimodaux

dérivations de l’exemple à l’origine de l’essor des travaux sur la multimodalité (« mets ça
ici »), et sont ainsi applicables à la majorité des phénomènes de référence multimodale. Si nos
propositions et les remarques afférentes constituent un produit de recherche en soi, elles
peuvent peut-être aussi être utiles à la réalisation de systèmes de dialogue, de par les questions
soulevées et les préoccupations détaillées.

Plusieurs aspects restent à étudier pour obtenir une méthodologie couvrant le champ occupé
actuellement par le dialogue multimodal. Un aspect concerne les pistes qui sont explorées en
ce moment pour simplifier la réalisation de systèmes multimodaux : comme les phénomènes
sont de plus en plus nombreux et les processus de plus en plus complexes, une solution
consiste à faciliter le travail des développeurs en leur fournissant des environnements de
développement capables d’automatiser certaines phases de conception. Un exemple en est
l’approche par dérivation et génération de modèles : en simplifiant un peu, les développeurs
écrivent des modèles, et l’environnement de développement génère automatiquement des
modules du système à partir de ces modèles. L’évaluation des systèmes obtenus doit alors
reposer non seulement sur le comportement du système face à des utilisateurs, mais aussi sur
la qualité des modèles initiaux et sur celle de la chaîne de dérivation de modèles. D’autre part,
quand les systèmes de dialogue multimodaux seront suffisamment nombreux, il nous apparaît
utile de revenir sur la méthode d’évaluation par défi. Son principe, que ce soit l’étape de
dérivation d’énoncés à partir d’un ensemble d’énoncés initiaux ou l’échange des rôles entre
différents concepteurs, nous apparaît en effet tout à fait pertinent pour le dialogue multimodal.
Références
ANTOINE J.-Y. (2003). Pour une ingénierie des langues plus linguistique. HDR informatique,
Université de Bretagne Sud, Vannes.

ANTOINE J.-Y., CAELEN J. (1999). Pour une évaluation objective, prédictive et générique de la
compréhension en CHM orale : le paradigme DCR (Demande, Contrôle, Résultat). Langues,
vol. 2, n° 2, 130-139.

BERINGER N., KARTAL U., LOUKA K., SCHIEL F., TÜRK U. (2002). PROMISE – A procedure
for multimodal interactive system evaluation. Proceedings of the LREC Workshop on
Multimodal Resources and Multimodal Systems Evaluation, Las Palmas, 77-80.

BERNSEN N.O., DYBKJÆR L. (2004). Evaluation of Spoken Multimodal Conversation.
Proceedings of the Sixth International Conference on Multimodal Interfaces (ICMI), Penn
State University, 38-45.

BOLT R.A. (1980). Put-That-There: Voice and gesture at the graphics interface. Proceedings
of the 7th Annual Conference on Computer Graphics and Interactive Techniques, Seattle.

BONNEAU-MAYNARD H., AYACHE C., BECHET F., DENIS A., KUHN A., LEFEVRE F., MOSTEFA
D., QUIGNARD M., ROSSET S., SERVAN C., VILLANEAU J. (2006). Results of the French Evalda-
Media Evaluation Campaign for Literal Understanding. Proceedings of the 5th International
Conference on Language Resources and Evaluation (LREC’06), Gênes, Italie.

CAELEN J., XUEREB A. (2007). Interaction et pragmatique. Paris : Hermès-Lavoisier.

CHAUDIRON S. (Ed.) (2004). Evaluation des systèmes de traitement de l’information. Paris :
Hermès-Lavoisier.
Frédéric Landragin

DEVILLERS L., MAYNARD H., PAROUBEK P. (2002). Méthodologies d’évaluation des systèmes
de dialogue parlé : réflexions et expériences autour de la compréhension. Traitement
Automatique des Langues, vol. 43, n° 2, 155-184.

DYBKJÆR L., BERNSEN N.O., DYBKJÆR H. (1998). A Methodology for diagnostic evaluation
of spoken human-machine dialogue, International Journal of Human Computer Studies,
vol. 48, 605-625.

DYBKJÆR L., BERNSEN N.O., MINKER W. (2004). Evaluation and usability of multimodal
spoken language dialogue systems, Speech Communication, vol. 43, n° 1-2, 33-54.

ECKERT W., LEVIN E., PIERACCINI R. (1998). Automatic evaluation of spoken dialogue
systems. Proceedings of the 2nd Workshop on Formal Semantics and Pragmatics of
Dialogue, University of Twente, Enschede, The Netherlands, 99-110.

GIBBON D., MERTINS I., MOORE R.K. (2000). Handbook of multimodal and spoken dialogue
systems: Resources, terminology and product evaluation. Kluwer Academic Publishers.

HIRSCHMAN L. (1992). Multi-Site Data Collection for a Spoken Language Corpus:
MADCOW. Proceedings of the DARPA Speech and Natural Language Workshop, New York.

LANDRAGIN F. (2004). Dialogue homme-machine multimodal. Paris : Hermès-Lavoisier.

LITMAN D.J., PAN S. (1999). Empirically evaluating an adaptable spoken dialogue system.
Proceedings of the 7th International Conference on User Modeling.

LOPEZ-COZAR R., ARAKI M. (2005). Spoken, multilingual and multimodal dialogue systems:
Development and assessment. John Wiley & Sons, Ltd.

LOPEZ-COZAR R., DE LA TORRE A., SEGURA J.C., RUBIO A.J. (2003). Assessment of dialogue
systems by means of a new simulation technique, Speech Communication, vol. 40, 387-407.

MÖLLER S., SMEELE P., BOLAND H., KREBBER J. (2007). Evaluating spoken dialogue systems
according to de-facto standards: A case study, Computer Speech and Language, vol. 21.

SABAH G. (1996). Le « carnet d’esquisses » : une mémoire interprétative dynamique. Actes du
colloque AFCET – AFIA, Rennes.

VUURPIJL L.G., TEN BOSCH L., ROSSIGNOL S., NEUMANN A., PFLEGER N., ENGEL R. (2004).
Evaluation of multimodal dialog systems. Proceedings of the LREC Workshop on Multimodal
Corpora and Evaluation, Lisbon, Portugal.

WALKER M.A., PASSONNEAU R., BOLAND J.E. (2001). Quantitative and Qualitative Evaluation
of Darpa Communicator Spoken Dialogue Systems. Meeting of the Association of
Computational Linguistics.

WALKER M.A., WHITTAKER S., STENT A., MALOOR P., MOORE J., JOHNSTON M., VASIREDDY
G. (2004). Generation and Evaluation of User Tailored Responses in Multimodal Dialogue.
Cognitive Science, vol. 28, n° 5, 811-840.

ZEILIGER J., ANTOINE J.-Y., CAELEN J. (2000). La méthodologie DQR d’évaluation qualitative
des systèmes de dialogue oral homme-machine, Dans Mariani J., Masson N., Néel F., Chibout
K. (Ed.) Ressources et Evaluations en Ingénierie de la Langue, AUF et De Boeck Université.
