@proceedings{TALN:2009,
  editor    = {Nazarenko, Adeline and Poibeau, Thierry},
  title     = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009}
}

@inproceedings{hathout:2009:TALN,
  author    = {Hathout, Nabil},
  title     = {Acquisition morphologique à partir d’un dictionnaire informatisé},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-001},
  language  = {french},
  resume    = {L’article propose un modèle linguistique et informatique permettant de faire émerger la structure morphologique dérivationnelle du lexique à partir des régularités sémantiques et formelles des mots qu’il contient. Ce modèle est radicalement lexématique. La structure morphologique est constituée par les relations que chaque mot entretient avec les autres unités du lexique et notamment avec les mots de sa famille morphologique et de sa série dérivationnelle. Ces relations forment des paradigmes analogiques. La modélisation a été testée sur le lexique du français en utilisant le dictionnaire informatisé TLFi.},
  abstract  = {The paper presents a linguistic and computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains. The model is word-based. The proposed morphological structure consists of (1) binary relations that connect each headword with words that are morphologically related, and especially with the members of its morphological family and its derivational series, and of (2) the analogies that hold between the words. The model has been tested on the lexicon of French using the TLFi machine readable dictionary.},
  motscles  = {Morphologie dérivationnelle, morphologie lexématique, similarité morphologique, analogie formelle},
  keywords  = {Derivational morphology, word-based morphology, morphological relatedness, formal analogy},
}

@inproceedings{leroux:2009:TALN,
  author    = {Le Roux, Joseph},
  title     = {Analyse déductive pour les grammaires d’interaction},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-002},
  language  = {french},
  resume    = {Nous proposons un algorithme d’analyse pour les grammaires d’interaction qui utilise le cadre formel de l’analyse déductive. Cette approche donne un point de vue nouveau sur ce problème puisque les méthodes précédentes réduisaient ce dernier à la réécriture de graphes et utilisaient des techniques de résolution de contraintes. D’autre part, cette présentation permet de décrire le processus de manière standard et d’exhiber les sources d’indéterminisme qui rendent ce problème difficile.},
  abstract  = {We propose a parsing algorithm for Interaction Grammars using the deductive parsing framework. This approach brings new perspectives on this problem, departing from previous methods relying on constraint-solving techniques to interpret it as a graph-rewriting problem. Furthermore, this presentation allows a standard description of the algorithm and a fine-grained inspection of the sources of non-determinism.},
  motscles  = {Analyse syntaxique, grammaires d’interaction},
  keywords  = {Parsing, Interaction Grammars},
}

@inproceedings{nasr-bechet:2009:TALN,
  author    = {Nasr, Alexis and Béchet, Frédéric},
  title     = {Analyse syntaxique en dépendances de l’oral spontané},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-003},
  language  = {french},
  resume    = {Cet article décrit un modèle d’analyse syntaxique de l’oral spontané axé sur la reconnaissance de cadres valenciels verbaux. Le modèle d’analyse se décompose en deux étapes : une étape générique, basée sur des ressources génériques du français et une étape de réordonnancement des solutions de l’analyseur réalisé par un modèle spécifique à une application. Le modèle est évalué sur le corpus MEDIA.},
  abstract  = {We describe in this paper a syntactic parser for spontaneous speech geared towards the identification of verbal subcategorization frames. The parser proceeds in two stages. The first stage is based on generic syntactic ressources for French. The second stage is a reranker which is specially trained for a given application. The parser is evaluated on the MEDIA corpus.},
  motscles  = {Analyse syntaxique, reconnaissance automatique de la parole},
  keywords  = {Syntactic parsing, automatic speech recognition},
}

@inproceedings{candito-EtAl:2009:TALN,
  author    = {Candito, Marie and Crabbé, Benoît and Denis, Pascal and Guérin, François},
  title     = {Analyse syntaxique du français : des constituants aux dépendances},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-004},
  language  = {french},
  resume    = {Cet article présente une technique d’analyse syntaxique statistique à la fois en constituants et en dépendances. L’analyse procède en ajoutant des étiquettes fonctionnelles aux sorties d’un analyseur en constituants, entraîné sur le French Treebank, pour permettre l’extraction de dépendances typées. D’une part, nous spécifions d’un point de vue formel et linguistique les structures de dépendances à produire, ainsi que la procédure de conversion du corpus en constituants (le French Treebank) vers un corpus cible annoté en dépendances, et partiellement validé. D’autre part, nous décrivons l’approche algorithmique qui permet de réaliser automatiquement le typage des dépendances. En particulier, nous nous focalisons sur les méthodes d’apprentissage discriminantes d’étiquetage en fonctions grammaticales.},
  abstract  = {This paper describes a technique for both constituent and dependency parsing. Parsing proceeds by adding functional labels to the output of a constituent parser trained on the French Treebank in order to further extract typed dependencies. On the one hand we specify on formal and linguistic grounds the nature of the dependencies to output as well as the conversion algorithm from the French Treebank to this dependency representation. On the other hand, we describe a class of algorithms that allows to perform the automatic labeling of the functions from the output of a constituent based parser. We specifically focus on discriminative learning methods for functional labelling.},
  motscles  = {Analyseur syntaxique statistique, analyse en constituants/dépendances, étiquetage en fonctions grammaticales},
  keywords  = {Statistical parsing, constituent/dependency parsing, grammatical function labeling},
}

@inproceedings{moreau-EtAl:2009:TALN,
  author    = {Moreau, Erwan and Tellier, Isabelle and Balvet, Antonio and Laurence, Grégoire and Rozenknop, Antoine and Poibeau, Thierry},
  title     = {Annotation fonctionnelle de corpus arborés avec des Champs Aléatoires Conditionnels},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-005},
  language  = {french},
  resume    = {L’objectif de cet article est d’évaluer dans quelle mesure les “fonctions syntaxiques” qui figurent dans une partie du corpus arboré de Paris 7 sont apprenables à partir d’exemples. La technique d’apprentissage automatique employée pour cela fait appel aux “Champs Aléatoires Conditionnels” (Conditional Random Fields ou CRF), dans une variante adaptée à l’annotation d’arbres. Les expériences menées sont décrites en détail et analysées. Moyennant un bon paramétrage, elles atteignent une F1-mesure de plus de 80%.},
  abstract  = {The purpose of this paper is to evaluatewhether the "syntactic functions" present in a part of the Paris 7 Treebank are learnable from examples. The learning technic used is the one of "Conditional Random Fields" (CRF), in an original variant adapted to tree labelling. The conducted experiments are extensively described and analyzed. With good parameters, a F1-mesure value of over 80% is reached.},
  motscles  = {fonctions syntaxiques, Conditional Random Fields, corpus arborés},
  keywords  = {syntactic functions, Conditional Random Fields, Treebanks},
}

@inproceedings{morin:2009:TALN,
  author    = {Morin, Emmanuel},
  title     = {Apport d’un corpus comparable déséquilibré à l’extraction de lexiques bilingues},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-006},
  language  = {french},
  resume    = {Les principaux travaux en extraction de lexiques bilingues à partir de corpus comparables reposent sur l’hypothèse implicite que ces corpus sont équilibrés. Cependant, les différentes méthodes computationnelles associées sont relativement insensibles à la taille de chaque partie du corpus. Dans ce contexte, nous étudions l’influence que peut avoir un corpus comparable déséquilibré sur la qualité des terminologies bilingues extraites à travers différentes expériences. Nos résultats montrent que sous certaines conditions l’utilisation d’un corpus comparable déséquilibré peut engendrer un gain significatif dans la qualité des lexiques extraits.},
  abstract  = {The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the different related approaches are relatively insensitive to sizes of each part of the comparable corpus. Within this context, we study the influence of unbalanced comparable corpora on the quality of bilingual terminology extraction through different experiments. Our results show the conditions under which the use of an unbalanced comparable corpus can induce a significant gain in the quality of extracted lexicons.},
  motscles  = {Multilinguisme, corpus comparable, extraction de lexiques bilingues},
  keywords  = {Multilingualism, comparable corpus, bilingual lexicon extraction},
}

@inproceedings{charton-torresmoreno:2009:TALN,
  author    = {Charton, Eric and Torres-Moreno, Juan-Manuel},
  title     = {Classification d’un contenu encyclopédique en vue d’un étiquetage par entités nommées},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-007},
  language  = {french},
  resume    = {On utilise souvent des ressources lexicales externes pour améliorer les performances des systèmes d’étiquetage d’entités nommées. Les contenus de ces ressources lexicales peuvent être variés : liste de noms propres, de lieux, de marques. On note cependant que la disponibilité de corpus encyclopédiques exhaustifs et ouverts de grande taille tels que Worldnet ou Wikipedia, a fait émerger de nombreuses propositions spécifiques d’exploitation de ces contenus par des systèmes d’étiquetage. Un problème demeure néanmoins ouvert avec ces ressources : celui de l’adaptation de leur taxonomie interne, complexe et composée de dizaines de milliers catégories, aux exigences particulières de l’étiquetage des entités nommées. Pour ces dernières, au plus de quelques centaines de classes sémantiques sont requises. Dans cet article nous explorons cette difficulté et proposons un système complet de transformation d’un arbre taxonomique encyclopédique en une système à classe sémantiques adapté à l’étiquetage d’entités nommées.},
  abstract  = {The advent of Wikipedia and WordNet aroused new interest in labeling by named entity aided by external resources. The availability of these large, multilingual, comprehensive and open digital encyclopaedic corpora suggests the development of labeling solutions that exploit the knowledge contained in these corpora. The mapping of a word sequence to an encyclopedic document is possible, however the classification of encyclopaedic entities and their related labels, is not yet fully resolved. The inconsistency of an open encyclopaedic corpus such as Wikipedia, makes sometimes difficult establishing a relationship between its entities and a restricted taxonomy. In this article we explore this problem and propose a complete system to meet this need.},
  motscles  = {Etiquetage, Entités nommées, classification, taxonomie},
  keywords  = {Named entity recognition, classification, taxonomie},
}

@inproceedings{langlais:2009:TALN,
  author    = {Langlais, Philippe},
  title     = {Étude quantitative de liens entre l’analogie formelle et la morphologie constructionnelle},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-008},
  language  = {french},
  resume    = {Plusieurs travaux ont récemment étudié l’apport de l’apprentissage analogique dans des applications du traitement automatique des langues comme la traduction automatique, ou la recherche d’information. Il est souvent admis que les relations analogiques de forme entre les mots capturent des informations de nature morphologique. Le but de cette étude est de présenter une analyse des points de rencontre entre l’analyse morphologique et les analogies de forme. C’est à notre connaissance la première étude de ce type portant sur des corpus de grande taille et sur plusieurs langues. Bien que notre étude ne soit pas dédiée à une tâche particulière du traitement des langues, nous montrons cependant que le principe d’analogie permet de segmenter des mots en morphèmes avec une bonne précision.},
  abstract  = {Several studies recently showed the interest of analogical learning for Natural Language processing tasks such as Machine Translation and Information Retrieval. It is often admitted that formal analogies between words capture morphological information. The purpose of this study os to quantify the correlations between morphological analysis and formal analogies. This is to our knowledge the first attempt to conduct such a quantitative analysis on large datasets and for several languages. Although this paper was not geared toward tackling a specific natural language processing task, we show that segmenting a word token into morphemes can be accomplished with a good precision by a simple strategy relying solely on formal analogy.},
  motscles  = {Apprentissage analogique, analogie formelle, analyse morphologique},
  keywords  = {Analogical Learning, Formal Analogies, Morphological Analysis},
}

@inproceedings{do-EtAl:2009:TALN,
  author    = {Do, Thi-Ngoc-Diep and Le, Viet-Bac and Bigi, Brigitte and Besacier, Laurent and Castelli, Eric},
  title     = {Exploitation d’un corpus bilingue pour la création d’un système de traduction probabiliste Vietnamien - Français},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-009},
  language  = {french},
  resume    = {Cet article présente nos premiers travaux en vue de la construction d’un système de traduction probabiliste pour le couple de langue vietnamien-français. La langue vietnamienne étant considérée comme une langue peu dotée, une des difficultés réside dans la constitution des corpus parallèles, indispensable à l’apprentissage des modèles. Nous nous concentrons sur la constitution d’un grand corpus parallèle vietnamien-français. La méthode d’identification automatique des paires de documents parallèles fondée sur la date de publication, les mots spéciaux et les scores d’alignements des phrases est appliquée. Cet article présente également la construction d’un premier système de traduction automatique probabiliste vietnamienfrançais et français-vietnamien à partir de ce corpus et discute l’opportunité d’utiliser des unités lexicales ou sous-lexicales pour le vietnamien (syllabes, mots, ou leurs combinaisons). Les performances du système sont encourageantes et se comparent avantageusement à celles du système de Google.},
  abstract  = {This paper presents our first attempt at constructing a Vietnamese-French statistical machine translation system. Since Vietnamese is considered as an under-resourced language, one of the difficulties is building a large Vietnamese-French parallel corpus, which is indispensable to train the models. We concentrate on building a large Vietnamese-French parallel corpus. The document alignment method based on publication date, special words and sentence alignment result is applied. The paper also presents an application of the obtained parallel corpus to the construction of a Vietnamese-French statistical machine translation system, where the use of different units for Vietnamese (syllables, words, or their combinations) is discussed. The performance of the system is encouraging and it compares favourably to that of Google Translate.},
  motscles  = {traduction probabiliste, corpus bilingue, alignement de documents, table de traduction},
  keywords  = {statistical machine translation, bilingual corpus, document alignment, phrase table},
}

@inproceedings{prochasson-morin:2009:TALN,
  author    = {Prochasson, Emmanuel and Morin, Emmanuel},
  title     = {Influence des points d’ancrage pour l’extraction lexicale bilingue à partir de corpus comparables spécialisés},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-010},
  language  = {french},
  resume    = {L’extraction de lexiques bilingues à partir de corpus comparables affiche de bonnes performances pour des corpus volumineux mais chute fortement pour des corpus d’une taille plus modeste. Pour pallier cette faiblesse, nous proposons une nouvelle contribution au processus d’alignement lexical à partir de corpus comparables spécialisés qui vise à renforcer la significativité des contextes lexicaux en s’appuyant sur le vocabulaire spécialisé du domaine étudié. Les expériences que nous avons réalisées en ce sens montrent qu’une meilleure prise en compte du vocabulaire spécialisé permet d’améliorer la qualité des lexiques extraits.},
  abstract  = {Bilingual lexicon extraction from comparable corpora gives good results for large corpora but drops significantly for small size corpora. In order to compensate this weakness, we suggest a new contribution dedicated to the lexical alignment from specialized comparable corpora that strengthens the representativeness of the lexical contexts based on domainspecific vocabulary. The experiments carried out in this way show that taking better account the specialized vocabulary induces a significant improvement in the quality of extracted lexicons.},
  motscles  = {Corpus comparable, extraction de lexiques bilingues, points d’ancrage},
  keywords  = {Comparable corpus, bilingual lexicon extraction, anchor points},
}

@inproceedings{huet-bourdaillet-langlais:2009:TALN,
  author    = {Huet, Stéphane and Bourdaillet, Julien and Langlais, Philippe},
  title     = {Intégration de l’alignement de mots dans le concordancier bilingue TransSearch},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-011},
  language  = {french},
  resume    = {Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Dans cet article, nous présentons nos travaux menés dans le but d’améliorer le concordancier bilingue TransSearch. Ce service, accessible sur le Web, repose principalement sur un alignement au niveau des phrases. Dans cette étude, nous discutons et évaluons l’intégration d’un alignement statistique au niveau des mots. Nous présentons deux nouvelles problématiques essentielles au succès de notre nouveau prototype : la détection des traductions erronées et le regroupement des variantes de traduction similaires.},
  abstract  = {Despite the impressive amount of recent studies devoted to improving the state of the art of machine translation, computer assisted translation tools remain the preferred solution of human translators when publication quality is of concern. In this paper, we present our ongoing efforts conducted within a project which aims at improving the commercial bilingual concordancer TransSearch. The core technology of this Web-based service mainly relies on sentence-level alignment. In this study, we discuss and evaluate the embedding of statistical word-level alignment. Two novel issues that are essential to the success of our new prototype are tackled: detecting erroneous translations and grouping together similar translations.},
  motscles  = {alignement au niveau des mots, concordancier bilingue, traduction automatique},
  keywords  = {word-level alignment, bilingual concordancer, machine translation},
}

@inproceedings{jackiewicz-charnois-ferrari:2009:TALN,
  author    = {Jackiewicz, Agata and Charnois, Thierry and Ferrari, Stéphane},
  title     = {Jugements d'évaluation et constituants périphériques},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-012},
  language  = {french},
  resume    = {L’article présente une étude portant sur des constituants détachés à valeur axiologique. Dans un premier temps, une analyse linguistique sur corpus met en évidence un ensemble de patrons caractéristiques du phénomène. Ensuite, une expérimentation informatique est proposée sur un corpus de plus grande taille afin de permettre l’observation des patrons en vue d’un retour sur le modèle linguistique. Ce travail s’inscrit dans un projet mené à l’interface de la linguistique et du TAL, qui se donne pour but d’enrichir, d’adapter au français et de formaliser le modèle général Appraisal de l’évaluation dans la langue.},
  abstract  = {In this paper, we present a study about peripheral constituent expressing some axiological value. First, a linguistic corpus analysis highlights some characteristic patterns for this phenomenon. Then, a computational experiment is carried out on a larger corpus in order to enable the observation of these patterns and to get a feedback on the linguistic model. This work takes part in a project at the intersection of Linguistics and NLP, which aims at enhancing, adapting to French language and formalizing the Appraisal generic model of the evaluation in language.},
  motscles  = {jugement d’évaluation, constituants extra-prédicatifs, constructions et lexiques subjectifs, Appraisal, implémentation informatique, portraits et biographies dans la presse de spécialité et la presse d’information},
  keywords  = {evaluative judgement, peripheral constituent, evaluative constructions and lexicon, Appraisal, computational implementation, biographies and portraits in specialized press and newspapers},
}

@inproceedings{portet-EtAl:2009:TALN,
  author    = {Portet, François and Gatt, Albert and Hunter, Jim and Reiter, Ehud and Sripada, Somayajulu},
  title     = {Le projet BabyTalk : génération de texte à partir de données hétérogènes pour la prise de décision en unité néonatale},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-013},
  language  = {french},
  resume    = {Notre société génère une masse d’information toujours croissante, que ce soit en médecine, en météorologie, etc. La méthode la plus employée pour analyser ces données est de les résumer sous forme graphique. Cependant, il a été démontré qu'un résumé textuel est aussi un mode de présentation efficace. L'objectif du prototype BT-45, développé dans le cadre du projet Babytalk, est de générer des résumés de 45 minutes de signaux physiologiques continus et d'événements temporels discrets en unité néonatale de soins intensifs (NICU). L'article présente l'aspect génération de texte de ce prototype. Une expérimentation clinique a montré que les résumés humains améliorent la prise de décision par rapport à l'approche graphique, tandis que les textes de BT-45 donnent des résultats similaires à l’approche graphique. Une analyse a identifié certaines des limitations de BT-45 mais en dépit de cellesci, notre travail montre qu'il est possible de produire automatiquement des résumés textuels efficaces de données complexes.},
  abstract  = {Nowadays large amount of data is produced every day in medicine, meteorology and other areas and the most common approach to analyse such data is to present it graphically. However, it has been shown that textual summarisation is also an effective approach. As part of the BabyTalk project, the prototype BT-45 was developed to generate summaries of 45 minutes of continuous physiological signals and discrete temporal events in a neonatal intensive care unit (NICU). The paper presents its architecture with an emphasis on its natural language generation part. A clinical experiment showed that human textual summaries led to better decision making than graphical presentation, whereas BT-45 texts led to similar results as visualisations. An analysis identified some of the reasons for the BT-45 texts inferiority, but, despite these deficiencies, our work shows that it is possible for computer systems to generate effective textual summaries of complex data.},
  motscles  = {Traitement automatique des langues naturelles, Génération de texte, Analyse de données, Unité de soins intensifs, Systèmes d'aide à la décision},
  keywords  = {Natural language processing, Natural language generation, Intelligent data analysis, Intensive care unit, Decision support systems},
}

@inproceedings{cartoni:2009:TALN,
  author    = {Cartoni, Bruno},
  title     = {Les adjectifs relationnels dans les lexiques informatisés : formalisation et exploitation dans un contexte multilingue},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-014},
  language  = {french},
  resume    = {Dans cet article, nous nous intéressons aux adjectifs dits relationnels et à leur statut en traitement automatique des langues naturelles (TALN). Nous montrons qu’ils constituent une « sous-classe » d’adjectifs rarement explicitée et donc rarement représentée dans les lexiques sur lesquels reposent les applications du TALN, alors qu’ils jouent un rôle important dans de nombreuses applications. Leur formation morphologique est source d’importantes divergences entre différentes langues, et c’est pourquoi ces adjectifs sont un véritable défi pour les applications informatiques multilingues. Dans une partie plus pratique, nous proposons une formalisation de ces adjectifs permettant de rendre compte de leurs liens avec leur base nominale. Nous tentons d’extraire ces informations dans les lexiques informatisés existants, puis nous les exploitons pour traduire les adjectifs relationnels préfixés de l’italien en français.},
  abstract  = {This article focuses on a particular type of adjectives, relational adjectives, and especially on the way they are processed in natural language processing systems. We show that this class of adjectives is barely recorded in an explicit manner in computer lexicons. There is an important discrepancy in the way those adjectives are morphologically constructed in different languages, and therefore, they are a real challenge for multilingual computing applications. On a more practical side, we propose a formalisation for the adjectives that shows their semantic link with their nominal base. We make an attempt to extract this kind of information in existing machine lexica, and we exploit their semantic links in the translation of prefixed relational adjectives from Italian into French.},
  motscles  = {Adjectifs relationnels, ressources lexicales, morphologie constructionnelle},
  keywords  = {Relational adjectives, lexical resources, constructional morphology},
}

@inproceedings{plantevit-charnois:2009:TALN,
  author    = {Plantevit, Marc and Charnois, Thierry},
  title     = {Motifs séquentiels pour l’extraction d’information : illustration sur le problème de la détection d’interactions entre gènes},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-015},
  language  = {french},
  resume    = {Face à la prolifération des publications en biologie et médecine (plus de 18 millions de publications actuellement recensées dans PubMed), l’extraction d’information automatique est devenue un enjeu crucial. Il existe de nombreux travaux dans le domaine du traitement de la langue appliquée à la biomédecine ("BioNLP"). Ces travaux se distribuent en deux grandes tendances. La première est fondée sur les méthodes d’apprentissage automatique de type numérique qui donnent de bons résultats mais ont un fonctionnement de type "boite noire". La deuxième tendance est celle du TALN à base d’analyses (lexicales, syntaxiques, voire sémantiques ou discursives) coûteuses en temps de développement des ressources nécessaires (lexiques, grammaires, etc.). Nous proposons dans cet article une approche basée sur la découverte de motifs séquentiels pour apprendre automatiquement les ressources linguistiques, en l’occurrence les patrons linguistiques qui permettent l’extraction de l’information dans les textes. Plusieurs aspects méritent d’être soulignés : cette approche permet de s’affranchir de l’analyse syntaxique de la phrase, elle ne nécessite pas de ressources en dehors du corpus d’apprentissage et elle ne demande que très peu d’intervention manuelle. Nous illustrons l’approche sur le problème de la détection d’interactions entre gènes et donnons les résultats obtenus sur des corpus biologiques qui montrent l’intérêt de ce type d’approche.},
  abstract  = {The proliferation of publications in biology andmedicine (more than 18million publications currently listed in PubMed) has lead to the crucial need of automatic information extraction. There are many work in the field of natural language processing applied to biomedicine (BioNLP). Two types of approaches tackle this problem. On the one hand, machine learning based approaches give good results but run as a "black box". On the second hand, NLP based approaches are highly time consuming for developing the resources (lexicons, grammars, etc.). In this paper, we propose an approach based on sequential pattern mining to automatically discover linguistic patterns that allow the information extraction in texts. This approach allows to overcome sentence parsing and it does not require resources outside the training data set. We illustrate the approach on the problem of detecting interactions between genes and give the results obtained on biological corpora that show the relevance of this type of approach.},
  motscles  = {Extraction d’information, fouille de textes, motifs séquentiels, interactions entre gènes},
  keywords  = {Information extraction, text mining, sequential patterns, gene interactions},
}

@inproceedings{max-maklhoufi-langlais:2009:TALN,
  author    = {Max, Aurélien and Maklhoufi, Rafik and Langlais, Philippe},
  title     = {Prise en compte de dépendances syntaxiques pour la traduction contextuelle de segments},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-016},
  language  = {french},
  resume    = {Dans un système standard de traduction statistique basé sur les segments, le score attribué aux différentes traductions d’un segment ne dépend pas du contexte dans lequel il apparaît. Plusieurs travaux récents tendent à montrer l’intérêt de prendre en compte le contexte source lors de la traduction, mais ces études portent sur des systèmes traduisant vers l’anglais, une langue faiblement fléchie. Dans cet article, nous décrivons nos expériences sur la prise en compte du contexte source dans un système statistique traduisant de l’anglais vers le français, basé sur l’approche proposée par Stroppa et al. (2007). Nous étudions l’impact de différents types d’indices capturant l’information contextuelle, dont des dépendances syntaxiques typées. Si les mesures automatiques d’évaluation de la qualité d’une traduction ne révèlent pas de gains significatifs de notre système par rapport à un système à l’état de l’art ne faisant pas usage du contexte, une évaluation manuelle conduite sur 100 phrases choisies aléatoirement est en faveur de notre système. Cette évaluation fait également ressortir que la prise en compte de certaines dépendances syntaxiques est bénéfique à notre système.},
  abstract  = {In standard phrase-based Statistical Machine Translation (PBSMT) systems, the score associated with each translation of a phrase does not depend on its context. While several works have shown the potential gain of exploiting source context, they all considered English, a morphologically poor language, as the target language. In this article, we describe experiments on exploiting the source context in an English -> French PBSMT system, inspired by the work of Stroppa et al. (2007). We report a study on the impact of various types of features that capture contextual information, including syntactic dependencies. While automatic metrics do not show significative gains relative to a baseline system, a manual evaluation of 100 randomly selected sentences concludes that our context-aware system performs consistently better. This evaluation also shows that some types of syntactic dependencies can participate to the gains observed.},
  motscles  = {Traduction automatique statistique, contexte source, dépendances syntaxiques},
  keywords  = {Statistical Machine Translation, source context, syntactic dependencies},
}

@inproceedings{ehrmann-hagege:2009:TALN,
  author    = {Ehrmann, Maud and Hagège, Caroline},
  title     = {Proposition de caractérisation et de typage des expressions temporelles en contexte},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-017},
  language  = {french},
  resume    = {Nous assistons actuellement en TAL à un regain d’intérêt pour le traitement de la temporalité véhiculée par les textes. Dans cet article, nous présentons une proposition de caractérisation et de typage des expressions temporelles tenant compte des travaux effectués dans ce domaine tout en cherchant à pallier les manques et incomplétudes de certains de ces travaux. Nous explicitons comment nous nous situons par rapport à l’existant et les raisons pour lesquelles parfois nous nous en démarquons. Le typage que nous définissons met en évidence de réelles différences dans l’interprétation et le mode de résolution référentielle d’expressions qui, en surface, paraissent similaires ou identiques. Nous proposons un ensemble des critères objectifs et linguistiquement motivés permettant de reconnaître, de segmenter et de typer ces expressions. Nous verrons que cela ne peut se réaliser sans considérer les procès auxquels ces expressions sont associées et un contexte parfois éloigné.},
  abstract  = {Temporal processing in texts is a topic of renewed interest in NLP. In this paper we present a new way of typing temporal expressions that takes into account both the state of the art of this domain and that also tries to be more precise and accurate that some of the current proposals. We explain into what extent our proposal is compatible and comparable with the state-of-the art and why sometimes we stray from it. The typing system that we define highlights real differences in the interpretation and reference calculus of these expressions. At the same time, by offering objective criteria, it fulfils the necessity of high inter-agreement between annotators. After having defined what we consider as temporal expressions, we will show that tokenization, characterization and typing of those expressions can only be done having into account processes to which these expressions are linked.},
  motscles  = {Temporalité, typage et caractérisation des expressions temporelles},
  keywords  = {temporal processing, temporal expressions characterization and typing},
}

@inproceedings{bestgen:2009:TALN,
  author    = {Bestgen, Yves},
  title     = {Quel indice pour mesurer l'efficacité en segmentation de textes?},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-018},
  language  = {french},
  resume    = {L'évaluation de l'efficacité d'algorithmes de segmentation thématique est généralement effectuée en quantifiant le degré d'accord entre une segmentation hypothétique et une segmentation de référence. Les indices classiques de précision et de rappel étant peu adaptés à ce domaine, WindowDiff (Pevzner, Hearst, 2002) s'est imposé comme l'indice de référence. Une analyse de cet indice montre toutefois qu'il présente plusieurs limitations. L'objectif de ce rapport est d'évaluer un indice proposé par Bookstein, Kulyukin et Raita (2002), la distance de Hamming généralisée, qui est susceptible de remédier à celles-ci. Les analyses montrent que celui-ci conserve tous les avantages de WindowDiff sans les limitations. De plus, contrairement à WindowDiff, il présente une interprétation simple puisqu'il correspond à une vraie distance entre les deux segmentations à comparer.},
  abstract  = {The evaluation of thematic segmentation algorithms is generally carried out by quantifying the degree of agreement between a hypothetical segmentation and a gold standard. The traditional indices of precision and recall being little adapted to this field, WindowDiff (Pevzner, Hearst, 2002) has become the standard for this kind of assessment. An analysis of this index shows however that it presents several limitations. The objective of this report is to evaluate an index developed by Bookstein, Kulyukin and Raita (2002), the Generalized Hamming Distance, which is likely to overcome these limitations. The analyzes show that it preserves all the advantages of WindowDiff without its limitations. Moreover, contrary to WindowDiff, it presents a simple interpretation since it corresponds to a true distance between the two segmentations.},
  motscles  = {Segmentation thématique, évaluation, distance de Hamming généralisée, WindowDiff},
  keywords  = {Thematic segmentation, evaluation, generalized Hamming distance, WindowDiff},
}

@inproceedings{laignelet-rioult:2009:TALN,
  author    = {Laignelet, Marion and Rioult, François},
  title     = {Repérer automatiquement les segments obsolescents à l’aide d’indices sémantiques et discursifs},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-019},
  language  = {french},
  resume    = {Cet article vise la description et le repérage automatique des segments d’obsolescence dans les documents de type encyclopédique. Nous supposons que des indices sémantiques et discursifs peuvent permettre le repérage de tels segments. Pour ce faire, nous travaillons sur un corpus annoté manuellement par des experts sur lequel nous projetons des indices repérés automatiquement. Les techniques statistiques de base ne permettent pas d’expliquer ce phénomène complexe. Nous proposons l’utilisation de techniques de fouille de données pour le caractériser et nous évaluons le pouvoir prédictif de nos indices. Nous montrons, à l’aide de techniques de classification supervisée et de calcul de l’aire sous la courbe ROC, que nos hypothèses sont pertinentes.},
  abstract  = {This paper deals with the description and automatic tracking of obsolescence in encyclopedic type of documents. We suppose that semantic and discursive cues may allow the tracking of these segments. For that purpose, we have worked on an expert manually annotated corpus, on which we have projected automatically tracked cues. Basic statistic techniques can not account for this complex phenomenon. We propose the use of techniques of data mining to characterize it, and we evaluate the predictive power of our cues. We show, using techniques of supervised classification and area under the ROC curve, that our hypotheses are relevant.},
  motscles  = {repérage automatique de l’obsolescence, indices sémantiques et discursifs, textes encyclopédiques, classification supervisée, aire sous la courbe ROC},
  keywords  = {automatic tracking of obsolescence, semantic and discursive cues, encyclopedic type of documents, supervised classification, area under the ROC curve},
}

@inproceedings{genereux-bossard:2009:TALN,
  author    = {Généreux, Michel and Bossard, Aurélien},
  title     = {Résumé automatique de textes d’opinions},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-020},
  language  = {french},
  resume    = {Le traitement des langues fait face à une demande croissante en matière d’analyse de textes véhiculant des critiques ou des opinions. Nous présentons ici un système de résumé automatique tourné vers l’analyse d’articles postés sur des blogues, où sont exprimées à la fois des informations factuelles et des prises de position sur les faits considérés. Nous montrons qu’une approche classique à base de traits de surface est tout à fait efficace dans ce cadre. Le système est évalué à travers une participation à la campagne d’évaluation internationale TAC (Text Analysis Conference) où notre système a réalisé des performances satisfaisantes.},
  abstract  = {There is currently a growing need concerning the analysis of texts expressing opinions or judgements. In this paper, we present a summarization system that is specifically designed to process blog posts, where factual information is mixed with opinions. We show that a classical approach based on surface cues is efficient to summarize this kind of texts. The system is evaluated through a participation to TAC (Text Analysis Conference), an international evaluation framework for automatic summarization, in which our system obtained good results.},
  motscles  = {résumé automatique, analyse de textes subjectifs, évaluation automatique},
  keywords  = {automatic summarization, analysis of subjective texts, automatic evaluation},
}

@inproceedings{falk-EtAl:2009:TALN,
  author    = {Falk, Ingrid and Gardent, Claire and Jacquey, Évelyne and Venant, Fabienne},
  title     = {Sens, synonymes et définitions},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-021},
  language  = {french},
  resume    = {Cet article décrit une méthodologie visant la réalisation d’une ressource sémantique en français centrée sur la synonymie. De manière complémentaire aux travaux existants, la méthode proposée n’a pas seulement pour objectif d’établir des liens de synonymie entre lexèmes, mais également d’apparier les sens possibles d’un lexème avec les ensembles de synonymes appropriés. En pratique, les sens possibles des lexèmes proviennent des définitions du TLFi et les synonymes de cinq dictionnaires accessibles à l’ATILF. Pour évaluer la méthode d’appariement entre sens d’un lexème et ensemble de synonymes, une ressource de référence a été réalisée pour 27 verbes du français par quatre lexicographes qui ont spécifié manuellement l’association entre verbe, sens (définition TLFi) et ensemble de synonymes. Relativement à ce standard étalon, la méthode d’appariement affiche une F-mesure de 0.706 lorsque l’ensemble des paramètres est pris en compte, notamment la distinction pronominal / non-pronominal pour les verbes du français et de 0.602 sans cette distinction.},
  abstract  = {We present a method for grouping the synonyms of a word into sets representing the possible meanings of that word. The possible meanings are given by the definitions of a general dictionary for French, the TLFi (Trésor de la langue française informatisé) and the method is applied to the synonyms of 5 synonym dictionnaries. To evaluate the method, we manually constructed a gold standard where for each (word, definition) pair, 4 lexicographers specified the set of synonyms they judge adequate. The method scores an F-measure of 0.602 when no distinction is made between pronominal and non-pronominal use and 0.706 when it is.},
  motscles  = {},
  keywords  = {},
}

@inproceedings{ailloud-klenner:2009:TALN,
  author    = {Ailloud, Étienne and Klenner, Manfred},
  title     = {Vers des contraintes plus linguistiques en résolution de coréférences},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-022},
  language  = {french},
  resume    = {Nous proposons un modèle filtrant de résolution de coréférences basé sur les notions de transitivité et d’exclusivité linguistique. À partir de l’hypothèse générale que les chaînes de coréférence demeurent cohérentes tout au long d’un texte, notre modèle assure le respect de certaines contraintes linguistiques (via des filtres) quant à la coréférence, ce qui améliore la résolution globale. Le filtrage a lieu à différentes étapes de l’approche standard (c-à-d. par apprentissage automatique), y compris avant l’apprentissage et avant la classification, accélérant et améliorant ce processus.},
  abstract  = {We propose a filter model of coreference resolution that is based on the notions of transitivity and linguistic exclusivity. Starting from the general assumption that coreference sets remain coherent throughout a text, our model enforces the checking of some compatibility criteria (filters) between coreference candidates, thereby improving resolution performance. This filtering is achieved at different stages of the workflow of machine-learning-based coreference resolution, including at the standard learning and testing steps, where it may help reduce the computational load and better distribute the actual occurrences to be learned.},
  motscles  = {Résolution de coréférences, apprentissage automatique, linguistique informatique par contraintes},
  keywords  = {Coreference resolution, Machine learning, Constraint-based NLP},
}

@inproceedings{nicolas-EtAl:2009:TALN,
  author    = {Nicolas, Lionel and Sagot, Benoît and Molinero, Miguel A. and Farré, Jacques and Villemonte De La Clergerie, Éric},
  title     = {Trouver et confondre les coupables : un processus sophistiqué de correction de lexique},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-023},
  language  = {french},
  resume    = {La couverture d’un analyseur syntaxique dépend avant tout de la grammaire et du lexique sur lequel il repose. Le développement d’un lexique complet et précis est une tâche ardue et de longue haleine, surtout lorsque le lexique atteint un certain niveau de qualité et de couverture. Dans cet article, nous présentons un processus capable de détecter automatiquement les entrées manquantes ou incomplètes d’un lexique, et de suggérer des corrections pour ces entrées. La détection se réalise au moyen de deux techniques reposant soit sur un modèle statistique, soit sur les informations fournies par un étiqueteur syntaxique. Les hypothèses de corrections pour les entrées lexicales détectées sont générées en étudiant les modifications qui permettent d’améliorer le taux d’analyse des phrases dans lesquelles ces entrées apparaissent. Le processus global met en oeuvre plusieurs techniques utilisant divers outils tels que des étiqueteurs et des analyseurs syntaxiques ou des classifieurs d’entropie. Son application au Lefff , un lexique morphologique et syntaxique à large couverture du français, nous a déjà permis de réaliser des améliorations notables.},
  abstract  = {The coverage of a parser depends mostly on the quality of the underlying grammar and lexicon. The development of a lexicon both complete and accurate is an intricate and demanding task, overall when achieving a certain level of quality and coverage. We introduce an automatic process able to detect missing or incomplete entries in a lexicon, and to suggest corrections hypotheses for these entries. The detection of dubious lexical entries is tackled by two techniques relying either on a specific statistical model, or on the information provided by a part-of-speech tagger. The generation of correction hypotheses for the detected entries is achieved by studying which modifications could improve the parse rate of the sentences in which the entries occur. This process brings together various techniques based on different tools such as taggers, parsers and entropy classifiers. Applying it on the Lefff , a large-coverage morphological and syntactic French lexicon, has already allowed us to perfom noticeable improvements.},
  motscles  = {Acquisition et correction lexicale, lexique à large couverture, fouille d’erreurs, étiqueteur syntaxique, classifieur d’entropie, analyseur syntaxique},
  keywords  = {Lexical acquisition and correction, wide coverage lexicon, error mining, tagger, entropy classifier, syntactic parser},
}

@inproceedings{trouilleux:2009:TALN,
  author    = {Trouilleux, François},
  title     = {Un analyseur de surface non déterministe pour le français},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-024},
  language  = {french},
  resume    = {Les analyseurs syntaxiques de surface à base de règles se caractérisent par un processus en deux temps : désambiguïsation lexicale, puis reconnaissance de patrons. Considérant que ces deux étapes introduisent une certaine redondance dans la description linguistique et une dilution des heuristiques dans les différents processus, nous proposons de définir un analyseur de surface qui fonctionne sur une entrée non désambiguïsée et produise l’ensemble des analyses possibles en termes de syntagmes noyau (chunks). L’analyseur, implanté avec NooJ, repose sur la définition de patrons étendus qui annotent des séquences de syntagmes noyau. Les résultats obtenus sur un corpus de développement d’environ 22 500 mots, avec un rappel proche de 100 %, montrent la faisabilité de l’approche et signalent quelques points d’ambiguïté à étudier plus particulièrement pour améliorer la précision.},
  abstract  = {Rule-based chunkers are characterized by a two-tier process : part-of-speech disambiguation, and pattern matching. Considering that these two stages introduce some redundancy in the linguistic description and a dilution of heuristics over the different processes, we propose to define a chunker which parses a non-disambiguated input, and produces all possible analysis in terms of chunks. The parser, implemented with NooJ, relies on the definition of extended patterns, which annotate sequences of chunks. The results obtained on an approx. 22500 word corpus, with almost 100 % recall, demonstrate the feasability of the approach, and signal which ambiguities should be further studied in order to improve precision.},
  motscles  = {Analyse syntaxique de surface, automates à états finis, déterminisme, désambiguïsation},
  keywords  = {Chunking, finite-state automata, determinism, disambiguation},
}

@inproceedings{bossard:2009:TALN,
  author    = {Bossard, Aurélien},
  title     = {Une approche mixte-statistique et structurelle - pour le résumé automatique de dépêches},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-025},
  language  = {french},
  resume    = {Les techniques de résumé automatique multi-documents par extraction ont récemment évolué vers des méthodes statistiques pour la sélection des phrases à extraire. Dans cet article, nous présentons un système conforme à l’« état de l’art » — CBSEAS — que nous avons développé pour les tâches Opinion (résumés d’opinions issues de blogs) et Update (résumés de dépêches et mise à jour du résumé à partir de nouvelles dépêches sur le même événement) de la campagne d’évaluation TAC 2008, et montrons l’intérêt d’analyses structurelles et linguistiques des documents à résumer. Nous présentons également notre étude sur la structure des dépêches et l’impact de son intégration à CBSEAS.},
  abstract  = {Automatic multi-document summarization techniques have recently evolved into statistical methods for selecting the sentences that will be used to generate the summary. In this paper, we present a system in accordance with « State-of-the-art » — CBSEAS — that we have developped for the « Opinion Task » (automatic summaries of opinions from blogs) and the « Update Task » (automatic summaries of newswire articles and information update) of the TAC 2008 evaluation campaign, and show the interest of structural and linguistic analysis of the documents to summarize .We also present our study on news structure and its integration to CBSEAS impact.},
  motscles  = {Résumé automatique, structure de documents},
  keywords  = {Automatic summarization, document structure},
}

@inproceedings{brun-EtAl:2009:TALN,
  author    = {Brun, Caroline and Dessaigne, Nicolas and Ehrmann, Maud and Gaillard, Baptiste and Guillemin-Lanne, Sylvie and Jacquet, Guillaume and Kaplan, Aaron and Kucharski, Marianna and Martineau, Claude and Migeotte, Aurélie and Nakamura, Takuya and Voyatzi, Stavroula},
  title     = {Une expérience de fusion pour l’annotation d'entités nommées},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-026},
  language  = {french},
  resume    = {Nous présentons une expérience de fusion d’annotations d’entités nommées provenant de différents annotateurs. Ce travail a été réalisé dans le cadre du projet Infom@gic, projet visant à l’intégration et à la validation d’applications opérationnelles autour de l’ingénierie des connaissances et de l’analyse de l’information, et soutenu par le pôle de compétitivité Cap Digital « Image, MultiMédia et Vie Numérique ». Nous décrivons tout d’abord les quatre annotateurs d’entités nommées à l’origine de cette expérience. Chacun d’entre eux fournit des annotations d’entités conformes à une norme développée dans le cadre du projet Infom@gic. L’algorithme de fusion des annotations est ensuite présenté ; il permet de gérer la compatibilité entre annotations et de mettre en évidence les conflits, et ainsi de fournir des informations plus fiables. Nous concluons en présentant et interprétant les résultats de la fusion, obtenus sur un corpus de référence annoté manuellement.},
  abstract  = {In this paper, we present an experiment aimed at merging named entity annotations provided by different annotators. This work has been performed as part of the Infom@gic project, whose goal is the integration and validation of knowledge engineering and information analysis applications, and which is supported by the pole of competitiveness Cap Digital « Image, MultiMédia et Vie Numérique ». We first describe the four annotators, which provide named entity annotations that conform to guidelines defined in the Infom@gic project. Then we present an algorithm for merging the different annotations. It uses information about the compatibility of various annotations and can point out conflicts, and thus yields annotations that are more reliable than those of any single annotator. We conclude by describing and interpreting the merging results obtained on a manually annotated reference corpus.},
  motscles  = {Entités nommées, fusion d’annotations, UIMA},
  keywords  = {Named entities, fusion of annotations, UIMA},
}

@inproceedings{leon:2009:TALN,
  author    = {Léon, Stéphanie},
  title     = {Un système modulaire d’acquisition automatique de traductions à partir du Web},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-027},
  language  = {french},
  resume    = {Nous présentons une méthode de Traduction Automatique d’Unités Lexicales Complexes (ULC) pour la construction de ressources bilingues français/anglais, basée sur un système modulaire qui prend en compte les propriétés linguistiques des unités sources (compositionnalité, polysémie, etc.). Notre système exploite les différentes « facettes » du Web multilingue pour valider des traductions candidates ou acquérir de nouvelles traductions. Après avoir collecté une base d’ULC en français à partir d’un corpus de pages Web, nous passons par trois phases de traduction qui s’appliquent à un cas linguistique, avec une méthode adaptée : les traductions compositionnelles non polysémiques, les traductions compositionnelles polysémiques et les traductions non compositionnelles et/ou inconnues. Notre évaluation sur un vaste échantillon d’ULC montre que l’exploitation du Web pour la traduction et la prise en compte des propriétés linguistiques au sein d’un système modulaire permet une acquisition automatique de traductions avec une excellente précision.},
  abstract  = {We present a method of automatic translation (French/English) of Complex Lexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular system is based on linguistic properties (compositionality, polysemy, etc.). Different aspects of the multilingual Web are used to validate candidate translations and collect new terms. We first build a French corpus of Web pages to collect CLU. Three adapted processing stages are applied for each linguistic property : compositional and non polysemous translations, compositional polysemous translations and non compositional translations. Our evaluation on a sample of CLU shows that our technique based on the Web can reach a very high precision.},
  motscles  = {Traduction Automatique, Unités Lexicales Complexes, Désambiguïsation lexicale, World Wide Web, Terminologie},
  keywords  = {Automatic translation, Complex lexical units, Lexical disambiguisation, World Wide Web, Terminology},
}

@inproceedings{blache:2009:TALN,
  author    = {Blache, Philippe},
  title     = {Des relations d’alignement pour décrire l’interaction des domaines linguistiques : vers des Grammaires Multimodales},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-028},
  language  = {french},
  resume    = {Un des problèmes majeurs de la linguistique aujourd’hui réside dans la prise en compte de phénomènes relevant de domaines et de modalités différentes. Dans la littérature, la réponse consiste à représenter les relations pouvant exister entre ces domaines de façon externe, en termes de relation de structure à structure, s’appuyant donc sur une description distincte de chaque domaine ou chaque modalité. Nous proposons dans cet article une approche différente permettant représenter ces phénomènes dans un cadre formel unique, permettant de rendre compte au sein d’une même grammaire tous les phénomènes concernés. Cette représentation précise de l’interaction entre domaines et modalités s’appuie sur la définition de relations d’alignement.},
  abstract  = {Linguistics is now faced with the question of representing information coming from different domains or modalities. The classical answer consists in representing separately each of these domains, building for each of them an independent structure, and then representing domain interaction in terms of relation between structures. We propose n this paper a different approach in which all information is represented within a unique and homogeneous framework, making it possible to represent into a same grammar all interaction phenomena. This precise representation of interaction relies on the definition of a new notion of alignment relations.},
  motscles  = {Multimodalité, interaction entre domaines, grammaire, corpus multimodaux},
  keywords  = {Multimodality, domains interaction, grammar, multimodal corpora},
}

@inproceedings{fort-ehrmann-nazarenko:2009:TALN,
  author    = {Fort, Karën and Ehrmann, Maud and Nazarenko, Adeline},
  title     = {Vers une méthodologie d’annotation des entités nommées en corpus ?},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-long-029},
  language  = {french},
  resume    = {La tâche, aujourd’hui considérée comme fondamentale, de reconnaissance d’entités nommées, présente des difficultés spécifiques en matière d’annotation. Nous les précisons ici, en les illustrant par des expériences d’annotation manuelle dans le domaine de la microbiologie. Ces problèmes nous amènent à reposer la question fondamentale de ce que les annotateurs doivent annoter et surtout, pour quoi faire. Nous identifions pour cela les applications nécessitant l’extraction d’entités nommées et, en fonction des besoins de ces applications, nous proposons de définir sémantiquement les éléments à annoter. Nous présentons ensuite un certain nombre de recommandations méthodologiques permettant d’assurer un cadre d’annotation cohérent et évaluable.},
  abstract  = {Today, the named entity recognition task is considered as fundamental, but it involves some specific difficulties in terms of annotation. We list them here, with illustrations taken from manual annotation experiments in microbiology. Those issues lead us to ask the fundamental question of what the annotators should annotate and, even more important, for which purpose. We thus identify the applications using named entity recognition and, according to the real needs of those applications, we propose to semantically define the elements to annotate. Finally, we put forward a number of methodological recommendations to ensure a coherent and reliable annotation scheme.},
  motscles  = {annotation, reconnaissance d’entités nommées},
  keywords  = {annotation, named entities extraction},
}

@inproceedings{ozdowska:2009:TALN,
  author    = {Ozdowska, Sylwia},
  title     = {Données bilingues pour la TAS français-anglais : impact de la langue source et direction de traduction originales sur la qualité de la traduction},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-position-001},
  language  = {french},
  resume    = {Dans cet article, nous prenons position par rapport à la question de la qualité des données bilingues destinées à la traduction automatique statistique en terme de langue source et direction de traduction originales à l’égard d’une tâche de traduction français-anglais. Nous montrons que l’entraînement sur un corpus contenant des textes qui ont été à l’origine traduits du français vers l’anglais améliore la qualité de la traduction. Inversement, l’entraînement sur un corpus contenant exclusivement des textes dont la langue source originale n’est ni le français ni l’anglais dégrade la traduction.},
  abstract  = {In this paper, we argue about the quality of bilingual data for statistical machine translation in terms of the original source language and translation direction in the context of a French to English translation task. We show that data containing original French and English translated from French improves translation quality. Conversely, using data comprising exclusively French and English translated from several other languages results in a clear-cut decrease in translation quality.},
  motscles  = {Traduction automatique statistique, corpus bilingue, direction de la traduction, langue source, langue cible},
  keywords  = {Statistical machine translation, bilingual corpus, translation direction, source language, target language},
}

@inproceedings{apidianaki:2009:TALN,
  author    = {Apidianaki, Marianna},
  title     = {La place de la désambiguïsation lexicale dans la Traduction Automatique Statistique},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-position-002},
  language  = {french},
  resume    = {L’étape de la désambiguïsation lexicale est souvent esquivée dans les systèmes de Traduction Automatique Statistique (Statistical Machine Translation (SMT)) car considérée comme non nécessaire à la sélection de traductions correctes. Le débat autour de cette nécessité est actuellement assez vif. Dans cet article, nous présentons les principales positions sur le sujet. Nous analysons les avantages et les inconvénients de la conception actuelle de la désambiguïsation dans le cadre de la SMT, d’après laquelle les sens des mots correspondent à leurs traductions dans des corpus parallèles. Ensuite, nous présentons des arguments en faveur d’une analyse plus poussée des informations sémantiques induites à partir de corpus parallèles et nous expliquons comment les résultats d’une telle analyse pourraient être exploités pour une évaluation plus flexible et concluante de l’impact de la désambiguïsation dans la SMT.},
  abstract  = {Word Sense Disambiguation (WSD) is often omitted in Statistical Machine Translation (SMT) systems, as it is considered unnecessary for lexical selection. The discussion on the need ofWSD is currently very active. In this article we present the main positions on the subject. We analyze the advantages and weaknesses of the current conception of WSD in SMT, according to which the senses of ambiguous words correspond to their translations in a parallel corpus. Then we present some arguments towards a more thorough analysis of the semantic information induced from parallel corpora and we explain how the results of this analysis could be exploited for a more flexible and conclusive evaluation of the impact of WSD on SMT.},
  motscles  = {Désambiguïsation lexicale, Traduction Automatique Statistique, sélection lexicale},
  keywords  = {Word Sense Disambiguation, Statistical Machine Translation, lexical selection},
}

@inproceedings{laurent-EtAl:2009:TALN,
  author    = {Laurent, Marianne and Putois, Ghislain and Bretier, Philippe and Moudenc, Thierry},
  title     = {Nouveau paradigme d’évaluation des systèmes de dialogue homme-machine},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-position-003},
  language  = {french},
  resume    = {L’évaluation des systèmes de dialogue homme-machine est un problème difficile et pour lequel ni les objectifs ni les solutions proposées ne font aujourd’hui l’unanimité. Les approches ergonomiques traditionnelles soumettent le système de dialogue au regard critique de l’utilisateur et tente d’en capter l’expression, mais l’absence d’un cadre objectivable des usages de ces utilisateurs empêche une comparaison entre systèmes différents, ou entre évolutions d’un même système. Nous proposons d’inverser cette vision et de mesurer le comportement de l’utilisateur au regard du système de dialogue. Aussi, au lieu d’évaluer l’adéquation du système à ses utilisateurs, nous mesurons l’adéquation des utilisateurs au système. Ce changement de paradigme permet un changement de référentiel qui n’est plus les usages des utilisateurs mais le cadre du système. Puisque le système est complètement défini, ce paradigme permet des approches quantitatives et donc des évaluations comparatives de systèmes.},
  abstract  = {Evaluation of a human-machine dialogue system is a difficult problem for which neither the objectives nor the proposed solutions gather a unanimous support. Traditional approaches in the ergonomics field evaluate the system by describing how it fits the user in the user referential of practices. However, the user referential is even more complicated to formalise, and one cannot ground a common use context to enable the comparison of two systems, even if they are merely an evolution of the same service. We propose to shift the point of view on the evaluation problem : instead of evaluating the system in interaction with the user in the user’s referential, we will now measure the user’s adequacy to the system in the system referential. This is our Copernician revolution : for the evaluation purpose, our system is no longer user-centric, because the user referential is not properly objectifiable, while the system referential is completely known by design.},
  motscles  = {Évaluation, Dialogue},
  keywords  = {Evaluation, Dialogue},
}

@inproceedings{brunetmanquat-goulian:2009:TALN,
  author    = {Brunet-Manquat, Francis and Goulian, Jérôme},
  title     = {ACOLAD un environnement pour l’édition de corpus de dépendances},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-001},
  language  = {french},
  resume    = {Dans cette démonstration, nous présentons le prototype d’un environnement open-source pour l’édition de corpus de dépendances. Cet environnement, nommé ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks).},
  abstract  = {},
  motscles  = {dépendances, chunk, édition},
  keywords  = {},
}

@inproceedings{bouamor-max-vilnat:2009:TALN,
  author    = {Bouamor, Houda and Max, Aurélien and Vilnat, Anne},
  title     = {Amener des utilisateurs à créer et évaluer des paraphrases par le jeu},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-002},
  language  = {french},
  resume    = {Dans cet article, nous présentons une application sur le web pour l’acquisition de paraphrases phrastiques et sous-phrastiques sous forme de jeu. L’application permet l’acquisition à la fois de paraphrases et de jugements humains multiples sur ces paraphrases, ce qui constitue des données particulièrement utiles pour les applications du TAL basées sur les phénomènes paraphrastiques.},
  abstract  = {In this article, we present a web application presented as a game for acquiring sentencial and phrasal paraphrases. It can be used both to acquire paraphrases and important quantities of human evaluations of their quality. These are particularly useful for NLP applications relying on paraphrasing.},
  motscles  = {Paraphrase, acquisition de données, évaluation de données},
  keywords  = {Paraphrasing, data acquisition, data evaluation},
}

@inproceedings{lardilleux-lepage:2009:TALN,
  author    = {Lardilleux, Adrien and Lepage, Yves},
  title     = {anymalign : un outil d’alignement sous-phrastique libre pour les êtres humains},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-003},
  language  = {french},
  resume    = {Nous présentons anymalign, un aligneur sous-phrastique grand public. Ses résultats ont une qualité qui rivalise avec le meilleur outil du domaine, GIZA++. Il est rapide et simple d’utilisation, et permet de produire dictionnaires et autres tables de traduction en une seule commande. À notre connaissance, c’est le seul outil au monde permettant d’aligner un nombre quelconque de langues simultanément. Il s’agit donc du premier aligneur sousphrastique réellement multilingue.},
  abstract  = {We present anymalign, a sub-sentential aligner oriented towards end users. It produces results that are competitive with the best known tool in the domain, GIZA++. It is fast and easy to use, and allows dictionaries or translation tables to be produced in a single command. To our knowledge, it is the only tool in the world capable of aligning any number of languages simultaneously. It is therefore the first truly multilingual sub-sentential aligner.},
  motscles  = {alignement sous-phrastique, multilinguisme, table de traduction},
  keywords  = {sub-sentential alignment, multilinguism, translation table},
}

@inproceedings{charlet-EtAl:2009:TALN,
  author    = {Charlet, Jean and Szulman, Sylvie and Aussenac-Gilles, Nathalie and Nazarenko, Adeline and Hernandez, Nathalie and Nadah, Nadia and Sardet, Éric and Delahousse, Jean and Pierra, Guy},
  title     = {Apport des outils de TAL à la construction d’ontologies : propositions au sein de la plateforme DaFOE},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-004},
  language  = {french},
  resume    = {La construction d’ontologie à partir de textes fait l’objet d’études depuis plusieurs années dans le domaine de l’ingénierie des ontologies. Un cadre méthodologique en quatre étapes (constitution d’un corpus de documents, analyse linguistique du corpus, conceptualisation, opérationnalisation de l’ontologie) est commun à la plupart des méthodes de construction d’ontologies à partir de textes. S’il existe plusieurs plateformes de traitement automatique de la langue (TAL) permettant d’analyser automatiquement les corpus et de les annoter tant du point de vue syntaxique que statistique, il n’existe actuellement aucune procédure généralement acceptée, ni a fortiori aucun ensemble cohérent d’outils supports, permettant de concevoir de façon progressive, explicite et traçable une ontologie de domaine à partir d’un ensemble de ressources informationnelles relevant de ce domaine. Le but de ce court article est de présenter les propositions développées, au sein du projet ANR DaFOE 4app, pour favoriser l’émergence d’un tel ensemble d’outils.},
  abstract  = {The concept of ontologies, appeared in the nineties, constitute a key point to represent and share the meaning carried out by formal symbols. Thus, the building of such an ontology is quite difficult. A way to do so is to use preexistent elements (textual corpus, taxonomies, norms or other ontologies) and operate them as a basis to define the ontology field. However, there is neither accepted process nor set of tools to progressively built ontologies from the available resources in a traceable and explicit way. We report in this paper several propositions developed within the framework of the ANR DaFOE4App project to support emergence of such tools.},
  motscles  = {Ontologie, construction d’ontologie, TALN},
  keywords  = {Ontology, Ontology building, NLP},
}

@inproceedings{weissenbacher-EtAl:2009:TALN,
  author    = {Weissenbacher, Davy and Pieri, Elisa and Ananiadou, Sophia and Rea, Brian and Vis, Farida and Lin, Yuwei and Procter, Rob and Halfpenny, Peter},
  title     = {ASSIST : un moteur de recherche spécialisé pour l’analyse des cadres d’expériences},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-005},
  language  = {french},
  resume    = {L’analyse qualitative des données demande au sociologue un important travail de sélection et d’interprétation des documents. Afin de faciliter ce travail, cette communauté c’est dotée d’outils informatique mais leur fonctionnalités sont encore limitées. Le projet ASSIST est une étude exploratoire pour préciser les modules de traitement automatique des langues (TAL) permettant d’assister le sociologue dans son travail d’analyse. Nous présentons le moteur de recherche réalisé et nous justifions le choix des composants de TAL intégrés au prototype.},
  abstract  = {Qualitative data analysis requiers from the sociologist an important work of selection and interpretation of the documents. To facilitate this work, several software have been created but their functionalities are still limitated. The ASSIST project is a preliminary work to define the natural language processing modules for helping the sociologist. We present the search engine realised and justify the NLP modules integrated in the prototype.},
  motscles  = {Recherche d’information, Extraction d’information, Terminologie},
  keywords  = {Information Retrieval, Information Extraction, Terminology},
}

@inproceedings{smilauer:2009:TALN,
  author    = {Šmilauer, Ivan},
  title     = {CETLEF.fr - diagnostic automatique des erreurs de déclinaison tchèque dans un outil ELAO},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-006},
  language  = {french},
  resume    = {CETLEF.fr – une application Web dynamique – propose des exercices de déclinaison tchèque avec un diagnostic automatique des erreurs. Le diagnostic a nécessité l'élaboration d'un modèle formel spécifique de la déclinaison contenant un classement des types paradigmatiques et des règles pour la réalisation des alternances morphématiques. Ce modèle est employé pour l'annotation des formes requises, nécessaire pour le diagnostic, mais également pour une présentation didactique sur la plateforme apprenant. Le diagnostic est effectué par comparaison d'une production erronée avec des formes hypothétiques générées à partir du radical de la forme requise et des différentes désinences casuelles. S'il existe une correspondance, l'erreur est interprétée d'après les différences dans les traits morphologiques de la forme requise et de la forme hypothétique. La majorité des erreurs commises peut être interprétée à l'aide de cette technique.},
  abstract  = {CETLEF.fr – a dynamic Web application – contains fill-in-the-blank exercises on Czech declension with an automatic error diagnosis. The diagnosis rendered necessary the definition of a specific formal model of nominal inflection containing a classification of the paradigms and the rules for the realization of morphemic alternations. This model has been employed for the morphological annotation of required forms, necessary for the error diagnosis as well as for a didactic presentation on the learning platform. Diagnosis is carried out by the comparison of an erroneous production with hypothetical forms generated from the radical of the required form and various haphazard endings. If a correspondence is found, the error is interpreted according to the differences in the morphological features of the required form and the hypothetical form. The majority of errors can be interpreted with the aid of this technique.},
  motscles  = {morphologie flexionnelle, déclinaison tchèque, acquisition d'une langue étrangère, diagnostic des erreurs et feedback, ELAO},
  keywords  = {inflectional morphology, Czech declension, second language acquisition, error diagnosis and feedback, CALL},
}

@inproceedings{fafiotte-falaise-goulian:2009:TALN,
  author    = {Fafiotte, Georges and Falaise, Achille and Goulian, Jérôme},
  title     = {CIFLI-SurviTra, deux facettes : démonstrateur de composants de TA fondée sur UNL, et phrasebook multilingue},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-007},
  language  = {french},
  resume    = {CIFLI-SurviTra ("Survival Translation" assistant) est une plate-forme destinée à favoriser l'ingénierie et la mise au point de composants UNL de TA, à partir d'une mémoire de traduction formée de livres de phrases multilingues avec variables lexicales. SurviTra est aussi un phrasebook digital multilingue, assistant linguistique pour voyageurs monolingues (français, hindi, tamoul, anglais) en situation de "survie linguistique". Le corpus d’un domaine-pilote ("Restaurant") a été structuré et construit : sous-domaines de phrases alignées et classes lexicales de locutions quadrilingues, graphes UNL, dictionnaires UW++/français et UW++/hindi par domaines. L’approche, générique, est applicable à d’autres langues. Le prototype d’assistant linguistique (application Web, à interface textuelle) peut évoluer vers une application UNL embarquée sur SmartPhone, avec Traitement de Parole et multimodalité.},
  abstract  = {},
  motscles  = {TA via UNL, démonstrateur de composants UNL, assistant linguistique sur le Web, phrasebook digital multilingue, mémoire de traduction, collecte collaborative de corpus},
  keywords  = {},
}

@inproceedings{petrakis-EtAl:2009:TALN,
  author    = {Petrakis, Stefanos and Klenner, Manfred and Ailloud, Étienne and Fahrni, Angela},
  title     = {Composition multilingue de sentiments},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-008},
  language  = {french},
  resume    = {Nous présentons ici PolArt, un outil multilingue pour l’analyse de sentiments qui aborde la composition des sentiments en appliquant des transducteurs en cascade. La compositionnalité est assurée au moyen de polarités préalables extraites d’un lexique et des règles de composition appliquées de manière incrémentielle.},
  abstract  = {We introduce PolArt, a multilingual tool for sentiment detection that copes with sentiment composition through the application of cascaded transducers. Compositionality is enabled by prior polarities taken from a polarity lexicon and the compositional rules applied incrementally.},
  motscles  = {Analyse de sentiments},
  keywords  = {Sentiment Detection},
}

@inproceedings{alrahabi-descles:2009:TALN,
  author    = {Alrahabi, Motasem and Desclés, Jean-Pierre},
  title     = {EXCOM : Plate-forme d'annotation sémantique de textes multilingues},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-009},
  language  = {french},
  resume    = {Nous proposons une plateforme d‟annotation sémantique, appelée « EXCOM ». Basée sur la méthode de l‟ « Exploration Contextuelle », elle permet, à travers une diversité de langues, de procéder à des annotations automatiques de segments textuels par l'analyse des formes de surface dans leur contexte. Les textes sont traités selon des « points de vue » discursifs dont les valeurs sont organisées dans une « carte sémantique ». L‟annotation se base sur un ensemble de règles linguistiques, écrites par un analyste, qui permettent d‟identifier les représentations textuelles sous-jacentes aux différentes catégories de la carte. Le système offre, à travers deux types d‟interfaces (développeur ou utilisateur), une chaîne de traitements automatiques de textes qui comprend la segmentation, l‟annotation et d‟autres fonctionnalités de post-traitement. Les documents annotés peuvent être utilisés, par exemple, pour des systèmes de recherche d‟information, de veille, de classification ou de résumé automatique.},
  abstract  = {We propose a platform for semantic annotation, called “EXCOM”. Based on the “Contextual Exploration” method, it enables, across a great range of languages, to perform automatic annotations of textual segments by analyzing surface forms in their context. Texts are approached through discursive “points of view”, of which values are organized into a “semantic map”. The annotation is based on a set of linguistic rules, manually constructed by an analyst, and that enables to automatically identify the textual representations underlying the different semantic categories of the map. The system provides through two sorts of user-friendly interfaces (analyst or end-user) a complete pipeline of automatic text processing which consists of segmentation, annotation and other post-processing functionalities. Annotated documents can be used, for instance, for information retrieval systems, classification or automatic summarization.},
  motscles  = {Plate-forme, Annotation automatique, Exploration Contextuelle, analyse sémantique, marqueurs discursifs, carte sémantique, multilinguisme},
  keywords  = {Platform, automatic annotation, Contextual Exploration, semantic analysis, discoursive markers, Semantic Map, multilingual approach},
}

@inproceedings{widlocher-mathet:2009:TALN,
  author    = {Widlöcher, Antoine and Mathet, Yann},
  title     = {La plate-forme d’annotation Glozz},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-010},
  language  = {french},
  resume    = {},
  abstract  = {},
  motscles  = {Linguistique de corpus, Annotation, Plate-forme logicielle},
  keywords  = {Corpus Linguistics, Annotation, Software Framework},
}

@inproceedings{raoul:2009:TALN,
  author    = {Raoul, Blin},
  title     = {SAGACE-v3.3 ; Analyseur de corpus pour langues non flexionnelles},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-011},
  language  = {french},
  resume    = {Nous présentons la dernière version du logiciel SAGACE, analyseur de corpus pour langues faiblement flexionnelles (par exemple japonais ou chinois). Ce logiciel est distribué avec un lexique où les catégories sont exprimées à l'aide de systèmes de traits.},
  abstract  = {We present a software program named SAGACE, designed to search for and extract word strings from a large corpus. It has been conceived for poor flexional languages, such as Japanese or Chinese. It is associated with a lexicon where categories are expressed with feature systems.},
  motscles  = {corpus, lexique, analyseur, japonais, chinois},
  keywords  = {corpus, lexicon, analyzer, japanese, chinese},
}

@inproceedings{hernandez-EtAl:2009:TALN,
  author    = {Hernandez, Nicolas and Poulard, Fabien and Afantenos, Stergos and Vernier, Matthieu and Rocheteau, Jérôme},
  title     = {Apache UIMA pour le Traitement Automatique des Langues},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-012},
  language  = {french},
  resume    = {L’objectif de la démonstration est d’une part de faire un retour d’expérience sur la solution logicielle Apache UIMA comme infrastructure de développement d’applications distribuées de TAL, et d’autre part de présenter les développements réalisés par l’équipe TALN du LINA pour permettre à la communauté de s’approprier ce « framework ».},
  abstract  = {Our objectives are twofold : First, based on some common use cases, we will discuss the interest of using UIMA as a middleware solution for developing Natural Language Processing systems. Second, we will present various preprocessing tools we have developed in order to facilitate the access to the framework for the French community.},
  motscles  = {Apache UIMA, Applications du TAL, Infrastructure logicielle},
  keywords  = {Apache UIMA, NLP applications, Middleware},
}

@inproceedings{lehuen-lemeunier:2009:TALN,
  author    = {Lehuen, Jérôme and Lemeunier, Thierry},
  title     = {Un Analyseur Sémantique pour le DHM},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-013},
  language  = {french},
  resume    = {},
  abstract  = {},
  motscles  = {},
  keywords  = {},
}

@inproceedings{vergne:2009:TALN,
  author    = {Vergne, Jacques},
  title     = {Un chunker multilingue endogène},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-demo-014},
  language  = {french},
  resume    = {Le chunking consiste à segmenter un texte en chunks, segments sous-phrastiques qu'Abney a défini approximativement comme des groupes accentuels. Traditionnellement, le chunking utilise des ressources monolingues, le plus souvent exhaustives, quelquefois partielles : des mots grammaticaux et des ponctuations, qui marquent souvent des débuts et fins de chunk. Mais cette méthode, si l'on veut l'étendre à de nombreuses langues, nécessite de multiplier les ressources monolingues. Nous présentons une nouvelle méthode : le chunking endogène, qui n'utilise aucune ressource hormis le texte analysé lui-même. Cette méthode prolonge les travaux de Zipf : la minimisation de l'effort de communication conduit les locuteurs à raccourcir les mots fréquents. On peut alors caractériser un chunk comme étant la période des fonctions périodiques correllées longueur et effectif des mots sur l'axe syntagmatique. Cette méthode originale présente l'avantage de s'appliquer à un grand nombre de langues d'écriture alphabétique, avec le même algorithme, sans aucune ressource.},
  abstract  = {Chunking is segmenting a text into chunks, sub-sentential segments, that Abney approximately defined as stress groups. Chunking usually uses monolingual resources, most often exhaustive, sometimes partial : function words and punctuations, which often mark beginnings and ends of chunks. But, to extend this method to other languages, monolingual resources have to be multiplied. We present a new method : endogenous chunking, which uses no other resource than the text to be parsed itself. The idea of this method comes from Zipf : to make the least communication effort, speakers are driven to shorten frequent words. A chunk then can be characterised as the period of the periodic correlated functions length and frequency of words on the syntagmatic axis. This original method takes its advantage to be applied to a great number of languages of alphabetic script, with the same algorithm, without any resource.},
  motscles  = {chunking, multilingue, endogène, longueur des mots, effectif des mots},
  keywords  = {chunking, multilingual, endogenous, word length, word frequency},
}

@inproceedings{seddah-candito-crabbe:2009:TALN,
  author    = {Seddah, Djamé and Candito, Marie and Crabbé, Benoît},
  title     = {Adaptation de parsers statistiques lexicalisés pour le français : Une évaluation complète sur corpus arborés},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-001},
  language  = {french},
  resume    = {Cet article présente les résultats d’une évaluation exhaustive des principaux analyseurs syntaxiques probabilistes dit “lexicalisés” initialement conçus pour l’anglais, adaptés pour le français et évalués sur le CORPUS ARBORÉ DU FRANÇAIS (Abeillé et al., 2003) et le MODIFIED FRENCH TREEBANK (Schluter \& van Genabith, 2007). Confirmant les résultats de (Crabbé \& Candito, 2008), nous montrons que les modèles lexicalisés, à travers les modèles de Charniak (Charniak, 2000), ceux de Collins (Collins, 1999) et le modèle des TIG Stochastiques (Chiang, 2000), présentent des performances moindres face à un analyseur PCFG à Annotation Latente (Petrov et al., 2006). De plus, nous montrons que le choix d’un jeu d’annotations issus de tel ou tel treebank oriente fortement les résultats d’évaluations tant en constituance qu’en dépendance non typée. Comparés à (Schluter \& van Genabith, 2008; Arun \& Keller, 2005), tous nos résultats sont state-of-the-art et infirment l’hypothèse d’une difficulté particulière qu’aurait le français en terme d’analyse syntaxique probabiliste et de sources de données.},
  abstract  = {This paper presents complete investigation results on the statistical parsing of French by bringing a complete evaluation on French data of the main based probabilistic lexicalized (Charniak, Collins, Chiang) and unlexicalized (Berkeley) parsers designed first on the Penn Treebank. We adapted the parsers on the two existing treebanks of French (Abeillé et al., 2003; Schluter \& van Genabith, 2007). To our knowledge, all the results reported here are state-of-the-art for the constituent parsing of French on every available treebank and invalidate the hypothesis of French being particularly difficult to parse. Regarding the algorithms, the comparisons show that lexicalized parsing models are outperformed by the unlexicalized Berkeley parser. Regarding the treebanks, we observe that a tag set with specific features has direct influences over evaluation results depending on the parsing model.},
  motscles  = {Analyse syntaxique probabiliste, corpus arborés, évaluation, analyse du français},
  keywords  = {Probabilistic parsing, treebanks, evaluation, French parsing},
}

@inproceedings{namer:2009:TALN,
  author    = {Namer, Fiammetta},
  title     = {Analyse automatique des noms déverbaux composés : pourquoi et comment faire interagir analogie et système de règles},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-002},
  language  = {french},
  resume    = {Cet article aborde deux problèmes d’analyse morpho-sémantique du lexique : (1) attribuer automatiquement une définition à des noms et verbes morphologiquement construits inconnus des dictionnaires mais présents dans les textes ; (2) proposer une analyse combinant règles et analogie, deux techniques généralement contradictoires. Les noms analysés sont apparemment suffixés et composés (HYDROMASSAGE). La plupart d’entre eux, massivement attestés dans les documents (journaux, Internet) sont absents des dictionnaires. Ils sont souvent reliés à des verbes (HYDROMASSER) également néologiques. Le nombre de ces noms et verbes est estimé à 5.400. L’analyse proposée leur attribue une définition par rapport à leur base, et enrichit un lexique de référence pour le TALN au moyen de cette base, si elle est néologique. L’implémentation des contraintes linguistiques qui régissent ces formations est reproductible dans d’autres langues européennes où sont rencontrés les mêmes types de données dont l’analyse reflète le même raisonnement que pour le français.},
  abstract  = {This paper addresses two morpho-semantic parsing issues: (1) to automatically provide morphologically complex unknown nouns and verbs with a definition; (2) to propose a methodology combining both rules and analogy, which are techniques usually seen as inconsistent with eachother. The analysed nouns look like both suffixed and compounded (HYDROMASSAGE). Most of them are not stored in dictionaries, although they are very frequent in newspapers or online documents. They are often related to verbs (HYDROMASSER), also lacking from dictionaries. The estimated amount of these nouns and verbs is 5,400. The proposed analysis assigns them a definition calculated according to their base meaning, and it increases the existing reference lexicon content with this base, from the moment that it is a new-coined form. The implementation of linguistic constraints which govern this word formations is reproducible in other West-European languages, where the same data type is found, subject to the same kind of analysis.},
  motscles  = {Analyse morphologique, Annotation sémantique, Composition savante, Noms déverbaux, Règles, Analogie},
  keywords  = {Morphological parsing, Semantic annotation, Neo-classical compounds, Deverbal nouns, Rules, Analogy},
}

@inproceedings{marchand-guillaume-perrier:2009:TALN,
  author    = {Marchand, Jonathan and Guillaume, Bruno and Perrier, Guy},
  title     = {Analyse en dépendances à l’aide des grammaires d’interaction},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-003},
  language  = {french},
  resume    = {Cet article propose une méthode pour extraire une analyse en dépendances d’un énoncé à partir de son analyse en constituants avec les grammaires d’interaction. Les grammaires d’interaction sont un formalisme grammatical qui exprime l’interaction entre les mots à l’aide d’un système de polarités. Le mécanisme de composition syntaxique est régi par la saturation des polarités. Les interactions s’effectuent entre les constituants, mais les grammaires étant lexicalisées, ces interactions peuvent se traduire sur les mots. La saturation des polarités lors de l’analyse syntaxique d’un énoncé permet d’extraire des relations de dépendances entre les mots, chaque dépendance étant réalisée par une saturation. Les structures de dépendances ainsi obtenues peuvent être vues comme un raffinement de l’analyse habituellement effectuée sous forme d’arbre de dépendance. Plus généralement, ce travail apporte un éclairage nouveau sur les liens entre analyse en constituants et analyse en dépendances.},
  abstract  = {This article proposes a method to extract dependency structures from phrasestructure level parsing with Interaction Grammars. Interaction Grammars are a formalism which expresses interactions among words using a polarity system. Syntactical composition is led by the saturation of polarities. Interactions take place between constituents, but as grammars are lexicalized, these interactions can be translated at the level of words. Dependency relations are extracted from the parsing process : every dependency is the consequence of a polarity saturation. The dependency relations we obtain can be seen as a refinement of the usual dependency tree. Generally speaking, this work sheds new light on links between phrase structure and dependency parsing.},
  motscles  = {Analyse syntaxique, grammaires de dépendances, grammaires d’interaction, polarité},
  keywords  = {Syntactic analysis, dependency grammars, interaction grammars, polarity},
}

@inproceedings{prost:2009:TALN,
  author    = {Prost, Jean-Philippe},
  title     = {Analyse relâchée à base de contraintes},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-004},
  language  = {french},
  resume    = {La question de la grammaticalité, et celle duale de l’agrammaticalité, sont des sujets délicats à aborder, dès lors que l’on souhaite intégrer différents degrés, tant de grammaticalité que d’agrammaticalité. En termes d’analyse automatique, les problèmes posés sont de l’ordre de la représentation des connaissances, du traitement, et bien évidement de l’évaluation. Dans cet article, nous nous concentrons sur l’aspect traitement, et nous nous penchons sur la question de l’analyse d’énoncés agrammaticaux. Nous explorons la possibilité de fournir une analyse la plus complète possible pour un énoncé agrammatical, sans l’apport d’information complémentaire telle que par le biais de mal-règles ou autre grammaire d’erreurs. Nous proposons une solution algorithmique qui permet l’analyse automatique d’un énoncé agrammatical, sur la seule base d’une grammaire modèle-théorique de bonne formation. Cet analyseur est prouvé générer une solution optimale, selon un critère numérique maximisé.},
  abstract  = {The question of grammaticality, and the dual one of ungrammaticality, are topics delicate to address when interested in modeling different degrees, whether of grammaticality or ungrammaticality. As far as parsing is concerned, the problems are with regard to knowledge representation, processing, and obviously evaluation. In this paper, we concentrate on the processing aspect and we address the question of parsing ungrammatical utterances. We explore the possibility to provide a full parse for an ungrammatical utterance without relying on any kind of additional information, which would be provided by mal-rules or other error grammar. We propose an algorithmic solution in order to parse an ungrammatical utterance using only a model-theoretic grammar of well-formedness. The parser is proven to generate an optimal solution, according to a maximised criterion.},
  motscles  = {grammaticalité, analyse syntaxique, contraintes, syntaxe modèle-théorique},
  keywords  = {grammaticality, syntactic parsing, constraints, Model-Theoretic Syntax},
}

@inproceedings{perywoodley-EtAl:2009:TALN,
  author    = {Péry-Woodley, Marie-Paule and Asher, Nicholas and Enjalbert, Patrice and Benamara, Farah and Bras, Myriam and Fabre, Cécile and Ferrari, Stéphane and Ho-Dac, Lydia-Mai and Le Draoulec, Anne and Mathet, Yann and Muller, Philippe and Prévot, Laurent and Rebeyrolle, Josette and Tanguy, Ludovic and Vergez-Couret, Marianne and Vieu, Laure and Widlöcher, Antoine},
  title     = {ANNODIS: une approche outillée de l'annotation de structures discursives},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-005},
  language  = {french},
  resume    = {Le projet ANNODIS vise la construction d’un corpus de textes annotés au niveau discursif ainsi que le développement d'outils pour l’annotation et l’exploitation de corpus. Les annotations adoptent deux points de vue complémentaires : une perspective ascendante part d'unités de discours minimales pour construire des structures complexes via un jeu de relations de discours ; une perspective descendante aborde le texte dans son entier et se base sur des indices pré-identifiés pour détecter des structures discursives de haut niveau. La construction du corpus est associée à la création de deux interfaces : la première assiste l'annotation manuelle des relations et structures discursives en permettant une visualisation du marquage issu des prétraitements ; une seconde sera destinée à l'exploitation des annotations. Nous présentons les modèles et protocoles d'annotation élaborés pour mettre en oeuvre, au travers de l'interface dédiée, la campagne d'annotation.},
  abstract  = {The ANNODIS project has two interconnected objectives: to produce a corpus of texts annotated at discourse-level, and to develop tools for corpus annotation and exploitation. Two sets of annotations are proposed, representing two complementary perspectives on discourse organisation: a bottom-up approach starting from minimal discourse units and building complex structures via a set of discourse relations; a top-down approach envisaging the text as a whole and using pre-identified cues to detect discourse macro-structures. The construction of the corpus goes hand in hand with the development of two interfaces: the first one supports manual annotation of discourse structures, and allows different views of the texts using NLPM-based pre-processing; another interface will support the exploitation of the annotations. We present the discourse models and annotation protocols, and the interface which embodies them.},
  motscles  = {annotation de corpus, structures de discours, interface d'annotation},
  keywords  = {corpus annotation, discourse structures, annotation tools},
}

@inproceedings{moriceau-tannier:2009:TALN,
  author    = {Moriceau, Véronique and Tannier, Xavier},
  title     = {Apport de la syntaxe dans un système de question-réponse : étude du système FIDJI.},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-006},
  language  = {french},
  resume    = {Cet article présente une série d’évaluations visant à étudier l’apport d’une analyse syntaxique robuste des questions et des documents dans un système de questions-réponses. Ces évaluations ont été effectuées sur le système FIDJI, qui utilise à la fois des informations syntaxiques et des techniques plus “traditionnelles”. La sélection des documents, l’extraction de la réponse ainsi que le comportement selon les différents types de questions ont été étudiés.},
  abstract  = {This paper presents some experiments aiming at estimating the contribution of a syntactic parser on both questions and documents in a question-answering system. This evaluation has been performed with the system FIDJI, which makes use of both syntactic information and more “traditional” techniques. Document selection, answer extraction as well as system behaviour on different types of questions have been experimented.},
  motscles  = {Systèmes de questions-réponses, analyse syntaxique, évaluation},
  keywords  = {Question-answering, syntactic analysis, evaluation},
}

@inproceedings{laurent-negre-seguela:2009:TALN,
  author    = {Laurent, Dominique and Nègre, Sophie and Séguéla, Patrick},
  title     = {Apport des cooccurrences à la correction et à l'analyse syntaxique},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-007},
  language  = {french},
  resume    = {Le correcteur grammatical Cordial utilise depuis de nombreuses années les cooccurrences pour la désambiguïsation sémantique. Un dictionnaire de cooccurrences ayant été constitué pour les utilisateurs du logiciel de correction et d'aides à la rédaction, la grande richesse de ce dictionnaire a incité à l'utiliser intensivement pour la correction, spécialement des homonymes et paronymes. Les résultats obtenus sont spectaculaires sur ces types d'erreurs mais la prise en compte des cooccurrences a également été utilisée avec profit pour la pure correction orthographique et pour le rattachement des groupes en analyse syntaxique.},
  abstract  = {For many years, the spellchecker named Cordial has been using cooccurrences for semantic disambiguation. Since a dictionary of co-occurrences had been established for users of the spellchecker and of the writing aid, the richness of this dictionary led us to use it intensively for the correction, especially for homonyms and paronyms. The results are impressive on this kind of errors but taking into account the cooccurrences proved to be very profitable for pure spellchecking and for the attachment of groups in syntactic parsing.},
  motscles  = {cooccurrences, collocations, correction grammaticale},
  keywords  = {collocation, grammar checking},
}

@inproceedings{daoud-daoud:2009:TALN,
  author    = {Daoud, Daoud and Daoud, Mohammad},
  title     = {},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-008},
  language  = {french},
  note      = {Arabic Disambiguation Using Dependency Grammar},
  resume    = {},
  abstract  = {In this paper, we present a new approach to disambiguation Arabic using a joint rule-based model which is conceptualized using Dependency Grammar. This approach helps in highly accurate analysis of sentences. The analysis produces a semantic net like structure expressed by means of Universal Networking Language (UNL) - a recently proposed interlingua. Extremely varied and complex phenomena of Arabic language have been addressed.},
  motscles  = {},
  keywords  = {Dependency Grammar, Arabic Language, Disambiguation, EnCo, UNL},
}

@inproceedings{deloupy-bagur-blancafort:2009:TALN,
  author    = {De Loupy, Claude and Bagur, Michaël and Blancafort, Helena},
  title     = {Association automatique de lemmes et de paradigmes de flexion à un mot inconnu},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-009},
  language  = {french},
  resume    = {La maintenance et l’enrichissement des lexiques morphosyntaxiques sont souvent des tâches fastidieuses. Dans cet article nous présentons la mise en place d’une procédure de guessing de flexion afin d’aider les linguistes dans leur travail de lexicographes. Le guesser développé ne fait pas qu’évaluer l’étiquette morphosyntaxique comme c’est généralement le cas. Il propose pour un mot français inconnu, un ou plusieurs candidats-lemmes, ainsi que les paradigmes de flexion associés (formes fléchies et étiquettes morphosyntaxiques). Dans cet article, nous décrivons le modèle probabiliste utilisé ainsi que les résultats obtenus. La méthode utilisée permet de réduire considérablement le nombre de règles à valider, permettant ainsi un gain de temps important.},
  abstract  = {Lexicon maintenance and lexicon enrichment is a labour-intensive task. In this paper, we present preliminary work on an inflectional guessing procedure for helping the linguist in lexicographic tasks. The guesser presented here does not only output morphosyntactic tags, but also suggests for an unknown French word one or more lemma candidates as well as their corresponding inflectional rules and morphosyntactic tags that the linguist has to validate. In this article, we present the probabilistic model we used as well as obtained results. The method allows a drastic reduction of the number of rules to validate.},
  motscles  = {guesser, lexiques morphosyntaxiques, aide aux linguistes, induction des règles de flexion},
  keywords  = {guesser, morphosyntactic lexica, aide to the linguist, induction of inflection rules},
}

@inproceedings{vernier-EtAl:2009:TALN,
  author    = {Vernier, Matthieu and Monceaux, Laura and Daille, Béatrice and Dubreil, Estelle},
  title     = {Catégorisation sémantico-discursive des évaluations exprimées dans la blogosphère},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-010},
  language  = {french},
  resume    = {Les blogs constituent un support d’observations idéal pour des applications liées à la fouille d’opinion. Toutefois, ils imposent de nouvelles problématiques et de nouveaux défis au regard des méthodes traditionnelles du domaine. De ce fait, nous proposons une méthode automatique pour la détection et la catégorisation des évaluations localement exprimées dans un corpus de blogs multi-domaine. Celle-ci rend compte des spécificités du langage évaluatif décrites dans deux théories linguistiques. L’outil développé au sein de la plateforme UIMA vise d’une part à construire automatiquement une grammaire du langage évaluatif, et d’autre part à utiliser cette grammaire pour la détection et la catégorisation des passages évaluatifs d’un texte. La catégorisation traite en particulier l’aspect axiologique de l’évaluation, sa configuration d’énonciation et sa modalité dans le discours.},
  abstract  = {Blogs are an ideal observation for applications related to the opinion mining task. However, they impose new problems and new challenges in this field. Therefore, we propose a method for automatic detection and classification of appraisal locally expressed in a multi-domain blogs corpus. It reflects the specific aspects of appraisal language described in two linguistic theories. The tool developed within the UIMA platform aims both to automatically build a grammar of the appraisal language, and the other part to use this grammar for the detection and categorization of evaluative segments in a text. Categorization especially deals with axiological aspect of an evaluative segments, enunciative configuration and its attitude in discourse.},
  motscles  = {fouille d’opinion, langage évaluatif, catégorisation des évaluations},
  keywords  = {opinion mining, appraisal language, appraisal classification},
}

@inproceedings{weiser-coste-amardeilh:2009:TALN,
  author    = {Weiser, Stéphanie and Coste, Martin and Amardeilh, Florence},
  title     = {Chaîne de traitement linguistique : du repérage d'expressions temporelles au peuplement d'une ontologie de tourisme},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-011},
  language  = {french},
  resume    = {Cet article présente la chaîne de traitement linguistique réalisée pour la mise en place d'une plateforme touristique sur Internet. Les premières étapes de cette chaîne sont le repérage et l'annotation des expressions temporelles présentes dans des pages Web. Ces deux tâches sont effectuées à l'aide de patrons linguistiques. Elles soulèvent de nombreux questionnements auxquels nous tentons de répondre, notamment au sujet de la définition des informations à extraire, du format d'annotation et des contraintes. L'étape suivante consiste en l'exploitation des données annotées pour le peuplement d'une ontologie du tourisme. Nous présentons les règles d'acquisition nécessaires pour alimenter la base de connaissance du projet. Enfin, nous exposons une évaluation du système d'annotation. Cette évaluation permet de juger aussi bien le repérage des expressions temporelles que leur annotation.},
  abstract  = {This paper presents the linguistic data processing sequence built for a tourism web portal. The first steps of this sequence are the detection and the annotation of the temporal expressions found in the web pages. These tasks are performed using linguistic patterns. They lead to many questions which we try to answer, such as the definition of information to detect, annotation format and constraints. In the next step this annotated data is used to populate a tourism ontology. We present the acquisition rules which are necessary to enrich the portal knowledge base. Then we present an evaluation of our annotation system. This evaluation is able to judge the detection of the temporal expressions and their annotation.},
  motscles  = {Annotation, expressions temporelles, ontologies, base de connaissance, tourisme},
  keywords  = {Annotation, temporal expressions, ontologies, knowledge base, tourism},
}

@inproceedings{ma-audibert:2009:TALN,
  author    = {Ma, Yue and Audibert, Laurent},
  title     = {Détection des contradictions dans les annotations sémantiques},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-012},
  language  = {french},
  resume    = {L’annotation sémantique a pour objectif d’apporter au texte une représentation explicite de son interprétation sémantique. Dans un précédent article, nous avons proposé d’étendre les ontologies par des règles d’annotation sémantique. Ces règles sont utilisées pour l’annotation sémantique d’un texte au regard d’une ontologie dans le cadre d’une plate-forme d’annotation linguistique automatique. Nous présentons dans cet article une mesure, basée sur la valeur de Shapley, permettant d’identifier les règles qui sont sources de contradiction dans l’annotation sémantique. Par rapport aux classiques mesures de précision et de rappel, l’intérêt de cette mesure est de ne pas nécessiter de corpus manuellement annoté, d’être entièrement automatisable et de permettre l’identification des règles qui posent problème.},
  abstract  = {The semantic annotation has the objective to bring to a text an explicit representation of its semantic interpretation. In a preceding article, we suggested extending ontologies by semantic annotation rules. These rules are used for the semantic annotation of a text with respect to an ontology within the framework of an automated linguistic annotation platform. We present in this article a measure, based on the Shapley value, allowing to identify the rules which are sources of contradictions in the semantic annotation. With regard to the classic measures, precision and recall, the interest of this measure is without the requirement of manually annotated corpus, completely automated and its ability to identify rules which raise problems.},
  motscles  = {Annotation sémantique, valeur de Shapley, plate-forme d’annotation},
  keywords  = {Semantic annotation, Shapley value, annotation platform},
}

@inproceedings{letallec-EtAl:2009:TALN,
  author    = {Le Tallec, Marc and Villaneau, Jeanne and Antoine, Jean-Yves and Savary, Agata and Syssau-Vaccarella, Arielle},
  title     = {Détection des émotions à partir du contenu linguistique d’énoncés oraux : application à un robot compagnon pour enfants fragilisés},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-013},
  language  = {french},
  resume    = {Le projet ANR Emotirob aborde la question de la détection des émotions sous un cadre original : concevoir un robot compagnon émotionnel pour enfants fragilisés. Notre approche consiste à combiner détection linguistique et prosodie. Nos expériences montrent qu'un sujet humain peut estimer de manière fiable la valence émotionnelle d'un énoncé à partir de son contenu propositionnel. Nous avons donc développé un premier modèle de détection linguistique qui repose sur le principe de compositionnalité des émotions : les mots simples ont une valence émotionnelle donnée et les prédicats modifient la valence de leurs arguments. Après une description succincte du système logique de compréhension dont les sorties sont utilisées pour le calcul global de l'émotion, cet article présente la construction d'une norme émotionnelle lexicale de référence, ainsi que d'une ontologie de classes émotionnelles de prédicats, pour des enfants de 5 et 7 ans.},
  abstract  = {Project ANR Emotirob aims at detecting emotions from an original point of view: realizing an emotional companion robot for weakened children. In our approach, linguistic detection and prosodie are combined. Our experiments show that human beings can estimate the emotional value of an utterance from its propositional content in a reliable way. So we have implemented a first model of linguistic detection, based on the principle that emotions can be compound: lexical words have an emotional value while predicates can modify emotional values of their arguments. This paper presents a short description of the logical understanding system, the outputs of which are used for the final emotional value calculus. Then, the creation of a lexical emotional reference standard is presented with an ontology of emotional predicate classes for children, aged between 5 and 7.},
  motscles  = {Emotion, valence émotionnelle, norme lexicale émotionnelle, robot compagnon, compréhension de parole},
  keywords  = {Emotion, Emotional valency, Emotional lexical standard, companion robot, spoken language understanding},
}

@inproceedings{gala-rey-tichit:2009:TALN,
  author    = {Gala, Nuria and Rey, Véronique and Tichit, Laurent},
  title     = {Dispersion sémantique dans des familles morpho-phonologiques : éléments théoriques et empiriques},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-014},
  language  = {french},
  resume    = {Traditionnellement, la morphologie lexicale a été diachronique et a permis de proposer le concept de famille de mots. Ce dernier est repris dans les études en synchronie et repose sur une forte cohérence sémantique entre les mots d’une même famille. Dans cet article, nous proposons une approche en synchronie fondée sur la notion de continuité à la fois phonologique et sémantique. Nous nous intéressons, d’une part, à la morpho-phonologie et, d’autre part, à la dispersion sémantique des mots dans les familles. Une première étude (Gala \& Rey, 2008) montrait que les familles de mots obtenues présentaient des espaces sémantiques soit de grande cohésion soit de grande dispersion. Afin de valider ces observations, nous présentons ici une méthode empirique qui permet de pondérer automatiquement les unités de sens d’un mot et d’une famille. Une expérience menée auprès de 30 locuteurs natifs valide notre approche et ouvre la voie pour une étude approfondie du lexique sur ces bases phonologiques et sémantiques.},
  abstract  = {Traditionally, lexical morphology has been diachronic and has established the notion of word families. This notion is reused in synchronic studies and implies strong semantic coherence within the words of a same family. In this paper, we propose an approach in synchrony which highlights phonological and semantic continuity. Our interests go on morphophonology and on the semantic dispersion of words in a family. A first study (Gala \& Rey, 2008) showed that the semantic spaces of the families displayed either a strong semantic cohesion or a strong dispersion. In order to validate this observation, we present here a corpus-based method that automatically weights the semantic units of a word and a family. An experience carried out with 30 native speakers validates our approach and allows us to foresee a thorough study of the lexicon based on phonological and semantic basis.},
  motscles  = {morpho-phonologie lexicale, traitement automatique des familles dérivationnelles, espaces sémantiques},
  keywords  = {lexicalmorpho-phonology, derivational families processing, semantic spaces},
}

@inproceedings{sejourne:2009:TALN,
  author    = {Séjourné, Kévin},
  title     = {Exploitation d’une structure pour les questions enchaînées},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-015},
  language  = {french},
  resume    = {Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses (SQR) utilisant des questions enchaînées. La recherche des documents dans un SQR est perturbée par l’absence des éléments utiles à la recherche dans les questions liées, éléments figurant dans les échanges précédents. Les récentes campagnes d’évaluation montrent que ce problème est sous-estimé, et n’a pas fait l’objet de technique dédiée. Afin d’améliorer la recherche des documents dans un SQR nous utilisons une méthode récente d’organisation des informations liées aux interactions entre questions. Celle-ci se base sur l’exploitation d’une structure de données adaptée à la transmission des informations des questions liées jusqu’au moteur d’interrogation. Le moteur d’interrogation doit alors être adapté afin de tirer partie de cette structure de données.},
  abstract  = {We present works realized in the field of the questions answering (QA) using chained questions. The documents search in QA system is disrupted because useful elements are missing for search using bound questions. Recents evaluation campaigns show this problem as underestimated, and this problem wasn’t solve by specific techniques. To improve documents search in a QA we use a recent information organization method for bound questions to the interactions between questions. This methode is bases on the operation of a special data structure. This data structure transmit informations from bound questions to the interrogation engine. Then the interrogation engine must be improve to take advantage of this data structure.},
  motscles  = {Question réponse enchaînée},
  keywords  = {chained question answering},
}

@inproceedings{denis-quignard:2009:TALN,
  author    = {Denis, Alexandre and Quignard, Matthieu},
  title     = {Exploitation du terrain commun pour la production d’expressions référentielles dans les systèmes de dialogue},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-016},
  language  = {french},
  resume    = {Cet article présente un moyen de contraindre la production d’expressions référentielles par un système de dialogue en fonction du terrain commun. Cette capacité, fondamentale pour atteindre la compréhension mutuelle, est trop souvent oubliée dans les systèmes de dialogue. Le modèle que nous proposons s’appuie sur une modélisation du processus d’ancrage (grounding process) en proposant un raffinement du statut d’ancrage appliqué à la description des référents. Il décrit quand et comment ce statut doit être révisé en fonction des jugements de compréhension des deux participants ainsi que son influence dans le choix d’une description partagée destinée à la génération d’une expression référentielle.},
  abstract  = {This paper presents a way to constraint the production of referring expressions by a dialogue system according to the common ground. This ability – fundamental for reaching mutual understanding – is often neglected in dialogue system design. The proposed model is based on a view of the grounding process and offering a refinement of the grounding status concerning the referent description. It explains how and when this status should be revised with respect to how participants evaluate their understanding and how this status may help to choose a shared description with which a referring expression can be generated.},
  motscles  = {Compréhension mutuelle, processus d’ancrage, référence, génération},
  keywords  = {Mutual understanding, grounding process, referring expression, generation},
}

@inproceedings{bahou-bayoudhi-hadrichbelguith:2009:TALN,
  author    = {Bahou, Younès and Bayoudhi, Amine and Hadrich Belguith, Lamia},
  title     = {Gestion de dialogue oral Homme-machine en arabe},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-017},
  language  = {french},
  resume    = {Dans le présent papier, nous présentons nos travaux sur la gestion du dialogue oral arabe Homme-machine. Ces travaux entrent dans le cadre de la réalisation du serveur vocal interactif SARF (Bahou et al., 2008) offrant des renseignements sur le transport ferroviaire tunisien en langue arabe standard moderne. Le gestionnaire de dialogue que nous proposons est basé sur une approche structurelle et est composé de deux modèles à savoir, le modèle de tâche et le modèle de dialogue. Le premier modèle permet de i) compléter et vérifier l’incohérence des structures sémantiques représentant les sens utiles des énoncés, ii) générer une requête vers l’application et iii) récupérer le résultat et de formuler une réponse à l’utilisateur en langage naturel. Quant au modèle de dialogue, il assure l’avancement du dialogue avec l’utilisateur et l’identification de ses intentions. L’interaction entre ces deux modèles est assurée grâce à un contexte du dialogue permettant le suivi et la mise à jour de l’historique du dialogue.},
  abstract  = {In this paper, we present our research work on Human-machine Arabic oral dialogue management. This work enters in the context of SARF system (Bahou et al., 2008) an interactive vocal server that provides information on Tunisian railway using modern standard Arabic. The dialogue manager that we propose is based on a structural approach and consists of two models namely, the task model and the dialogue model. The first model is used to i) complete and verify the incoherence of semantic structures representing the useful meaning of utterances, ii) generate a query to the application and iii) get back the results and formulate an answer to the user in natural language. As for the dialogue model, it assures the dialogue progress with the user and the identification of her or his intentions. The interaction between these two models is assured by a dialogue context that allows monitoring and updating the dialogue history.},
  motscles  = {gestion du dialogue Homme-machine, dialogue oral arabe, modèle de tâche, modèle de dialogue},
  keywords  = {Human-machine dialogue management, Arabic oral dialogue, task model, dialogue model},
}

@inproceedings{clement-gerdes-marlet:2009:TALN,
  author    = {Clément, Lionel and Gerdes, Kim and Marlet, Renaud},
  title     = {Grammaires d’erreur – correction grammaticale avec analyse profonde et proposition de corrections minimales},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-018},
  language  = {french},
  resume    = {Nous présentons un système de correction grammatical ouvert, basé sur des analyses syntaxiques profondes. La spécification grammaticale est une grammaire hors-contexte équipée de structures de traits plates. Après une analyse en forêt partagée où les contraintes d’accord de traits sont relâchées, la détection d’erreur minimise globalement les corrections à effectuer et des phrases alternatives correctes sont automatiquement proposées.},
  abstract  = {We present an open system for grammar checking, based on deep parsing. The grammatical specification is a contex-free grammar with flat feature structures. After a sharedforest analysis where feature agreement constraints are relaxed, error detection globally minimizes the number of fixes and alternate correct sentences are automatically proposed.},
  motscles  = {Correcteur grammatical, analyse syntaxique, forêt partagée},
  keywords  = {Grammar checker, parsing, shared forest},
}

@inproceedings{bittar-danlos:2009:TALN,
  author    = {Bittar, André and Danlos, Laurence},
  title     = {Intégration des constructions à verbe support dans TimeML},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-019},
  language  = {french},
  resume    = {Le langage TimeML a été conçu pour l’annotation des informations temporelles dans les textes, notamment les événements, les expressions de temps et les relations entre les deux. Des consignes d’annotation générales ont été élaborées afin de guider l’annotateur dans cette tâche, mais certains phénomènes linguistiques restent à traiter en détail. Un problème commun dans les tâches de TAL, que ce soit en traduction, en génération ou en compréhension, est celui de l’encodage des constructions à verbe support. Relativement peu d’attention a été portée, jusqu’à maintenant, sur ce problème dans le cadre du langage TimeML. Dans cet article, nous proposons des consignes d’annotation pour les constructions à verbe support.},
  abstract  = {TimeML is a markup language developed for the annotation of temporal information in texts, in particular events, temporal expressions and the relations which hold between the two. General annotation guidelines have been developed to guide the annotator in this task, but certain linguistic phenomena have yet to be dealt with in detail. A common problem in NLP tasks, whether in translation, generation or understanding, is that of the encoding of light verb constructions. Relatively little attention has been paid to this problem, until now, in the TimeML framework. In this article, we propose annotation guidelines for light verb constructions.},
  motscles  = {TimeML, verbes support, discours, sémantique},
  keywords  = {TimeML, light verbs, discourse, semantics},
}

@inproceedings{sagot-tolone:2009:TALN,
  author    = {Sagot, Benoît and Tolone, Elsa},
  title     = {Intégrer les tables du Lexique-Grammaire à un analyseur syntaxique robuste à grande échelle},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-020},
  language  = {french},
  resume    = {Dans cet article, nous montrons comment nous avons converti les tables du Lexique-Grammaire en un format TAL, celui du lexique Lefff, permettant ainsi son intégration dans l’analyseur syntaxique FRMG. Nous présentons les fondements linguistiques de ce processus de conversion et le lexique obtenu. Nous validons le lexique obtenu en évaluant l’analyseur syntaxique FRMG sur le corpus de référence de la campagne EASy selon qu’il utilise les entrées verbales du Lefff ou celles des tables des verbes du Lexique-Grammaire ainsi converties.},
  abstract  = {In this paper, we describe how we converted the lexicon-grammar tables into an NLP format, that of the Lefff lexicon, which allowed us to integrate it into the FRMG parser. We decribe the linguistic basis of this conversion process, and the resulting lexicon.We validate the resulting lexicon by evaluating the FRMG parser on the EASy reference corpus depending on the set of verbal entries it relies on, namely those of the Lefff or those of the converted lexicon-grammar verb tables.},
  motscles  = {Lexiques syntaxiques, Lexique-Grammaire, analyse syntaxique},
  keywords  = {Syntactic lexica, Lexicon-Grammar, parsing},
}

@inproceedings{messiant-nakamura-voyatzi:2009:TALN,
  author    = {Messiant, Cédric and Nakamura, Takuya and Voyatzi, Stavroula},
  title     = {La complémentarité des approches manuelle et automatique en acquisition lexicale},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-021},
  language  = {french},
  resume    = {Les ressources lexicales sont essentielles pour obtenir des systèmes de traitement des langues performants. Ces ressources peuvent être soit construites à la main, soit acquises automatiquement à partir de gros corpus. Dans cet article, nous montrons la complémentarité de ces deux approches. Pour ce faire, nous utilisons l’exemple de la sous-catégorisation verbale en comparant un lexique acquis par des méthodes automatiques (LexSchem) avec un lexique construit manuellement (Le Lexique-Grammaire). Nous montrons que les informations acquises par ces deux méthodes sont bien distinctes et qu’elles peuvent s’enrichir mutuellement.},
  abstract  = {Lexical resources are essentially created to obtain efficient text-processing systems. These resources can be constructed either manually or automatically from large corpora. In this paper, we show the complementarity of these two types of approaches, comparing an automatically constructed lexicon (LexSchem) to a manually constructed one (Lexique-Grammaire), on examples of verbal subcategorization. The results show that the information retained by these two resources is in fact different and that they can be mutually enhanced.},
  motscles  = {verbe, syntaxe, lexique, sous-catégorisation},
  keywords  = {verb, syntax, lexicon, subcategorization},
}

@inproceedings{laurent-negre-seguela:2009:TALN,
  author    = {Laurent, Dominique and Nègre, Sophie and Séguéla, Patrick},
  title     = {L'analyseur syntaxique Cordial dans Passage},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-022},
  language  = {french},
  resume    = {Cordial est un analyseur syntaxique et sémantique développé par la société Synapse Développement. Largement utilisé par les laboratoires de TALN depuis plus de dix ans, cet analyseur participe à la campagne Passage ("Produire des Annotations Syntaxiques à Grande Échelle"). Comment fonctionne cet analyseur ? Quels résultats a-t-il obtenu lors de la première phase d'évaluation de cette campagne ? Au-delà de ces questions, cet article montre en quoi les contraintes industrielles façonnent les outils d'analyse automatique du langage naturel.},
  abstract  = {Cordial is a syntactic and semantic parser developed by Synapse Développement. Widely used by the laboratories of NLP for over ten years, this analyzer is involved in the Passage campaign ("Producing Syntactic Annotations on a Large Scale"). How does this parser work? What were the results obtained during the first phase of the evaluation? Beyond these issues, this article shows how the industrial constraints condition the tools for Natural Language Procesing.},
  motscles  = {Analyse syntaxique, analyse sémantique, évaluation, Passage},
  keywords  = {Parsing, semantic analysis, evaluation},
}

@inproceedings{widlocher-mathet:2009:TALN,
  author    = {Widlöcher, Antoine and Mathet, Yann},
  title     = {La plate-forme Glozz : environnement d’annotation et d’exploration de corpus},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-023},
  language  = {french},
  resume    = {La nécessité d’une interaction systématique entre modèles, traitements et corpus impose la disponibilité d’annotations de référence auxquelles modèles et traitements pourront être confrontés. Or l’établissement de telles annotations requiert un cadre formel permettant la représentation d’objets linguistiques variés, et des applications permettant à l’annotateur de localiser sur corpus et de caractériser les occurrences des phénomènes observés. Si différents outils d’annotation ont vu le jour, ils demeurent souvent fortement liés à un modèle théorique et à des objets linguistiques particuliers, et ne permettent que marginalement d’explorer certaines structures plus récemment appréhendées expérimentalement, notamment à granularité élevée et en matière d’analyse du discours. La plate-forme Glozz répond à ces différentes contraintes et propose un environnement d’exploration de corpus et d’annotation fortement configurable et non limité a priori au contexte discursif dans lequel elle a initialement vu le jour.},
  abstract  = {The need for a systematic confrontation between models and corpora make it necessary to have - and consequently, to produce - reference annotations to which linguistic models could be compared. Creating such annotations requires both a formal framework which copes with various linguistic objects, and specific manual annotation tools, in order to make it possible to locate, identify and feature linguistic phenomena in texts. Though several annotation tools do already exist, they are mostly dedicated to a given theory and to a given set of structures. The Glozz platform, described in this paper, tries to address all of these needs, and provides a highly versatile corpus exploration and annotation framework.},
  motscles  = {Linguistique de corpus, Annotation, Plate-forme logicielle},
  keywords  = {Corpus Linguistics, Annotation, Software Framework},
}

@inproceedings{buvet-EtAl:2009:TALN,
  author    = {Buvet, Pierre-André and Cartier, Emmanuel and Issac, Fabrice and Madiouni, Yassine and Mathieu-Colas, Michel and Mejri, Salah},
  title     = {Morfetik, ressource lexicale pour le TAL},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-024},
  language  = {french},
  resume    = {Le traitement automatique des langues exige un recensement lexical aussi rigoureux que possible. Dans ce but, nous avons développé un dictionnaire morphologique du français, conçu comme le point de départ d’un système modulaire (Morfetik) incluant un moteur de flexion, des interfaces de consultation et d’interrogation et des outils d’exploitation. Nous présentons dans cet article, après une brève description du dictionnaire de base (lexique des mots simples), quelques-uns des outils informatiques liés à cette ressource : un moteur de recherche des lemmes et des formes fléchies ; un moteur de flexion XML et MySQL ; des outils NLP permettant d’exploiter le dictionnaire ainsi généré ; nous présentons notamment un analyseur linguistique développé dans notre laboratoire. Nous comparons dans une dernière partie Morfetik avec d’autres ressources analogues du français : Morphalou, Lexique3 et le DELAF.},
  abstract  = {Automatic language processing requires as rigorous a lexical inventory as possible. For this purpose, we have developed a morphological dictionary for French, conceived as the starting point of a modular system (Morfetik) which includes an inflection generator, user interfaces and operating tools. In this paper, we briefly describe the basic dictionary (lexicon of simple words) and detail some of the computing tools based on the dictionary. The computing tools built on this resource include: a lemma / inflected forms search engine; an XML and MySQL engine to build the inflected forms; the generated dictionary can then be used by various NLP Tools; in this article, we present the use of the dictionary in a linguistic analyser developed at the laboratory. Finally, we compare Morfetik to similar resources : Morphalou, Lexique3 and DELAF.},
  motscles  = {dictionnaire morphologique du français, CMLF, analyse linguistique des textes},
  keywords  = {French morphological dictionary, XML, CMLF, Linguistical analysis, Morfetik},
}

@inproceedings{poulard-afantenos-hernandez:2009:TALN,
  author    = {Poulard, Fabien and Afantenos, Stergos and Hernandez, Nicolas},
  title     = {Nouvelles considérations pour la détection de réutilisation de texte},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-025},
  language  = {french},
  resume    = {Dans cet article nous nous intéressons au problème de la détection de réutilisation de texte. Plus particulièrement, étant donné un document original et un ensemble de documents candidats — thématiquement similaires au premier — nous cherchons à classer ceux qui sont dérivés du document original et ceux qui ne le sont pas. Nous abordons le problème selon deux approches : dans la première, nous nous intéressons aux similarités discursives entre les documents, dans la seconde au recouvrement de n-grams hapax. Nous présentons le résultat d’expérimentations menées sur un corpus de presse francophone construit dans le cadre du projet ANR PIITHIE.},
  abstract  = {In this article we are interested in the problem of text reuse. More specifically, given an original document and a set of candidate documents—which are thematically similar to the first one — we are interested in classifying them into those that have been derived from the original document and those that are not. We are approaching the problem in two ways : firstly we are interested in the discourse similarities between the documents, and secondly we are interested in the overlap of n-grams that are hapax. We are presenting the results of the experiments that we have performed on a corpus constituted from articles of the French press which has been created in the context of the PIITHIE project funded by the French National Agency for Research (Agence National de la Recherche, ANR).},
  motscles  = {réutilisation de texte, recouvrement de n-grams hapax, similarités discursives, corpus journalistique francophone},
  keywords  = {text reuse, hapax n-grams overlap, discourse similarities, french journalistic corpus},
}

@inproceedings{garciafernandez-rosset-vilnat:2009:TALN,
  author    = {Garcia-Fernandez, Anne and Rosset, Sophie and Vilnat, Anne},
  title     = {Collecte et analyses de réponses naturelles pour les systèmes de questions-réponses},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-026},
  language  = {french},
  resume    = {Notre travail se situe dans le cadre des systèmes de réponse a une question et à pour but de fournir une réponse en langue naturelle aux questions posées en langue naturelle. Cet article présente une expérience permettant d’analyser les réponses de locuteurs du français à des questions que nous leur posons. L’expérience se déroule à l’écrit comme à l’oral et propose à des locuteurs français des questions relevant de différents types sémantiques et syntaxiques. Nous mettons en valeur une large variabilité dans les formes de réponses possibles en langue française. D’autre part nous établissons un certain nombre de liens entre formulation de question et formulation de réponse. Nous proposons d’autre part une comparaison des réponses selon la modalité oral / écrit. Ces résultats peuvent être intégrés à des systèmes existants pour produire une réponse en langue naturelle de façon dynamique.},
  abstract  = {Situated within the domain of interactive question-answering, our work is to increase the naturalness of natural language answers. This paper presents an experiment aiming at observing the formulation of answers by French speakers. The system asked simple questions to which the humans had to answer. Two modalities were used : text (web) and speech (phone). We present and analyze the collected corpus. Within the large variability of answer forms in French, we point some links between the answer form and the question form. Moreover we present a preliminary study on the observed variation between modalities. We expect these results to be integrable in existing systems to dynamically produce adpated natural language answers.},
  motscles  = {systèmes de réponse à une question, expérience, variations linguistiques, réponse en langue naturelle},
  keywords  = {question-answering systems, experimentation, linguistics variations, natural language answer},
}

@inproceedings{claveau:2009:TALN,
  author    = {Claveau, Vincent},
  title     = {La /fOnetizasjc/ comme un problème de translittération},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-027},
  language  = {french},
  resume    = {La phonétisation est une étape essentielle pour le traitement de l’oral. Dans cet article, nous décrivons un système automatique de phonétisation de mots isolés qui est simple, portable et performant. Il repose sur une approche par apprentissage ; le système est donc construit à partir d’exemples de mots et de leur représentation phonétique. Nous utilisons pour cela une technique d’inférence de règles de réécriture initialement développée pour la translittération et la traduction. Pour évaluer les performances de notre approche, nous avons utilisé plusieurs jeux de données couvrant différentes langues et divers alphabets phonétiques, tirés du challenge Pascal Pronalsyl. Les très bons résultats obtenus égalent ou dépassent ceux des meilleurs systèmes de l’état de l’art.},
  abstract  = {Phonetizing is a crucial step to process oral documents. In this paper, a new word-based phonetization approach is proposed ; it is automatic, simple, portable and efficient. It relies on machine learning ; thus, the system is built from examples of words with their phonetic representations. More precisely, it makes the most of a technique inferring rewriting rules initially developed for transliteration and translation. In order to evaluate the performances of this approach, we used several datasets from the Pronalsyl Pascal challenge, including different languages. The obtained results equal or outperform those of the best known systems.},
  motscles  = {Phonétisation, phonémisation, inférence de règles de réécriture, challenge Pronalsyl, conversion graphème-phonème, translittération},
  keywords  = {Phonetization, phonemization, inference of rewriting rule, Pronalsyl challenge, grapheme-phoneme conversion, transliteration},
}

@inproceedings{crego-max-yvon:2009:TALN,
  author    = {Crego, Josep Maria and Max, Aurélien and Yvon, François},
  title     = {Plusieurs langues (bien choisies) valent mieux qu’une : traduction statistique multi-source par renforcement lexical},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-028},
  language  = {french},
  resume    = {Les systèmes de traduction statistiques intègrent différents types de modèles dont les prédictions sont combinées, lors du décodage, afin de produire les meilleures traductions possibles. Traduire correctement des mots polysémiques, comme, par exemple, le mot avocat du français vers l’anglais (lawyer ou avocado), requiert l’utilisation de modèles supplémentaires, dont l’estimation et l’intégration s’avèrent complexes. Une alternative consiste à tirer parti de l’observation selon laquelle les ambiguïtés liées à la polysémie ne sont pas les mêmes selon les langues source considérées. Si l’on dispose, par exemple, d’une traduction vers l’espagnol dans laquelle avocat a été traduit par aguacate, alors la traduction de ce mot vers l’anglais n’est plus ambiguë. Ainsi, la connaissance d’une traduction français!espagnol permet de renforcer la sélection de la traduction avocado pour le système français!anglais. Dans cet article, nous proposons d’utiliser des documents en plusieurs langues pour renforcer les choix lexicaux effectués par un système de traduction automatique. En particulier, nous montrons une amélioration des performances sur plusieurs métriques lorsque les traductions auxiliaires utilisées sont obtenues manuellement.},
  abstract  = {Statistical Machine Translation (SMT) systems integrate various models that exploit all available features during decoding to produce the best possible translation hypotheses. Correctly translating polysemous words, such as the French word avocat into English (lawyer or avocado) requires integrating complex models. Such translation lexical ambiguities, however, depend on the language pair considered. If one knows, for instance, that avocat was translated into Spanish as aguacate, then translating it into English is no longer ambiguous (avocado). Thus, in this example, the knowledge of the Spanish translation allows to reinforce the choice of the appropriate English word for the French!English system. In this article, we present an approach in which documents available in several languages are used to reinforce the lexical choices made by a SMT system. In particular, we show that gains can be obtained on several metrics when using auxiliary translations produced by human translators.},
  motscles  = {Traduction automatique statistique, désambiguïsation lexicale, réévaluation de listes d’hypothèses},
  keywords  = {Statistical Machine Translation, Word Sense Disambiguation, N-best list rescoring},
}

@inproceedings{bouraoui-EtAl:2009:TALN,
  author    = {Bouraoui, Jean-Léon and Boissière, Philippe and Mojahid, Mustapha and Vigouroux, Nadine and Lagarrigue, Aurélie and Vella, Frédéric and Nespoulous, Jean-Luc},
  title     = {Problématique d'analyse et de modélisation des erreurs en production écrite. Approche interdisciplinaire},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-029},
  language  = {french},
  resume    = {L'objectif du travail présenté ici est la modélisation de la détection et la correction des erreurs orthographiques et dactylographiques, plus particulièrement dans le contexte des handicaps langagiers. Le travail est fondé sur une analyse fine des erreurs d’écriture commises. La première partie de cet article est consacrée à une description précise de la faute. Dans la seconde partie, nous analysons l’erreur (1) en déterminant la nature de la faute (typographique, orthographique, ou grammaticale) et (2) en explicitant sa conséquence sur le niveau de perturbation linguistique (phonologique, orthographique, morphologique ou syntaxique). Il résulte de ce travail un modèle général des erreurs (une grille) que nous présenterons, ainsi que les résultats statistiques correspondants. Enfin, nous montrerons sur des exemples, l’utilité de l’apport de cette grille, en soumettant ces types de fautes à quelques correcteurs. Nous envisageons également les implications informatiques de ce travail.},
  abstract  = {The aim of our work is modeling the detection and the correction of spelling and typing errors, especially in the linguistic disabilities context. The work is based on a fine analysis of clerical errors committed. The first part of this article is devoted to a detailed description of error. In the second part, we analyze error in (1) determining the nature of the fault (typographical, spelling, or grammar) and (2) by explaining its consequences on the level of linguistic disturbance (phonological, orthographic, morphological and syntactic). The outcome of this work is a general model of errors (a grid) that we present, as well as the corresponding statistical results. Finally, we show on examples, the usefulness of this grid, by submitting these types of errors to a few spellcheckers. We also envisage the computer implications of this work.},
  motscles  = {Typologie et analyse d’erreurs textuelles, assistance à la saisie de textes},
  keywords  = {Typology and analyze of textual errors, writing assitance systems},
}

@inproceedings{kessler-EtAl:2009:TALN,
  author    = {Kessler, Rémy and Béchet, Nicolas and Torres-Moreno, Juan-Manuel and Roche, Mathieu and El-Bèze, Marc},
  title     = {Profilage de candidatures assisté par Relevance Feedback},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-030},
  language  = {french},
  resume    = {Le marché d’offres d’emploi et des candidatures sur Internet connaît une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de texte libre) qu’il n’est plus possible de traiter manuellement. Une analyse et catégorisation assistées nous semble pertinente en réponse à cette problématique. Nous proposons E-Gen, système qui a pour but l’analyse et catégorisation assistés d’offres d’emploi et des réponses des candidats. Dans cet article nous présentons plusieurs stratégies, reposant sur les modèles vectoriel et probabiliste, afin de résoudre la problématique du profilage des candidatures en fonction d’une offre précise. Nous avons évalué une palette de mesures de similarité afin d’effectuer un classement pertinent des candidatures au moyen des courbes ROC. L’utilisation d’une forme de relevance feedback a permis de surpasser nos résultats sur ce problème difficile et sujet à une grande subjectivité.},
  abstract  = {The market of online job search sites has grown exponentially. This implies volumes of information (mostly in the form of free text) manually impossible to process. An analysis and assisted categorization seems relevant to address this issue. We present E-Gen, a system which aims to perform assisted analysis and categorization of job offers and the responses of candidates. This paper presents several strategies based on vectorial and probabilistic models to solve the problem of profiling applications according to a specific job offer. We have evaluated a range of measures of similarity to rank candidatures by using ROC curves. Relevance feedback approach allows surpass our previous results on this task, difficult and higly subjective.},
  motscles  = {Classification, recherche d’information, Ressources humaines, modèle probabiliste, mesures de similarité, Relevance Feedback},
  keywords  = {Classification,Information Retrieval, Human Ressources, Probabilistic Model, Similarity measure, Relevance Feedback},
}

@inproceedings{hamon-grabar:2009:TALN,
  author    = {Hamon, Thierry and Grabar, Natalia},
  title     = {Profilage sémantique endogène des relations de synonymie au sein de Gene Ontology},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-031},
  language  = {french},
  resume    = {Le calcul de la similarité sémantique entre les termes repose sur l’existence et l’utilisation de ressources sémantiques. Cependant de telles ressources, qui proposent des équivalences entre entités, souvent des relations de synonymie, doivent elles-mêmes être d’abord analysées afin de définir des zones de fiabilité où la similarité sémantique est plus forte. Nous proposons une méthode d’acquisition de synonymes élémentaires grâce à l’exploitation des terminologies structurées au travers l’analyse de la structure syntaxique des termes complexes et de leur compositionnalité. Les synonymes acquis sont ensuite profilés grâce aux indicateurs endogènes inférés automatiquement à partir de ces mêmes terminologies (d’autres types de relations, inclusions lexicales, productivité, forme des composantes connexes). Dans le domaine biomédical, il existe de nombreuses terminologies structurées qui peuvent être exploitées pour la constitution de ressources sémantiques. Le travail présenté ici exploite une de ces terminologies, Gene Ontology.},
  abstract  = {Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be first analyzed and weighted in order to define within them the reliability zones where the semantic similarity shows to be stronger. We propose a method for acquisition of elementary synonyms which is based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositionality. The acquired synonyms are then profiled thanks to endogenous indicators (other types of relations, lexical inclusions, productivity, form of connected components), which are automatically inferred within the same terminologies. In the biomedical area, several structured terminologies have been built and can be exploited for the construction of semantic resources. The work we present in this paper, is applied to terms of one of these terminologies, i.e. the Gene Ontology.},
  motscles  = {Terminologie, distance sémantique, relations sémantiques, synonymie},
  keywords  = {Terminology, semantic distance, semantic relations, synonymy},
}

@inproceedings{benfraj-benothmanezribi-benahmed:2009:TALN,
  author    = {Ben Fraj, Fériel and Ben Othmane Zribi, Chiraz and Ben Ahmed, Mohamed},
  title     = {Quels attributs discriminants pour une analyse syntaxique par classification de textes en langue arabe ?},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-032},
  language  = {french},
  resume    = {Dans le cadre dune approche déterministe et incrémentale danalyse syntaxique par classification de textes en langue arabe, nous avons prévu de prendre en considération un ensemble varié dattributs discriminants afin de mieux assister la procédure de classification dans ses prises de décisions à travers les différentes étapes danalyse. Ainsi, en plus des attributs morpho-syntaxiques du mot en cours danalyse et des informations contextuelles des mots lavoisinant, nous avons ajouté des informations compositionnelles extraites du fragment de larbre syntaxique déjà construit lors de létape précédente de lanalyse en cours. Ce papier présente notre approche danalyse syntaxique par classification et vise lexposition dune justification expérimentale de lapport de chaque type dattributs discriminants et spécialement ceux compositionnels dans ladite analyse syntaxique.},
  abstract  = {For parsing Arabic texts in a deterministic and incremental classification approach, we suggest that varying discriminative attributes is helpful in disambiguation between structures to classify. Thats why; we consider morpho-syntactic information of the current analyzed word and its surrounding context. In addition, we add a new information type: the compositional one. It consists of the portion of the syntactic tree already constructed until the previous analysis step. In this paper, we expose our parsing approach with classification basis and we justify the utility of the different discriminative attributes and especially the compositional ones.},
  motscles  = {analyse syntaxique incrémentale, langue arabe, apprentissage automatique, classification, attributs discriminants},
  keywords  = {incremental parsing, Arabic language, machine learning, classification, discriminative attributes},
}

@inproceedings{beaufort-EtAl:2009:TALN,
  author    = {Beaufort, Richard and Dister, Anne and Naets, Hubert and Macé, Kévin and Fairon, Cédrick},
  title     = {Recto /Verso Un système de conversion automatique ancienne / nouvelle orthographe à visée linguistique et didactique},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-033},
  language  = {french},
  resume    = {Cet article présente Recto /Verso, un système de traitement automatique du langage dédié à l’application des rectifications orthographiques de 1990. Ce système a été développé dans le cadre de la campagne de sensibilisation réalisée en mars dernier par le Service et le Conseil de la langue française et de la politique linguistique de la Communauté française de Belgique. Nous commençons par rappeler les motivations et le contenu de la réforme proposée, et faisons le point sur les principes didactiques retenus dans le cadre de la campagne. La plus grande partie de l’article est ensuite consacrée à l’implémentation du système. Nous terminons enfin par une première analyse de l’impact de la campagne sur les utilisateurs.},
  abstract  = {This paper presents Recto /Verso, a natural language processing system dedicated to the application of the 1990 French spelling rectifications. This system was developed for supporting the awareness-raising campaign promoted last March by the Superior council of the French language in Belgium. We first remind the motivations and the content of the reform, and we draw up the didactic principles followed during the campaign. The most important part of this paper is then focused on the system’s implementation.We finally end by a short analysis of the campaign’s impact on the users.},
  motscles  = {Rectifications orthographiques de 1990, conversion ancienne / nouvelle orthographe, objectifs didactiques, machines à états finis},
  keywords  = {1990 French spelling rectifications, ancient / new spelling conversion, didactic purposes, finite-state machines},
}

@inproceedings{malaise-EtAl:2009:TALN,
  author    = {Malaisé, Véronique and Gazendam, Luit and Heeren, Willemijn and Ordelman, Roeland and Brugman, Hennie},
  title     = {},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-034},
  language  = {french},
  note      = {Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs},
  resume    = {L’accès aux documents multimédia, dans une archive audiovisuelle, dépend en grande partie de la quantité et de la qualité des métadonnées attachées aux documents, notamment la description de leur contenu. Cependant, l’annotation manuelle des collections est astreignante pour le personnel. De nombreuses archives évoluent vers des méthodes d’annotation (semi-)automatiques pour la création et/ou l’amélioration des métadonnées. Le project CATCH-CHOICE, fondé par NWO, s’est penché sur l’extraction de mots clés à partir de resources textuelles liées aux programmes TV destinés à être archivés (péritextes), en collaboration avec les archives audiovisuelles néerlandaises, Sound and Vision. Cet article se penche sur la question de l’adéquation des transcriptions de Reconnaissance Automatique de la Parole développés dans le projet CATCH-CHoral pour la génération automatique de mots-clés : les mots-clés extraits de ces ressources sont évalués par rapport à des annotations manuelles et par rapport à des mots-clés générés à partir de péritextes décrivant les programmes télévisuels.},
  abstract  = {Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, the manual annotation of collections puts heavy demands on resources. A large number of archives are introducing (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords from textual resources related to TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. This paper investigates the suitability of Automatic Speech Recognition transcripts produced in the CATCH-CHoral project for generating such keywords, which we evaluate against manual annotations of the documents, and against keywords automatically generated from context documents describing the TV programs’ content.},
  motscles  = {Extraction de mots clés, Reconnaissance Automatique de la Parole, Documents Audiovisuels},
  keywords  = {Keyword extraction, Automatic Speech Recognition, Audiovisual Documents},
}

@inproceedings{boudin-torresmoreno:2009:TALN,
  author    = {Boudin, Florian and Torres-Moreno, Juan-Manuel},
  title     = {Résumé automatique multi-document et indépendance de la langue : une première évaluation en français},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-035},
  language  = {french},
  resume    = {Le résumé automatique de texte est une problématique difficile, fortement dépendante de la langue et qui peut nécessiter un ensemble de données d’apprentissage conséquent. L’approche par extraction peut aider à surmonter ces difficultés. (Mihalcea, 2004) a démontré l’intérêt des approches à base de graphes pour l’extraction de segments de texte importants. Dans cette étude, nous décrivons une approche indépendante de la langue pour la problématique du résumé automatique multi-documents. L’originalité de notre méthode repose sur l’utilisation d’une mesure de similarité permettant le rapprochement de segments morphologiquement proches. De plus, c’est à notre connaissance la première fois que l’évaluation d’une approche de résumé automatique multi-document est conduite sur des textes en français.},
  abstract  = {Automatic text summarization is a difficult task, highly language-dependent and which may require a large training dataset. Recently, (Mihalcea, 2004) has shown that graph-based approaches applied to the sentence extraction issue can achieve good results. In this paper, we describe a language-independent approach for automatic multi-document text summarization. The main originality of our approach is the use of an hybrid similarity measure during the graph building process that can identify morphologically similar words. Moreover, this is as far as we know, the first time that the evaluation of a summarization approach is conducted on French documents.},
  motscles  = {Résumé automatique de texte, Approches à base de graphes, Extraction d’information},
  keywords  = {Text summarization, Graph-Based approaches, Information Extraction},
}

@inproceedings{bozzi-suignard-waastrichard:2009:TALN,
  author    = {Bozzi, Laurent and Suignard, Philippe and Waast-Richard, Claire},
  title     = {Segmentation et classification non supervisée de conversations téléphoniques automatiquement retranscrites},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-036},
  language  = {french},
  resume    = {Cette étude porte sur l’analyse de conversations entre des clients et des téléconseillers d’EDF. Elle propose une chaîne de traitements permettant d’automatiser la détection des sujets abordés dans chaque conversation. L’aspect multi-thématique des conversations nous incite à trouver une unité de documents entre le simple tour de parole et la conversation entière. Cette démarche enchaîne une étape de segmentation de la conversation en thèmes homogènes basée sur la notion de cohésion lexicale, puis une étape de text-mining comportant une analyse linguistique enrichie d’un vocabulaire métier spécifique à EDF, et enfin une classification non supervisée des segments obtenus. Plusieurs algorithmes de segmentation ont été évalués sur un corpus de test, segmenté et annoté manuellement : le plus « proche » de la segmentation de référence est C99. Cette démarche, appliquée à la fois sur un corpus de conversations transcrites à la main, et sur les mêmes conversations décodées par un moteur de reconnaissance vocale, aboutit quasiment à l’obtention des 20 mêmes classes thématiques.},
  abstract  = {This study focuses on the analysis of conversations and between clients and EDF agent. It offers a range of treatments designed to automate the detection of the topics covered in each conversation. As the conversations are multi-thematic we have to find a document unit, between the simple turn of speech and the whole conversation. The proposed approach starts with a step of segmentation of the conversation (based on lexical cohesion), and then a stage of text-mining, including a language enriched by a vocabulary specific to EDF, and finally a clustering of the segments. Several segmentation algorithms were tested on a test corpus, manually annotated and segmented : the "closest" to the reference segmentation is C99. This approach, applied to both a corpus of conversations transcribed manually, and on the same conversations decoded by a voice recognition engine, leads to almost obtain the same 200 clusters.},
  motscles  = {audio-mining, text mining, segmentation, classification, catégorisation, reconnaissance vocale, données textuelles, conversations téléphoniques, centre d’appel},
  keywords  = {audio-mining, text mining, segmentation, clustering, categorization, voice recognition, textual data, phone conversation, call center},
}

@inproceedings{seng-EtAl:2009:TALN,
  author    = {Seng, Sopheap and Besacier, Laurent and Bigi, Brigitte and Castelli, Eric},
  title     = {Segmentation multiple d’un flux de données textuelles pour la modélisation statistique du langage},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-037},
  language  = {french},
  resume    = {Dans cet article, nous traitons du problème de la modélisation statistique du langage pour les langues peu dotées et sans segmentation entre les mots. Tandis que le manque de données textuelles a un impact sur la performance des modèles, les erreurs introduites par la segmentation automatique peuvent rendre ces données encore moins exploitables. Pour exploiter au mieux les données textuelles, nous proposons une méthode qui effectue des segmentations multiples sur le corpus d’apprentissage au lieu d’une segmentation unique. Cette méthode basée sur les automates d’état finis permet de retrouver les n-grammes non trouvés par la segmentation unique et de générer des nouveaux n-grammes pour l’apprentissage de modèle du langage. L’application de cette approche pour l’apprentissage des modèles de langage pour les systèmes de reconnaissance automatique de la parole en langue khmère et vietnamienne s’est montrée plus performante que la méthode par segmentation unique, à base de règles.},
  abstract  = {In this article we deal with the problem of statistical language modelling for under-resourced language with a writing system without word boundary delimiters. While the lack of text resources has an impact on the performance of language models, the errors produced by the word segmentation makes those data less usable. To better exploit the text resources, we propose a method to make multiples segmentations on the training corpus instead of a unique segmentation. This method based on finite state machine allows obtaining the n-grams not found by the unique segmentation and generate new n-grams. We use this approach to train the language models for automatic speech recognition systems of Khmer and Vietnamese languages and it proves better performance than the unique segmentation method.},
  motscles  = {segmentation multiple, langue non segmentée, modélisation statistique du langage},
  keywords  = {multiple segmentation, unsegmented language, statistical language modeling},
}

@inproceedings{lafourcade-joubert-riou:2009:TALN,
  author    = {Lafourcade, Mathieu and Joubert, Alain and Riou, Stéphane},
  title     = {Sens et usages d’un terme dans un réseau lexical évolutif},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-038},
  language  = {french},
  resume    = {L’obtention d’informations lexicales fiables est un enjeu primordial en TALN, mais cette collecte peut s’avérer difficile. L’approche présentée ici vise à pallier les écueils de cette difficulté en faisant participer un grand nombre de personnes à un projet contributif via des jeux accessibles sur le web. Ainsi, les joueurs vont construire le réseau lexical, en fournissant de plusieurs manières possibles des associations de termes à partir d'un terme cible et d'une consigne correspondant à une relation typée. Le réseau lexical ainsi produit est de grande taille et comporte une trentaine de types de relations. A partir de cette ressource, nous abordons la question de la détermination des différents sens et usages d’un terme. Ceci est réalisé en analysant les relations entre ce terme et ses voisins immédiats dans le réseau et en calculant des cliques ou des quasi-cliques. Ceci nous amène naturellement à introduire la notion de similarité entre cliques, que nous interprétons comme une mesure de similarité entre ces différents sens et usages. Nous pouvons ainsi construire pour un terme son arbre des usages, qui est une structure de données exploitable en désambiguïsation de sens. Nous présentons quelques résultats obtenus en soulignant leur caractère évolutif.},
  abstract  = {Obtaining reliable lexical information is an essential task in NLP, but it can prove a difficult task. The approach we present here aims at lessening the difficulty: it consists in having people take part in a collective project by offering them playful applications accessible on the web. The players themselves thus build the lexical network, by supplying (in various possible ways) associations between terms from a target term and an instruction concerning a typed relation. The lexical network thus obtained is large and it includes about thirty types of relations. From this network, we then discuss the question of meaning and word usage determination for a term, by searching relations between this term and its neighbours in the network and by computing cliques or quasicliques. This leads us to introduce the notion of similarity between cliques, which can be interpreted as a measure of similarity between these different meanings and word usages. We are thus able to build the tree of word usages for a term: it is a data structure that can be used to disambiguate meaning. Finally, we briefly present some of the results obtained, putting the emphasis on their evolutionary aspect.},
  motscles  = {Traitement Automatique du Langage Naturel, réseau lexical évolutif, relations typées pondérées, similarité entre sens et usages, arbre des usages},
  keywords  = {Natural Language Processing, evolutionary lexical network, typed and weighted relations, meaning and word usage similarity, tree of word usages},
}

@inproceedings{bouraoui-vigouroux:2009:TALN,
  author    = {Bouraoui, Jean-Léon and Vigouroux, Nadine},
  title     = {Traitement automatique de disfluences dans un corpus linguistiquement contraint},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-039},
  language  = {french},
  resume    = {Cet article présente un travail de modélisation et de détection des phénomènes de disfluence. Une des spécificité de ce travail est le cadre dans lequel il se situe: le contrôle de la navigation aérienne. Nous montrons ce que ce cadre particulier implique certains choix concernant la modélisation et l'implémentation. Ainsi, nous constatons que la modélisation fondée sur la syntaxe, souvent utilisée dans le traitement des langues naturelles, n'est pas la plus appropriée ici. Nous expliquons la façon dont l'implémentation a été réalisée. Dans une dernière partie, nous présentons la validation de ce dispositif, effectuée sur 400 énoncés.},
  abstract  = {This article presents a work of modeling and detection of phenomena disfluences. One of the specificity of this work is its framework: the air traffic control. We show that this particular framework implies certain choices about modeling and implementation. Thus, we find that modeling based on the syntax, often used in natural language processing, is not the most appropriate here. We explain how the implementation has been completed. In a final section, we present the validation of this device, made of 400 utterances.},
  motscles  = {Dialogue oral spontané, Analyse linguistique de corpus, Compréhension robuste, Contrôle Aérien, Phraséologie, Disfluences, Modèles de langage, Traitement Automatique du Langage Naturel},
  keywords  = {Spontaneous speech dialog, corpus linguistic analysis, robust understanding, Air Traffic Control, phraseology, disfluencies, language models, Natural Language Processing},
}

@inproceedings{kallmeyer-maier-parmentier:2009:TALN,
  author    = {Kallmeyer, Laura and Maier, Wolfgang and Parmentier, Yannick},
  title     = {Un Algorithme d’Analyse de Type Earley pour Grammaires à Concaténation d’Intervalles},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-040},
  language  = {french},
  resume    = {Nous présentons ici différents algorithmes d’analyse pour grammaires à concaténation d’intervalles (Range Concatenation Grammar, RCG), dont un nouvel algorithme de type Earley, dans le paradigme de l’analyse déductive. Notre travail est motivé par l’intérêt porté récemment à ce type de grammaire, et comble un manque dans la littérature existante.},
  abstract  = {We present several different parsing algorithms for Range Concatenation Grammar (RCG), inter alia an entirely novel Earley-style algorithm, using the deductive parsing framework. Our work is motivated by recent interest in range concatenation grammar in general and fills a gap in the existing literature.},
  motscles  = {Analyse syntaxique déductive, grammaires à concaténation d’intervalles},
  keywords  = {Deductive parsing, range concatenation grammar},
}

@inproceedings{lehuen-lemeunier:2009:TALN,
  author    = {Lehuen, Jérôme and Lemeunier, Thierry},
  title     = {Un Analyseur Sémantique pour le DHM Modélisation – Réalisation – Évaluation},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-041},
  language  = {french},
  resume    = {Cet article décrit un modèle de langage dédié au dialogue homme-machine, son implémentation en CLIPS, ainsi qu’une évaluation comparative. Notre problématique n’est ni d’analyser des grands corpus, ni de proposer une grammaire à grande couverture. Notre objectif est de produire des représentations sémantiques utilisables par un module de dialogue à partir d’énoncés oraux courts, le plus souvent agrammaticaux. Une démarche pragmatique nous a conduit à fonder l’analyse sur des principes simples mais efficaces dans le cadre que nous nous sommes fixé. L’algorithme retenu s’inspire de l’analyse tabulaire. L’évaluation que nous présentons repose sur le corpus MEDIA qui a fait l’objet d’une annotation sémantique manuelle de référence pour une campagne d’évaluation d’analyseurs sémantiques pour le dialogue. Les résultats que nous obtenons place notre analyseur dans le trio de tête des systèmes évalués lors de la campagne de juin 2005, et nous confortent dans nos choix d’algorithme et de représentation des connaissances.},
  abstract  = {This article describes a natural language model dedicated to man-machine dialogue, its implementation in CLIPS, as well as a comparative evaluation. Our problematic is neither to analyze large corpora nor to propose a large-coverage grammar. Our objective is to produce semantic representations usable for a dialog module from short oral utterances that are rather often ungrammatical. A pragmatic approach leads us to base parsing on simple but efficient principles within the man-machine dialog framework. Chart parsing influences the algorithm we have chosen. The evaluation that we present here uses the MEDIA corpora. It has been manually annotated and represents a standard usable in an evaluation campaign for semantic parsers dedicated to the dialog. With the results that we obtain our parser is in the three bests of the systems evaluated in the June 2005 campaign. It confirms our choices of algorithm and of knowledge representation.},
  motscles  = {Analyse sémantique tabulaire, contexte dialogique, évaluation},
  keywords  = {Semantic chart parsing, dialogue context, evaluation},
}

@inproceedings{fernandezsabido-torresmoreno:2009:TALN,
  author    = {Fernández Sabido, Silvia and Torres-Moreno, Juan-Manuel},
  title     = {Une approche exploratoire de compression automatique de phrases basée sur des critères thermodynamiques},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-042},
  language  = {french},
  resume    = {Nous présentons une approche exploratoire basée sur des notions thermodynamiques de la Physique statistique pour la compression de phrases. Nous décrivons le modèle magnétique des verres de spins, adapté à notre conception de la problématique. Des simulations Métropolis Monte-Carlo permettent d’introduire des fluctuations thermiques pour piloter la compression. Des comparaisons intéressantes de notre méthode ont été réalisées sur un corpus en français.},
  abstract  = {We present an exploratory approach based on thermodynamic concepts of Statistical Physics for sentence compression.We describe the magnetic model of spin glasses, well suited to our conception of problem. The Metropolis Monte-Carlo simulations allow to introduce thermal fluctuations to drive the compression. Interesting comparisons of our method were performed on a French text corpora.},
  motscles  = {Compression de phrases, Résumé automatique, Résumé par extraction, Enertex, Mécanique statistique},
  keywords  = {Sentence Compression, Automatic Summarization, Extraction Summarization, Enertex, Statistical Mechanics},
}

@inproceedings{penasaldarriaga-morin-viardgaudin:2009:TALN,
  author    = {Peña Saldarriaga, Sebastián and Morin, Emmanuel and Viard-Gaudin, Christian},
  title     = {Un nouveau schéma de pondération pour la catégorisation de documents manuscrits},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-043},
  language  = {french},
  resume    = {Les schémas de pondération utilisés habituellement en catégorisation de textes, et plus généralement en recherche d’information (RI), ne sont pas adaptés à l’utilisation de données liées à des textes issus d’un processus de reconnaissance de l’écriture. En particulier, les candidats-mot à la reconnaissance ne pourraient être exploités sans introduire de fausses occurrences de termes dans le document. Dans cet article nous présentons un nouveau schéma de pondération permettant d’exploiter les listes de candidats-mot. Il permet d’estimer le pouvoir discriminant d’un terme en fonction de la probabilité a posteriori d’un candidat-mot dans une liste de candidats. Les résultats montrent que le taux de classification de documents fortement dégradés peut être amélioré en utilisant le schéma proposé.},
  abstract  = {The traditional weighting schemes used in information retrieval, and especially in text categorization cannot exploit information intrinsic to texts obtained through an on-line handwriting recognition process. In particular, top n (n > 1) candidates could not be used without introducing false occurrences of spurious terms thus making the resulting text noisier. In this paper, we propose an improved weighting scheme for text categorization, that estimates a term importance from the posterior probabilities of the top n candidates. The experimental results show that the categorization rate of poorly recognized texts increases when our weighting model is applied.},
  motscles  = {Catégorisation de textes, écriture en-ligne, n-best candidats, pondération},
  keywords  = {Text categorization, on-line handwriting, n-best candidates, weighting},
}

@inproceedings{scherrer:2009:TALN,
  author    = {Scherrer, Yves},
  title     = {Un système de traduction automatique paramétré par des atlas dialectologiques},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-044},
  language  = {french},
  resume    = {Contrairement à la plupart des systèmes de traitement du langage, qui s’appliquent à des langues écrites et standardisées, nous présentons ici un système de traduction automatique qui prend en compte les spécificités des dialectes. En général, les dialectes se caractérisent par une variation continue et un manque de données textuelles en qualité et quantité suffisantes. En même temps, du moins en Europe, les dialectologues ont étudié en détail les caractéristiques linguistiques des dialectes. Nous soutenons que des données provenant d’atlas dialectologiques peuvent être utilisées pour paramétrer un système de traduction automatique. Nous illustrons cette idée avec le prototype d’un système de traduction basé sur des règles, qui traduit de l’allemand standard vers les différents dialectes de Suisse allemande. Quelques exemples linguistiquement motivés serviront à exposer l’architecture de ce système.},
  abstract  = {Most natural language processing systems apply to written, standardized language varieties. In contrast, we present a machine translation system that takes into account some specificites of dialects : dialect areas show continuous variation along all levels of linguistic analysis, and textual data is often not available in sufficient quality and quantity. At the same time, many European dialect areas are well studied by dialectologists. We argue that data from dialectological atlases can be used to parametrize a machine translation system. We illustrate this idea by presenting the prototype of a rule-based machine translation system that translates from Standard German into the Swiss German dialect continuum. Its architecture is explained with some linguistically motivated examples.},
  motscles  = {Traduction automatique, dialectes, langues proches, langues germaniques},
  keywords  = {Machine translation, dialects, closely related languages, Germanic languages},
}

@inproceedings{chappelier-eckard:2009:TALN,
  author    = {Chappelier, Jean-Cédric and Eckard, Emmanuel},
  title     = {Utilisation de PLSI en recherche d’information Représentation des requêtes},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-045},
  language  = {french},
  resume    = {Le modèle PLSI (« Probabilistic Latent Semantic Indexing ») offre une approche de l’indexation de documents fondée sur des modèles probabilistes de catégories sémantiques latentes et a conduit à des applications dans différents domaines. Toutefois, ce modèle rend impossible le traitement de documents inconnus au moment de l’apprentissage, problème particulièrement sensible pour la représentation des requêtes dans le cadre de la recherche d’information. Une méthode, dite de « folding-in », permet dans une certaine mesure de contourner ce problème, mais présente des faiblesses. Cet article introduit nouvelle une mesure de similarité document-requête pour PLSI, fondée sur lesmodèles de langue, où le problème du « folding-in » ne se pose pas. Nous comparons cette nouvelle similarité aux noyaux de Fisher, l’état de l’art en la matière. Nous présentons aussi une évaluation de PLSI sur un corpus de recherche d’information de près de 7500 documents et de plus d’un million d’occurrences de termes provenant de la collection TREC–AP, une taille considérable dans le cadre de PLSI.},
  abstract  = {The PLSI model (“Probabilistic Latent Semantic Indexing”) offers a document indexing scheme based on probabilistic latent category models. It entailed applications in diverse fields, notably in information retrieval (IR). Nevertheless, PLSI cannot process documents not seen during parameter inference, a major liability for queries in IR. A method known as “folding-in” allows to circumvent this problem up to a point, but has its own weaknesses. The present paper introduces a new document-query similarity measure for PLSI based on language models that entirely avoids the problem a query projection.We compare this similarity to Fisher kernels, the state of the art similarities for PLSI. Moreover, we present an evaluation of PLSI on a particularly large training set of almost 7500 document and over one million term occurrence large, created from the TREC–AP collection.},
  motscles  = {},
  keywords  = {},
}

@inproceedings{ferret:2009:TALN,
  author    = {Ferret, Olivier},
  title     = {Utiliser des sens de mots pour la segmentation thématique ?},
  booktitle = {Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2009},
  address   = {Senlis, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2009/taln-2009-court-046},
  language  = {french},
  resume    = {La segmentation thématique est un domaine de l’analyse discursive ayant donné lieu à de nombreux travaux s’appuyant sur la notion de cohésion lexicale. La plupart d’entre eux n’exploitent que la simple récurrence lexicale mais quelques uns ont néanmoins exploré l’usage de connaissances rendant compte de cette cohésion lexicale. Celles-ci prennent généralement la forme de réseaux lexicaux, soit construits automatiquement à partir de corpus, soit issus de dictionnaires élaborés manuellement. Dans cet article, nous examinons dans quelle mesure une ressource d’une nature un peu différente peut être utilisée pour caractériser la cohésion lexicale des textes. Il s’agit en l’occurrence de sens de mots induits automatiquement à partir de corpus, à l’instar de ceux produits par la tâche «Word Sense Induction and Discrimination » de l’évaluation SemEval 2007. Ce type de ressources apporte une structuration des réseaux lexicaux au niveau sémantique dont nous évaluons l’apport pour la segmentation thématique.},
  abstract  = {Many topic segmenters rely on lexical cohesion. Most of them only exploit lexical recurrence but some of them makes use of knowledge sources about lexical cohesion. These sources are generally lexical networks built either by hand or automatically from corpora. In this article, we study to what extent a new source of knowledge about lexical cohesion can be used for topic segmentation. This source is a set of word senses that were automatically discriminated from corpora, as the word senses resulting from the Word Sense Induction and Discrimination task of the SemEval 2007 evaluation. Such a resource is a way to structurate lexical networks at a semantic level. The impact of this structuring on topic segmentation is evaluated in this article.},
  motscles  = {Segmentation thématique, désambiguïsation sémantique},
  keywords  = {Topic segmentation, word sense disambiguation},
}