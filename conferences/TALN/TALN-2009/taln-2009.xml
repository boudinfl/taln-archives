<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>TALN'2009</acronyme>
		<titre>16ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Senlis</ville>
		<pays>France</pays>
		<dateDebut>2009-06-24</dateDebut>
		<dateFin>2009-06-26</dateFin>
		<presidents>
			<president>
				<prenom>Adeline</prenom>
				<nom>Nazarenko</nom>
			</president>
			<president>
				<prenom>Thierry</prenom>
				<nom>Poibeau</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="position">Prise de position</type>
			<type id="court">Papiers courts</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<siteWeb>http://lipn.univ-paris13.fr/taln09/</siteWeb>
	</edition>
	<articles>
		<article id="taln-2009-long-001" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Nabil</prenom>
					<nom>Hathout</nom>
					<email>Nabil.Hathout@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Toulouse</affiliation>
			</affiliations>
			<titre>Acquisition morphologique à partir d’un dictionnaire informatisé</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’article propose un modèle linguistique et informatique permettant de faire émerger la structure morphologique dérivationnelle du lexique à partir des régularités sémantiques et formelles des mots qu’il contient. Ce modèle est radicalement lexématique. La structure morphologique est constituée par les relations que chaque mot entretient avec les autres unités du lexique et notamment avec les mots de sa famille morphologique et de sa série dérivationnelle. Ces relations forment des paradigmes analogiques. La modélisation a été testée sur le lexique du français en utilisant le dictionnaire informatisé TLFi.</resume>
			<mots_cles>Morphologie dérivationnelle, morphologie lexématique, similarité morphologique, analogie formelle</mots_cles>
			<title></title>
			<abstract>The paper presents a linguistic and computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains. The model is word-based. The proposed morphological structure consists of (1) binary relations that connect each headword with words that are morphologically related, and especially with the members of its morphological family and its derivational series, and of (2) the analogies that hold between the words. The model has been tested on the lexicon of French using the TLFi machine readable dictionary.</abstract>
			<keywords>Derivational morphology, word-based morphology, morphological relatedness, formal analogy</keywords>
		</article>
		<article id="taln-2009-long-002" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Joseph</prenom>
					<nom>Le Roux</nom>
					<email>jleroux@computing.dcu.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">NCLT, Dublin City University</affiliation>
			</affiliations>
			<titre>Analyse déductive pour les grammaires d’interaction</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous proposons un algorithme d’analyse pour les grammaires d’interaction qui utilise le cadre formel de l’analyse déductive. Cette approche donne un point de vue nouveau sur ce problème puisque les méthodes précédentes réduisaient ce dernier à la réécriture de graphes et utilisaient des techniques de résolution de contraintes. D’autre part, cette présentation permet de décrire le processus de manière standard et d’exhiber les sources d’indéterminisme qui rendent ce problème difficile.</resume>
			<mots_cles>Analyse syntaxique, grammaires d’interaction</mots_cles>
			<title></title>
			<abstract>We propose a parsing algorithm for Interaction Grammars using the deductive parsing framework. This approach brings new perspectives on this problem, departing from previous methods relying on constraint-solving techniques to interpret it as a graph-rewriting problem. Furthermore, this presentation allows a standard description of the algorithm and a fine-grained inspection of the sources of non-determinism.</abstract>
			<keywords>Parsing, Interaction Grammars</keywords>
		</article>
		<article id="taln-2009-long-003" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Alexis</prenom>
					<nom>Nasr</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Béchet</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF - CNRS - Université Aix Marseille</affiliation>
				<affiliation affiliationId="2">LIA - Université d’Avignon</affiliation>
			</affiliations>
			<titre>Analyse syntaxique en dépendances de l’oral spontané</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article décrit un modèle d’analyse syntaxique de l’oral spontané axé sur la reconnaissance de cadres valenciels verbaux. Le modèle d’analyse se décompose en deux étapes : une étape générique, basée sur des ressources génériques du français et une étape de réordonnancement des solutions de l’analyseur réalisé par un modèle spécifique à une application. Le modèle est évalué sur le corpus MEDIA.</resume>
			<mots_cles>Analyse syntaxique, reconnaissance automatique de la parole</mots_cles>
			<title></title>
			<abstract>We describe in this paper a syntactic parser for spontaneous speech geared towards the identification of verbal subcategorization frames. The parser proceeds in two stages. The first stage is based on generic syntactic ressources for French. The second stage is a reranker which is specially trained for a given application. The parser is evaluated on the MEDIA corpus.</abstract>
			<keywords>Syntactic parsing, automatic speech recognition</keywords>
		</article>
		<article id="taln-2009-long-004" session="plénière">
			<auteurs>
				<auteur>
					<prenom>Marie</prenom>
					<nom>Candito</nom>
					<email>mcandito@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Crabbé</nom>
					<email>bcrabbe@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Denis</nom>
					<email>pascal.denis@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Guérin</nom>
					<email>francois.guerin@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 7/INRIA (Alpage), 30 rue du Château des Rentiers, 75013 Paris</affiliation>
				<affiliation affiliationId="2">INRIA (Alpage), Domaine de Voluceau Rocquencourt - B.P. 105 78153 Le Chesnay</affiliation>
			</affiliations>
			<titre>Analyse syntaxique du français : des constituants aux dépendances</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une technique d’analyse syntaxique statistique à la fois en constituants et en dépendances. L’analyse procède en ajoutant des étiquettes fonctionnelles aux sorties d’un analyseur en constituants, entraîné sur le French Treebank, pour permettre l’extraction de dépendances typées. D’une part, nous spécifions d’un point de vue formel et linguistique les structures de dépendances à produire, ainsi que la procédure de conversion du corpus en constituants (le French Treebank) vers un corpus cible annoté en dépendances, et partiellement validé. D’autre part, nous décrivons l’approche algorithmique qui permet de réaliser automatiquement le typage des dépendances. En particulier, nous nous focalisons sur les méthodes d’apprentissage discriminantes d’étiquetage en fonctions grammaticales.</resume>
			<mots_cles>Analyseur syntaxique statistique, analyse en constituants/dépendances, étiquetage en fonctions grammaticales</mots_cles>
			<title></title>
			<abstract>This paper describes a technique for both constituent and dependency parsing. Parsing proceeds by adding functional labels to the output of a constituent parser trained on the French Treebank in order to further extract typed dependencies. On the one hand we specify on formal and linguistic grounds the nature of the dependencies to output as well as the conversion algorithm from the French Treebank to this dependency representation. On the other hand, we describe a class of algorithms that allows to perform the automatic labeling of the functions from the output of a constituent based parser. We specifically focus on discriminative learning methods for functional labelling.</abstract>
			<keywords>Statistical parsing, constituent/dependency parsing, grammatical function labeling</keywords>
		</article>
		<article id="taln-2009-long-005" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Erwan</prenom>
					<nom>Moreau</nom>
					<email>erwan.moreau@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Tellier</nom>
					<email>isabelle.tellier@univ-orleans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antonio</prenom>
					<nom>Balvet</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Grégoire</prenom>
					<nom>Laurence</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Rozenknop</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Poibeau</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN, université de Paris 13</affiliation>
				<affiliation affiliationId="2">LIFO, université d’Orléans</affiliation>
				<affiliation affiliationId="3">UMR STL 8163, université de Lille</affiliation>
				<affiliation affiliationId="4">LIFL, Inria Lille-nord Europe</affiliation>
			</affiliations>
			<titre>Annotation fonctionnelle de corpus arborés avec des Champs Aléatoires Conditionnels</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’objectif de cet article est d’évaluer dans quelle mesure les “fonctions syntaxiques” qui figurent dans une partie du corpus arboré de Paris 7 sont apprenables à partir d’exemples. La technique d’apprentissage automatique employée pour cela fait appel aux “Champs Aléatoires Conditionnels” (Conditional Random Fields ou CRF), dans une variante adaptée à l’annotation d’arbres. Les expériences menées sont décrites en détail et analysées. Moyennant un bon paramétrage, elles atteignent une F1-mesure de plus de 80%.</resume>
			<mots_cles>fonctions syntaxiques, Conditional Random Fields, corpus arborés</mots_cles>
			<title></title>
			<abstract>The purpose of this paper is to evaluatewhether the "syntactic functions" present in a part of the Paris 7 Treebank are learnable from examples. The learning technic used is the one of "Conditional Random Fields" (CRF), in an original variant adapted to tree labelling. The conducted experiments are extensively described and analyzed. With good parameters, a F1-mesure value of over 80% is reached.</abstract>
			<keywords>syntactic functions, Conditional Random Fields, Treebanks</keywords>
		</article>
		<article id="taln-2009-long-006" session="Traduction &amp; alignement">
			<auteurs>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes, LINA - UMR CNRS 6241, 2 rue de la Houssinière, BP 92208, 44322 Nantes Cedex 03</affiliation>
			</affiliations>
			<titre>Apport d’un corpus comparable déséquilibré à l’extraction de lexiques bilingues</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les principaux travaux en extraction de lexiques bilingues à partir de corpus comparables reposent sur l’hypothèse implicite que ces corpus sont équilibrés. Cependant, les différentes méthodes computationnelles associées sont relativement insensibles à la taille de chaque partie du corpus. Dans ce contexte, nous étudions l’influence que peut avoir un corpus comparable déséquilibré sur la qualité des terminologies bilingues extraites à travers différentes expériences. Nos résultats montrent que sous certaines conditions l’utilisation d’un corpus comparable déséquilibré peut engendrer un gain significatif dans la qualité des lexiques extraits.</resume>
			<mots_cles>Multilinguisme, corpus comparable, extraction de lexiques bilingues</mots_cles>
			<title></title>
			<abstract>The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the different related approaches are relatively insensitive to sizes of each part of the comparable corpus. Within this context, we study the influence of unbalanced comparable corpora on the quality of bilingual terminology extraction through different experiments. Our results show the conditions under which the use of an unbalanced comparable corpus can induce a significant gain in the quality of extracted lexicons.</abstract>
			<keywords>Multilingualism, comparable corpus, bilingual lexicon extraction</keywords>
		</article>
		<article id="taln-2009-long-007" session="Entités nommées">
			<auteurs>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Charton</nom>
					<email>eric.charton@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Juan-Manuel</prenom>
					<nom>Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
			</affiliations>
			<titre>Classification d’un contenu encyclopédique en vue d’un étiquetage par entités nommées</titre>
			<type>long</type>
			<pages></pages>
			<resume>On utilise souvent des ressources lexicales externes pour améliorer les performances des systèmes d’étiquetage d’entités nommées. Les contenus de ces ressources lexicales peuvent être variés : liste de noms propres, de lieux, de marques. On note cependant que la disponibilité de corpus encyclopédiques exhaustifs et ouverts de grande taille tels que Worldnet ou Wikipedia, a fait émerger de nombreuses propositions spécifiques d’exploitation de ces contenus par des systèmes d’étiquetage. Un problème demeure néanmoins ouvert avec ces ressources : celui de l’adaptation de leur taxonomie interne, complexe et composée de dizaines de milliers catégories, aux exigences particulières de l’étiquetage des entités nommées. Pour ces dernières, au plus de quelques centaines de classes sémantiques sont requises. Dans cet article nous explorons cette difficulté et proposons un système complet de transformation d’un arbre taxonomique encyclopédique en une système à classe sémantiques adapté à l’étiquetage d’entités nommées.</resume>
			<mots_cles>Etiquetage, Entités nommées, classification, taxonomie</mots_cles>
			<title></title>
			<abstract>The advent of Wikipedia and WordNet aroused new interest in labeling by named entity aided by external resources. The availability of these large, multilingual, comprehensive and open digital encyclopaedic corpora suggests the development of labeling solutions that exploit the knowledge contained in these corpora. The mapping of a word sequence to an encyclopedic document is possible, however the classification of encyclopaedic entities and their related labels, is not yet fully resolved. The inconsistency of an open encyclopaedic corpus such as Wikipedia, makes sometimes difficult establishing a relationship between its entities and a restricted taxonomy. In this article we explore this problem and propose a complete system to meet this need.</abstract>
			<keywords>Named entity recognition, classification, taxonomie</keywords>
		</article>
		<article id="taln-2009-long-008" session="Lexique">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département d’Informatique et de Recherche Opérationnelle, Université de montreal, C.P. 6128 Suc. Centre-Ville, Montréal, H3C3J7, Qc, Canada</affiliation>
			</affiliations>
			<titre>Étude quantitative de liens entre l’analogie formelle et la morphologie constructionnelle</titre>
			<type>long</type>
			<pages></pages>
			<resume>Plusieurs travaux ont récemment étudié l’apport de l’apprentissage analogique dans des applications du traitement automatique des langues comme la traduction automatique, ou la recherche d’information. Il est souvent admis que les relations analogiques de forme entre les mots capturent des informations de nature morphologique. Le but de cette étude est de présenter une analyse des points de rencontre entre l’analyse morphologique et les analogies de forme. C’est à notre connaissance la première étude de ce type portant sur des corpus de grande taille et sur plusieurs langues. Bien que notre étude ne soit pas dédiée à une tâche particulière du traitement des langues, nous montrons cependant que le principe d’analogie permet de segmenter des mots en morphèmes avec une bonne précision.</resume>
			<mots_cles>Apprentissage analogique, analogie formelle, analyse morphologique</mots_cles>
			<title></title>
			<abstract>Several studies recently showed the interest of analogical learning for Natural Language processing tasks such as Machine Translation and Information Retrieval. It is often admitted that formal analogies between words capture morphological information. The purpose of this study os to quantify the correlations between morphological analysis and formal analogies. This is to our knowledge the first attempt to conduct such a quantitative analysis on large datasets and for several languages. Although this paper was not geared toward tackling a specific natural language processing task, we show that segmenting a word token into morphemes can be accomplished with a good precision by a simple strategy relying solely on formal analogy.</abstract>
			<keywords>Analogical Learning, Formal Analogies, Morphological Analysis</keywords>
		</article>
		<article id="taln-2009-long-009" session="Traduction &amp; alignement">
			<auteurs>
				<auteur>
					<prenom>Thi-Ngoc-Diep</prenom>
					<nom>Do</nom>
					<email>thi-ngoc-diep.do@imag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Viet-Bac</prenom>
					<nom>Le</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Bigi</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Castelli</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LIG, GETALP, Grenoble, France</affiliation>
				<affiliation affiliationId="2">Centre MICA, CNRS/UMI-2954, Hanoi, Vietnam</affiliation>
			</affiliations>
			<titre>Exploitation d’un corpus bilingue pour la création d’un système de traduction probabiliste Vietnamien - Français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente nos premiers travaux en vue de la construction d’un système de traduction probabiliste pour le couple de langue vietnamien-français. La langue vietnamienne étant considérée comme une langue peu dotée, une des difficultés réside dans la constitution des corpus parallèles, indispensable à l’apprentissage des modèles. Nous nous concentrons sur la constitution d’un grand corpus parallèle vietnamien-français. La méthode d’identification automatique des paires de documents parallèles fondée sur la date de publication, les mots spéciaux et les scores d’alignements des phrases est appliquée. Cet article présente également la construction d’un premier système de traduction automatique probabiliste vietnamienfrançais et français-vietnamien à partir de ce corpus et discute l’opportunité d’utiliser des unités lexicales ou sous-lexicales pour le vietnamien (syllabes, mots, ou leurs combinaisons). Les performances du système sont encourageantes et se comparent avantageusement à celles du système de Google.</resume>
			<mots_cles>traduction probabiliste, corpus bilingue, alignement de documents, table de traduction</mots_cles>
			<title></title>
			<abstract>This paper presents our first attempt at constructing a Vietnamese-French statistical machine translation system. Since Vietnamese is considered as an under-resourced language, one of the difficulties is building a large Vietnamese-French parallel corpus, which is indispensable to train the models. We concentrate on building a large Vietnamese-French parallel corpus. The document alignment method based on publication date, special words and sentence alignment result is applied. The paper also presents an application of the obtained parallel corpus to the construction of a Vietnamese-French statistical machine translation system, where the use of different units for Vietnamese (syllables, words, or their combinations) is discussed. The performance of the system is encouraging and it compares favourably to that of Google Translate.</abstract>
			<keywords>statistical machine translation, bilingual corpus, document alignment, phrase table</keywords>
		</article>
		<article id="taln-2009-long-010" session="Traduction &amp; alignement">
			<auteurs>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Prochasson</nom>
					<email>emmanuel.prochasson@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes, LINA - UMR CNRS 6241, 2 rue de la Houssinière, BP 92208, 44322 Nantes Cedex 03</affiliation>
			</affiliations>
			<titre>Influence des points d’ancrage pour l’extraction lexicale bilingue à partir de corpus comparables spécialisés</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’extraction de lexiques bilingues à partir de corpus comparables affiche de bonnes performances pour des corpus volumineux mais chute fortement pour des corpus d’une taille plus modeste. Pour pallier cette faiblesse, nous proposons une nouvelle contribution au processus d’alignement lexical à partir de corpus comparables spécialisés qui vise à renforcer la significativité des contextes lexicaux en s’appuyant sur le vocabulaire spécialisé du domaine étudié. Les expériences que nous avons réalisées en ce sens montrent qu’une meilleure prise en compte du vocabulaire spécialisé permet d’améliorer la qualité des lexiques extraits.</resume>
			<mots_cles>Corpus comparable, extraction de lexiques bilingues, points d’ancrage</mots_cles>
			<title></title>
			<abstract>Bilingual lexicon extraction from comparable corpora gives good results for large corpora but drops significantly for small size corpora. In order to compensate this weakness, we suggest a new contribution dedicated to the lexical alignment from specialized comparable corpora that strengthens the representativeness of the lexical contexts based on domainspecific vocabulary. The experiments carried out in this way show that taking better account the specialized vocabulary induces a significant improvement in the quality of extracted lexicons.</abstract>
			<keywords>Comparable corpus, bilingual lexicon extraction, anchor points</keywords>
		</article>
		<article id="taln-2009-long-011" session="Traduction &amp; alignement">
			<auteurs>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Huet</nom>
					<email>huetstep@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Bourdaillet</nom>
					<email>bourdaij@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">DIRO - Université de Montréal, C.P. 6128, succursale centre-ville, H3C 3J7, Montréal, Québec, Canada</affiliation>
			</affiliations>
			<titre>Intégration de l’alignement de mots dans le concordancier bilingue TransSearch</titre>
			<type>long</type>
			<pages></pages>
			<resume>Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Dans cet article, nous présentons nos travaux menés dans le but d’améliorer le concordancier bilingue TransSearch. Ce service, accessible sur le Web, repose principalement sur un alignement au niveau des phrases. Dans cette étude, nous discutons et évaluons l’intégration d’un alignement statistique au niveau des mots. Nous présentons deux nouvelles problématiques essentielles au succès de notre nouveau prototype : la détection des traductions erronées et le regroupement des variantes de traduction similaires.</resume>
			<mots_cles>alignement au niveau des mots, concordancier bilingue, traduction automatique</mots_cles>
			<title></title>
			<abstract>Despite the impressive amount of recent studies devoted to improving the state of the art of machine translation, computer assisted translation tools remain the preferred solution of human translators when publication quality is of concern. In this paper, we present our ongoing efforts conducted within a project which aims at improving the commercial bilingual concordancer TransSearch. The core technology of this Web-based service mainly relies on sentence-level alignment. In this study, we discuss and evaluate the embedding of statistical word-level alignment. Two novel issues that are essential to the success of our new prototype are tackled: detecting erroneous translations and grouping together similar translations.</abstract>
			<keywords>word-level alignment, bilingual concordancer, machine translation</keywords>
		</article>
		<article id="taln-2009-long-012" session="Résumé et analyse d'opinion">
			<auteurs>
				<auteur>
					<prenom>Agata</prenom>
					<nom>Jackiewicz</nom>
					<email>Agata.Jackiewicz@paris-sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Charnois</nom>
					<email>Thierry.Charnois@info.unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Ferrari</nom>
					<email>Stephane.Ferrari@info.unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LALIC – Université Paris-Sorbonne</affiliation>
				<affiliation affiliationId="2">GREYC – Université de Caen Basse-Normandie</affiliation>
			</affiliations>
			<titre>Jugements d'évaluation et constituants périphériques</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’article présente une étude portant sur des constituants détachés à valeur axiologique. Dans un premier temps, une analyse linguistique sur corpus met en évidence un ensemble de patrons caractéristiques du phénomène. Ensuite, une expérimentation informatique est proposée sur un corpus de plus grande taille afin de permettre l’observation des patrons en vue d’un retour sur le modèle linguistique. Ce travail s’inscrit dans un projet mené à l’interface de la linguistique et du TAL, qui se donne pour but d’enrichir, d’adapter au français et de formaliser le modèle général Appraisal de l’évaluation dans la langue.</resume>
			<mots_cles>jugement d’évaluation, constituants extra-prédicatifs, constructions et lexiques subjectifs, Appraisal, implémentation informatique, portraits et biographies dans la presse de spécialité et la presse d’information</mots_cles>
			<title></title>
			<abstract>In this paper, we present a study about peripheral constituent expressing some axiological value. First, a linguistic corpus analysis highlights some characteristic patterns for this phenomenon. Then, a computational experiment is carried out on a larger corpus in order to enable the observation of these patterns and to get a feedback on the linguistic model. This work takes part in a project at the intersection of Linguistics and NLP, which aims at enhancing, adapting to French language and formalizing the Appraisal generic model of the evaluation in language.</abstract>
			<keywords>evaluative judgement, peripheral constituent, evaluative constructions and lexicon, Appraisal, computational implementation, biographies and portraits in specialized press and newspapers</keywords>
		</article>
		<article id="taln-2009-long-013" session="Sémantique et applications">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Portet</nom>
					<email>portet@imag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Albert</prenom>
					<nom>Gatt</nom>
					<email>a.gatt@abdn.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jim</prenom>
					<nom>Hunter</nom>
					<email>j.hunter@abdn.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ehud</prenom>
					<nom>Reiter</nom>
					<email>e.reiter@abdn.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Somayajulu</prenom>
					<nom>Sripada</nom>
					<email>yaji.sripada@abdn.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Department of Computing Science, University of Aberdeen, Écosse</affiliation>
				<affiliation affiliationId="2">LIG, UMR 5217, Université de Grenoble, France</affiliation>
			</affiliations>
			<titre>Le projet BabyTalk : génération de texte à partir de données hétérogènes pour la prise de décision en unité néonatale</titre>
			<type>long</type>
			<pages></pages>
			<resume>Notre société génère une masse d’information toujours croissante, que ce soit en médecine, en météorologie, etc. La méthode la plus employée pour analyser ces données est de les résumer sous forme graphique. Cependant, il a été démontré qu'un résumé textuel est aussi un mode de présentation efficace. L'objectif du prototype BT-45, développé dans le cadre du projet Babytalk, est de générer des résumés de 45 minutes de signaux physiologiques continus et d'événements temporels discrets en unité néonatale de soins intensifs (NICU). L'article présente l'aspect génération de texte de ce prototype. Une expérimentation clinique a montré que les résumés humains améliorent la prise de décision par rapport à l'approche graphique, tandis que les textes de BT-45 donnent des résultats similaires à l’approche graphique. Une analyse a identifié certaines des limitations de BT-45 mais en dépit de cellesci, notre travail montre qu'il est possible de produire automatiquement des résumés textuels efficaces de données complexes.</resume>
			<mots_cles>Traitement automatique des langues naturelles, Génération de texte, Analyse de données, Unité de soins intensifs, Systèmes d'aide à la décision</mots_cles>
			<title></title>
			<abstract>Nowadays large amount of data is produced every day in medicine, meteorology and other areas and the most common approach to analyse such data is to present it graphically. However, it has been shown that textual summarisation is also an effective approach. As part of the BabyTalk project, the prototype BT-45 was developed to generate summaries of 45 minutes of continuous physiological signals and discrete temporal events in a neonatal intensive care unit (NICU). The paper presents its architecture with an emphasis on its natural language generation part. A clinical experiment showed that human textual summaries led to better decision making than graphical presentation, whereas BT-45 texts led to similar results as visualisations. An analysis identified some of the reasons for the BT-45 texts inferiority, but, despite these deficiencies, our work shows that it is possible for computer systems to generate effective textual summaries of complex data.</abstract>
			<keywords>Natural language processing, Natural language generation, Intelligent data analysis, Intensive care unit, Decision support systems</keywords>
		</article>
		<article id="taln-2009-long-014" session="Lexique">
			<auteurs>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Cartoni</nom>
					<email>cartonib@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève</affiliation>
			</affiliations>
			<titre>Les adjectifs relationnels dans les lexiques informatisés : formalisation et exploitation dans un contexte multilingue</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous nous intéressons aux adjectifs dits relationnels et à leur statut en traitement automatique des langues naturelles (TALN). Nous montrons qu’ils constituent une « sous-classe » d’adjectifs rarement explicitée et donc rarement représentée dans les lexiques sur lesquels reposent les applications du TALN, alors qu’ils jouent un rôle important dans de nombreuses applications. Leur formation morphologique est source d’importantes divergences entre différentes langues, et c’est pourquoi ces adjectifs sont un véritable défi pour les applications informatiques multilingues. Dans une partie plus pratique, nous proposons une formalisation de ces adjectifs permettant de rendre compte de leurs liens avec leur base nominale. Nous tentons d’extraire ces informations dans les lexiques informatisés existants, puis nous les exploitons pour traduire les adjectifs relationnels préfixés de l’italien en français.</resume>
			<mots_cles>Adjectifs relationnels, ressources lexicales, morphologie constructionnelle</mots_cles>
			<title></title>
			<abstract>This article focuses on a particular type of adjectives, relational adjectives, and especially on the way they are processed in natural language processing systems. We show that this class of adjectives is barely recorded in an explicit manner in computer lexicons. There is an important discrepancy in the way those adjectives are morphologically constructed in different languages, and therefore, they are a real challenge for multilingual computing applications. On a more practical side, we propose a formalisation for the adjectives that shows their semantic link with their nominal base. We make an attempt to extract this kind of information in existing machine lexica, and we exploit their semantic links in the translation of prefixed relational adjectives from Italian into French.</abstract>
			<keywords>Relational adjectives, lexical resources, constructional morphology</keywords>
		</article>
		<article id="taln-2009-long-015" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Marc</prenom>
					<nom>Plantevit</nom>
					<email>Marc.Plantevit@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Charnois</nom>
					<email>Thierry.Charnois@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC – CNRS UMR 6072, Université de Caen – Bd Mal. Juin, 14032 CAEN Cedex</affiliation>
			</affiliations>
			<titre>Motifs séquentiels pour l’extraction d’information : illustration sur le problème de la détection d’interactions entre gènes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Face à la prolifération des publications en biologie et médecine (plus de 18 millions de publications actuellement recensées dans PubMed), l’extraction d’information automatique est devenue un enjeu crucial. Il existe de nombreux travaux dans le domaine du traitement de la langue appliquée à la biomédecine ("BioNLP"). Ces travaux se distribuent en deux grandes tendances. La première est fondée sur les méthodes d’apprentissage automatique de type numérique qui donnent de bons résultats mais ont un fonctionnement de type "boite noire". La deuxième tendance est celle du TALN à base d’analyses (lexicales, syntaxiques, voire sémantiques ou discursives) coûteuses en temps de développement des ressources nécessaires (lexiques, grammaires, etc.). Nous proposons dans cet article une approche basée sur la découverte de motifs séquentiels pour apprendre automatiquement les ressources linguistiques, en l’occurrence les patrons linguistiques qui permettent l’extraction de l’information dans les textes. Plusieurs aspects méritent d’être soulignés : cette approche permet de s’affranchir de l’analyse syntaxique de la phrase, elle ne nécessite pas de ressources en dehors du corpus d’apprentissage et elle ne demande que très peu d’intervention manuelle. Nous illustrons l’approche sur le problème de la détection d’interactions entre gènes et donnons les résultats obtenus sur des corpus biologiques qui montrent l’intérêt de ce type d’approche.</resume>
			<mots_cles>Extraction d’information, fouille de textes, motifs séquentiels, interactions entre gènes</mots_cles>
			<title></title>
			<abstract>The proliferation of publications in biology andmedicine (more than 18million publications currently listed in PubMed) has lead to the crucial need of automatic information extraction. There are many work in the field of natural language processing applied to biomedicine (BioNLP). Two types of approaches tackle this problem. On the one hand, machine learning based approaches give good results but run as a "black box". On the second hand, NLP based approaches are highly time consuming for developing the resources (lexicons, grammars, etc.). In this paper, we propose an approach based on sequential pattern mining to automatically discover linguistic patterns that allow the information extraction in texts. This approach allows to overcome sentence parsing and it does not require resources outside the training data set. We illustrate the approach on the problem of detecting interactions between genes and give the results obtained on biological corpora that show the relevance of this type of approach.</abstract>
			<keywords>Information extraction, text mining, sequential patterns, gene interactions</keywords>
		</article>
		<article id="taln-2009-long-016" session="Traduction &amp; alignement">
			<auteurs>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Rafik</prenom>
					<nom>Maklhoufi</nom>
					<email>rafik.makhloufi@utt.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS et Université Paris-Sud 11, Orsay, France</affiliation>
				<affiliation affiliationId="2">Université de Technologie de Troyes, France</affiliation>
				<affiliation affiliationId="3">DIRO, Université de Montréal, Canada</affiliation>
			</affiliations>
			<titre>Prise en compte de dépendances syntaxiques pour la traduction contextuelle de segments</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans un système standard de traduction statistique basé sur les segments, le score attribué aux différentes traductions d’un segment ne dépend pas du contexte dans lequel il apparaît. Plusieurs travaux récents tendent à montrer l’intérêt de prendre en compte le contexte source lors de la traduction, mais ces études portent sur des systèmes traduisant vers l’anglais, une langue faiblement fléchie. Dans cet article, nous décrivons nos expériences sur la prise en compte du contexte source dans un système statistique traduisant de l’anglais vers le français, basé sur l’approche proposée par Stroppa et al. (2007). Nous étudions l’impact de différents types d’indices capturant l’information contextuelle, dont des dépendances syntaxiques typées. Si les mesures automatiques d’évaluation de la qualité d’une traduction ne révèlent pas de gains significatifs de notre système par rapport à un système à l’état de l’art ne faisant pas usage du contexte, une évaluation manuelle conduite sur 100 phrases choisies aléatoirement est en faveur de notre système. Cette évaluation fait également ressortir que la prise en compte de certaines dépendances syntaxiques est bénéfique à notre système.</resume>
			<mots_cles>Traduction automatique statistique, contexte source, dépendances syntaxiques</mots_cles>
			<title></title>
			<abstract>In standard phrase-based Statistical Machine Translation (PBSMT) systems, the score associated with each translation of a phrase does not depend on its context. While several works have shown the potential gain of exploiting source context, they all considered English, a morphologically poor language, as the target language. In this article, we describe experiments on exploiting the source context in an English -> French PBSMT system, inspired by the work of Stroppa et al. (2007). We report a study on the impact of various types of features that capture contextual information, including syntactic dependencies. While automatic metrics do not show significative gains relative to a baseline system, a manual evaluation of 100 randomly selected sentences concludes that our context-aware system performs consistently better. This evaluation also shows that some types of syntactic dependencies can participate to the gains observed.</abstract>
			<keywords>Statistical Machine Translation, source context, syntactic dependencies</keywords>
		</article>
		<article id="taln-2009-long-017" session="Sémantique et applications">
			<auteurs>
				<auteur>
					<prenom>Maud</prenom>
					<nom>Ehrmann</nom>
					<email>Maud.Ehrmann@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Hagège</nom>
					<email>Caroline.Hagege@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Center Europe - XRCE, 6, Chemin de Maupertuis, 38240 Meylan</affiliation>
			</affiliations>
			<titre>Proposition de caractérisation et de typage des expressions temporelles en contexte</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous assistons actuellement en TAL à un regain d’intérêt pour le traitement de la temporalité véhiculée par les textes. Dans cet article, nous présentons une proposition de caractérisation et de typage des expressions temporelles tenant compte des travaux effectués dans ce domaine tout en cherchant à pallier les manques et incomplétudes de certains de ces travaux. Nous explicitons comment nous nous situons par rapport à l’existant et les raisons pour lesquelles parfois nous nous en démarquons. Le typage que nous définissons met en évidence de réelles différences dans l’interprétation et le mode de résolution référentielle d’expressions qui, en surface, paraissent similaires ou identiques. Nous proposons un ensemble des critères objectifs et linguistiquement motivés permettant de reconnaître, de segmenter et de typer ces expressions. Nous verrons que cela ne peut se réaliser sans considérer les procès auxquels ces expressions sont associées et un contexte parfois éloigné.</resume>
			<mots_cles>Temporalité, typage et caractérisation des expressions temporelles</mots_cles>
			<title></title>
			<abstract>Temporal processing in texts is a topic of renewed interest in NLP. In this paper we present a new way of typing temporal expressions that takes into account both the state of the art of this domain and that also tries to be more precise and accurate that some of the current proposals. We explain into what extent our proposal is compatible and comparable with the state-of-the art and why sometimes we stray from it. The typing system that we define highlights real differences in the interpretation and reference calculus of these expressions. At the same time, by offering objective criteria, it fulfils the necessity of high inter-agreement between annotators. After having defined what we consider as temporal expressions, we will show that tokenization, characterization and typing of those expressions can only be done having into account processes to which these expressions are linked.</abstract>
			<keywords>temporal processing, temporal expressions characterization and typing</keywords>
		</article>
		<article id="taln-2009-long-018" session="Segmentation et résolution d'anaphores">
			<auteurs>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Bestgen</nom>
					<email>yves.bestgen@psp.ucl.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CECL / PSOR – Université catholique de Louvain, Place du Cardinal Mercier, 10 — B-1348 Louvain-la-Neuve — Belgique</affiliation>
			</affiliations>
			<titre>Quel indice pour mesurer l'efficacité en segmentation de textes?</titre>
			<type>long</type>
			<pages></pages>
			<resume>L'évaluation de l'efficacité d'algorithmes de segmentation thématique est généralement effectuée en quantifiant le degré d'accord entre une segmentation hypothétique et une segmentation de référence. Les indices classiques de précision et de rappel étant peu adaptés à ce domaine, WindowDiff (Pevzner, Hearst, 2002) s'est imposé comme l'indice de référence. Une analyse de cet indice montre toutefois qu'il présente plusieurs limitations. L'objectif de ce rapport est d'évaluer un indice proposé par Bookstein, Kulyukin et Raita (2002), la distance de Hamming généralisée, qui est susceptible de remédier à celles-ci. Les analyses montrent que celui-ci conserve tous les avantages de WindowDiff sans les limitations. De plus, contrairement à WindowDiff, il présente une interprétation simple puisqu'il correspond à une vraie distance entre les deux segmentations à comparer.</resume>
			<mots_cles>Segmentation thématique, évaluation, distance de Hamming généralisée, WindowDiff</mots_cles>
			<title></title>
			<abstract>The evaluation of thematic segmentation algorithms is generally carried out by quantifying the degree of agreement between a hypothetical segmentation and a gold standard. The traditional indices of precision and recall being little adapted to this field, WindowDiff (Pevzner, Hearst, 2002) has become the standard for this kind of assessment. An analysis of this index shows however that it presents several limitations. The objective of this report is to evaluate an index developed by Bookstein, Kulyukin and Raita (2002), the Generalized Hamming Distance, which is likely to overcome these limitations. The analyzes show that it preserves all the advantages of WindowDiff without its limitations. Moreover, contrary to WindowDiff, it presents a simple interpretation since it corresponds to a true distance between the two segmentations.</abstract>
			<keywords>Thematic segmentation, evaluation, generalized Hamming distance, WindowDiff</keywords>
		</article>
		<article id="taln-2009-long-019" session="Segmentation et résolution d'anaphores">
			<auteurs>
				<auteur>
					<prenom>Marion</prenom>
					<nom>Laignelet</nom>
					<email>marion.laignelet@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Rioult</nom>
					<email>Francois.Rioult@info.unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS / Université de Toulouse 2 Le Mirail</affiliation>
				<affiliation affiliationId="2">GREYC (CNRS UMR6072), Université de Caen Basse-Normandie</affiliation>
			</affiliations>
			<titre>Repérer automatiquement les segments obsolescents à l’aide d’indices sémantiques et discursifs</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article vise la description et le repérage automatique des segments d’obsolescence dans les documents de type encyclopédique. Nous supposons que des indices sémantiques et discursifs peuvent permettre le repérage de tels segments. Pour ce faire, nous travaillons sur un corpus annoté manuellement par des experts sur lequel nous projetons des indices repérés automatiquement. Les techniques statistiques de base ne permettent pas d’expliquer ce phénomène complexe. Nous proposons l’utilisation de techniques de fouille de données pour le caractériser et nous évaluons le pouvoir prédictif de nos indices. Nous montrons, à l’aide de techniques de classification supervisée et de calcul de l’aire sous la courbe ROC, que nos hypothèses sont pertinentes.</resume>
			<mots_cles>repérage automatique de l’obsolescence, indices sémantiques et discursifs, textes encyclopédiques, classification supervisée, aire sous la courbe ROC</mots_cles>
			<title></title>
			<abstract>This paper deals with the description and automatic tracking of obsolescence in encyclopedic type of documents. We suppose that semantic and discursive cues may allow the tracking of these segments. For that purpose, we have worked on an expert manually annotated corpus, on which we have projected automatically tracked cues. Basic statistic techniques can not account for this complex phenomenon. We propose the use of techniques of data mining to characterize it, and we evaluate the predictive power of our cues. We show, using techniques of supervised classification and area under the ROC curve, that our hypotheses are relevant.</abstract>
			<keywords>automatic tracking of obsolescence, semantic and discursive cues, encyclopedic type of documents, supervised classification, area under the ROC curve</keywords>
		</article>
		<article id="taln-2009-long-020" session="Résumé et analyse d'opinion">
			<auteurs>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Généreux</nom>
					<email>Michel.Genereux@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Bossard</nom>
					<email>Aurelien.Bossard@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de Paris-Nord (CNRS UMR 7030 et Université Paris 13), 99, av. J.-B. Clément – 93430 Villetaneuse</affiliation>
			</affiliations>
			<titre>Résumé automatique de textes d’opinions</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le traitement des langues fait face à une demande croissante en matière d’analyse de textes véhiculant des critiques ou des opinions. Nous présentons ici un système de résumé automatique tourné vers l’analyse d’articles postés sur des blogues, où sont exprimées à la fois des informations factuelles et des prises de position sur les faits considérés. Nous montrons qu’une approche classique à base de traits de surface est tout à fait efficace dans ce cadre. Le système est évalué à travers une participation à la campagne d’évaluation internationale TAC (Text Analysis Conference) où notre système a réalisé des performances satisfaisantes.</resume>
			<mots_cles>résumé automatique, analyse de textes subjectifs, évaluation automatique</mots_cles>
			<title></title>
			<abstract>There is currently a growing need concerning the analysis of texts expressing opinions or judgements. In this paper, we present a summarization system that is specifically designed to process blog posts, where factual information is mixed with opinions. We show that a classical approach based on surface cues is efficient to summarize this kind of texts. The system is evaluated through a participation to TAC (Text Analysis Conference), an international evaluation framework for automatic summarization, in which our system obtained good results.</abstract>
			<keywords>automatic summarization, analysis of subjective texts, automatic evaluation</keywords>
		</article>
		<article id="taln-2009-long-021" session="Sémantique et applications">
			<auteurs>
				<auteur>
					<prenom>Ingrid</prenom>
					<nom>Falk</nom>
					<email>ingrid.falk@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claire</prenom>
					<nom>Gardent</nom>
					<email>claire.gardent@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Évelyne</prenom>
					<nom>Jacquey</nom>
					<email>evelyne.jacquey@atilf.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabienne</prenom>
					<nom>Venant</nom>
					<email>fabienne.venant@loria.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA / Université Nancy 2</affiliation>
				<affiliation affiliationId="2">CNRS / LORIA, Nancy</affiliation>
				<affiliation affiliationId="3">CNRS / ATILF, Nancy</affiliation>
				<affiliation affiliationId="4">Université Nancy 2</affiliation>
			</affiliations>
			<titre>Sens, synonymes et définitions</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article décrit une méthodologie visant la réalisation d’une ressource sémantique en français centrée sur la synonymie. De manière complémentaire aux travaux existants, la méthode proposée n’a pas seulement pour objectif d’établir des liens de synonymie entre lexèmes, mais également d’apparier les sens possibles d’un lexème avec les ensembles de synonymes appropriés. En pratique, les sens possibles des lexèmes proviennent des définitions du TLFi et les synonymes de cinq dictionnaires accessibles à l’ATILF. Pour évaluer la méthode d’appariement entre sens d’un lexème et ensemble de synonymes, une ressource de référence a été réalisée pour 27 verbes du français par quatre lexicographes qui ont spécifié manuellement l’association entre verbe, sens (définition TLFi) et ensemble de synonymes. Relativement à ce standard étalon, la méthode d’appariement affiche une F-mesure de 0.706 lorsque l’ensemble des paramètres est pris en compte, notamment la distinction pronominal / non-pronominal pour les verbes du français et de 0.602 sans cette distinction.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>We present a method for grouping the synonyms of a word into sets representing the possible meanings of that word. The possible meanings are given by the definitions of a general dictionary for French, the TLFi (Trésor de la langue française informatisé) and the method is applied to the synonyms of 5 synonym dictionnaries. To evaluate the method, we manually constructed a gold standard where for each (word, definition) pair, 4 lexicographers specified the set of synonyms they judge adequate. The method scores an F-measure of 0.602 when no distinction is made between pronominal and non-pronominal use and 0.706 when it is.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2009-long-022" session="Segmentation et résolution d'anaphores">
			<auteurs>
				<auteur>
					<prenom>Étienne</prenom>
					<nom>Ailloud</nom>
					<email>ailloud@cl.uzh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Manfred</prenom>
					<nom>Klenner</nom>
					<email>klenner@cl.uzh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institute of Computational Linguistics, Zurich University, Zurich, Switzerland</affiliation>
			</affiliations>
			<titre>Vers des contraintes plus linguistiques en résolution de coréférences</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous proposons un modèle filtrant de résolution de coréférences basé sur les notions de transitivité et d’exclusivité linguistique. À partir de l’hypothèse générale que les chaînes de coréférence demeurent cohérentes tout au long d’un texte, notre modèle assure le respect de certaines contraintes linguistiques (via des filtres) quant à la coréférence, ce qui améliore la résolution globale. Le filtrage a lieu à différentes étapes de l’approche standard (c-à-d. par apprentissage automatique), y compris avant l’apprentissage et avant la classification, accélérant et améliorant ce processus.</resume>
			<mots_cles>Résolution de coréférences, apprentissage automatique, linguistique informatique par contraintes</mots_cles>
			<title></title>
			<abstract>We propose a filter model of coreference resolution that is based on the notions of transitivity and linguistic exclusivity. Starting from the general assumption that coreference sets remain coherent throughout a text, our model enforces the checking of some compatibility criteria (filters) between coreference candidates, thereby improving resolution performance. This filtering is achieved at different stages of the workflow of machine-learning-based coreference resolution, including at the standard learning and testing steps, where it may help reduce the computational load and better distribute the actual occurrences to be learned.</abstract>
			<keywords>Coreference resolution, Machine learning, Constraint-based NLP</keywords>
		</article>
		<article id="taln-2009-long-023" session="Lexique">
			<auteurs>
				<auteur>
					<prenom>Lionel</prenom>
					<nom>Nicolas</nom>
					<email>lnicolas@i3s.unice.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Miguel A.</prenom>
					<nom>Molinero</nom>
					<email>mmolinero@udc.es</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jacques</prenom>
					<nom>Farré</nom>
					<email>jf@i3s.unice.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Villemonte De La Clergerie</nom>
					<email>Eric.De_La_Clergerie@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Équipe RL, Laboratoire I3S, UNSA+CNRS, France</affiliation>
				<affiliation affiliationId="2">Projet ALPAGE, INRIA Rocquencourt + Paris 7, France</affiliation>
				<affiliation affiliationId="3">Grupo LYS, Univ. de A Coruña, España</affiliation>
			</affiliations>
			<titre>Trouver et confondre les coupables : un processus sophistiqué de correction de lexique</titre>
			<type>long</type>
			<pages></pages>
			<resume>La couverture d’un analyseur syntaxique dépend avant tout de la grammaire et du lexique sur lequel il repose. Le développement d’un lexique complet et précis est une tâche ardue et de longue haleine, surtout lorsque le lexique atteint un certain niveau de qualité et de couverture. Dans cet article, nous présentons un processus capable de détecter automatiquement les entrées manquantes ou incomplètes d’un lexique, et de suggérer des corrections pour ces entrées. La détection se réalise au moyen de deux techniques reposant soit sur un modèle statistique, soit sur les informations fournies par un étiqueteur syntaxique. Les hypothèses de corrections pour les entrées lexicales détectées sont générées en étudiant les modifications qui permettent d’améliorer le taux d’analyse des phrases dans lesquelles ces entrées apparaissent. Le processus global met en oeuvre plusieurs techniques utilisant divers outils tels que des étiqueteurs et des analyseurs syntaxiques ou des classifieurs d’entropie. Son application au Lefff , un lexique morphologique et syntaxique à large couverture du français, nous a déjà permis de réaliser des améliorations notables.</resume>
			<mots_cles>Acquisition et correction lexicale, lexique à large couverture, fouille d’erreurs, étiqueteur syntaxique, classifieur d’entropie, analyseur syntaxique</mots_cles>
			<title></title>
			<abstract>The coverage of a parser depends mostly on the quality of the underlying grammar and lexicon. The development of a lexicon both complete and accurate is an intricate and demanding task, overall when achieving a certain level of quality and coverage. We introduce an automatic process able to detect missing or incomplete entries in a lexicon, and to suggest corrections hypotheses for these entries. The detection of dubious lexical entries is tackled by two techniques relying either on a specific statistical model, or on the information provided by a part-of-speech tagger. The generation of correction hypotheses for the detected entries is achieved by studying which modifications could improve the parse rate of the sentences in which the entries occur. This process brings together various techniques based on different tools such as taggers, parsers and entropy classifiers. Applying it on the Lefff , a large-coverage morphological and syntactic French lexicon, has already allowed us to perfom noticeable improvements.</abstract>
			<keywords>Lexical acquisition and correction, wide coverage lexicon, error mining, tagger, entropy classifier, syntactic parser</keywords>
		</article>
		<article id="taln-2009-long-024" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Trouilleux</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LRL, Université Blaise-Pascal</affiliation>
			</affiliations>
			<titre>Un analyseur de surface non déterministe pour le français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les analyseurs syntaxiques de surface à base de règles se caractérisent par un processus en deux temps : désambiguïsation lexicale, puis reconnaissance de patrons. Considérant que ces deux étapes introduisent une certaine redondance dans la description linguistique et une dilution des heuristiques dans les différents processus, nous proposons de définir un analyseur de surface qui fonctionne sur une entrée non désambiguïsée et produise l’ensemble des analyses possibles en termes de syntagmes noyau (chunks). L’analyseur, implanté avec NooJ, repose sur la définition de patrons étendus qui annotent des séquences de syntagmes noyau. Les résultats obtenus sur un corpus de développement d’environ 22 500 mots, avec un rappel proche de 100 %, montrent la faisabilité de l’approche et signalent quelques points d’ambiguïté à étudier plus particulièrement pour améliorer la précision.</resume>
			<mots_cles>Analyse syntaxique de surface, automates à états finis, déterminisme, désambiguïsation</mots_cles>
			<title></title>
			<abstract>Rule-based chunkers are characterized by a two-tier process : part-of-speech disambiguation, and pattern matching. Considering that these two stages introduce some redundancy in the linguistic description and a dilution of heuristics over the different processes, we propose to define a chunker which parses a non-disambiguated input, and produces all possible analysis in terms of chunks. The parser, implemented with NooJ, relies on the definition of extended patterns, which annotate sequences of chunks. The results obtained on an approx. 22500 word corpus, with almost 100 % recall, demonstrate the feasability of the approach, and signal which ambiguities should be further studied in order to improve precision.</abstract>
			<keywords>Chunking, finite-state automata, determinism, disambiguation</keywords>
		</article>
		<article id="taln-2009-long-025" session="Résumé et analyse d'opinion">
			<auteurs>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Bossard</nom>
					<email>aurelien.bossard@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN - UMR 7030, CNRS - Université Paris 13, F-93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Une approche mixte-statistique et structurelle - pour le résumé automatique de dépêches</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les techniques de résumé automatique multi-documents par extraction ont récemment évolué vers des méthodes statistiques pour la sélection des phrases à extraire. Dans cet article, nous présentons un système conforme à l’« état de l’art » — CBSEAS — que nous avons développé pour les tâches Opinion (résumés d’opinions issues de blogs) et Update (résumés de dépêches et mise à jour du résumé à partir de nouvelles dépêches sur le même événement) de la campagne d’évaluation TAC 2008, et montrons l’intérêt d’analyses structurelles et linguistiques des documents à résumer. Nous présentons également notre étude sur la structure des dépêches et l’impact de son intégration à CBSEAS.</resume>
			<mots_cles>Résumé automatique, structure de documents</mots_cles>
			<title></title>
			<abstract>Automatic multi-document summarization techniques have recently evolved into statistical methods for selecting the sentences that will be used to generate the summary. In this paper, we present a system in accordance with « State-of-the-art » — CBSEAS — that we have developped for the « Opinion Task » (automatic summaries of opinions from blogs) and the « Update Task » (automatic summaries of newswire articles and information update) of the TAC 2008 evaluation campaign, and show the interest of structural and linguistic analysis of the documents to summarize .We also present our study on news structure and its integration to CBSEAS impact.</abstract>
			<keywords>Automatic summarization, document structure</keywords>
		</article>
		<article id="taln-2009-long-026" session="Entités nommées">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Brun</nom>
					<email>Caroline.Brun@xrce.xerox.com</email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Dessaigne</nom>
					<email>Nicolas.Dessaigne@arisem.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maud</prenom>
					<nom>Ehrmann</nom>
					<email>Maud.Ehrmann@xrce.xerox.com</email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Baptiste</prenom>
					<nom>Gaillard</nom>
					<email>Baptiste.gaillard@fr.thalesgroup.com</email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sylvie</prenom>
					<nom>Guillemin-Lanne</nom>
					<email>sylvie.guillemin-lanne@temis.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Jacquet</nom>
					<email>Guillaume.Jacquet@xrce.xerox.com</email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aaron</prenom>
					<nom>Kaplan</nom>
					<email>Aaron.Kaplan@xrce.xerox.com</email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marianna</prenom>
					<nom>Kucharski</nom>
					<email>marianna.kucharski@temis.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>Martineau</nom>
					<email>claude.martineau@univ-mlv.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Migeotte</nom>
					<email>Aurelie.Migeotte@arisem.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Takuya</prenom>
					<nom>Nakamura</nom>
					<email>takuya.nakamura@univ-mlv.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stavroula</prenom>
					<nom>Voyatzi</nom>
					<email>stavroula.voyatzi@univ-mlv.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Arisem – 1-5 rue Carnot- 91883 Massy cedex</affiliation>
				<affiliation affiliationId="2">IGM-LabInfo Université Paris-Est – 77454 Marne-la-Vallée Cedex 2</affiliation>
				<affiliation affiliationId="3">Temis –Tour Gamma B -193-197 rue de Bercy, 75582 Paris Cedex</affiliation>
				<affiliation affiliationId="4">Thales Communication – 1-5 Avenue Carnot, 91883 Massy</affiliation>
				<affiliation affiliationId="5">XRCE – 6 chemin de Maupertuis, 38240 Meylan</affiliation>
			</affiliations>
			<titre>Une expérience de fusion pour l’annotation d'entités nommées</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons une expérience de fusion d’annotations d’entités nommées provenant de différents annotateurs. Ce travail a été réalisé dans le cadre du projet Infom@gic, projet visant à l’intégration et à la validation d’applications opérationnelles autour de l’ingénierie des connaissances et de l’analyse de l’information, et soutenu par le pôle de compétitivité Cap Digital « Image, MultiMédia et Vie Numérique ». Nous décrivons tout d’abord les quatre annotateurs d’entités nommées à l’origine de cette expérience. Chacun d’entre eux fournit des annotations d’entités conformes à une norme développée dans le cadre du projet Infom@gic. L’algorithme de fusion des annotations est ensuite présenté ; il permet de gérer la compatibilité entre annotations et de mettre en évidence les conflits, et ainsi de fournir des informations plus fiables. Nous concluons en présentant et interprétant les résultats de la fusion, obtenus sur un corpus de référence annoté manuellement.</resume>
			<mots_cles>Entités nommées, fusion d’annotations, UIMA</mots_cles>
			<title></title>
			<abstract>In this paper, we present an experiment aimed at merging named entity annotations provided by different annotators. This work has been performed as part of the Infom@gic project, whose goal is the integration and validation of knowledge engineering and information analysis applications, and which is supported by the pole of competitiveness Cap Digital « Image, MultiMédia et Vie Numérique ». We first describe the four annotators, which provide named entity annotations that conform to guidelines defined in the Infom@gic project. Then we present an algorithm for merging the different annotations. It uses information about the compatibility of various annotations and can point out conflicts, and thus yields annotations that are more reliable than those of any single annotator. We conclude by describing and interpreting the merging results obtained on a manually annotated reference corpus.</abstract>
			<keywords>Named entities, fusion of annotations, UIMA</keywords>
		</article>
		<article id="taln-2009-long-027" session="Traduction &amp; alignement">
			<auteurs>
				<auteur>
					<prenom>Stéphanie</prenom>
					<nom>Léon</nom>
					<email>stephanie.leon@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, 161 rue Ada, 34392 Montpellier Cedex 5</affiliation>
			</affiliations>
			<titre>Un système modulaire d’acquisition automatique de traductions à partir du Web</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons une méthode de Traduction Automatique d’Unités Lexicales Complexes (ULC) pour la construction de ressources bilingues français/anglais, basée sur un système modulaire qui prend en compte les propriétés linguistiques des unités sources (compositionnalité, polysémie, etc.). Notre système exploite les différentes « facettes » du Web multilingue pour valider des traductions candidates ou acquérir de nouvelles traductions. Après avoir collecté une base d’ULC en français à partir d’un corpus de pages Web, nous passons par trois phases de traduction qui s’appliquent à un cas linguistique, avec une méthode adaptée : les traductions compositionnelles non polysémiques, les traductions compositionnelles polysémiques et les traductions non compositionnelles et/ou inconnues. Notre évaluation sur un vaste échantillon d’ULC montre que l’exploitation du Web pour la traduction et la prise en compte des propriétés linguistiques au sein d’un système modulaire permet une acquisition automatique de traductions avec une excellente précision.</resume>
			<mots_cles>Traduction Automatique, Unités Lexicales Complexes, Désambiguïsation lexicale, World Wide Web, Terminologie</mots_cles>
			<title></title>
			<abstract>We present a method of automatic translation (French/English) of Complex Lexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular system is based on linguistic properties (compositionality, polysemy, etc.). Different aspects of the multilingual Web are used to validate candidate translations and collect new terms. We first build a French corpus of Web pages to collect CLU. Three adapted processing stages are applied for each linguistic property : compositional and non polysemous translations, compositional polysemous translations and non compositional translations. Our evaluation on a sample of CLU shows that our technique based on the Web can reach a very high precision.</abstract>
			<keywords>Automatic translation, Complex lexical units, Lexical disambiguisation, World Wide Web, Terminology</keywords>
		</article>
		<article id="taln-2009-long-028" session="plénière">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>blache@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS &amp; Université de Provence</affiliation>
			</affiliations>
			<titre>Des relations d’alignement pour décrire l’interaction des domaines linguistiques : vers des Grammaires Multimodales</titre>
			<type>long</type>
			<pages></pages>
			<resume>Un des problèmes majeurs de la linguistique aujourd’hui réside dans la prise en compte de phénomènes relevant de domaines et de modalités différentes. Dans la littérature, la réponse consiste à représenter les relations pouvant exister entre ces domaines de façon externe, en termes de relation de structure à structure, s’appuyant donc sur une description distincte de chaque domaine ou chaque modalité. Nous proposons dans cet article une approche différente permettant représenter ces phénomènes dans un cadre formel unique, permettant de rendre compte au sein d’une même grammaire tous les phénomènes concernés. Cette représentation précise de l’interaction entre domaines et modalités s’appuie sur la définition de relations d’alignement.</resume>
			<mots_cles>Multimodalité, interaction entre domaines, grammaire, corpus multimodaux</mots_cles>
			<title></title>
			<abstract>Linguistics is now faced with the question of representing information coming from different domains or modalities. The classical answer consists in representing separately each of these domains, building for each of them an independent structure, and then representing domain interaction in terms of relation between structures. We propose n this paper a different approach in which all information is represented within a unique and homogeneous framework, making it possible to represent into a same grammar all interaction phenomena. This precise representation of interaction relies on the definition of a new notion of alignment relations.</abstract>
			<keywords>Multimodality, domains interaction, grammar, multimodal corpora</keywords>
		</article>
		<article id="taln-2009-long-029" session="Entités nommées">
			<auteurs>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email>karen.fort@inist.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maud</prenom>
					<nom>Ehrmann</nom>
					<email>maud.ehrmann@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Adeline</prenom>
					<nom>Nazarenko</nom>
					<email>adeline.nazarenko@lipn.univ-paris13.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INIST, 2 allée du Parc de Brabois, 54500 Vandoeuvre-lès-Nancy</affiliation>
				<affiliation affiliationId="2">XRCE, 6 Chemin de Maupertuis, 38240 Meylan</affiliation>
				<affiliation affiliationId="3">LIPN, Université Paris 13 &amp; CNRS, 99 av. J.B. Clément, 93430 Villetaneuse</affiliation>
			</affiliations>
			<titre>Vers une méthodologie d’annotation des entités nommées en corpus ?</titre>
			<type>long</type>
			<pages></pages>
			<resume>La tâche, aujourd’hui considérée comme fondamentale, de reconnaissance d’entités nommées, présente des difficultés spécifiques en matière d’annotation. Nous les précisons ici, en les illustrant par des expériences d’annotation manuelle dans le domaine de la microbiologie. Ces problèmes nous amènent à reposer la question fondamentale de ce que les annotateurs doivent annoter et surtout, pour quoi faire. Nous identifions pour cela les applications nécessitant l’extraction d’entités nommées et, en fonction des besoins de ces applications, nous proposons de définir sémantiquement les éléments à annoter. Nous présentons ensuite un certain nombre de recommandations méthodologiques permettant d’assurer un cadre d’annotation cohérent et évaluable.</resume>
			<mots_cles>annotation, reconnaissance d’entités nommées</mots_cles>
			<title></title>
			<abstract>Today, the named entity recognition task is considered as fundamental, but it involves some specific difficulties in terms of annotation. We list them here, with illustrations taken from manual annotation experiments in microbiology. Those issues lead us to ask the fundamental question of what the annotators should annotate and, even more important, for which purpose. We thus identify the applications using named entity recognition and, according to the real needs of those applications, we propose to semantically define the elements to annotate. Finally, we put forward a number of methodological recommendations to ensure a coherent and reliable annotation scheme.</abstract>
			<keywords>annotation, named entities extraction</keywords>
		</article>
		<article id="taln-2009-position-001" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Sylwia</prenom>
					<nom>Ozdowska</nom>
					<email>sozdowska@computing.dcu.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">National Centre for Language Technology – Dublin City University, Glasnevin, Dublin 9, Ireland</affiliation>
			</affiliations>
			<titre>Données bilingues pour la TAS français-anglais : impact de la langue source et direction de traduction originales sur la qualité de la traduction</titre>
			<type>position</type>
			<pages></pages>
			<resume>Dans cet article, nous prenons position par rapport à la question de la qualité des données bilingues destinées à la traduction automatique statistique en terme de langue source et direction de traduction originales à l’égard d’une tâche de traduction français-anglais. Nous montrons que l’entraînement sur un corpus contenant des textes qui ont été à l’origine traduits du français vers l’anglais améliore la qualité de la traduction. Inversement, l’entraînement sur un corpus contenant exclusivement des textes dont la langue source originale n’est ni le français ni l’anglais dégrade la traduction.</resume>
			<mots_cles>Traduction automatique statistique, corpus bilingue, direction de la traduction, langue source, langue cible</mots_cles>
			<title></title>
			<abstract>In this paper, we argue about the quality of bilingual data for statistical machine translation in terms of the original source language and translation direction in the context of a French to English translation task. We show that data containing original French and English translated from French improves translation quality. Conversely, using data comprising exclusively French and English translated from several other languages results in a clear-cut decrease in translation quality.</abstract>
			<keywords>Statistical machine translation, bilingual corpus, translation direction, source language, target language</keywords>
		</article>
		<article id="taln-2009-position-002" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Marianna</prenom>
					<nom>Apidianaki</nom>
					<email>mapidianaki@computing.dcu.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">National Centre for Language Technology, Dublin City University</affiliation>
			</affiliations>
			<titre>La place de la désambiguïsation lexicale dans la Traduction Automatique Statistique</titre>
			<type>position</type>
			<pages></pages>
			<resume>L’étape de la désambiguïsation lexicale est souvent esquivée dans les systèmes de Traduction Automatique Statistique (Statistical Machine Translation (SMT)) car considérée comme non nécessaire à la sélection de traductions correctes. Le débat autour de cette nécessité est actuellement assez vif. Dans cet article, nous présentons les principales positions sur le sujet. Nous analysons les avantages et les inconvénients de la conception actuelle de la désambiguïsation dans le cadre de la SMT, d’après laquelle les sens des mots correspondent à leurs traductions dans des corpus parallèles. Ensuite, nous présentons des arguments en faveur d’une analyse plus poussée des informations sémantiques induites à partir de corpus parallèles et nous expliquons comment les résultats d’une telle analyse pourraient être exploités pour une évaluation plus flexible et concluante de l’impact de la désambiguïsation dans la SMT.</resume>
			<mots_cles>Désambiguïsation lexicale, Traduction Automatique Statistique, sélection lexicale</mots_cles>
			<title></title>
			<abstract>Word Sense Disambiguation (WSD) is often omitted in Statistical Machine Translation (SMT) systems, as it is considered unnecessary for lexical selection. The discussion on the need ofWSD is currently very active. In this article we present the main positions on the subject. We analyze the advantages and weaknesses of the current conception of WSD in SMT, according to which the senses of ambiguous words correspond to their translations in a parallel corpus. Then we present some arguments towards a more thorough analysis of the semantic information induced from parallel corpora and we explain how the results of this analysis could be exploited for a more flexible and conclusive evaluation of the impact of WSD on SMT.</abstract>
			<keywords>Word Sense Disambiguation, Statistical Machine Translation, lexical selection</keywords>
		</article>
		<article id="taln-2009-position-003" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Marianne</prenom>
					<nom>Laurent</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ghislain</prenom>
					<nom>Putois</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Bretier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Moudenc</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, Lannion, 2 avenue Pierre Marzin, 22307 Lannion Cedex</affiliation>
			</affiliations>
			<titre>Nouveau paradigme d’évaluation des systèmes de dialogue homme-machine</titre>
			<type>position</type>
			<pages></pages>
			<resume>L’évaluation des systèmes de dialogue homme-machine est un problème difficile et pour lequel ni les objectifs ni les solutions proposées ne font aujourd’hui l’unanimité. Les approches ergonomiques traditionnelles soumettent le système de dialogue au regard critique de l’utilisateur et tente d’en capter l’expression, mais l’absence d’un cadre objectivable des usages de ces utilisateurs empêche une comparaison entre systèmes différents, ou entre évolutions d’un même système. Nous proposons d’inverser cette vision et de mesurer le comportement de l’utilisateur au regard du système de dialogue. Aussi, au lieu d’évaluer l’adéquation du système à ses utilisateurs, nous mesurons l’adéquation des utilisateurs au système. Ce changement de paradigme permet un changement de référentiel qui n’est plus les usages des utilisateurs mais le cadre du système. Puisque le système est complètement défini, ce paradigme permet des approches quantitatives et donc des évaluations comparatives de systèmes.</resume>
			<mots_cles>Évaluation, Dialogue</mots_cles>
			<title></title>
			<abstract>Evaluation of a human-machine dialogue system is a difficult problem for which neither the objectives nor the proposed solutions gather a unanimous support. Traditional approaches in the ergonomics field evaluate the system by describing how it fits the user in the user referential of practices. However, the user referential is even more complicated to formalise, and one cannot ground a common use context to enable the comparison of two systems, even if they are merely an evolution of the same service. We propose to shift the point of view on the evaluation problem : instead of evaluating the system in interaction with the user in the user’s referential, we will now measure the user’s adequacy to the system in the system referential. This is our Copernician revolution : for the evaluation purpose, our system is no longer user-centric, because the user referential is not properly objectifiable, while the system referential is completely known by design.</abstract>
			<keywords>Evaluation, Dialogue</keywords>
		</article>
		<article id="taln-2009-demo-001" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Francis</prenom>
					<nom>Brunet-Manquat</nom>
					<email>Francis.Brunet-Manquat@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jérôme</prenom>
					<nom>Goulian</nom>
					<email>Jerome.Goulian@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG-GETALP, BP 53 38041 Grenoble cedex 9, FRANCE</affiliation>
			</affiliations>
			<titre>ACOLAD un environnement pour l’édition de corpus de dépendances</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Dans cette démonstration, nous présentons le prototype d’un environnement open-source pour l’édition de corpus de dépendances. Cet environnement, nommé ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks).</resume>
			<mots_cles>dépendances, chunk, édition</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2009-demo-002" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Bouamor</nom>
					<email>Houda.Bouamor@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>Aurelien.Max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS et Université Paris-Sud 11 Orsay, France</affiliation>
			</affiliations>
			<titre>Amener des utilisateurs à créer et évaluer des paraphrases par le jeu</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une application sur le web pour l’acquisition de paraphrases phrastiques et sous-phrastiques sous forme de jeu. L’application permet l’acquisition à la fois de paraphrases et de jugements humains multiples sur ces paraphrases, ce qui constitue des données particulièrement utiles pour les applications du TAL basées sur les phénomènes paraphrastiques.</resume>
			<mots_cles>Paraphrase, acquisition de données, évaluation de données</mots_cles>
			<title></title>
			<abstract>In this article, we present a web application presented as a game for acquiring sentencial and phrasal paraphrases. It can be used both to acquire paraphrases and important quantities of human evaluations of their quality. These are particularly useful for NLP applications relying on paraphrasing.</abstract>
			<keywords>Paraphrasing, data acquisition, data evaluation</keywords>
		</article>
		<article id="taln-2009-demo-003" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Adrien</prenom>
					<nom>Lardilleux</nom>
					<email>Adrien.Lardilleux@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Lepage</nom>
					<email>Yves.Lepage@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, Université de Caen Basse-Normandie</affiliation>
			</affiliations>
			<titre>anymalign : un outil d’alignement sous-phrastique libre pour les êtres humains</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous présentons anymalign, un aligneur sous-phrastique grand public. Ses résultats ont une qualité qui rivalise avec le meilleur outil du domaine, GIZA++. Il est rapide et simple d’utilisation, et permet de produire dictionnaires et autres tables de traduction en une seule commande. À notre connaissance, c’est le seul outil au monde permettant d’aligner un nombre quelconque de langues simultanément. Il s’agit donc du premier aligneur sousphrastique réellement multilingue.</resume>
			<mots_cles>alignement sous-phrastique, multilinguisme, table de traduction</mots_cles>
			<title></title>
			<abstract>We present anymalign, a sub-sentential aligner oriented towards end users. It produces results that are competitive with the best known tool in the domain, GIZA++. It is fast and easy to use, and allows dictionaries or translation tables to be produced in a single command. To our knowledge, it is the only tool in the world capable of aligning any number of languages simultaneously. It is therefore the first truly multilingual sub-sentential aligner.</abstract>
			<keywords>sub-sentential alignment, multilinguism, translation table</keywords>
		</article>
		<article id="taln-2009-demo-004" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Jean</prenom>
					<nom>Charlet</nom>
					<email>Jean.Charlet@spim.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sylvie</prenom>
					<nom>Szulman</nom>
					<email>Sylvie.Szulman@lipn.univ-paris13.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Aussenac-Gilles</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Adeline</prenom>
					<nom>Nazarenko</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Hernandez</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nadia</prenom>
					<nom>Nadah</nom>
					<email></email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Sardet</nom>
					<email></email>
					<affiliationId>6</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean</prenom>
					<nom>Delahousse</nom>
					<email></email>
					<affiliationId>7</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Pierra</nom>
					<email></email>
					<affiliationId>6</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INSERM UMR_S 872, Eq. 20, Paris</affiliation>
				<affiliation affiliationId="2">Université Pierre et Marie Curie ; AP-HP, Paris</affiliation>
				<affiliation affiliationId="3">LIPN - UMR 7030, Université Paris 13 - CNRS</affiliation>
				<affiliation affiliationId="4">CNRS/IRIT et Université de Toulouse</affiliation>
				<affiliation affiliationId="5">Heudiasyc CNRS/UMR 6599, Université de Technologie de Compiègne</affiliation>
				<affiliation affiliationId="6">LISI-ENSMA et CRITT-Informatique, Poitiers</affiliation>
				<affiliation affiliationId="7">MONDECA, Paris</affiliation>
			</affiliations>
			<titre>Apport des outils de TAL à la construction d’ontologies : propositions au sein de la plateforme DaFOE</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>La construction d’ontologie à partir de textes fait l’objet d’études depuis plusieurs années dans le domaine de l’ingénierie des ontologies. Un cadre méthodologique en quatre étapes (constitution d’un corpus de documents, analyse linguistique du corpus, conceptualisation, opérationnalisation de l’ontologie) est commun à la plupart des méthodes de construction d’ontologies à partir de textes. S’il existe plusieurs plateformes de traitement automatique de la langue (TAL) permettant d’analyser automatiquement les corpus et de les annoter tant du point de vue syntaxique que statistique, il n’existe actuellement aucune procédure généralement acceptée, ni a fortiori aucun ensemble cohérent d’outils supports, permettant de concevoir de façon progressive, explicite et traçable une ontologie de domaine à partir d’un ensemble de ressources informationnelles relevant de ce domaine. Le but de ce court article est de présenter les propositions développées, au sein du projet ANR DaFOE 4app, pour favoriser l’émergence d’un tel ensemble d’outils.</resume>
			<mots_cles>Ontologie, construction d’ontologie, TALN</mots_cles>
			<title></title>
			<abstract>The concept of ontologies, appeared in the nineties, constitute a key point to represent and share the meaning carried out by formal symbols. Thus, the building of such an ontology is quite difficult. A way to do so is to use preexistent elements (textual corpus, taxonomies, norms or other ontologies) and operate them as a basis to define the ontology field. However, there is neither accepted process nor set of tools to progressively built ontologies from the available resources in a traceable and explicit way. We report in this paper several propositions developed within the framework of the ANR DaFOE4App project to support emergence of such tools.</abstract>
			<keywords>Ontology, Ontology building, NLP</keywords>
		</article>
		<article id="taln-2009-demo-005" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Davy</prenom>
					<nom>Weissenbacher</nom>
					<email>Davy.Weissenbacher@manchester.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Elisa</prenom>
					<nom>Pieri</nom>
					<email>Elisa.Pieri@manchester.ac.uk</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophia</prenom>
					<nom>Ananiadou</nom>
					<email>Sophia.Ananiadou@manchester.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brian</prenom>
					<nom>Rea</nom>
					<email>Brian.Rea@manchester.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Farida</prenom>
					<nom>Vis</nom>
					<email>Farida.Vis@manchester.ac.uk</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yuwei</prenom>
					<nom>Lin</nom>
					<email>Yuwei.Lin@manchester.ac.uk</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Rob</prenom>
					<nom>Procter</nom>
					<email>Rob.Procter@manchester.ac.uk</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Peter</prenom>
					<nom>Halfpenny</nom>
					<email>Peter.Halfpenny@manchester.ac.uk</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">National Centre for Text Mining, University of Manchester, 131 Princess Street, M1 7DN UK</affiliation>
				<affiliation affiliationId="2">National Centre for e-Social Science, University of Manchester, Oxford Rd, Manchester M13 9PL,UK</affiliation>
			</affiliations>
			<titre>ASSIST : un moteur de recherche spécialisé pour l’analyse des cadres d’expériences</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>L’analyse qualitative des données demande au sociologue un important travail de sélection et d’interprétation des documents. Afin de faciliter ce travail, cette communauté c’est dotée d’outils informatique mais leur fonctionnalités sont encore limitées. Le projet ASSIST est une étude exploratoire pour préciser les modules de traitement automatique des langues (TAL) permettant d’assister le sociologue dans son travail d’analyse. Nous présentons le moteur de recherche réalisé et nous justifions le choix des composants de TAL intégrés au prototype.</resume>
			<mots_cles>Recherche d’information, Extraction d’information, Terminologie</mots_cles>
			<title></title>
			<abstract>Qualitative data analysis requiers from the sociologist an important work of selection and interpretation of the documents. To facilitate this work, several software have been created but their functionalities are still limitated. The ASSIST project is a preliminary work to define the natural language processing modules for helping the sociologist. We present the search engine realised and justify the NLP modules integrated in the prototype.</abstract>
			<keywords>Information Retrieval, Information Extraction, Terminology</keywords>
		</article>
		<article id="taln-2009-demo-006" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Ivan</prenom>
					<nom>Šmilauer</nom>
					<email>smilauer@cetlef.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INALCO, LaLIC-CERTAL, 49bis avenue de la Belle Gabrielle, 75012 Paris</affiliation>
			</affiliations>
			<titre>CETLEF.fr - diagnostic automatique des erreurs de déclinaison tchèque dans un outil ELAO</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>CETLEF.fr – une application Web dynamique – propose des exercices de déclinaison tchèque avec un diagnostic automatique des erreurs. Le diagnostic a nécessité l'élaboration d'un modèle formel spécifique de la déclinaison contenant un classement des types paradigmatiques et des règles pour la réalisation des alternances morphématiques. Ce modèle est employé pour l'annotation des formes requises, nécessaire pour le diagnostic, mais également pour une présentation didactique sur la plateforme apprenant. Le diagnostic est effectué par comparaison d'une production erronée avec des formes hypothétiques générées à partir du radical de la forme requise et des différentes désinences casuelles. S'il existe une correspondance, l'erreur est interprétée d'après les différences dans les traits morphologiques de la forme requise et de la forme hypothétique. La majorité des erreurs commises peut être interprétée à l'aide de cette technique.</resume>
			<mots_cles>morphologie flexionnelle, déclinaison tchèque, acquisition d'une langue étrangère, diagnostic des erreurs et feedback, ELAO</mots_cles>
			<title></title>
			<abstract>CETLEF.fr – a dynamic Web application – contains fill-in-the-blank exercises on Czech declension with an automatic error diagnosis. The diagnosis rendered necessary the definition of a specific formal model of nominal inflection containing a classification of the paradigms and the rules for the realization of morphemic alternations. This model has been employed for the morphological annotation of required forms, necessary for the error diagnosis as well as for a didactic presentation on the learning platform. Diagnosis is carried out by the comparison of an erroneous production with hypothetical forms generated from the radical of the required form and various haphazard endings. If a correspondence is found, the error is interpreted according to the differences in the morphological features of the required form and the hypothetical form. The majority of errors can be interpreted with the aid of this technique.</abstract>
			<keywords>inflectional morphology, Czech declension, second language acquisition, error diagnosis and feedback, CALL</keywords>
		</article>
		<article id="taln-2009-demo-007" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Georges</prenom>
					<nom>Fafiotte</nom>
					<email>georges.fafiotte@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Achille</prenom>
					<nom>Falaise</nom>
					<email>achille.falaise@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jérôme</prenom>
					<nom>Goulian</nom>
					<email>jerome.goulian@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG-GETALP, UJF Grenoble 1 BP 53, 38041 – GRENOBLE cedex 9</affiliation>
			</affiliations>
			<titre>CIFLI-SurviTra, deux facettes : démonstrateur de composants de TA fondée sur UNL, et phrasebook multilingue</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>CIFLI-SurviTra ("Survival Translation" assistant) est une plate-forme destinée à favoriser l'ingénierie et la mise au point de composants UNL de TA, à partir d'une mémoire de traduction formée de livres de phrases multilingues avec variables lexicales. SurviTra est aussi un phrasebook digital multilingue, assistant linguistique pour voyageurs monolingues (français, hindi, tamoul, anglais) en situation de "survie linguistique". Le corpus d’un domaine-pilote ("Restaurant") a été structuré et construit : sous-domaines de phrases alignées et classes lexicales de locutions quadrilingues, graphes UNL, dictionnaires UW++/français et UW++/hindi par domaines. L’approche, générique, est applicable à d’autres langues. Le prototype d’assistant linguistique (application Web, à interface textuelle) peut évoluer vers une application UNL embarquée sur SmartPhone, avec Traitement de Parole et multimodalité.</resume>
			<mots_cles>TA via UNL, démonstrateur de composants UNL, assistant linguistique sur le Web, phrasebook digital multilingue, mémoire de traduction, collecte collaborative de corpus</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2009-demo-008" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Stefanos</prenom>
					<nom>Petrakis</nom>
					<email>petrakis@ifi.uzh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Manfred</prenom>
					<nom>Klenner</nom>
					<email>klenner@ifi.uzh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Étienne</prenom>
					<nom>Ailloud</nom>
					<email>ailloud@ifi.uzh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Angela</prenom>
					<nom>Fahrni</nom>
					<email>angela.fahrni@swissonline.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institute of Computational Linguistics, University of Zurich, Switzerland</affiliation>
			</affiliations>
			<titre>Composition multilingue de sentiments</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous présentons ici PolArt, un outil multilingue pour l’analyse de sentiments qui aborde la composition des sentiments en appliquant des transducteurs en cascade. La compositionnalité est assurée au moyen de polarités préalables extraites d’un lexique et des règles de composition appliquées de manière incrémentielle.</resume>
			<mots_cles>Analyse de sentiments</mots_cles>
			<title></title>
			<abstract>We introduce PolArt, a multilingual tool for sentiment detection that copes with sentiment composition through the application of cascaded transducers. Compositionality is enabled by prior polarities taken from a polarity lexicon and the compositional rules applied incrementally.</abstract>
			<keywords>Sentiment Detection</keywords>
		</article>
		<article id="taln-2009-demo-009" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Motasem</prenom>
					<nom>Alrahabi</nom>
					<email>motasem.alrahabi@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Pierre</prenom>
					<nom>Desclés</nom>
					<email>jean-pierre.descles@paris-sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LaLIC – Université Paris-Sorbonne</affiliation>
			</affiliations>
			<titre>EXCOM : Plate-forme d'annotation sémantique de textes multilingues</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous proposons une plateforme d‟annotation sémantique, appelée « EXCOM ». Basée sur la méthode de l‟ « Exploration Contextuelle », elle permet, à travers une diversité de langues, de procéder à des annotations automatiques de segments textuels par l'analyse des formes de surface dans leur contexte. Les textes sont traités selon des « points de vue » discursifs dont les valeurs sont organisées dans une « carte sémantique ». L‟annotation se base sur un ensemble de règles linguistiques, écrites par un analyste, qui permettent d‟identifier les représentations textuelles sous-jacentes aux différentes catégories de la carte. Le système offre, à travers deux types d‟interfaces (développeur ou utilisateur), une chaîne de traitements automatiques de textes qui comprend la segmentation, l‟annotation et d‟autres fonctionnalités de post-traitement. Les documents annotés peuvent être utilisés, par exemple, pour des systèmes de recherche d‟information, de veille, de classification ou de résumé automatique.</resume>
			<mots_cles>Plate-forme, Annotation automatique, Exploration Contextuelle, analyse sémantique, marqueurs discursifs, carte sémantique, multilinguisme</mots_cles>
			<title></title>
			<abstract>We propose a platform for semantic annotation, called “EXCOM”. Based on the “Contextual Exploration” method, it enables, across a great range of languages, to perform automatic annotations of textual segments by analyzing surface forms in their context. Texts are approached through discursive “points of view”, of which values are organized into a “semantic map”. The annotation is based on a set of linguistic rules, manually constructed by an analyst, and that enables to automatically identify the textual representations underlying the different semantic categories of the map. The system provides through two sorts of user-friendly interfaces (analyst or end-user) a complete pipeline of automatic text processing which consists of segmentation, annotation and other post-processing functionalities. Annotated documents can be used, for instance, for information retrieval systems, classification or automatic summarization.</abstract>
			<keywords>Platform, automatic annotation, Contextual Exploration, semantic analysis, discoursive markers, Semantic Map, multilingual approach</keywords>
		</article>
		<article id="taln-2009-demo-010" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>antoine.widlocher@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>yann.mathet@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire GREYC, CNRS UMR 6072, Université de Caen</affiliation>
			</affiliations>
			<titre>La plate-forme d’annotation Glozz</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles>Linguistique de corpus, Annotation, Plate-forme logicielle</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords>Corpus Linguistics, Annotation, Software Framework</keywords>
		</article>
		<article id="taln-2009-demo-011" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Blin</prenom>
					<nom>Raoul</nom>
					<email>blin@ehess.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CRLAO - CNRS , 54 Raspail 75006 Paris</affiliation>
			</affiliations>
			<titre>SAGACE-v3.3 ; Analyseur de corpus pour langues non flexionnelles</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous présentons la dernière version du logiciel SAGACE, analyseur de corpus pour langues faiblement flexionnelles (par exemple japonais ou chinois). Ce logiciel est distribué avec un lexique où les catégories sont exprimées à l'aide de systèmes de traits.</resume>
			<mots_cles>corpus, lexique, analyseur, japonais, chinois</mots_cles>
			<title></title>
			<abstract>We present a software program named SAGACE, designed to search for and extract word strings from a large corpus. It has been conceived for poor flexional languages, such as Japanese or Chinese. It is associated with a lexicon where categories are expressed with feature systems.</abstract>
			<keywords>corpus, lexicon, analyzer, japanese, chinese</keywords>
		</article>
		<article id="taln-2009-demo-012" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Hernandez</nom>
					<email>Nicolas.Hernandez@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabien</prenom>
					<nom>Poulard</nom>
					<email>Fabien.Poulard@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stergos</prenom>
					<nom>Afantenos</nom>
					<email>Stergos.Afantenos@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Matthieu</prenom>
					<nom>Vernier</nom>
					<email>Matthieu.Vernier@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jérôme</prenom>
					<nom>Rocheteau</nom>
					<email>Jerome.Rocheteau@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA (CNRS - UMR 6241), 2 rue de la Houssinière – B.P. 92208, 44322 NANTES Cedex 3</affiliation>
			</affiliations>
			<titre>Apache UIMA pour le Traitement Automatique des Langues</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>L’objectif de la démonstration est d’une part de faire un retour d’expérience sur la solution logicielle Apache UIMA comme infrastructure de développement d’applications distribuées de TAL, et d’autre part de présenter les développements réalisés par l’équipe TALN du LINA pour permettre à la communauté de s’approprier ce « framework ».</resume>
			<mots_cles>Apache UIMA, Applications du TAL, Infrastructure logicielle</mots_cles>
			<title></title>
			<abstract>Our objectives are twofold : First, based on some common use cases, we will discuss the interest of using UIMA as a middleware solution for developing Natural Language Processing systems. Second, we will present various preprocessing tools we have developed in order to facilitate the access to the framework for the French community.</abstract>
			<keywords>Apache UIMA, NLP applications, Middleware</keywords>
		</article>
		<article id="taln-2009-demo-013" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Jérôme</prenom>
					<nom>Lehuen</nom>
					<email>Jerome.Lehuen@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Lemeunier</nom>
					<email>Thierry.Lemeunier@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIUM - Université du Maine, Avenue Laënnec, 72085 Le Mans Cedex 9</affiliation>
			</affiliations>
			<titre>Un Analyseur Sémantique pour le DHM</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2009-demo-014" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Jacques</prenom>
					<nom>Vergne</nom>
					<email>Jacques.Vergne@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC – Université de Caen, campus 2 - BP 5186, F-14032 CAEN cedex</affiliation>
			</affiliations>
			<titre>Un chunker multilingue endogène</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Le chunking consiste à segmenter un texte en chunks, segments sous-phrastiques qu'Abney a défini approximativement comme des groupes accentuels. Traditionnellement, le chunking utilise des ressources monolingues, le plus souvent exhaustives, quelquefois partielles : des mots grammaticaux et des ponctuations, qui marquent souvent des débuts et fins de chunk. Mais cette méthode, si l'on veut l'étendre à de nombreuses langues, nécessite de multiplier les ressources monolingues. Nous présentons une nouvelle méthode : le chunking endogène, qui n'utilise aucune ressource hormis le texte analysé lui-même. Cette méthode prolonge les travaux de Zipf : la minimisation de l'effort de communication conduit les locuteurs à raccourcir les mots fréquents. On peut alors caractériser un chunk comme étant la période des fonctions périodiques correllées longueur et effectif des mots sur l'axe syntagmatique. Cette méthode originale présente l'avantage de s'appliquer à un grand nombre de langues d'écriture alphabétique, avec le même algorithme, sans aucune ressource.</resume>
			<mots_cles>chunking, multilingue, endogène, longueur des mots, effectif des mots</mots_cles>
			<title></title>
			<abstract>Chunking is segmenting a text into chunks, sub-sentential segments, that Abney approximately defined as stress groups. Chunking usually uses monolingual resources, most often exhaustive, sometimes partial : function words and punctuations, which often mark beginnings and ends of chunks. But, to extend this method to other languages, monolingual resources have to be multiplied. We present a new method : endogenous chunking, which uses no other resource than the text to be parsed itself. The idea of this method comes from Zipf : to make the least communication effort, speakers are driven to shorten frequent words. A chunk then can be characterised as the period of the periodic correlated functions length and frequency of words on the syntagmatic axis. This original method takes its advantage to be applied to a great number of languages of alphabetic script, with the same algorithm, without any resource.</abstract>
			<keywords>chunking, multilingual, endogenous, word length, word frequency</keywords>
		</article>
		<article id="taln-2009-court-001" session="Posters 1">
			<auteurs>
				<auteur>
					<prenom>Djamé</prenom>
					<nom>Seddah</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie</prenom>
					<nom>Candito</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Crabbé</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LALIC et INRIA (Alpage), Université Paris-Sorbonne</affiliation>
				<affiliation affiliationId="2">UFRL et INRIA (Alpage), Université Paris 7</affiliation>
			</affiliations>
			<titre>Adaptation de parsers statistiques lexicalisés pour le français : Une évaluation complète sur corpus arborés</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente les résultats d’une évaluation exhaustive des principaux analyseurs syntaxiques probabilistes dit “lexicalisés” initialement conçus pour l’anglais, adaptés pour le français et évalués sur le CORPUS ARBORÉ DU FRANÇAIS (Abeillé et al., 2003) et le MODIFIED FRENCH TREEBANK (Schluter &amp; van Genabith, 2007). Confirmant les résultats de (Crabbé &amp; Candito, 2008), nous montrons que les modèles lexicalisés, à travers les modèles de Charniak (Charniak, 2000), ceux de Collins (Collins, 1999) et le modèle des TIG Stochastiques (Chiang, 2000), présentent des performances moindres face à un analyseur PCFG à Annotation Latente (Petrov et al., 2006). De plus, nous montrons que le choix d’un jeu d’annotations issus de tel ou tel treebank oriente fortement les résultats d’évaluations tant en constituance qu’en dépendance non typée. Comparés à (Schluter &amp; van Genabith, 2008; Arun &amp; Keller, 2005), tous nos résultats sont state-of-the-art et infirment l’hypothèse d’une difficulté particulière qu’aurait le français en terme d’analyse syntaxique probabiliste et de sources de données.</resume>
			<mots_cles>Analyse syntaxique probabiliste, corpus arborés, évaluation, analyse du français</mots_cles>
			<title></title>
			<abstract>This paper presents complete investigation results on the statistical parsing of French by bringing a complete evaluation on French data of the main based probabilistic lexicalized (Charniak, Collins, Chiang) and unlexicalized (Berkeley) parsers designed first on the Penn Treebank. We adapted the parsers on the two existing treebanks of French (Abeillé et al., 2003; Schluter &amp; van Genabith, 2007). To our knowledge, all the results reported here are state-of-the-art for the constituent parsing of French on every available treebank and invalidate the hypothesis of French being particularly difficult to parse. Regarding the algorithms, the comparisons show that lexicalized parsing models are outperformed by the unlexicalized Berkeley parser. Regarding the treebanks, we observe that a tag set with specific features has direct influences over evaluation results depending on the parsing model.</abstract>
			<keywords>Probabilistic parsing, treebanks, evaluation, French parsing</keywords>
		</article>
		<article id="taln-2009-court-002" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Fiammetta</prenom>
					<nom>Namer</nom>
					<email>fiammetta.namer@univ-nancy2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR 7118 « ATILF » - Nancy Université</affiliation>
			</affiliations>
			<titre>Analyse automatique des noms déverbaux composés : pourquoi et comment faire interagir analogie et système de règles</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article aborde deux problèmes d’analyse morpho-sémantique du lexique : (1) attribuer automatiquement une définition à des noms et verbes morphologiquement construits inconnus des dictionnaires mais présents dans les textes ; (2) proposer une analyse combinant règles et analogie, deux techniques généralement contradictoires. Les noms analysés sont apparemment suffixés et composés (HYDROMASSAGE). La plupart d’entre eux, massivement attestés dans les documents (journaux, Internet) sont absents des dictionnaires. Ils sont souvent reliés à des verbes (HYDROMASSER) également néologiques. Le nombre de ces noms et verbes est estimé à 5.400. L’analyse proposée leur attribue une définition par rapport à leur base, et enrichit un lexique de référence pour le TALN au moyen de cette base, si elle est néologique. L’implémentation des contraintes linguistiques qui régissent ces formations est reproductible dans d’autres langues européennes où sont rencontrés les mêmes types de données dont l’analyse reflète le même raisonnement que pour le français.</resume>
			<mots_cles>Analyse morphologique, Annotation sémantique, Composition savante, Noms déverbaux, Règles, Analogie</mots_cles>
			<title></title>
			<abstract>This paper addresses two morpho-semantic parsing issues: (1) to automatically provide morphologically complex unknown nouns and verbs with a definition; (2) to propose a methodology combining both rules and analogy, which are techniques usually seen as inconsistent with eachother. The analysed nouns look like both suffixed and compounded (HYDROMASSAGE). Most of them are not stored in dictionaries, although they are very frequent in newspapers or online documents. They are often related to verbs (HYDROMASSER), also lacking from dictionaries. The estimated amount of these nouns and verbs is 5,400. The proposed analysis assigns them a definition calculated according to their base meaning, and it increases the existing reference lexicon content with this base, from the moment that it is a new-coined form. The implementation of linguistic constraints which govern this word formations is reproducible in other West-European languages, where the same data type is found, subject to the same kind of analysis.</abstract>
			<keywords>Morphological parsing, Semantic annotation, Neo-classical compounds, Deverbal nouns, Rules, Analogy</keywords>
		</article>
		<article id="taln-2009-court-003" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Jonathan</prenom>
					<nom>Marchand</nom>
					<email>Jonathan.Marchand@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email>Bruno.Guillaume@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Perrier</nom>
					<email>Guy.Perrier@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA / Université Nancy 2</affiliation>
				<affiliation affiliationId="2">LORIA / INRIA Nancy Grand-Est</affiliation>
			</affiliations>
			<titre>Analyse en dépendances à l’aide des grammaires d’interaction</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article propose une méthode pour extraire une analyse en dépendances d’un énoncé à partir de son analyse en constituants avec les grammaires d’interaction. Les grammaires d’interaction sont un formalisme grammatical qui exprime l’interaction entre les mots à l’aide d’un système de polarités. Le mécanisme de composition syntaxique est régi par la saturation des polarités. Les interactions s’effectuent entre les constituants, mais les grammaires étant lexicalisées, ces interactions peuvent se traduire sur les mots. La saturation des polarités lors de l’analyse syntaxique d’un énoncé permet d’extraire des relations de dépendances entre les mots, chaque dépendance étant réalisée par une saturation. Les structures de dépendances ainsi obtenues peuvent être vues comme un raffinement de l’analyse habituellement effectuée sous forme d’arbre de dépendance. Plus généralement, ce travail apporte un éclairage nouveau sur les liens entre analyse en constituants et analyse en dépendances.</resume>
			<mots_cles>Analyse syntaxique, grammaires de dépendances, grammaires d’interaction, polarité</mots_cles>
			<title></title>
			<abstract>This article proposes a method to extract dependency structures from phrasestructure level parsing with Interaction Grammars. Interaction Grammars are a formalism which expresses interactions among words using a polarity system. Syntactical composition is led by the saturation of polarities. Interactions take place between constituents, but as grammars are lexicalized, these interactions can be translated at the level of words. Dependency relations are extracted from the parsing process : every dependency is the consequence of a polarity saturation. The dependency relations we obtain can be seen as a refinement of the usual dependency tree. Generally speaking, this work sheds new light on links between phrase structure and dependency parsing.</abstract>
			<keywords>Syntactic analysis, dependency grammars, interaction grammars, polarity</keywords>
		</article>
		<article id="taln-2009-court-004" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Prost</nom>
					<email>JPProst@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIFO, Université d’Orléans</affiliation>
			</affiliations>
			<titre>Analyse relâchée à base de contraintes</titre>
			<type>court</type>
			<pages></pages>
			<resume>La question de la grammaticalité, et celle duale de l’agrammaticalité, sont des sujets délicats à aborder, dès lors que l’on souhaite intégrer différents degrés, tant de grammaticalité que d’agrammaticalité. En termes d’analyse automatique, les problèmes posés sont de l’ordre de la représentation des connaissances, du traitement, et bien évidement de l’évaluation. Dans cet article, nous nous concentrons sur l’aspect traitement, et nous nous penchons sur la question de l’analyse d’énoncés agrammaticaux. Nous explorons la possibilité de fournir une analyse la plus complète possible pour un énoncé agrammatical, sans l’apport d’information complémentaire telle que par le biais de mal-règles ou autre grammaire d’erreurs. Nous proposons une solution algorithmique qui permet l’analyse automatique d’un énoncé agrammatical, sur la seule base d’une grammaire modèle-théorique de bonne formation. Cet analyseur est prouvé générer une solution optimale, selon un critère numérique maximisé.</resume>
			<mots_cles>grammaticalité, analyse syntaxique, contraintes, syntaxe modèle-théorique</mots_cles>
			<title></title>
			<abstract>The question of grammaticality, and the dual one of ungrammaticality, are topics delicate to address when interested in modeling different degrees, whether of grammaticality or ungrammaticality. As far as parsing is concerned, the problems are with regard to knowledge representation, processing, and obviously evaluation. In this paper, we concentrate on the processing aspect and we address the question of parsing ungrammatical utterances. We explore the possibility to provide a full parse for an ungrammatical utterance without relying on any kind of additional information, which would be provided by mal-rules or other error grammar. We propose an algorithmic solution in order to parse an ungrammatical utterance using only a model-theoretic grammar of well-formedness. The parser is proven to generate an optimal solution, according to a maximised criterion.</abstract>
			<keywords>grammaticality, syntactic parsing, constraints, Model-Theoretic Syntax</keywords>
		</article>
		<article id="taln-2009-court-005" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Marie-Paule</prenom>
					<nom>Péry-Woodley</nom>
					<email>pery@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicholas</prenom>
					<nom>Asher</nom>
					<email>asher@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Enjalbert</nom>
					<email>patrice.enjalbert@info.unicaen.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Farah</prenom>
					<nom>Benamara</nom>
					<email>benamara@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Myriam</prenom>
					<nom>Bras</nom>
					<email>bras@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cécile</prenom>
					<nom>Fabre</nom>
					<email>cecile.fabre@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Ferrari</nom>
					<email>stephane.ferrari@info.unicaen.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lydia-Mai</prenom>
					<nom>Ho-Dac</nom>
					<email>hodac@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Le Draoulec</nom>
					<email>draoulec@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>mathet@info.unicaen.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Muller</nom>
					<email>philippe.muller@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Prévot</nom>
					<email>laurent.prevot@lpl-aix.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Josette</prenom>
					<nom>Rebeyrolle</nom>
					<email>rebeyrol@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ludovic</prenom>
					<nom>Tanguy</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marianne</prenom>
					<nom>Vergez-Couret</nom>
					<email>vergez@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laure</prenom>
					<nom>Vieu</nom>
					<email>vieu@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>awidloch@info.unicaen.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS – Université de Toulouse UTM</affiliation>
				<affiliation affiliationId="2">IRIT – Université de Toulouse UPS</affiliation>
				<affiliation affiliationId="3">GREYC – Université de Caen</affiliation>
				<affiliation affiliationId="4">Laboratoire Parole et Langage – Université de Provence</affiliation>
			</affiliations>
			<titre>ANNODIS: une approche outillée de l'annotation de structures discursives</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le projet ANNODIS vise la construction d’un corpus de textes annotés au niveau discursif ainsi que le développement d'outils pour l’annotation et l’exploitation de corpus. Les annotations adoptent deux points de vue complémentaires : une perspective ascendante part d'unités de discours minimales pour construire des structures complexes via un jeu de relations de discours ; une perspective descendante aborde le texte dans son entier et se base sur des indices pré-identifiés pour détecter des structures discursives de haut niveau. La construction du corpus est associée à la création de deux interfaces : la première assiste l'annotation manuelle des relations et structures discursives en permettant une visualisation du marquage issu des prétraitements ; une seconde sera destinée à l'exploitation des annotations. Nous présentons les modèles et protocoles d'annotation élaborés pour mettre en oeuvre, au travers de l'interface dédiée, la campagne d'annotation.</resume>
			<mots_cles>annotation de corpus, structures de discours, interface d'annotation</mots_cles>
			<title></title>
			<abstract>The ANNODIS project has two interconnected objectives: to produce a corpus of texts annotated at discourse-level, and to develop tools for corpus annotation and exploitation. Two sets of annotations are proposed, representing two complementary perspectives on discourse organisation: a bottom-up approach starting from minimal discourse units and building complex structures via a set of discourse relations; a top-down approach envisaging the text as a whole and using pre-identified cues to detect discourse macro-structures. The construction of the corpus goes hand in hand with the development of two interfaces: the first one supports manual annotation of discourse structures, and allows different views of the texts using NLPM-based pre-processing; another interface will support the exploitation of the annotations. We present the discourse models and annotation protocols, and the interface which embodies them.</abstract>
			<keywords>corpus annotation, discourse structures, annotation tools</keywords>
		</article>
		<article id="taln-2009-court-006" session="Posters 3">
			<auteurs>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Moriceau</nom>
					<email>moriceau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xavier</prenom>
					<nom>Tannier</nom>
					<email>xtannier@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris-Sud 11 / LIMSI-CNRS BP 133, 91403 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre>Apport de la syntaxe dans un système de question-réponse : étude du système FIDJI.</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente une série d’évaluations visant à étudier l’apport d’une analyse syntaxique robuste des questions et des documents dans un système de questions-réponses. Ces évaluations ont été effectuées sur le système FIDJI, qui utilise à la fois des informations syntaxiques et des techniques plus “traditionnelles”. La sélection des documents, l’extraction de la réponse ainsi que le comportement selon les différents types de questions ont été étudiés.</resume>
			<mots_cles>Systèmes de questions-réponses, analyse syntaxique, évaluation</mots_cles>
			<title></title>
			<abstract>This paper presents some experiments aiming at estimating the contribution of a syntactic parser on both questions and documents in a question-answering system. This evaluation has been performed with the system FIDJI, which makes use of both syntactic information and more “traditional” techniques. Document selection, answer extraction as well as system behaviour on different types of questions have been experimented.</abstract>
			<keywords>Question-answering, syntactic analysis, evaluation</keywords>
		</article>
		<article id="taln-2009-court-007" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Dominique</prenom>
					<nom>Laurent</nom>
					<email>dlaurent@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Nègre</nom>
					<email>sophie.negre@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Séguéla</nom>
					<email>patrick.seguela@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Synapse Développement</affiliation>
			</affiliations>
			<titre>Apport des cooccurrences à la correction et à l'analyse syntaxique</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le correcteur grammatical Cordial utilise depuis de nombreuses années les cooccurrences pour la désambiguïsation sémantique. Un dictionnaire de cooccurrences ayant été constitué pour les utilisateurs du logiciel de correction et d'aides à la rédaction, la grande richesse de ce dictionnaire a incité à l'utiliser intensivement pour la correction, spécialement des homonymes et paronymes. Les résultats obtenus sont spectaculaires sur ces types d'erreurs mais la prise en compte des cooccurrences a également été utilisée avec profit pour la pure correction orthographique et pour le rattachement des groupes en analyse syntaxique.</resume>
			<mots_cles>cooccurrences, collocations, correction grammaticale</mots_cles>
			<title></title>
			<abstract>For many years, the spellchecker named Cordial has been using cooccurrences for semantic disambiguation. Since a dictionary of co-occurrences had been established for users of the spellchecker and of the writing aid, the richness of this dictionary led us to use it intensively for the correction, especially for homonyms and paronyms. The results are impressive on this kind of errors but taking into account the cooccurrences proved to be very profitable for pure spellchecking and for the attachment of groups in syntactic parsing.</abstract>
			<keywords>collocation, grammar checking</keywords>
		</article>
		<article id="taln-2009-court-008" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Daoud</prenom>
					<nom>Daoud</nom>
					<email>daoud@batelco.jo</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohammad</prenom>
					<nom>Daoud</nom>
					<email>Mohammad.Daoud@imag.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Princess Sumaya University for Technology, P.O.Box 1438 Al-Jubaiha, 11941 Jordan</affiliation>
				<affiliation affiliationId="2">Laboratoire LIG - Université Joseph Fourier 85, rue de la Bibliothèque,, 38041 Grenoble, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Arabic Disambiguation Using Dependency Grammar</title>
			<abstract>In this paper, we present a new approach to disambiguation Arabic using a joint rule-based model which is conceptualized using Dependency Grammar. This approach helps in highly accurate analysis of sentences. The analysis produces a semantic net like structure expressed by means of Universal Networking Language (UNL) - a recently proposed interlingua. Extremely varied and complex phenomena of Arabic language have been addressed.</abstract>
			<keywords>Dependency Grammar, Arabic Language, Disambiguation, EnCo, UNL</keywords>
		</article>
		<article id="taln-2009-court-009" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Claude</prenom>
					<nom>De Loupy</nom>
					<email>loupy@syllabs.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michaël</prenom>
					<nom>Bagur</nom>
					<email>bagur@syllabs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Helena</prenom>
					<nom>Blancafort</nom>
					<email>blancafort@syllabs.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Syllabs – 15, rue Jean Baptiste Berlier, 75013 Paris</affiliation>
				<affiliation affiliationId="2">MoDyCo – Université Paris 10, 200 Av. de la République, Nanterre</affiliation>
				<affiliation affiliationId="3">Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona</affiliation>
			</affiliations>
			<titre>Association automatique de lemmes et de paradigmes de flexion à un mot inconnu</titre>
			<type>court</type>
			<pages></pages>
			<resume>La maintenance et l’enrichissement des lexiques morphosyntaxiques sont souvent des tâches fastidieuses. Dans cet article nous présentons la mise en place d’une procédure de guessing de flexion afin d’aider les linguistes dans leur travail de lexicographes. Le guesser développé ne fait pas qu’évaluer l’étiquette morphosyntaxique comme c’est généralement le cas. Il propose pour un mot français inconnu, un ou plusieurs candidats-lemmes, ainsi que les paradigmes de flexion associés (formes fléchies et étiquettes morphosyntaxiques). Dans cet article, nous décrivons le modèle probabiliste utilisé ainsi que les résultats obtenus. La méthode utilisée permet de réduire considérablement le nombre de règles à valider, permettant ainsi un gain de temps important.</resume>
			<mots_cles>guesser, lexiques morphosyntaxiques, aide aux linguistes, induction des règles de flexion</mots_cles>
			<title></title>
			<abstract>Lexicon maintenance and lexicon enrichment is a labour-intensive task. In this paper, we present preliminary work on an inflectional guessing procedure for helping the linguist in lexicographic tasks. The guesser presented here does not only output morphosyntactic tags, but also suggests for an unknown French word one or more lemma candidates as well as their corresponding inflectional rules and morphosyntactic tags that the linguist has to validate. In this article, we present the probabilistic model we used as well as obtained results. The method allows a drastic reduction of the number of rules to validate.</abstract>
			<keywords>guesser, morphosyntactic lexica, aide to the linguist, induction of inflection rules</keywords>
		</article>
		<article id="taln-2009-court-010" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Matthieu</prenom>
					<nom>Vernier</nom>
					<email>Matthieu.Vernier@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laura</prenom>
					<nom>Monceaux</nom>
					<email>Laura.Monceaux@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Daille</nom>
					<email>Beatrice.Daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Estelle</prenom>
					<nom>Dubreil</nom>
					<email>Estelle.Dubreil@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA / CNRS UMR 6241, Université de Nantes</affiliation>
			</affiliations>
			<titre>Catégorisation sémantico-discursive des évaluations exprimées dans la blogosphère</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les blogs constituent un support d’observations idéal pour des applications liées à la fouille d’opinion. Toutefois, ils imposent de nouvelles problématiques et de nouveaux défis au regard des méthodes traditionnelles du domaine. De ce fait, nous proposons une méthode automatique pour la détection et la catégorisation des évaluations localement exprimées dans un corpus de blogs multi-domaine. Celle-ci rend compte des spécificités du langage évaluatif décrites dans deux théories linguistiques. L’outil développé au sein de la plateforme UIMA vise d’une part à construire automatiquement une grammaire du langage évaluatif, et d’autre part à utiliser cette grammaire pour la détection et la catégorisation des passages évaluatifs d’un texte. La catégorisation traite en particulier l’aspect axiologique de l’évaluation, sa configuration d’énonciation et sa modalité dans le discours.</resume>
			<mots_cles>fouille d’opinion, langage évaluatif, catégorisation des évaluations</mots_cles>
			<title></title>
			<abstract>Blogs are an ideal observation for applications related to the opinion mining task. However, they impose new problems and new challenges in this field. Therefore, we propose a method for automatic detection and classification of appraisal locally expressed in a multi-domain blogs corpus. It reflects the specific aspects of appraisal language described in two linguistic theories. The tool developed within the UIMA platform aims both to automatically build a grammar of the appraisal language, and the other part to use this grammar for the detection and categorization of evaluative segments in a text. Categorization especially deals with axiological aspect of an evaluative segments, enunciative configuration and its attitude in discourse.</abstract>
			<keywords>opinion mining, appraisal language, appraisal classification</keywords>
		</article>
		<article id="taln-2009-court-011" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Stéphanie</prenom>
					<nom>Weiser</nom>
					<email>sweiser@u-paris10.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Martin</prenom>
					<nom>Coste</nom>
					<email>martin.coste@mondeca.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Florence</prenom>
					<nom>Amardeilh</nom>
					<email>florence.amardeilh@mondeca.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">MoDyCo – CNRS, Université Paris Ouest Nanterre La Défense – 200, avenue de la République, 92001 Nanterre</affiliation>
				<affiliation affiliationId="2">Mondeca – 3, cité Nollez, 75018 Paris</affiliation>
			</affiliations>
			<titre>Chaîne de traitement linguistique : du repérage d'expressions temporelles au peuplement d'une ontologie de tourisme</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente la chaîne de traitement linguistique réalisée pour la mise en place d'une plateforme touristique sur Internet. Les premières étapes de cette chaîne sont le repérage et l'annotation des expressions temporelles présentes dans des pages Web. Ces deux tâches sont effectuées à l'aide de patrons linguistiques. Elles soulèvent de nombreux questionnements auxquels nous tentons de répondre, notamment au sujet de la définition des informations à extraire, du format d'annotation et des contraintes. L'étape suivante consiste en l'exploitation des données annotées pour le peuplement d'une ontologie du tourisme. Nous présentons les règles d'acquisition nécessaires pour alimenter la base de connaissance du projet. Enfin, nous exposons une évaluation du système d'annotation. Cette évaluation permet de juger aussi bien le repérage des expressions temporelles que leur annotation.</resume>
			<mots_cles>Annotation, expressions temporelles, ontologies, base de connaissance, tourisme</mots_cles>
			<title></title>
			<abstract>This paper presents the linguistic data processing sequence built for a tourism web portal. The first steps of this sequence are the detection and the annotation of the temporal expressions found in the web pages. These tasks are performed using linguistic patterns. They lead to many questions which we try to answer, such as the definition of information to detect, annotation format and constraints. In the next step this annotated data is used to populate a tourism ontology. We present the acquisition rules which are necessary to enrich the portal knowledge base. Then we present an evaluation of our annotation system. This evaluation is able to judge the detection of the temporal expressions and their annotation.</abstract>
			<keywords>Annotation, temporal expressions, ontologies, knowledge base, tourism</keywords>
		</article>
		<article id="taln-2009-court-012" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Yue</prenom>
					<nom>Ma</nom>
					<email>Yue.Ma@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Audibert</nom>
					<email>Laurent.Audibert@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de l’université Paris-Nord (LIPN) - UMR 7030, Université Paris 13 - CNRS, 99, avenue Jean-Baptiste Clément - F-93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Détection des contradictions dans les annotations sémantiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>L’annotation sémantique a pour objectif d’apporter au texte une représentation explicite de son interprétation sémantique. Dans un précédent article, nous avons proposé d’étendre les ontologies par des règles d’annotation sémantique. Ces règles sont utilisées pour l’annotation sémantique d’un texte au regard d’une ontologie dans le cadre d’une plate-forme d’annotation linguistique automatique. Nous présentons dans cet article une mesure, basée sur la valeur de Shapley, permettant d’identifier les règles qui sont sources de contradiction dans l’annotation sémantique. Par rapport aux classiques mesures de précision et de rappel, l’intérêt de cette mesure est de ne pas nécessiter de corpus manuellement annoté, d’être entièrement automatisable et de permettre l’identification des règles qui posent problème.</resume>
			<mots_cles>Annotation sémantique, valeur de Shapley, plate-forme d’annotation</mots_cles>
			<title></title>
			<abstract>The semantic annotation has the objective to bring to a text an explicit representation of its semantic interpretation. In a preceding article, we suggested extending ontologies by semantic annotation rules. These rules are used for the semantic annotation of a text with respect to an ontology within the framework of an automated linguistic annotation platform. We present in this article a measure, based on the Shapley value, allowing to identify the rules which are sources of contradictions in the semantic annotation. With regard to the classic measures, precision and recall, the interest of this measure is without the requirement of manually annotated corpus, completely automated and its ability to identify rules which raise problems.</abstract>
			<keywords>Semantic annotation, Shapley value, annotation platform</keywords>
		</article>
		<article id="taln-2009-court-013" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Marc</prenom>
					<nom>Le Tallec</nom>
					<email>Marc.letallec@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jeanne</prenom>
					<nom>Villaneau</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Yves</prenom>
					<nom>Antoine</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Agata</prenom>
					<nom>Savary</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Arielle</prenom>
					<nom>Syssau-Vaccarella</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais Tours – LI</affiliation>
				<affiliation affiliationId="2">Université Européenne de Bretagne – VALORIA</affiliation>
				<affiliation affiliationId="3">Université Montpellier 3</affiliation>
			</affiliations>
			<titre>Détection des émotions à partir du contenu linguistique d’énoncés oraux : application à un robot compagnon pour enfants fragilisés</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le projet ANR Emotirob aborde la question de la détection des émotions sous un cadre original : concevoir un robot compagnon émotionnel pour enfants fragilisés. Notre approche consiste à combiner détection linguistique et prosodie. Nos expériences montrent qu'un sujet humain peut estimer de manière fiable la valence émotionnelle d'un énoncé à partir de son contenu propositionnel. Nous avons donc développé un premier modèle de détection linguistique qui repose sur le principe de compositionnalité des émotions : les mots simples ont une valence émotionnelle donnée et les prédicats modifient la valence de leurs arguments. Après une description succincte du système logique de compréhension dont les sorties sont utilisées pour le calcul global de l'émotion, cet article présente la construction d'une norme émotionnelle lexicale de référence, ainsi que d'une ontologie de classes émotionnelles de prédicats, pour des enfants de 5 et 7 ans.</resume>
			<mots_cles>Emotion, valence émotionnelle, norme lexicale émotionnelle, robot compagnon, compréhension de parole</mots_cles>
			<title></title>
			<abstract>Project ANR Emotirob aims at detecting emotions from an original point of view: realizing an emotional companion robot for weakened children. In our approach, linguistic detection and prosodie are combined. Our experiments show that human beings can estimate the emotional value of an utterance from its propositional content in a reliable way. So we have implemented a first model of linguistic detection, based on the principle that emotions can be compound: lexical words have an emotional value while predicates can modify emotional values of their arguments. This paper presents a short description of the logical understanding system, the outputs of which are used for the final emotional value calculus. Then, the creation of a lexical emotional reference standard is presented with an ontology of emotional predicate classes for children, aged between 5 and 7.</abstract>
			<keywords>Emotion, Emotional valency, Emotional lexical standard, companion robot, spoken language understanding</keywords>
		</article>
		<article id="taln-2009-court-014" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Nuria</prenom>
					<nom>Gala</nom>
					<email>nuria.gala@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Rey</nom>
					<email>veronique.rey@univ-provence.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Tichit</nom>
					<email>tichit@iml.univ-mrs.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF, Marseille, CNRS UMR 6166</affiliation>
				<affiliation affiliationId="2">SHADYC, Marseille, CNRS et EHESS, UMR 8562</affiliation>
				<affiliation affiliationId="3">IML, Marseille, CNRS UMR 6206</affiliation>
			</affiliations>
			<titre>Dispersion sémantique dans des familles morpho-phonologiques : éléments théoriques et empiriques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Traditionnellement, la morphologie lexicale a été diachronique et a permis de proposer le concept de famille de mots. Ce dernier est repris dans les études en synchronie et repose sur une forte cohérence sémantique entre les mots d’une même famille. Dans cet article, nous proposons une approche en synchronie fondée sur la notion de continuité à la fois phonologique et sémantique. Nous nous intéressons, d’une part, à la morpho-phonologie et, d’autre part, à la dispersion sémantique des mots dans les familles. Une première étude (Gala &amp; Rey, 2008) montrait que les familles de mots obtenues présentaient des espaces sémantiques soit de grande cohésion soit de grande dispersion. Afin de valider ces observations, nous présentons ici une méthode empirique qui permet de pondérer automatiquement les unités de sens d’un mot et d’une famille. Une expérience menée auprès de 30 locuteurs natifs valide notre approche et ouvre la voie pour une étude approfondie du lexique sur ces bases phonologiques et sémantiques.</resume>
			<mots_cles>morpho-phonologie lexicale, traitement automatique des familles dérivationnelles, espaces sémantiques</mots_cles>
			<title></title>
			<abstract>Traditionally, lexical morphology has been diachronic and has established the notion of word families. This notion is reused in synchronic studies and implies strong semantic coherence within the words of a same family. In this paper, we propose an approach in synchrony which highlights phonological and semantic continuity. Our interests go on morphophonology and on the semantic dispersion of words in a family. A first study (Gala &amp; Rey, 2008) showed that the semantic spaces of the families displayed either a strong semantic cohesion or a strong dispersion. In order to validate this observation, we present here a corpus-based method that automatically weights the semantic units of a word and a family. An experience carried out with 30 native speakers validates our approach and allows us to foresee a thorough study of the lexicon based on phonological and semantic basis.</abstract>
			<keywords>lexicalmorpho-phonology, derivational families processing, semantic spaces</keywords>
		</article>
		<article id="taln-2009-court-015" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Kévin</prenom>
					<nom>Séjourné</nom>
					<email>kevin.sejourne@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Paris Sud XI, Limsi/CNRS</affiliation>
			</affiliations>
			<titre>Exploitation d’une structure pour les questions enchaînées</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses (SQR) utilisant des questions enchaînées. La recherche des documents dans un SQR est perturbée par l’absence des éléments utiles à la recherche dans les questions liées, éléments figurant dans les échanges précédents. Les récentes campagnes d’évaluation montrent que ce problème est sous-estimé, et n’a pas fait l’objet de technique dédiée. Afin d’améliorer la recherche des documents dans un SQR nous utilisons une méthode récente d’organisation des informations liées aux interactions entre questions. Celle-ci se base sur l’exploitation d’une structure de données adaptée à la transmission des informations des questions liées jusqu’au moteur d’interrogation. Le moteur d’interrogation doit alors être adapté afin de tirer partie de cette structure de données.</resume>
			<mots_cles>Question réponse enchaînée</mots_cles>
			<title></title>
			<abstract>We present works realized in the field of the questions answering (QA) using chained questions. The documents search in QA system is disrupted because useful elements are missing for search using bound questions. Recents evaluation campaigns show this problem as underestimated, and this problem wasn’t solve by specific techniques. To improve documents search in a QA we use a recent information organization method for bound questions to the interactions between questions. This methode is bases on the operation of a special data structure. This data structure transmit informations from bound questions to the interrogation engine. Then the interrogation engine must be improve to take advantage of this data structure.</abstract>
			<keywords>chained question answering</keywords>
		</article>
		<article id="taln-2009-court-016" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Denis</nom>
					<email>alexandre.denis@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Matthieu</prenom>
					<nom>Quignard</nom>
					<email>matthieu.quignard@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR 7503 LORIA/CNRS – Campus scientifique, 56 506 Vandoeuvre-lès-Nancy Cedex</affiliation>
			</affiliations>
			<titre>Exploitation du terrain commun pour la production d’expressions référentielles dans les systèmes de dialogue</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente un moyen de contraindre la production d’expressions référentielles par un système de dialogue en fonction du terrain commun. Cette capacité, fondamentale pour atteindre la compréhension mutuelle, est trop souvent oubliée dans les systèmes de dialogue. Le modèle que nous proposons s’appuie sur une modélisation du processus d’ancrage (grounding process) en proposant un raffinement du statut d’ancrage appliqué à la description des référents. Il décrit quand et comment ce statut doit être révisé en fonction des jugements de compréhension des deux participants ainsi que son influence dans le choix d’une description partagée destinée à la génération d’une expression référentielle.</resume>
			<mots_cles>Compréhension mutuelle, processus d’ancrage, référence, génération</mots_cles>
			<title></title>
			<abstract>This paper presents a way to constraint the production of referring expressions by a dialogue system according to the common ground. This ability – fundamental for reaching mutual understanding – is often neglected in dialogue system design. The proposed model is based on a view of the grounding process and offering a refinement of the grounding status concerning the referent description. It explains how and when this status should be revised with respect to how participants evaluate their understanding and how this status may help to choose a shared description with which a referring expression can be generated.</abstract>
			<keywords>Mutual understanding, grounding process, referring expression, generation</keywords>
		</article>
		<article id="taln-2009-court-017" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Younès</prenom>
					<nom>Bahou</nom>
					<email>bahou_younes@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Amine</prenom>
					<nom>Bayoudhi</nom>
					<email>bayoudhi.amine@gmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lamia</prenom>
					<nom>Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LARIS-MIRACL, FSEGS – Université de Sfax, Tunisie.</affiliation>
			</affiliations>
			<titre>Gestion de dialogue oral Homme-machine en arabe</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans le présent papier, nous présentons nos travaux sur la gestion du dialogue oral arabe Homme-machine. Ces travaux entrent dans le cadre de la réalisation du serveur vocal interactif SARF (Bahou et al., 2008) offrant des renseignements sur le transport ferroviaire tunisien en langue arabe standard moderne. Le gestionnaire de dialogue que nous proposons est basé sur une approche structurelle et est composé de deux modèles à savoir, le modèle de tâche et le modèle de dialogue. Le premier modèle permet de i) compléter et vérifier l’incohérence des structures sémantiques représentant les sens utiles des énoncés, ii) générer une requête vers l’application et iii) récupérer le résultat et de formuler une réponse à l’utilisateur en langage naturel. Quant au modèle de dialogue, il assure l’avancement du dialogue avec l’utilisateur et l’identification de ses intentions. L’interaction entre ces deux modèles est assurée grâce à un contexte du dialogue permettant le suivi et la mise à jour de l’historique du dialogue.</resume>
			<mots_cles>gestion du dialogue Homme-machine, dialogue oral arabe, modèle de tâche, modèle de dialogue</mots_cles>
			<title></title>
			<abstract>In this paper, we present our research work on Human-machine Arabic oral dialogue management. This work enters in the context of SARF system (Bahou et al., 2008) an interactive vocal server that provides information on Tunisian railway using modern standard Arabic. The dialogue manager that we propose is based on a structural approach and consists of two models namely, the task model and the dialogue model. The first model is used to i) complete and verify the incoherence of semantic structures representing the useful meaning of utterances, ii) generate a query to the application and iii) get back the results and formulate an answer to the user in natural language. As for the dialogue model, it assures the dialogue progress with the user and the identification of her or his intentions. The interaction between these two models is assured by a dialogue context that allows monitoring and updating the dialogue history.</abstract>
			<keywords>Human-machine dialogue management, Arabic oral dialogue, task model, dialogue model</keywords>
		</article>
		<article id="taln-2009-court-018" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Lionel</prenom>
					<nom>Clément</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kim</prenom>
					<nom>Gerdes</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Renaud</prenom>
					<nom>Marlet</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Bordeaux 1, LaBRI</affiliation>
				<affiliation affiliationId="2">ILPGA, LPP, Sorbonne Nouvelle</affiliation>
				<affiliation affiliationId="3">INRIA, LaBRI</affiliation>
			</affiliations>
			<titre>Grammaires d’erreur – correction grammaticale avec analyse profonde et proposition de corrections minimales</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous présentons un système de correction grammatical ouvert, basé sur des analyses syntaxiques profondes. La spécification grammaticale est une grammaire hors-contexte équipée de structures de traits plates. Après une analyse en forêt partagée où les contraintes d’accord de traits sont relâchées, la détection d’erreur minimise globalement les corrections à effectuer et des phrases alternatives correctes sont automatiquement proposées.</resume>
			<mots_cles>Correcteur grammatical, analyse syntaxique, forêt partagée</mots_cles>
			<title></title>
			<abstract>We present an open system for grammar checking, based on deep parsing. The grammatical specification is a contex-free grammar with flat feature structures. After a sharedforest analysis where feature agreement constraints are relaxed, error detection globally minimizes the number of fixes and alternate correct sentences are automatically proposed.</abstract>
			<keywords>Grammar checker, parsing, shared forest</keywords>
		</article>
		<article id="taln-2009-court-019" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>André</prenom>
					<nom>Bittar</nom>
					<email>andre.bittar@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>laurence.danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, INRIA Rocquencourt, 78150 Le Chesnay, Université Paris Diderot, 75013 Paris</affiliation>
			</affiliations>
			<titre>Intégration des constructions à verbe support dans TimeML</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le langage TimeML a été conçu pour l’annotation des informations temporelles dans les textes, notamment les événements, les expressions de temps et les relations entre les deux. Des consignes d’annotation générales ont été élaborées afin de guider l’annotateur dans cette tâche, mais certains phénomènes linguistiques restent à traiter en détail. Un problème commun dans les tâches de TAL, que ce soit en traduction, en génération ou en compréhension, est celui de l’encodage des constructions à verbe support. Relativement peu d’attention a été portée, jusqu’à maintenant, sur ce problème dans le cadre du langage TimeML. Dans cet article, nous proposons des consignes d’annotation pour les constructions à verbe support.</resume>
			<mots_cles>TimeML, verbes support, discours, sémantique</mots_cles>
			<title></title>
			<abstract>TimeML is a markup language developed for the annotation of temporal information in texts, in particular events, temporal expressions and the relations which hold between the two. General annotation guidelines have been developed to guide the annotator in this task, but certain linguistic phenomena have yet to be dealt with in detail. A common problem in NLP tasks, whether in translation, generation or understanding, is that of the encoding of light verb constructions. Relatively little attention has been paid to this problem, until now, in the TimeML framework. In this article, we propose annotation guidelines for light verb constructions.</abstract>
			<keywords>TimeML, light verbs, discourse, semantics</keywords>
		</article>
		<article id="taln-2009-court-020" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Elsa</prenom>
					<nom>Tolone</nom>
					<email>elsa.tolone@univ-paris-est.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA / Paris 7, 30 rue du Ch. des rentiers, 75013 Paris, France</affiliation>
				<affiliation affiliationId="2">IGM, Université Paris-Est, 77454 Marne-la-Vallée Cedex, France</affiliation>
			</affiliations>
			<titre>Intégrer les tables du Lexique-Grammaire à un analyseur syntaxique robuste à grande échelle</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous montrons comment nous avons converti les tables du Lexique-Grammaire en un format TAL, celui du lexique Lefff, permettant ainsi son intégration dans l’analyseur syntaxique FRMG. Nous présentons les fondements linguistiques de ce processus de conversion et le lexique obtenu. Nous validons le lexique obtenu en évaluant l’analyseur syntaxique FRMG sur le corpus de référence de la campagne EASy selon qu’il utilise les entrées verbales du Lefff ou celles des tables des verbes du Lexique-Grammaire ainsi converties.</resume>
			<mots_cles>Lexiques syntaxiques, Lexique-Grammaire, analyse syntaxique</mots_cles>
			<title></title>
			<abstract>In this paper, we describe how we converted the lexicon-grammar tables into an NLP format, that of the Lefff lexicon, which allowed us to integrate it into the FRMG parser. We decribe the linguistic basis of this conversion process, and the resulting lexicon.We validate the resulting lexicon by evaluating the FRMG parser on the EASy reference corpus depending on the set of verbal entries it relies on, namely those of the Lefff or those of the converted lexicon-grammar verb tables.</abstract>
			<keywords>Syntactic lexica, Lexicon-Grammar, parsing</keywords>
		</article>
		<article id="taln-2009-court-021" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Cédric</prenom>
					<nom>Messiant</nom>
					<email>cedric.messiant@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Takuya</prenom>
					<nom>Nakamura</nom>
					<email>takuya.nakamura@univ-mlv.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stavroula</prenom>
					<nom>Voyatzi</nom>
					<email>voyatzi@univ-mlv.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de Paris-Nord, CNRS UMR 7030 et Université Paris 13, 99, avenue Jean-Baptiste Clément, F-93430 Villetaneuse France</affiliation>
				<affiliation affiliationId="2">Laboratoire d’Informatique Gaspard-Monge, CNRS UMR 8049 IGM-LabInfo et Université de Marne-la-Vallée, 5 Bd Descartes, Champs-sur-Marne, 77454 Marne-la-Vallée Cedex 2</affiliation>
			</affiliations>
			<titre>La complémentarité des approches manuelle et automatique en acquisition lexicale</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les ressources lexicales sont essentielles pour obtenir des systèmes de traitement des langues performants. Ces ressources peuvent être soit construites à la main, soit acquises automatiquement à partir de gros corpus. Dans cet article, nous montrons la complémentarité de ces deux approches. Pour ce faire, nous utilisons l’exemple de la sous-catégorisation verbale en comparant un lexique acquis par des méthodes automatiques (LexSchem) avec un lexique construit manuellement (Le Lexique-Grammaire). Nous montrons que les informations acquises par ces deux méthodes sont bien distinctes et qu’elles peuvent s’enrichir mutuellement.</resume>
			<mots_cles>verbe, syntaxe, lexique, sous-catégorisation</mots_cles>
			<title></title>
			<abstract>Lexical resources are essentially created to obtain efficient text-processing systems. These resources can be constructed either manually or automatically from large corpora. In this paper, we show the complementarity of these two types of approaches, comparing an automatically constructed lexicon (LexSchem) to a manually constructed one (Lexique-Grammaire), on examples of verbal subcategorization. The results show that the information retained by these two resources is in fact different and that they can be mutually enhanced.</abstract>
			<keywords>verb, syntax, lexicon, subcategorization</keywords>
		</article>
		<article id="taln-2009-court-022" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Dominique</prenom>
					<nom>Laurent</nom>
					<email>dlaurent@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Nègre</nom>
					<email>sophie.negre@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Séguéla</nom>
					<email>patrick.seguela@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Synapse Développement</affiliation>
			</affiliations>
			<titre>L'analyseur syntaxique Cordial dans Passage</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cordial est un analyseur syntaxique et sémantique développé par la société Synapse Développement. Largement utilisé par les laboratoires de TALN depuis plus de dix ans, cet analyseur participe à la campagne Passage ("Produire des Annotations Syntaxiques à Grande Échelle"). Comment fonctionne cet analyseur ? Quels résultats a-t-il obtenu lors de la première phase d'évaluation de cette campagne ? Au-delà de ces questions, cet article montre en quoi les contraintes industrielles façonnent les outils d'analyse automatique du langage naturel.</resume>
			<mots_cles>Analyse syntaxique, analyse sémantique, évaluation, Passage</mots_cles>
			<title></title>
			<abstract>Cordial is a syntactic and semantic parser developed by Synapse Développement. Widely used by the laboratories of NLP for over ten years, this analyzer is involved in the Passage campaign ("Producing Syntactic Annotations on a Large Scale"). How does this parser work? What were the results obtained during the first phase of the evaluation? Beyond these issues, this article shows how the industrial constraints condition the tools for Natural Language Procesing.</abstract>
			<keywords>Parsing, semantic analysis, evaluation</keywords>
		</article>
		<article id="taln-2009-court-023" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>antoine.widlocher@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>yann.mathet@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire GREYC, CNRS UMR 6072, Université de Caen</affiliation>
			</affiliations>
			<titre>La plate-forme Glozz : environnement d’annotation et d’exploration de corpus</titre>
			<type>court</type>
			<pages></pages>
			<resume>La nécessité d’une interaction systématique entre modèles, traitements et corpus impose la disponibilité d’annotations de référence auxquelles modèles et traitements pourront être confrontés. Or l’établissement de telles annotations requiert un cadre formel permettant la représentation d’objets linguistiques variés, et des applications permettant à l’annotateur de localiser sur corpus et de caractériser les occurrences des phénomènes observés. Si différents outils d’annotation ont vu le jour, ils demeurent souvent fortement liés à un modèle théorique et à des objets linguistiques particuliers, et ne permettent que marginalement d’explorer certaines structures plus récemment appréhendées expérimentalement, notamment à granularité élevée et en matière d’analyse du discours. La plate-forme Glozz répond à ces différentes contraintes et propose un environnement d’exploration de corpus et d’annotation fortement configurable et non limité a priori au contexte discursif dans lequel elle a initialement vu le jour.</resume>
			<mots_cles>Linguistique de corpus, Annotation, Plate-forme logicielle</mots_cles>
			<title></title>
			<abstract>The need for a systematic confrontation between models and corpora make it necessary to have - and consequently, to produce - reference annotations to which linguistic models could be compared. Creating such annotations requires both a formal framework which copes with various linguistic objects, and specific manual annotation tools, in order to make it possible to locate, identify and feature linguistic phenomena in texts. Though several annotation tools do already exist, they are mostly dedicated to a given theory and to a given set of structures. The Glozz platform, described in this paper, tries to address all of these needs, and provides a highly versatile corpus exploration and annotation framework.</abstract>
			<keywords>Corpus Linguistics, Annotation, Software Framework</keywords>
		</article>
		<article id="taln-2009-court-024" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Pierre-André</prenom>
					<nom>Buvet</nom>
					<email>pabuvet@ldi.univparis13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Cartier</nom>
					<email>ecartier@ldi.univparis13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Issac</nom>
					<email>fissac@ldi.univparis13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yassine</prenom>
					<nom>Madiouni</nom>
					<email>ymadiouni@ldi.univparis13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Mathieu-Colas</nom>
					<email>mmathieu-colas@ldi.univparis13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Salah</prenom>
					<nom>Mejri</nom>
					<email>smejri@ldi.univparis13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS LDI UMR 7187 – Université Paris 13, 99 avenue Jean-Baptiste Clément, 93430 Villetaneuse</affiliation>
			</affiliations>
			<titre>Morfetik, ressource lexicale pour le TAL</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le traitement automatique des langues exige un recensement lexical aussi rigoureux que possible. Dans ce but, nous avons développé un dictionnaire morphologique du français, conçu comme le point de départ d’un système modulaire (Morfetik) incluant un moteur de flexion, des interfaces de consultation et d’interrogation et des outils d’exploitation. Nous présentons dans cet article, après une brève description du dictionnaire de base (lexique des mots simples), quelques-uns des outils informatiques liés à cette ressource : un moteur de recherche des lemmes et des formes fléchies ; un moteur de flexion XML et MySQL ; des outils NLP permettant d’exploiter le dictionnaire ainsi généré ; nous présentons notamment un analyseur linguistique développé dans notre laboratoire. Nous comparons dans une dernière partie Morfetik avec d’autres ressources analogues du français : Morphalou, Lexique3 et le DELAF.</resume>
			<mots_cles>dictionnaire morphologique du français, CMLF, analyse linguistique des textes</mots_cles>
			<title></title>
			<abstract>Automatic language processing requires as rigorous a lexical inventory as possible. For this purpose, we have developed a morphological dictionary for French, conceived as the starting point of a modular system (Morfetik) which includes an inflection generator, user interfaces and operating tools. In this paper, we briefly describe the basic dictionary (lexicon of simple words) and detail some of the computing tools based on the dictionary. The computing tools built on this resource include: a lemma / inflected forms search engine; an XML and MySQL engine to build the inflected forms; the generated dictionary can then be used by various NLP Tools; in this article, we present the use of the dictionary in a linguistic analyser developed at the laboratory. Finally, we compare Morfetik to similar resources : Morphalou, Lexique3 and DELAF.</abstract>
			<keywords>French morphological dictionary, XML, CMLF, Linguistical analysis, Morfetik</keywords>
		</article>
		<article id="taln-2009-court-025" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Fabien</prenom>
					<nom>Poulard</nom>
					<email>Fabien.Poulard@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stergos</prenom>
					<nom>Afantenos</nom>
					<email>Stergos.Afantenos@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Hernandez</nom>
					<email>Nicolas.Hernandez@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA (CNRS - UMR 6241), 2 rue de la Houssinière – B.P. 92208, 44322 NANTES Cedex 3</affiliation>
			</affiliations>
			<titre>Nouvelles considérations pour la détection de réutilisation de texte</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article nous nous intéressons au problème de la détection de réutilisation de texte. Plus particulièrement, étant donné un document original et un ensemble de documents candidats — thématiquement similaires au premier — nous cherchons à classer ceux qui sont dérivés du document original et ceux qui ne le sont pas. Nous abordons le problème selon deux approches : dans la première, nous nous intéressons aux similarités discursives entre les documents, dans la seconde au recouvrement de n-grams hapax. Nous présentons le résultat d’expérimentations menées sur un corpus de presse francophone construit dans le cadre du projet ANR PIITHIE.</resume>
			<mots_cles>réutilisation de texte, recouvrement de n-grams hapax, similarités discursives, corpus journalistique francophone</mots_cles>
			<title></title>
			<abstract>In this article we are interested in the problem of text reuse. More specifically, given an original document and a set of candidate documents—which are thematically similar to the first one — we are interested in classifying them into those that have been derived from the original document and those that are not. We are approaching the problem in two ways : firstly we are interested in the discourse similarities between the documents, and secondly we are interested in the overlap of n-grams that are hapax. We are presenting the results of the experiments that we have performed on a corpus constituted from articles of the French press which has been created in the context of the PIITHIE project funded by the French National Agency for Research (Agence National de la Recherche, ANR).</abstract>
			<keywords>text reuse, hapax n-grams overlap, discourse similarities, french journalistic corpus</keywords>
		</article>
		<article id="taln-2009-court-026" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Garcia-Fernandez</nom>
					<email>annegf@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Rosset</nom>
					<email>vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS / Université Paris Sud 11 Orsay</affiliation>
			</affiliations>
			<titre>Collecte et analyses de réponses naturelles pour les systèmes de questions-réponses</titre>
			<type>court</type>
			<pages></pages>
			<resume>Notre travail se situe dans le cadre des systèmes de réponse a une question et à pour but de fournir une réponse en langue naturelle aux questions posées en langue naturelle. Cet article présente une expérience permettant d’analyser les réponses de locuteurs du français à des questions que nous leur posons. L’expérience se déroule à l’écrit comme à l’oral et propose à des locuteurs français des questions relevant de différents types sémantiques et syntaxiques. Nous mettons en valeur une large variabilité dans les formes de réponses possibles en langue française. D’autre part nous établissons un certain nombre de liens entre formulation de question et formulation de réponse. Nous proposons d’autre part une comparaison des réponses selon la modalité oral / écrit. Ces résultats peuvent être intégrés à des systèmes existants pour produire une réponse en langue naturelle de façon dynamique.</resume>
			<mots_cles>systèmes de réponse à une question, expérience, variations linguistiques, réponse en langue naturelle</mots_cles>
			<title></title>
			<abstract>Situated within the domain of interactive question-answering, our work is to increase the naturalness of natural language answers. This paper presents an experiment aiming at observing the formulation of answers by French speakers. The system asked simple questions to which the humans had to answer. Two modalities were used : text (web) and speech (phone). We present and analyze the collected corpus. Within the large variability of answer forms in French, we point some links between the answer form and the question form. Moreover we present a preliminary study on the observed variation between modalities. We expect these results to be integrable in existing systems to dynamically produce adpated natural language answers.</abstract>
			<keywords>question-answering systems, experimentation, linguistics variations, natural language answer</keywords>
		</article>
		<article id="taln-2009-court-027" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Claveau</nom>
					<email>IRISA-CNRS, Campus de Beaulieu, 35042 Rennes cedex</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">vincent.claveau@irisa.fr</affiliation>
			</affiliations>
			<titre>La /fOnetizasjc/ comme un problème de translittération</titre>
			<type>court</type>
			<pages></pages>
			<resume>La phonétisation est une étape essentielle pour le traitement de l’oral. Dans cet article, nous décrivons un système automatique de phonétisation de mots isolés qui est simple, portable et performant. Il repose sur une approche par apprentissage ; le système est donc construit à partir d’exemples de mots et de leur représentation phonétique. Nous utilisons pour cela une technique d’inférence de règles de réécriture initialement développée pour la translittération et la traduction. Pour évaluer les performances de notre approche, nous avons utilisé plusieurs jeux de données couvrant différentes langues et divers alphabets phonétiques, tirés du challenge Pascal Pronalsyl. Les très bons résultats obtenus égalent ou dépassent ceux des meilleurs systèmes de l’état de l’art.</resume>
			<mots_cles>Phonétisation, phonémisation, inférence de règles de réécriture, challenge Pronalsyl, conversion graphème-phonème, translittération</mots_cles>
			<title></title>
			<abstract>Phonetizing is a crucial step to process oral documents. In this paper, a new word-based phonetization approach is proposed ; it is automatic, simple, portable and efficient. It relies on machine learning ; thus, the system is built from examples of words with their phonetic representations. More precisely, it makes the most of a technique inferring rewriting rules initially developed for transliteration and translation. In order to evaluate the performances of this approach, we used several datasets from the Pronalsyl Pascal challenge, including different languages. The obtained results equal or outperform those of the best known systems.</abstract>
			<keywords>Phonetization, phonemization, inference of rewriting rule, Pronalsyl challenge, grapheme-phoneme conversion, transliteration</keywords>
		</article>
		<article id="taln-2009-court-028" session="Poster 1">
			<auteurs>
				<auteur>
					<prenom>Josep Maria</prenom>
					<nom>Crego</nom>
					<email>jmcrego@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>amax@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Orsay</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud 11, Orsay</affiliation>
			</affiliations>
			<titre>Plusieurs langues (bien choisies) valent mieux qu’une : traduction statistique multi-source par renforcement lexical</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les systèmes de traduction statistiques intègrent différents types de modèles dont les prédictions sont combinées, lors du décodage, afin de produire les meilleures traductions possibles. Traduire correctement des mots polysémiques, comme, par exemple, le mot avocat du français vers l’anglais (lawyer ou avocado), requiert l’utilisation de modèles supplémentaires, dont l’estimation et l’intégration s’avèrent complexes. Une alternative consiste à tirer parti de l’observation selon laquelle les ambiguïtés liées à la polysémie ne sont pas les mêmes selon les langues source considérées. Si l’on dispose, par exemple, d’une traduction vers l’espagnol dans laquelle avocat a été traduit par aguacate, alors la traduction de ce mot vers l’anglais n’est plus ambiguë. Ainsi, la connaissance d’une traduction français!espagnol permet de renforcer la sélection de la traduction avocado pour le système français!anglais. Dans cet article, nous proposons d’utiliser des documents en plusieurs langues pour renforcer les choix lexicaux effectués par un système de traduction automatique. En particulier, nous montrons une amélioration des performances sur plusieurs métriques lorsque les traductions auxiliaires utilisées sont obtenues manuellement.</resume>
			<mots_cles>Traduction automatique statistique, désambiguïsation lexicale, réévaluation de listes d’hypothèses</mots_cles>
			<title></title>
			<abstract>Statistical Machine Translation (SMT) systems integrate various models that exploit all available features during decoding to produce the best possible translation hypotheses. Correctly translating polysemous words, such as the French word avocat into English (lawyer or avocado) requires integrating complex models. Such translation lexical ambiguities, however, depend on the language pair considered. If one knows, for instance, that avocat was translated into Spanish as aguacate, then translating it into English is no longer ambiguous (avocado). Thus, in this example, the knowledge of the Spanish translation allows to reinforce the choice of the appropriate English word for the French!English system. In this article, we present an approach in which documents available in several languages are used to reinforce the lexical choices made by a SMT system. In particular, we show that gains can be obtained on several metrics when using auxiliary translations produced by human translators.</abstract>
			<keywords>Statistical Machine Translation, Word Sense Disambiguation, N-best list rescoring</keywords>
		</article>
		<article id="taln-2009-court-029" session="Posters 2">
			<auteurs>
				<auteur>
					<prenom>Jean-Léon</prenom>
					<nom>Bouraoui</nom>
					<email>bouraoui@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Boissière</nom>
					<email>boissier@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mustapha</prenom>
					<nom>Mojahid</nom>
					<email>mojahid@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nadine</prenom>
					<nom>Vigouroux</nom>
					<email>vigourou@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Lagarrigue</nom>
					<email>alagarri@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Vella</nom>
					<email>vella@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Luc</prenom>
					<nom>Nespoulous</nom>
					<email>nespoulo@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT (Institut de Recherche en Informatique de Toulouse), UMR CNRS 5505, Université Paul Sabatier, 118, Route de Narbonne, F-31062 Toulouse Cedex</affiliation>
				<affiliation affiliationId="2">OCTOGONE, (E.A 4156) / Laboratoire Jacques Lordat, Université de Toulouse II - Le Mirail Pavillon de la Recherche, 5, Allées Antonio-Machado, F-31058 Toulouse Cedex</affiliation>
			</affiliations>
			<titre>Problématique d'analyse et de modélisation des erreurs en production écrite. Approche interdisciplinaire</titre>
			<type>court</type>
			<pages></pages>
			<resume>L'objectif du travail présenté ici est la modélisation de la détection et la correction des erreurs orthographiques et dactylographiques, plus particulièrement dans le contexte des handicaps langagiers. Le travail est fondé sur une analyse fine des erreurs d’écriture commises. La première partie de cet article est consacrée à une description précise de la faute. Dans la seconde partie, nous analysons l’erreur (1) en déterminant la nature de la faute (typographique, orthographique, ou grammaticale) et (2) en explicitant sa conséquence sur le niveau de perturbation linguistique (phonologique, orthographique, morphologique ou syntaxique). Il résulte de ce travail un modèle général des erreurs (une grille) que nous présenterons, ainsi que les résultats statistiques correspondants. Enfin, nous montrerons sur des exemples, l’utilité de l’apport de cette grille, en soumettant ces types de fautes à quelques correcteurs. Nous envisageons également les implications informatiques de ce travail.</resume>
			<mots_cles>Typologie et analyse d’erreurs textuelles, assistance à la saisie de textes</mots_cles>
			<title></title>
			<abstract>The aim of our work is modeling the detection and the correction of spelling and typing errors, especially in the linguistic disabilities context. The work is based on a fine analysis of clerical errors committed. The first part of this article is devoted to a detailed description of error. In the second part, we analyze error in (1) determining the nature of the fault (typographical, spelling, or grammar) and (2) by explaining its consequences on the level of linguistic disturbance (phonological, orthographic, morphological and syntactic). The outcome of this work is a general model of errors (a grid) that we present, as well as the corresponding statistical results. Finally, we show on examples, the usefulness of this grid, by submitting these types of errors to a few spellcheckers. We also envisage the computer implications of this work.</abstract>
			<keywords>Typology and analyze of textual errors, writing assitance systems</keywords>
		</article>
		<article id="taln-2009-court-030" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Rémy</prenom>
					<nom>Kessler</nom>
					<email>remy.kessler@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Béchet</nom>
					<email>nicolas.bechet@lirmm.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Juan-Manuel</prenom>
					<nom>Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Roche</nom>
					<email>mathieu.roche@lirmm.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marc</prenom>
					<nom>El-Bèze</nom>
					<email>marc.elbeze@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
				<affiliation affiliationId="2">LIRMM - UMR 5506, CNRS - Université Montpellier 2 - France</affiliation>
			</affiliations>
			<titre>Profilage de candidatures assisté par Relevance Feedback</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le marché d’offres d’emploi et des candidatures sur Internet connaît une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de texte libre) qu’il n’est plus possible de traiter manuellement. Une analyse et catégorisation assistées nous semble pertinente en réponse à cette problématique. Nous proposons E-Gen, système qui a pour but l’analyse et catégorisation assistés d’offres d’emploi et des réponses des candidats. Dans cet article nous présentons plusieurs stratégies, reposant sur les modèles vectoriel et probabiliste, afin de résoudre la problématique du profilage des candidatures en fonction d’une offre précise. Nous avons évalué une palette de mesures de similarité afin d’effectuer un classement pertinent des candidatures au moyen des courbes ROC. L’utilisation d’une forme de relevance feedback a permis de surpasser nos résultats sur ce problème difficile et sujet à une grande subjectivité.</resume>
			<mots_cles>Classification, recherche d’information, Ressources humaines, modèle probabiliste, mesures de similarité, Relevance Feedback</mots_cles>
			<title></title>
			<abstract>The market of online job search sites has grown exponentially. This implies volumes of information (mostly in the form of free text) manually impossible to process. An analysis and assisted categorization seems relevant to address this issue. We present E-Gen, a system which aims to perform assisted analysis and categorization of job offers and the responses of candidates. This paper presents several strategies based on vectorial and probabilistic models to solve the problem of profiling applications according to a specific job offer. We have evaluated a range of measures of similarity to rank candidatures by using ROC curves. Relevance feedback approach allows surpass our previous results on this task, difficult and higly subjective.</abstract>
			<keywords>Classification,Information Retrieval, Human Ressources, Probabilistic Model, Similarity measure, Relevance Feedback</keywords>
		</article>
		<article id="taln-2009-court-031" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Hamon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN – UMR 7030, Université Paris 13 – CNRS, 99 av. J-B Clément, F-93430 Villetaneuse, France</affiliation>
				<affiliation affiliationId="2">Centre de Recherche des Cordeliers, Université Pierre et Marie Curie - Paris6, UMR_S 872, Paris, F-75006 France ; Université Paris Descartes, UMR_S 872, Paris, F-75006 France ; INSERM, U872, Paris, F-75006 France ; HEGP AP-HP</affiliation>
			</affiliations>
			<titre>Profilage sémantique endogène des relations de synonymie au sein de Gene Ontology</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le calcul de la similarité sémantique entre les termes repose sur l’existence et l’utilisation de ressources sémantiques. Cependant de telles ressources, qui proposent des équivalences entre entités, souvent des relations de synonymie, doivent elles-mêmes être d’abord analysées afin de définir des zones de fiabilité où la similarité sémantique est plus forte. Nous proposons une méthode d’acquisition de synonymes élémentaires grâce à l’exploitation des terminologies structurées au travers l’analyse de la structure syntaxique des termes complexes et de leur compositionnalité. Les synonymes acquis sont ensuite profilés grâce aux indicateurs endogènes inférés automatiquement à partir de ces mêmes terminologies (d’autres types de relations, inclusions lexicales, productivité, forme des composantes connexes). Dans le domaine biomédical, il existe de nombreuses terminologies structurées qui peuvent être exploitées pour la constitution de ressources sémantiques. Le travail présenté ici exploite une de ces terminologies, Gene Ontology.</resume>
			<mots_cles>Terminologie, distance sémantique, relations sémantiques, synonymie</mots_cles>
			<title></title>
			<abstract>Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be first analyzed and weighted in order to define within them the reliability zones where the semantic similarity shows to be stronger. We propose a method for acquisition of elementary synonyms which is based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositionality. The acquired synonyms are then profiled thanks to endogenous indicators (other types of relations, lexical inclusions, productivity, form of connected components), which are automatically inferred within the same terminologies. In the biomedical area, several structured terminologies have been built and can be exploited for the construction of semantic resources. The work we present in this paper, is applied to terms of one of these terminologies, i.e. the Gene Ontology.</abstract>
			<keywords>Terminology, semantic distance, semantic relations, synonymy</keywords>
		</article>
		<article id="taln-2009-court-032" session="Posters 2">
			<auteurs>
				<auteur>
					<prenom>Fériel</prenom>
					<nom>Ben Fraj</nom>
					<email>Feriel.BenFraj@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Chiraz</prenom>
					<nom>Ben Othmane Zribi</nom>
					<email>Chiraz.BenOthmane@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Ben Ahmed</nom>
					<email>Mohamed.BenAhmed@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire RIADI  Université La Manouba, ENSI, La Manouba, Tunisie</affiliation>
			</affiliations>
			<titre>Quels attributs discriminants pour une analyse syntaxique par classification de textes en langue arabe ?</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans le cadre dune approche déterministe et incrémentale danalyse syntaxique par classification de textes en langue arabe, nous avons prévu de prendre en considération un ensemble varié dattributs discriminants afin de mieux assister la procédure de classification dans ses prises de décisions à travers les différentes étapes danalyse. Ainsi, en plus des attributs morpho-syntaxiques du mot en cours danalyse et des informations contextuelles des mots lavoisinant, nous avons ajouté des informations compositionnelles extraites du fragment de larbre syntaxique déjà construit lors de létape précédente de lanalyse en cours. Ce papier présente notre approche danalyse syntaxique par classification et vise lexposition dune justification expérimentale de lapport de chaque type dattributs discriminants et spécialement ceux compositionnels dans ladite analyse syntaxique.</resume>
			<mots_cles>analyse syntaxique incrémentale, langue arabe, apprentissage automatique, classification, attributs discriminants</mots_cles>
			<title></title>
			<abstract>For parsing Arabic texts in a deterministic and incremental classification approach, we suggest that varying discriminative attributes is helpful in disambiguation between structures to classify. Thats why; we consider morpho-syntactic information of the current analyzed word and its surrounding context. In addition, we add a new information type: the compositional one. It consists of the portion of the syntactic tree already constructed until the previous analysis step. In this paper, we expose our parsing approach with classification basis and we justify the utility of the different discriminative attributes and especially the compositional ones.</abstract>
			<keywords>incremental parsing, Arabic language, machine learning, classification, discriminative attributes</keywords>
		</article>
		<article id="taln-2009-court-033" session="Posters 2">
			<auteurs>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Beaufort</nom>
					<email>richard.beaufort@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Dister</nom>
					<email>anne.dister@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hubert</prenom>
					<nom>Naets</nom>
					<email>hubert.naets@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kévin</prenom>
					<nom>Macé</nom>
					<email>kevin.mace@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cédrick</prenom>
					<nom>Fairon</nom>
					<email>cedrick.fairon@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL / Université Catholique de Louvain, 1 Place Blaise Pascal, B-1348 Louvain-la-Neuve, Belgique</affiliation>
			</affiliations>
			<titre>Recto /Verso Un système de conversion automatique ancienne / nouvelle orthographe à visée linguistique et didactique</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente Recto /Verso, un système de traitement automatique du langage dédié à l’application des rectifications orthographiques de 1990. Ce système a été développé dans le cadre de la campagne de sensibilisation réalisée en mars dernier par le Service et le Conseil de la langue française et de la politique linguistique de la Communauté française de Belgique. Nous commençons par rappeler les motivations et le contenu de la réforme proposée, et faisons le point sur les principes didactiques retenus dans le cadre de la campagne. La plus grande partie de l’article est ensuite consacrée à l’implémentation du système. Nous terminons enfin par une première analyse de l’impact de la campagne sur les utilisateurs.</resume>
			<mots_cles>Rectifications orthographiques de 1990, conversion ancienne / nouvelle orthographe, objectifs didactiques, machines à états finis</mots_cles>
			<title></title>
			<abstract>This paper presents Recto /Verso, a natural language processing system dedicated to the application of the 1990 French spelling rectifications. This system was developed for supporting the awareness-raising campaign promoted last March by the Superior council of the French language in Belgium. We first remind the motivations and the content of the reform, and we draw up the didactic principles followed during the campaign. The most important part of this paper is then focused on the system’s implementation.We finally end by a short analysis of the campaign’s impact on the users.</abstract>
			<keywords>1990 French spelling rectifications, ancient / new spelling conversion, didactic purposes, finite-state machines</keywords>
		</article>
		<article id="taln-2009-court-034" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Malaisé</nom>
					<email>vmalaise@few.vu.nl</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Luit</prenom>
					<nom>Gazendam</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Willemijn</prenom>
					<nom>Heeren</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Roeland</prenom>
					<nom>Ordelman</nom>
					<email></email>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hennie</prenom>
					<nom>Brugman</nom>
					<email></email>
					<affiliationId>5</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">VU University Amsterdam</affiliation>
				<affiliation affiliationId="2">Telematica Instituut, Enschede</affiliation>
				<affiliation affiliationId="3">University of Twente, Enschede</affiliation>
				<affiliation affiliationId="4">Netherlands Institute for Sound and Vision, Hilversum</affiliation>
				<affiliation affiliationId="5">MPI for Psycholinguistics, Nijmegen</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume>L’accès aux documents multimédia, dans une archive audiovisuelle, dépend en grande partie de la quantité et de la qualité des métadonnées attachées aux documents, notamment la description de leur contenu. Cependant, l’annotation manuelle des collections est astreignante pour le personnel. De nombreuses archives évoluent vers des méthodes d’annotation (semi-)automatiques pour la création et/ou l’amélioration des métadonnées. Le project CATCH-CHOICE, fondé par NWO, s’est penché sur l’extraction de mots clés à partir de resources textuelles liées aux programmes TV destinés à être archivés (péritextes), en collaboration avec les archives audiovisuelles néerlandaises, Sound and Vision. Cet article se penche sur la question de l’adéquation des transcriptions de Reconnaissance Automatique de la Parole développés dans le projet CATCH-CHoral pour la génération automatique de mots-clés : les mots-clés extraits de ces ressources sont évalués par rapport à des annotations manuelles et par rapport à des mots-clés générés à partir de péritextes décrivant les programmes télévisuels.</resume>
			<mots_cles>Extraction de mots clés, Reconnaissance Automatique de la Parole, Documents Audiovisuels</mots_cles>
			<title>Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs</title>
			<abstract>Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, the manual annotation of collections puts heavy demands on resources. A large number of archives are introducing (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords from textual resources related to TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. This paper investigates the suitability of Automatic Speech Recognition transcripts produced in the CATCH-CHoral project for generating such keywords, which we evaluate against manual annotations of the documents, and against keywords automatically generated from context documents describing the TV programs’ content.</abstract>
			<keywords>Keyword extraction, Automatic Speech Recognition, Audiovisual Documents</keywords>
		</article>
		<article id="taln-2009-court-035" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Florian</prenom>
					<nom>Boudin</nom>
					<email>florian.boudin@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Juan-Manuel</prenom>
					<nom>Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
				<affiliation affiliationId="2">École Polytechnique de Montréal, CP 6079 Succ. Centre-ville, Montréal</affiliation>
			</affiliations>
			<titre>Résumé automatique multi-document et indépendance de la langue : une première évaluation en français</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le résumé automatique de texte est une problématique difficile, fortement dépendante de la langue et qui peut nécessiter un ensemble de données d’apprentissage conséquent. L’approche par extraction peut aider à surmonter ces difficultés. (Mihalcea, 2004) a démontré l’intérêt des approches à base de graphes pour l’extraction de segments de texte importants. Dans cette étude, nous décrivons une approche indépendante de la langue pour la problématique du résumé automatique multi-documents. L’originalité de notre méthode repose sur l’utilisation d’une mesure de similarité permettant le rapprochement de segments morphologiquement proches. De plus, c’est à notre connaissance la première fois que l’évaluation d’une approche de résumé automatique multi-document est conduite sur des textes en français.</resume>
			<mots_cles>Résumé automatique de texte, Approches à base de graphes, Extraction d’information</mots_cles>
			<title></title>
			<abstract>Automatic text summarization is a difficult task, highly language-dependent and which may require a large training dataset. Recently, (Mihalcea, 2004) has shown that graph-based approaches applied to the sentence extraction issue can achieve good results. In this paper, we describe a language-independent approach for automatic multi-document text summarization. The main originality of our approach is the use of an hybrid similarity measure during the graph building process that can identify morphologically similar words. Moreover, this is as far as we know, the first time that the evaluation of a summarization approach is conducted on French documents.</abstract>
			<keywords>Text summarization, Graph-Based approaches, Information Extraction</keywords>
		</article>
		<article id="taln-2009-court-036" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Bozzi</nom>
					<email>Laurent.Bozzi@edf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Suignard</nom>
					<email>Philippe.Suignard@edf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claire</prenom>
					<nom>Waast-Richard</nom>
					<email>Claire.Waast-Richard@edf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">EDF R&amp;D, 1, avenue du Général de Gaulle, 92141 Clamart Cedex</affiliation>
			</affiliations>
			<titre>Segmentation et classification non supervisée de conversations téléphoniques automatiquement retranscrites</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cette étude porte sur l’analyse de conversations entre des clients et des téléconseillers d’EDF. Elle propose une chaîne de traitements permettant d’automatiser la détection des sujets abordés dans chaque conversation. L’aspect multi-thématique des conversations nous incite à trouver une unité de documents entre le simple tour de parole et la conversation entière. Cette démarche enchaîne une étape de segmentation de la conversation en thèmes homogènes basée sur la notion de cohésion lexicale, puis une étape de text-mining comportant une analyse linguistique enrichie d’un vocabulaire métier spécifique à EDF, et enfin une classification non supervisée des segments obtenus. Plusieurs algorithmes de segmentation ont été évalués sur un corpus de test, segmenté et annoté manuellement : le plus « proche » de la segmentation de référence est C99. Cette démarche, appliquée à la fois sur un corpus de conversations transcrites à la main, et sur les mêmes conversations décodées par un moteur de reconnaissance vocale, aboutit quasiment à l’obtention des 20 mêmes classes thématiques.</resume>
			<mots_cles>audio-mining, text mining, segmentation, classification, catégorisation, reconnaissance vocale, données textuelles, conversations téléphoniques, centre d’appel</mots_cles>
			<title></title>
			<abstract>This study focuses on the analysis of conversations and between clients and EDF agent. It offers a range of treatments designed to automate the detection of the topics covered in each conversation. As the conversations are multi-thematic we have to find a document unit, between the simple turn of speech and the whole conversation. The proposed approach starts with a step of segmentation of the conversation (based on lexical cohesion), and then a stage of text-mining, including a language enriched by a vocabulary specific to EDF, and finally a clustering of the segments. Several segmentation algorithms were tested on a test corpus, manually annotated and segmented : the "closest" to the reference segmentation is C99. This approach, applied to both a corpus of conversations transcribed manually, and on the same conversations decoded by a voice recognition engine, leads to almost obtain the same 200 clusters.</abstract>
			<keywords>audio-mining, text mining, segmentation, clustering, categorization, voice recognition, textual data, phone conversation, call center</keywords>
		</article>
		<article id="taln-2009-court-037" session="Poster 2">
			<auteurs>
				<auteur>
					<prenom>Sopheap</prenom>
					<nom>Seng</nom>
					<email>Sopheap.Seng@imag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email>Laurent.Besacier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Bigi</nom>
					<email>Brigitte.Bigi@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Castelli</nom>
					<email>Eric.Castelli@mica.edu.vn</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LIG/GETALP, Grenoble France</affiliation>
				<affiliation affiliationId="2">Laboratoire MICA, CNRS/UMI-2954, Hanoi Vietnam</affiliation>
			</affiliations>
			<titre>Segmentation multiple d’un flux de données textuelles pour la modélisation statistique du langage</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous traitons du problème de la modélisation statistique du langage pour les langues peu dotées et sans segmentation entre les mots. Tandis que le manque de données textuelles a un impact sur la performance des modèles, les erreurs introduites par la segmentation automatique peuvent rendre ces données encore moins exploitables. Pour exploiter au mieux les données textuelles, nous proposons une méthode qui effectue des segmentations multiples sur le corpus d’apprentissage au lieu d’une segmentation unique. Cette méthode basée sur les automates d’état finis permet de retrouver les n-grammes non trouvés par la segmentation unique et de générer des nouveaux n-grammes pour l’apprentissage de modèle du langage. L’application de cette approche pour l’apprentissage des modèles de langage pour les systèmes de reconnaissance automatique de la parole en langue khmère et vietnamienne s’est montrée plus performante que la méthode par segmentation unique, à base de règles.</resume>
			<mots_cles>segmentation multiple, langue non segmentée, modélisation statistique du langage</mots_cles>
			<title></title>
			<abstract>In this article we deal with the problem of statistical language modelling for under-resourced language with a writing system without word boundary delimiters. While the lack of text resources has an impact on the performance of language models, the errors produced by the word segmentation makes those data less usable. To better exploit the text resources, we propose a method to make multiples segmentations on the training corpus instead of a unique segmentation. This method based on finite state machine allows obtaining the n-grams not found by the unique segmentation and generate new n-grams. We use this approach to train the language models for automatic speech recognition systems of Khmer and Vietnamese languages and it proves better performance than the unique segmentation method.</abstract>
			<keywords>multiple segmentation, unsegmented language, statistical language modeling</keywords>
		</article>
		<article id="taln-2009-court-038" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alain</prenom>
					<nom>Joubert</nom>
					<email>joubert@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Riou</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM – Université Montpellier 2 - CNRS, Laboratoire d’Informatique, Robotique et Microélectronique de Montpellier, 161, rue Ada, F-34392 Montpellier cedex 5</affiliation>
			</affiliations>
			<titre>Sens et usages d’un terme dans un réseau lexical évolutif</titre>
			<type>court</type>
			<pages></pages>
			<resume>L’obtention d’informations lexicales fiables est un enjeu primordial en TALN, mais cette collecte peut s’avérer difficile. L’approche présentée ici vise à pallier les écueils de cette difficulté en faisant participer un grand nombre de personnes à un projet contributif via des jeux accessibles sur le web. Ainsi, les joueurs vont construire le réseau lexical, en fournissant de plusieurs manières possibles des associations de termes à partir d'un terme cible et d'une consigne correspondant à une relation typée. Le réseau lexical ainsi produit est de grande taille et comporte une trentaine de types de relations. A partir de cette ressource, nous abordons la question de la détermination des différents sens et usages d’un terme. Ceci est réalisé en analysant les relations entre ce terme et ses voisins immédiats dans le réseau et en calculant des cliques ou des quasi-cliques. Ceci nous amène naturellement à introduire la notion de similarité entre cliques, que nous interprétons comme une mesure de similarité entre ces différents sens et usages. Nous pouvons ainsi construire pour un terme son arbre des usages, qui est une structure de données exploitable en désambiguïsation de sens. Nous présentons quelques résultats obtenus en soulignant leur caractère évolutif.</resume>
			<mots_cles>Traitement Automatique du Langage Naturel, réseau lexical évolutif, relations typées pondérées, similarité entre sens et usages, arbre des usages</mots_cles>
			<title></title>
			<abstract>Obtaining reliable lexical information is an essential task in NLP, but it can prove a difficult task. The approach we present here aims at lessening the difficulty: it consists in having people take part in a collective project by offering them playful applications accessible on the web. The players themselves thus build the lexical network, by supplying (in various possible ways) associations between terms from a target term and an instruction concerning a typed relation. The lexical network thus obtained is large and it includes about thirty types of relations. From this network, we then discuss the question of meaning and word usage determination for a term, by searching relations between this term and its neighbours in the network and by computing cliques or quasicliques. This leads us to introduce the notion of similarity between cliques, which can be interpreted as a measure of similarity between these different meanings and word usages. We are thus able to build the tree of word usages for a term: it is a data structure that can be used to disambiguate meaning. Finally, we briefly present some of the results obtained, putting the emphasis on their evolutionary aspect.</abstract>
			<keywords>Natural Language Processing, evolutionary lexical network, typed and weighted relations, meaning and word usage similarity, tree of word usages</keywords>
		</article>
		<article id="taln-2009-court-039" session="Poster 2">
			<auteurs>
				<auteur>
					<prenom>Jean-Léon</prenom>
					<nom>Bouraoui</nom>
					<email>bouraoui@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nadine</prenom>
					<nom>Vigouroux</nom>
					<email>vigourou@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT – Paul Sabatier, 118, route de Narbonne, 31062 Toulouse, France</affiliation>
			</affiliations>
			<titre>Traitement automatique de disfluences dans un corpus linguistiquement contraint</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente un travail de modélisation et de détection des phénomènes de disfluence. Une des spécificité de ce travail est le cadre dans lequel il se situe: le contrôle de la navigation aérienne. Nous montrons ce que ce cadre particulier implique certains choix concernant la modélisation et l'implémentation. Ainsi, nous constatons que la modélisation fondée sur la syntaxe, souvent utilisée dans le traitement des langues naturelles, n'est pas la plus appropriée ici. Nous expliquons la façon dont l'implémentation a été réalisée. Dans une dernière partie, nous présentons la validation de ce dispositif, effectuée sur 400 énoncés.</resume>
			<mots_cles>Dialogue oral spontané, Analyse linguistique de corpus, Compréhension robuste, Contrôle Aérien, Phraséologie, Disfluences, Modèles de langage, Traitement Automatique du Langage Naturel</mots_cles>
			<title></title>
			<abstract>This article presents a work of modeling and detection of phenomena disfluences. One of the specificity of this work is its framework: the air traffic control. We show that this particular framework implies certain choices about modeling and implementation. Thus, we find that modeling based on the syntax, often used in natural language processing, is not the most appropriate here. We explain how the implementation has been completed. In a final section, we present the validation of this device, made of 400 utterances.</abstract>
			<keywords>Spontaneous speech dialog, corpus linguistic analysis, robust understanding, Air Traffic Control, phraseology, disfluencies, language models, Natural Language Processing</keywords>
		</article>
		<article id="taln-2009-court-040" session="Poster 2">
			<auteurs>
				<auteur>
					<prenom>Laura</prenom>
					<nom>Kallmeyer</nom>
					<email>lk@sfs.uni-tuebingen.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Wolfgang</prenom>
					<nom>Maier</nom>
					<email>wo.maier@uni-tuebingen.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yannick</prenom>
					<nom>Parmentier</nom>
					<email>parmenti@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">SFB 441 / Universität Tübingen, Nauklerstr. 35, D-72074 Tübingen</affiliation>
				<affiliation affiliationId="2">LORIA / Nancy Université, Campus Scientifique, BP 239, F-54506 Vandoeuvre-Lès-Nancy Cedex</affiliation>
			</affiliations>
			<titre>Un Algorithme d’Analyse de Type Earley pour Grammaires à Concaténation d’Intervalles</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous présentons ici différents algorithmes d’analyse pour grammaires à concaténation d’intervalles (Range Concatenation Grammar, RCG), dont un nouvel algorithme de type Earley, dans le paradigme de l’analyse déductive. Notre travail est motivé par l’intérêt porté récemment à ce type de grammaire, et comble un manque dans la littérature existante.</resume>
			<mots_cles>Analyse syntaxique déductive, grammaires à concaténation d’intervalles</mots_cles>
			<title></title>
			<abstract>We present several different parsing algorithms for Range Concatenation Grammar (RCG), inter alia an entirely novel Earley-style algorithm, using the deductive parsing framework. Our work is motivated by recent interest in range concatenation grammar in general and fills a gap in the existing literature.</abstract>
			<keywords>Deductive parsing, range concatenation grammar</keywords>
		</article>
		<article id="taln-2009-court-041" session="Démos">
			<auteurs>
				<auteur>
					<prenom>Jérôme</prenom>
					<nom>Lehuen</nom>
					<email>Jerome.Lehuen@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Lemeunier</nom>
					<email>Thierry.Lemeunier@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIUM - Université du Maine, Avenue Laënnec, 72085 Le Mans Cedex 9</affiliation>
			</affiliations>
			<titre>Un Analyseur Sémantique pour le DHM Modélisation – Réalisation – Évaluation</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article décrit un modèle de langage dédié au dialogue homme-machine, son implémentation en CLIPS, ainsi qu’une évaluation comparative. Notre problématique n’est ni d’analyser des grands corpus, ni de proposer une grammaire à grande couverture. Notre objectif est de produire des représentations sémantiques utilisables par un module de dialogue à partir d’énoncés oraux courts, le plus souvent agrammaticaux. Une démarche pragmatique nous a conduit à fonder l’analyse sur des principes simples mais efficaces dans le cadre que nous nous sommes fixé. L’algorithme retenu s’inspire de l’analyse tabulaire. L’évaluation que nous présentons repose sur le corpus MEDIA qui a fait l’objet d’une annotation sémantique manuelle de référence pour une campagne d’évaluation d’analyseurs sémantiques pour le dialogue. Les résultats que nous obtenons place notre analyseur dans le trio de tête des systèmes évalués lors de la campagne de juin 2005, et nous confortent dans nos choix d’algorithme et de représentation des connaissances.</resume>
			<mots_cles>Analyse sémantique tabulaire, contexte dialogique, évaluation</mots_cles>
			<title></title>
			<abstract>This article describes a natural language model dedicated to man-machine dialogue, its implementation in CLIPS, as well as a comparative evaluation. Our problematic is neither to analyze large corpora nor to propose a large-coverage grammar. Our objective is to produce semantic representations usable for a dialog module from short oral utterances that are rather often ungrammatical. A pragmatic approach leads us to base parsing on simple but efficient principles within the man-machine dialog framework. Chart parsing influences the algorithm we have chosen. The evaluation that we present here uses the MEDIA corpora. It has been manually annotated and represents a standard usable in an evaluation campaign for semantic parsers dedicated to the dialog. With the results that we obtain our parser is in the three bests of the systems evaluated in the June 2005 campaign. It confirms our choices of algorithm and of knowledge representation.</abstract>
			<keywords>Semantic chart parsing, dialogue context, evaluation</keywords>
		</article>
		<article id="taln-2009-court-042" session="Poster 2">
			<auteurs>
				<auteur>
					<prenom>Silvia</prenom>
					<nom>Fernández Sabido</nom>
					<email>silvia.fernandez@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Juan-Manuel</prenom>
					<nom>Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d’Avignon, BP 1228 84911 Avignon</affiliation>
				<affiliation affiliationId="2">Laboratoire de Physique de Matériaux, UHP-Nancy, 54506 Vandoeuvre</affiliation>
			</affiliations>
			<titre>Une approche exploratoire de compression automatique de phrases basée sur des critères thermodynamiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous présentons une approche exploratoire basée sur des notions thermodynamiques de la Physique statistique pour la compression de phrases. Nous décrivons le modèle magnétique des verres de spins, adapté à notre conception de la problématique. Des simulations Métropolis Monte-Carlo permettent d’introduire des fluctuations thermiques pour piloter la compression. Des comparaisons intéressantes de notre méthode ont été réalisées sur un corpus en français.</resume>
			<mots_cles>Compression de phrases, Résumé automatique, Résumé par extraction, Enertex, Mécanique statistique</mots_cles>
			<title></title>
			<abstract>We present an exploratory approach based on thermodynamic concepts of Statistical Physics for sentence compression.We describe the magnetic model of spin glasses, well suited to our conception of problem. The Metropolis Monte-Carlo simulations allow to introduce thermal fluctuations to drive the compression. Interesting comparisons of our method were performed on a French text corpora.</abstract>
			<keywords>Sentence Compression, Automatic Summarization, Extraction Summarization, Enertex, Statistical Mechanics</keywords>
		</article>
		<article id="taln-2009-court-043" session="Poster 2">
			<auteurs>
				<auteur>
					<prenom>Sebastián</prenom>
					<nom>Peña Saldarriaga</nom>
					<email>sebastian.pena-saldarriaga@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>Emmanuel.Morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Viard-Gaudin</nom>
					<email>Christian.Viard-Gaudin@univ-nantes.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA - UMR CNRS 6241, Université de Nantes</affiliation>
				<affiliation affiliationId="2">IRCCyN - UMR CNRS 6597, Université de Nantes</affiliation>
			</affiliations>
			<titre>Un nouveau schéma de pondération pour la catégorisation de documents manuscrits</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les schémas de pondération utilisés habituellement en catégorisation de textes, et plus généralement en recherche d’information (RI), ne sont pas adaptés à l’utilisation de données liées à des textes issus d’un processus de reconnaissance de l’écriture. En particulier, les candidats-mot à la reconnaissance ne pourraient être exploités sans introduire de fausses occurrences de termes dans le document. Dans cet article nous présentons un nouveau schéma de pondération permettant d’exploiter les listes de candidats-mot. Il permet d’estimer le pouvoir discriminant d’un terme en fonction de la probabilité a posteriori d’un candidat-mot dans une liste de candidats. Les résultats montrent que le taux de classification de documents fortement dégradés peut être amélioré en utilisant le schéma proposé.</resume>
			<mots_cles>Catégorisation de textes, écriture en-ligne, n-best candidats, pondération</mots_cles>
			<title></title>
			<abstract>The traditional weighting schemes used in information retrieval, and especially in text categorization cannot exploit information intrinsic to texts obtained through an on-line handwriting recognition process. In particular, top n (n > 1) candidates could not be used without introducing false occurrences of spurious terms thus making the resulting text noisier. In this paper, we propose an improved weighting scheme for text categorization, that estimates a term importance from the posterior probabilities of the top n candidates. The experimental results show that the categorization rate of poorly recognized texts increases when our weighting model is applied.</abstract>
			<keywords>Text categorization, on-line handwriting, n-best candidates, weighting</keywords>
		</article>
		<article id="taln-2009-court-044" session="Poster 2">
			<auteurs>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Scherrer</nom>
					<email>yves.scherrer@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATL, Université de Genève, Rue de Candolle 5, 1211 Genève 4, Suisse</affiliation>
			</affiliations>
			<titre>Un système de traduction automatique paramétré par des atlas dialectologiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Contrairement à la plupart des systèmes de traitement du langage, qui s’appliquent à des langues écrites et standardisées, nous présentons ici un système de traduction automatique qui prend en compte les spécificités des dialectes. En général, les dialectes se caractérisent par une variation continue et un manque de données textuelles en qualité et quantité suffisantes. En même temps, du moins en Europe, les dialectologues ont étudié en détail les caractéristiques linguistiques des dialectes. Nous soutenons que des données provenant d’atlas dialectologiques peuvent être utilisées pour paramétrer un système de traduction automatique. Nous illustrons cette idée avec le prototype d’un système de traduction basé sur des règles, qui traduit de l’allemand standard vers les différents dialectes de Suisse allemande. Quelques exemples linguistiquement motivés serviront à exposer l’architecture de ce système.</resume>
			<mots_cles>Traduction automatique, dialectes, langues proches, langues germaniques</mots_cles>
			<title></title>
			<abstract>Most natural language processing systems apply to written, standardized language varieties. In contrast, we present a machine translation system that takes into account some specificites of dialects : dialect areas show continuous variation along all levels of linguistic analysis, and textual data is often not available in sufficient quality and quantity. At the same time, many European dialect areas are well studied by dialectologists. We argue that data from dialectological atlases can be used to parametrize a machine translation system. We illustrate this idea by presenting the prototype of a rule-based machine translation system that translates from Standard German into the Swiss German dialect continuum. Its architecture is explained with some linguistically motivated examples.</abstract>
			<keywords>Machine translation, dialects, closely related languages, Germanic languages</keywords>
		</article>
		<article id="taln-2009-court-045" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Jean-Cédric</prenom>
					<nom>Chappelier</nom>
					<email>jean-cedric.chappelier@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Eckard</nom>
					<email>emmanuel.eckard@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Intelligence Artificielle, École polytechnique fédérale de Lausanne, Suisse</affiliation>
			</affiliations>
			<titre>Utilisation de PLSI en recherche d’information Représentation des requêtes</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le modèle PLSI (« Probabilistic Latent Semantic Indexing ») offre une approche de l’indexation de documents fondée sur des modèles probabilistes de catégories sémantiques latentes et a conduit à des applications dans différents domaines. Toutefois, ce modèle rend impossible le traitement de documents inconnus au moment de l’apprentissage, problème particulièrement sensible pour la représentation des requêtes dans le cadre de la recherche d’information. Une méthode, dite de « folding-in », permet dans une certaine mesure de contourner ce problème, mais présente des faiblesses. Cet article introduit nouvelle une mesure de similarité document-requête pour PLSI, fondée sur lesmodèles de langue, où le problème du « folding-in » ne se pose pas. Nous comparons cette nouvelle similarité aux noyaux de Fisher, l’état de l’art en la matière. Nous présentons aussi une évaluation de PLSI sur un corpus de recherche d’information de près de 7500 documents et de plus d’un million d’occurrences de termes provenant de la collection TREC–AP, une taille considérable dans le cadre de PLSI.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>The PLSI model (“Probabilistic Latent Semantic Indexing”) offers a document indexing scheme based on probabilistic latent category models. It entailed applications in diverse fields, notably in information retrieval (IR). Nevertheless, PLSI cannot process documents not seen during parameter inference, a major liability for queries in IR. A method known as “folding-in” allows to circumvent this problem up to a point, but has its own weaknesses. The present paper introduces a new document-query similarity measure for PLSI based on language models that entirely avoids the problem a query projection.We compare this similarity to Fisher kernels, the state of the art similarities for PLSI. Moreover, we present an evaluation of PLSI on a particularly large training set of almost 7500 document and over one million term occurrence large, created from the TREC–AP collection.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2009-court-046" session="Poster 3">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, 18 route du Panorama, BP6, Fontenay-aux-Roses, F-92265 France</affiliation>
			</affiliations>
			<titre>Utiliser des sens de mots pour la segmentation thématique ?</titre>
			<type>court</type>
			<pages></pages>
			<resume>La segmentation thématique est un domaine de l’analyse discursive ayant donné lieu à de nombreux travaux s’appuyant sur la notion de cohésion lexicale. La plupart d’entre eux n’exploitent que la simple récurrence lexicale mais quelques uns ont néanmoins exploré l’usage de connaissances rendant compte de cette cohésion lexicale. Celles-ci prennent généralement la forme de réseaux lexicaux, soit construits automatiquement à partir de corpus, soit issus de dictionnaires élaborés manuellement. Dans cet article, nous examinons dans quelle mesure une ressource d’une nature un peu différente peut être utilisée pour caractériser la cohésion lexicale des textes. Il s’agit en l’occurrence de sens de mots induits automatiquement à partir de corpus, à l’instar de ceux produits par la tâche «Word Sense Induction and Discrimination » de l’évaluation SemEval 2007. Ce type de ressources apporte une structuration des réseaux lexicaux au niveau sémantique dont nous évaluons l’apport pour la segmentation thématique.</resume>
			<mots_cles>Segmentation thématique, désambiguïsation sémantique</mots_cles>
			<title></title>
			<abstract>Many topic segmenters rely on lexical cohesion. Most of them only exploit lexical recurrence but some of them makes use of knowledge sources about lexical cohesion. These sources are generally lexical networks built either by hand or automatically from corpora. In this article, we study to what extent a new source of knowledge about lexical cohesion can be used for topic segmentation. This source is a set of word senses that were automatically discriminated from corpora, as the word senses resulting from the Word Sense Induction and Discrimination task of the SemEval 2007 evaluation. Such a resource is a way to structurate lexical networks at a semantic level. The impact of this structuring on topic segmentation is evaluated in this article.</abstract>
			<keywords>Topic segmentation, word sense disambiguation</keywords>
		</article>
	</articles>
</conference>
