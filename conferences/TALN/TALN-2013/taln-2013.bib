@proceedings{TALN:2013,
  editor    = {Morin, Emmanuel and Estève, Yannick and Estève, Yannick},
  title     = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013}
}

@inproceedings{fraser:2013:TALN,
  author    = {Fraser, Alexander},
  title     = {Améliorer la traduction des langages morphologiquement riches},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {1--1},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-invite-001},
  language  = {french},
  note      = {Improving Translation to Morphologically Rich Languages},
  resume    = {Si les techniques statistiques pour la traduction automatique ont fait des progrès significatifs au cours des 20 dernières années, les résultats pour la traduction de langues morphologiquement riches sont toujours mitigés par rapport aux précédentes générations de systèmes à base de règles. Les recherches actuelles en traduction statistique de langues morphologiquement riches varient grandement en fonction de la quantité de connaissances linguistiques utilisées et de la nature de ces connaissances. Cette variation est plus importante en langue cible (par exemple, les ressources utilisées en traduction automatique statistique respectueuse de linguistique en arabe, en français et en allemand sont très différentes). La conférence portera sur les techniques état de l’art dédiées à la tâche de traduction statistique pour une langue cible qui est morphologiquement plus riche que la langue source.},
  abstract  = {While statistical techniques for machine translation have made significant progress in the last 20 years, results for translating to morphologically rich languages are still mixed versus previous generation rule-based systems. Current research in statistical techniques for translating to morphologically rich languages varies greatly in the amount of linguistic knowledge used and the form of this linguistic knowledge. This varies most strongly by target language (e.g., the resources used for linguistically-aware statistical machine translation to Arabic, French, German are very different). The talk will discuss state-of-the-art techniques for statistical translation tasks involving translating to a target language which is morphologically richer than the source language.},
  motscles  = {traduction statistique, langages morphologiquement riches, connaissances linguistiques},
  keywords  = {statistical translation, morphologically rich languages, linguistic knowledge},
}

@inproceedings{mothe:2013:TALN,
  author    = {Mothe, Josiane},
  title     = {Recherche d’Information et Traitement Automatique des Langues Naturelles},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {2--2},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-invite-002},
  language  = {french},
  note      = {Information Retrieval and Natural Language Processing},
  resume    = {La recherche d’information s’intéresse à l’accès aux documents et une majorité de travaux dans le domaine s’appuie sur les éléments textuels de ces documents écrits en langage naturel. Les requêtes soumisses par les utilisateurs de moteurs de recherche sont également textuelles, même si elles sont très pauvres d’un point de vue linguistique. Il parait donc naturel que les travaux en recherche d’information cherchent à s’alimenter par les avancées et les résultats en traitement automatique des langues naturelles. Malgré les espoirs déçus des années 80, l’engouement pour l’utilisation du traitement du langage naturel en recherche d’information reste intact, poussé par les nouvelles perspectives offertes. Dans cette conférence, nous balayerons les aspects de la recherche d’information qui se sont le plus appuyés sur des éléments du traitement automatique des langues naturelles. Nous présenterons en particulier quelques résultats relatifs à la reformulation automatique de requêtes, à la prédiction de la difficulté des requêtes, au résumé automatique et à la contextualisation de textes courts ainsi que les perspectives actuelles offertes en particulier par les travaux en linguistique computationnelle.},
  abstract  = {Information retrieval aims at providing means to access documents. Most of current work in the domain relies on the textual elements of these documents which are written in natural language. Users’ queries are also generally textual, even if the queries are very poor from a linguistic point of view. As a results information retrieval field aimed at feeding on advances and results from natural language processing field. In spite of the disappointed hopes of the 80s, the enthusiasm for using natural language processing in information retrieval remains high, pushed by the new perspectives. In this talk, we will mention the various aspects of information retrieval which rely, at various levels, on natural language processing components. We will present in particular some results regardless automatic query reformulation, query difficulty prediction, automatic summarization and short text contextualization as well as some perspectives offered in particular considering computational linguistics.},
  motscles  = {Recherche d’information, traitement automatique des langues, reformulation de requêtes, difficulté des requêtes, résumé automatique},
  keywords  = {Information retrieval, natural language processing, query reformulation, query difficulty, automatic summarization},
}

@inproceedings{fort-couillault:2013:TALN,
  author    = {Fort, Karën and Couillault, Alain},
  title     = {La Charte Éthique et Big Data : pour des ressources pour le TAL (enfin !) traçables et pérennes},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {3--4},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-charte-001},
  language  = {french},
  note      = {The Ethics \& Big Data Charter : for tractable and lasting NLP resources},
  resume    = {La charte Ethique \& Big Data a été conçue à l’initiative de l’ATALA, de l’AFCP, de l’APROGED et de CAP DIGITAL, au sein d’un groupe de travail mixte réunissant d’autres partenaires académiques et industriels (tels que le CERSA-CNRS, Digital Ethics, Eptica-Lingway, le cabinet Itéanu ou ELRA/ELDA). Elle se donne comme objectif de fournir des garanties concernant la traçabilité des données (notamment des ressources langagières), leur qualité et leur impact sur l’emploi. Cette charte a été adoptée par Cap Digital (co-rédacteur). Nous avons également proposé à la DGLFLF et à l’ANR de l’utiliser. Elle est aujourd’hui disponible sous forme de wiki, de fichier pdf et il en existe une version en anglais. La charte est décrite en détails dans (Couillault et Fort, 2013).},
  abstract  = {The Ethics \& Big Data Charter was designed by ATALA, AFCP, APROGED and CAP DIGITAL, in a working group including other academic and industrial partners (such as CERSA-CNRS, Digital Ethics, Eptica-Lingway, Itéanu office or ELRA/ELDA). Its aims at ensuring the traceability and quality of the data (including language resources), how they are produced and their impact on working conditions. This charter has been adopted by Cap Digital (co-writer). We also proposed it to DGLFLF and ANR. As of today, it is available as a wiki, a pdf file and an English version 6. The charter is detailled in (Couillault et Fort, 2013).},
  motscles  = {éthique, big data, ressources langagières},
  keywords  = {ethics, big data, language resources},
}

@inproceedings{nejme-boulaknadel-aboutajdine:2013:TALN,
  author    = {Nejme, Fatima Zahra and Boulaknadel, Siham and Aboutajdine, Driss},
  title     = {Analyse Automatique de la Morphologie Nominale Amazighe},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {5--18},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-001},
  language  = {french},
  note      = {Morphological analysis of the standard Amazigh language using NooJ platform},
  resume    = {Dans le but de préserver le patrimoine amazighe et éviter qu’il soit menacé de disparition, il semble opportun de doter cette langue de moyens nécessaires pour faire face aux enjeux de l'accès au domaine de l'Information et de la Communication (TIC). Dans ce contexte, et dans la perspective de construire des outils et des ressources linguistiques pour le traitement automatique de cette langue, nous avons entrepris de construire un système d’analyse morphologique pour l’amazighe standard du Maroc. Ce système profite des apports des modèles \{ états finis au sein de l’environnement linguistique de développement NooJ en faisant appel à des règles grammaticales à large couverture.},
  abstract  = {In the aim of safeguarding the Amazigh heritage from being threathned of disappearance, it seems opportune to equip this language of necessary means to confront the stakes of access to the domain of New Information and Communication Technologies (ICT). In this context, and in the perspective to build tools and linguistic resources for the automatic processing of Amazigh language, we have undertaken to develop a system of a morphological description for standard Amazigh of Morocco. This system uses finite state technology, within the linguistic developmental environment NooJ by using a large-coverage of morphological grammars covering all grammatical rules.},
  motscles  = {La langue amazighe, TALN, NooJ, analyse morphologique, morphologie flexionnelle, morphologie dérivationnelle},
  keywords  = {Amazigh language, NLP, NooJ, morphological analysis, inflectional morphology, derivational morphology},
}

@inproceedings{tellier-dupont:2013:TALN,
  author    = {Tellier, Isabelle and Dupont, Yoann},
  title     = {Apprentissage symbolique et statistique pour le chunking:comparaison et combinaisons},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {19--32},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-002},
  language  = {french},
  note      = {Symbolic and statistical learning for chunking : comparison and combinations},
  resume    = {Nous décrivons dans cet article l’utilisation d’algorithmes d’inférence grammaticale pour la tâche de chunking, pour ensuite les comparer et les combiner avec des CRF (Conditional Random Fields), à l’efficacité éprouvée pour cette tâche. Notre corpus est extrait du French TreeBank. Nous proposons et évaluons deux manières différentes de combiner modèle symbolique et modèle statistique appris par un CRF et montrons qu’ils bénéficient dans les deux cas l’un de l’autre.},
  abstract  = {We describe in this paper how to use grammatical inference algorithms for chunking, then compare and combine them to CRFs (Conditional Random Fields) which are known efficient for this task. Our corpus is extracted from the FrenchTreebank. We propose and evaluate two ways of combining a symbolic model and a statistical model learnt by a CRF, and show that in both cases they benefit from one another.},
  motscles  = {apprentissage automatique, chunking, CRF, inférence grammaticale, k-RI, FrenchTreeBank},
  keywords  = {machine learning, chunking, CRF, grammatical inference, k-RI, French TreeBank},
}

@inproceedings{chali-hasan-mojahid:2013:TALN,
  author    = {Chali, Yllias and Hasan, Sadid A. and Mojahid, Mustapha},
  title     = {L’utilisation des POMDP pour les résumés multi-documents orientés par une thématique},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {33--47},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-003},
  language  = {french},
  note      = {Using POMDPs for Topic-Focused Multi-Document Summarization},
  resume    = {L’objectif principal du résumé multi-documents orienté par une thématique est de générer un résumé à partir de documents sources en réponse à une requête formulée par l’utilisateur. Cette tâche est difficile car il n’existe pas de méthode efficace pour mesurer la satisfaction de l’utilisateur. Cela introduit ainsi une incertitude dans le processus de génération de résumé. Dans cet article, nous proposons une modélisation de l’incertitude en formulant notre système de résumé comme un processus de décision markovien partiellement observables (POMDP) car dans de nombreux domaines on a montré que les POMDP permettent de gérer efficacement les incertitudes. Des expériences approfondies sur les jeux de données du banc d’essai DUC ont démontré l’efficacité de notre approche.},
  abstract  = {The main goal of topic-focused multidocument summarization is to generate a summary from the source documents in response to a given query or particular information requested by the user. This task is difficult in large part because there is no significant way of measuring whether the user is satisfied with the information provided. This introduces uncertainty in the current state of the summary generation procedure. In this paper, we model the uncertainty explicitly by formulating our summarization system as a Partially Observable Markov Decision Process (POMDP) since researchers in many areas have shown that POMDPs can deal with uncertainty successfully. Extensive experiments on the DUC benchmark datasets demonstrate the effectiveness of our approach.},
  motscles  = {Résumé multi-document, résumé orienté requête, POMDP},
  keywords  = {Topic-focused multi-document summarization, POMDP},
}

@inproceedings{ferret:2013:TALN,
  author    = {Ferret, Olivier},
  title     = {Sélection non supervisée de relations sémantiques pour améliorer un thésaurus distributionnel},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {48--61},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-004},
  language  = {french},
  note      = {Unsupervised selection of semantic relations for improving a distributional thesaurus},
  resume    = {Les travaux se focalisant sur la construction de thésaurus distributionnels ont montré que les relations sémantiques qu’ils recèlent sont principalement fiables pour les mots de forte fréquence. Dans cet article, nous proposons une méthode pour rééquilibrer de tels thésaurus en faveur des mots de fréquence faible sur la base d’un mécanisme d’amorçage : un ensemble d’exemples et de contre-exemples de mots sémantiquement similaires sont sélectionnés de façon non supervisée et utilisés pour entraîner un classifieur supervisé. Celui-ci est ensuite appliqué pour réordonner les voisins sémantiques du thésaurus utilisé pour sélectionner les exemples et contre-exemples. Nous montrons comment les relations entre les constituants de noms composés similaires peuvent être utilisées pour réaliser une telle sélection et comment conjuguer ce critère à un critère déjà expérimenté sur la symétrie des relations sémantiques. Nous évaluons l’intérêt de cette procédure sur un large ensemble de noms en anglais couvrant un vaste spectre de fréquence.},
  abstract  = {Work about distributional thesauri has shown that the relations in these thesauri are mainly reliable for high frequency words. In this article, we propose a method for improving such a thesaurus through its re-balancing in favor of low frequency words. This method is based on a bootstrapping mechanism : a set of positive and negative examples of semantically similar words are selected in an unsupervised way and used for training a supervised classifier. This classifier is then applied for reranking the semantic neighbors of the thesaurus used for example selection. We show how the relations between the mono-terms of similar nominal compounds can be used for performing this selection and how to associate this criterion with an already tested criterion based on the symmetry of semantic relations. We evaluate the interest of the global procedure for a large set of English nouns with various frequencies.},
  motscles  = {Sémantique lexicale, similarité sémantique, thésaurus},
  keywords  = {Lexical semantics, semantic similarity, distributional thesauri},
}

@inproceedings{dupuch-hamon-grabar:2013:TALN,
  author    = {Dupuch, Marie and Hamon, Thierry and Grabar, Natalia},
  title     = {Groupement de termes basé sur des régularités linguistiques et sémantiques dans un contexte cross-langue},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {62--75},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-005},
  language  = {french},
  note      = {Grouping of terms based on linguistic and semantic regularities in a cross-lingual context},
  resume    = {Nous proposons d’exploiter des méthodes du Traitement Automatique de Langues dédiées à la structuration de terminologie indépendamment dans deux langues (anglais et français) et de fusionner ensuite les résultats obtenus dans chaque langue. Les termes sont groupés en clusters grâce aux relations générées. L’évaluation de ces relations est effectuée au travers de la comparaison des clusters avec des données de référence et la baseline, tandis que la complémentarité des relations est analysée au travers de leur implication dans la création de clusters de termes. Les résultats obtenus indiquent que : chaque langue contribue de manière équilibrée aux résultats, le nombre de relations hiérarchiques communes est plus grand que le nombre de relations synonymiques communes. Globalement, les résultats montrent que, dans un contexte cross-langue, chaque langue permet de détecter des régularités linguistiques et sémantiques complémentaires. L’union des résultats obtenus dans les deux langues améliore la qualité globale des clusters.},
  abstract  = {We propose to exploit the Natural Language Processing methods dedicated to terminology structuring independently in two languages (English and French) and then to merge the results obtained in each language. The terms are grouped into clusters thanks to the generated relations. The evaluation of the relations is done via the comparison of the clusters with the reference data and the baseline, while the complementarity of the relations is analyzed through their involvement in the clusters of terms. Our results indicate that : each language contributes almost equally to the generated results ; the number of common hierarchical relations is greater than the number of common synonym relations. On the whole, the obtained results point out that in a cross-language context, each language brings additional linguistic and semantic regularities. The union of the results obtained in each language improves the overall quality of the clusters.},
  motscles  = {Relations sémantiques, termes, domaine de spécialité, médecine, contexte crosslangue},
  keywords  = {Semantic relations, terms, specialized areas, medicine, cross-lingual context},
}

@inproceedings{pradet-EtAl:2013:TALN,
  author    = {Pradet, Quentin and Baguenier-Desormeaux, Jeanne and de Chalendar, Gaël and Danlos, Laurence},
  title     = {WoNeF : amélioration, extension et évaluation d’une traduction française automatique de WordNet},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {76--89},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-006},
  language  = {french},
  note      = {WoNeF, an improved, extended and evaluated automatic French translation of WordNet},
  resume    = {Identifier les sens possibles des mots du vocabulaire est un problème difficile demandant un travail manuel très conséquent. Ce travail a été entrepris pour l’anglais : le résultat est la base de données lexicale WordNet, pour laquelle il n’existe encore que peu d’équivalents dans d’autres langues. Néanmoins, des traductions automatiques de WordNet vers de nombreuses langues cibles existent, notamment pour le français. JAWS est une telle traduction automatique utilisant des dictionnaires et un modèle de langage syntaxique. Nous améliorons cette traduction, la complétons avec les verbes et adjectifs de WordNet, et démontrons la validité de notre approche via une nouvelle évaluation manuelle. En plus de la version principale nommée WoNeF, nous produisons deux versions supplémentaires : une version à haute précision (93% de précision, jusqu’à 97% pour les noms), et une version à haute couverture contenant 109 447 paires (littéral, synset).},
  abstract  = {Identifying the various possible meanings of each word of the vocabulary is a difficult problem that requires a lot of manual work. It has been tackled by the WordNet lexical semantics database in English, but there are still few resources available for other languages. Automatic translations of WordNet have been tried to many target languages such as French. JAWS is such an automatic translation of WordNet nouns to French using bilingual dictionaries and a syntactic langage model. We improve the existing translation precision and coverage, complete it with translations of verbs and adjectives and enhance its evaluation method, demonstrating the validity of the approach. In addition to the main result called WoNeF, we produce two additional versions : a high-precision version with 93% precision (up to 97% on nouns) and a high-coverage version with 109,447 (literal, synset) pairs.},
  motscles  = {WordNet, désambiguïsation lexicale, traduction, ressource},
  keywords  = {WordNet, Word Sense Disambiguation, translation, resource},
}

@inproceedings{jabaian-lefevre-besacier:2013:TALN,
  author    = {Jabaian, Bassam and Lefèvre, Fabrice and Besacier, Laurent},
  title     = {Approches statistiques discriminantes pour l’interprétation sémantique multilingue de la parole},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {90--103},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-007},
  language  = {french},
  note      = {Discriminative statistical approaches for multilingual speech understanding},
  resume    = {Les approches statistiques sont maintenant très répandues dans les différentes applications du traitement automatique de la langue et le choix d’une approche particulière dépend généralement de la tâche visée. Dans le cadre de l’interprétation sémantique multilingue, cet article présente une comparaison entre les méthodes utilisées pour la traduction automatique et celles utilisées pour la compréhension de la parole. Cette comparaison permet de proposer une approche unifiée afin de réaliser un décodage conjoint qui à la fois traduit une phrase et lui attribue ses étiquettes sémantiques. Ce décodage est obtenu par une approche à base de transducteurs à états finis qui permet de composer un graphe de traduction avec un graphe de compréhension. Cette représentation peut être généralisée pour permettre des transmissions d’informations riches entre les composants d’un système d’interaction vocale homme-machine.},
  abstract  = {Statistical approaches are now widespread in the various applications of natural language processing and the elicitation of an approach usually depends on the targeted task. This paper presents a comparison between the methods used for machine translation and speech understanding. This comparison allows to propose a unified approach to perform a joint decoding which translates a sentence and assign semantic tags to the translation at the same time. This decoding is achieved through a finite-state transducer approach which allows to compose a translation graph with an understanding graph. This representation can be generalized to allow the rich transmission of information between the components of a human-machine vocal interface.},
  motscles  = {compréhension multilingue, système de dialogue, CRF, graphes d’hypothèses},
  keywords  = {multilingual understanding, dialogue system, CRF, hypothesis graphs},
}

@inproceedings{braud-denis:2013:TALN,
  author    = {Braud, Chloé and Denis, Pascal},
  title     = {Identification automatique des relations discursives « implicites » à partir de données annotées et de corpus bruts},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {104--117},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-008},
  language  = {french},
  note      = {Automatically identifying implicit discourse relations using annotated data and raw corpora},
  resume    = {Cet article présente un système d’identification des relations discursives dites « implicites » (à savoir, non explicitement marquées par un connecteur) pour le français. Etant donné le faible volume de données annotées disponibles, notre système s’appuie sur des données étiquetées automatiquement en supprimant les connecteurs non ambigus pris comme annotation d’une relation, une méthode introduite par (Marcu et Echihabi, 2002). Comme l’ont montré (Sporleder et Lascarides, 2008) pour l’anglais, cette approche ne généralise pas très bien aux exemples de relations implicites tels qu’annotés par des humains. Nous arrivons au même constat pour le français et, partant du principe que le problème vient d’une différence de distribution entre les deux types de données, nous proposons une série de méthodes assez simples, inspirées par l’adaptation de domaine, qui visent à combiner efficacement données annotées et données artificielles. Nous évaluons empiriquement les différentes approches sur le corpus ANNODIS : nos meilleurs résultats sont de l’ordre de 45.6% d’exactitude, avec un gain significatif de 5.9% par rapport à un système n’utilisant que les données annotées manuellement.},
  abstract  = {This paper presents a system for identifying « implicit » discourse relations (that is, relations that are not marked by a discourse connective). Given the little amount of available annotated data for this task, our system also resorts to additional automatically labeled data wherein unambiguous connectives have been suppressed and used as relation labels, a method introduced by (Marcu et Echihabi, 2002). As shown by (Sporleder et Lascarides, 2008) for English, this approach doesn’t generalize well to implicit relations as annotated by humans. We show that the same conclusion applies to French due to important distribution differences between the two types of data. In consequence, we propose various simple methods, all inspired from work on domain adaptation, with the aim of better combining annotated data and artificial data. We evaluate these methods through various experiments carried out on the ANNODIS corpus : our best system reaches a labeling accuracy of 45.6%, corresponding to a 5.9% significant gain over a system solely trained on manually labeled data.},
  motscles  = {analyse du discours, relations implicites, apprentissage automatique},
  keywords  = {discourse analysis, implicit relations, machine learning},
}

@inproceedings{lassalle-denis:2013:TALN,
  author    = {Lassalle, Emmanuel and Denis, Pascal},
  title     = {Apprentissage d’une hiérarchie de modèles à paires spécialisés pour la résolution de la coréférence},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {118--131},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-009},
  language  = {french},
  note      = {Learning a hierarchy of specialized pairwise models for coreference resolution},
  resume    = {Nous proposons une nouvelle méthode pour améliorer significativement la performance des modèles à paires de mentions pour la résolution de la coréférence. Étant donné un ensemble d’indicateurs, notre méthode apprend à séparer au mieux des types de paires de mentions en classes d’équivalence, chacune de celles-ci donnant lieu à un modèle de classification spécifique. La procédure algorithmique proposée trouve le meilleur espace de traits (créé à partir de combinaisons de traits élémentaires et d’indicateurs) pour discriminer les paires de mentions coréférentielles. Bien que notre approche explore un très vaste ensemble d’espaces de trait, elle reste efficace en exploitant la structure des hiérarchies construites à partir des indicateurs. Nos expériences sur les données anglaises de la CoNLL-2012 Shared Task indiquent que notre méthode donne des gains de performance par rapport au modèle initial utilisant seulement les traits élémentaires, et ce, quelque soit la méthode de formation des chaînes ou la métrique d’évaluation choisie. Notre meilleur système obtient une moyenne de 67.2 en F1-mesure MUC, B3 et CEAF ce qui, malgré sa simplicité, le situe parmi les meilleurs systèmes testés sur ces données.},
  abstract  = {This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds the best feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible features spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our experiments on the CoNLL-2012 shared task English datasets indicate that our method is robust to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains 67.2 of average F1 over MUC, B3, and CEAF which, despite its simplicity, places it among the best performing systems on these datasets.},
  motscles  = {résolution de la coréférence, apprentissage automatique},
  keywords  = {coreference resolution, machine learning},
}

@inproceedings{fauconnier-EtAl:2013:TALN,
  author    = {Fauconnier, Jean-Philippe and Kamel, Mouna and Rothenburger, Bernard and Aussenac-Gilles, Nathalie},
  title     = {Apprentissage supervisé pour l’identification de relations sémantiques au sein de structures énumératives parallèles},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {132--145},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-010},
  language  = {french},
  note      = {A Supervised learning for the identification of semantic relations in parallel enumerative structures},
  resume    = {Ce travail s’inscrit dans le cadre de la construction et l’enrichissement d’ontologies à partir de textes de type encyclopédique ou scientifique. L’originalité de notre travail réside dans l’extraction de relations sémantiques exprimées au-delà de la linéarité du texte. Pour cela, nous nous appuyons sur la sémantique véhiculée par les caractères typo-dispositionels qui ont pour fonction de suppléer des formulations strictement linguistiques qui seraient plus difficilement exploitables. L’étude que nous proposons concerne les relations sémantiques portées par les structures énumératives parallèles qui, bien qu’affichant des discontinuités entre ses différents composants, présentent un tout sur le plan sémantique. Ce sont des structures textuelles qui sont propices aux relations hiérarchiques. Après avoir défini une typologie des relations portées par ce type de structure, nous proposons une approche par apprentissage visant à leur identification. Sur la base de traits incorporant informations lexico-syntaxiques et typo-dispositionnelles, les premiers résultats aboutissent à une exactitude de 61,1%.},
  abstract  = {This work falls within the framework of ontology engineering and learning from encyclopedic or scientific texts. Our original contribution lies within the extraction of semantic relations expressed beyond the text linearity. To this end, we relied on the semantics behind the typo-dispositional characters whose function is to supplement the strictly linguistic formulations that could be more difficult to exploit. The work reported here is dealing with the semantic relations carried by the parallel enumerative structures. Although they display discontinuities between their various components, these enumerative structures form a whole at the semantic level. They are textual structures that are prone to hierarchic relations. After defining a typology of the relationships carried by this type of structure, we are proposing a learning approach aimed at their identification. Based on features including lexico-syntactic and typo-dispositional informations, the first results led an accuracy of 61.1%.},
  motscles  = {extraction de relations, structures énumératives parallèles, mise en forme matérielle, apprentissage supervisé, construction d’ontologies},
  keywords  = {relationship extraction, parallel enumerative structures, material shaping, supervised learning, ontology learning},
}

@inproceedings{jacques-hartwell-falaise:2013:TALN,
  author    = {Jacques, Marie-Paule and Hartwell, Laura and Falaise, Achille},
  title     = {Techniques de TAL et corpus pour faciliter les formulations en anglais scientifique écrit},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {146--159},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-011},
  language  = {french},
  note      = {NLP and corpus techniques for finding formulations that facilitate scientific writing in English},
  resume    = {Nous présentons l'adaptation de la base d'écrits scientifiques en ligne Scientext pour un « nouveau » public : chercheurs et autres auteurs français d'écrits scientifiques, ayant besoin de rédiger en anglais. Cette adaptation a consisté à ajouter dans la base des requêtes précodées qui permettent d'afficher les contextes dans lesquels les auteurs d'articles scientifiques en anglais expriment leur objectif de recherche et à enrichir l'interface ScienQuest de nouvelles fonctionnalités pour mémoriser et réafficher les contextes pertinents, pour faciliter la consultation par un public plus large. Les nombreuses descriptions linguistiques de la rhétorique des articles scientifiques insistent sur l'importance de la création et de l'occupation d'une « niche » de recherche. Chercheurs et doctorants ont ici un moyen d'en visualiser des exemples sans connaître sa formulation a priori, via nos requêtes. Notre évaluation sur le corpus de test en donne une précision globale de 86,5 %.},
  abstract  = {This paper presents adaptations of the query options integrated into the online corpus Scientext so as to better serve a new audience: French scientists writing in English. We added pre-coded queries that display the contexts in which authors of scientific articles in English state their research objective. Furthermore, new functional options enrich the ScienQuest interface allowing results to be filtered for noise and then saved for consultation by a larger public. Previous studies on the scientific discourse and rhetoric of scientific articles have highlighted the importance of establishing and occupying a research niche. Here, francophone researchers and doctoral students without prior discursive knowledge, can access authentic and multiple ways of formulating a research objective. Our evaluation of a test corpus showed an overall accuracy of 86.5 %.},
  motscles  = {anglais, patrons lexico-syntaxiques, ScienQuest, Scientext},
  keywords  = {ESP, lexico-syntactic patterns, ScienQuest, Scientext},
}

@inproceedings{hernandez-boudin:2013:TALN,
  author    = {Hernandez, Nicolas and Boudin, Florian},
  title     = {Construction d’un large corpus écrit libre annoté morpho-syntaxiquement en français},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {160--173},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-012},
  language  = {french},
  note      = {Construction of a Free Large Part-of-Speech Annotated Corpus in French},
  resume    = {Cet article étudie la possibilité de créer un nouveau corpus écrit en français annoté morphosyntaxiquement à partir d’un corpus annoté existant. Nos objectifs sont de se libérer de la licence d’exploitation contraignante du corpus d’origine et d’obtenir une modernisation perpétuelle des textes. Nous montrons qu’un corpus pré-annoté automatiquement peut permettre d’entraîner un étiqueteur produisant des performances état-de-l’art, si ce corpus est suffisamment grand.},
  abstract  = {This paper studies the possibility of creating a new part-of-speech annotated corpus in French from an existing one. The objectives are to propose an exit from the restrictive licence of the source corpus and to obtain a perpetual modernisation of texts. Results show that it is possible to train a state-of-the-art POS-tagger from an automatically tagged corpus if this one is large enough.},
  motscles  = {corpus arboré, construction de corpus, étiquetage morpho-syntaxique},
  keywords  = {French treebank, Building a corpus, Part-of-Speech Tagging},
}

@inproceedings{abeille-crabbe:2013:TALN,
  author    = {Abeillé, Anne and Crabbé, Benoît},
  title     = {Vers un treebank du français parlé},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {174--187},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-013},
  language  = {french},
  note      = {Towards a treebank of spoken French},
  resume    = {Nous présentons les premiers résultats d’un corpus arboré pour le français parlé. Il a été réalisé dans le cadre du projet ANR Etape (resp. G. Gravier) en 2011 et 2012. Contrairement à d’autres langues comme l’anglais (voir le Switchboard treebank de (Meteer, 1995)), il n’existe pas de grand corpus oral du francais annoté et validé pour les constituants et les fonctions syntaxiques. Nous souhaitons construire une ressource comparable, qui serait une extension naturelle du Corpus arboré de Paris 7 (FTB : (Abeillé et al., 2003))) basé sur des textes du journal Le Monde. Nous serons ainsi en mesure de comparer, avec des annotations comparables, l’écrit et l’oral. Les premiers résultats, qui consistent à réutiliser l’analyseur de (Petrov et al., 2006) entraîné sur l’écrit, avec une phase de correction manuelle, sont encourageants.},
  abstract  = {We present the first results of an attempt to build a spoken treebank for French. It has been conducted as part of the ANR project Etape (resp. G. Gravier). Contrary to other languages such as English (see the Switchboard treebank (Meteer, 1995)), there is no sizable spoken corpus for French annotated for syntactic constituents and grammatical functions. Our project is to build such a resource which will be a natural extension of the Paris 7 treebank (FTB : (Abeillé et al., 2003))) for written French, in order to be able to compare with similar annotations written and spoken French. We have reused and adapted the parser (Petrov et al., 2006) which has been trained on the written treebank, with manual correction and validation. The first results are promising.},
  motscles  = {Corpus arboré, français parlé, corpus oral, analyse syntaxique automatique},
  keywords  = {Treebank, spoken French, spoken corpus, parsing},
}

@inproceedings{urieli-tanguy:2013:TALN,
  author    = {Urieli, Assaf and Tanguy, Ludovic},
  title     = {L’apport du faisceau dans l’analyse syntaxique en dépendances par transitions : études de cas avec l’analyseur Talismane},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {188--201},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-014},
  language  = {french},
  note      = {Applying a Beam Search to Transition-based Dependency Parsing: a Case Study for French with the Talismane Suite},
  resume    = {L’analyse syntaxique (ou parsing) en dépendances par transitions se fait souvent de façon déterministe, où chaque étape du parsing propose une seule solution comme entrée de l’étape suivante. Il en va de même pour la chaîne complète d’analyse qui transforme un texte brut en graphe de dépendances, généralement décomposé en quatre modules (segmentation en phrases, en mots, étiquetage et parsing) : chaque module ne fournit qu’une seule solution au module suivant. On sait cependant que certaines ambiguïtés ne peuvent pas être levées sans prendre en considération le niveau supérieur. Dans cet article, nous présentons l’analyseur Talismane, outil libre et complet d’analyse syntaxique probabiliste du français, et nous étudions plus précisément l’apport d’une recherche par faisceau (beam search) à l’analyse syntaxique. Les résultats nous permettent à la fois de dégager la taille de faisceau la plus adaptée (qui permet d’atteindre un score de 88,5 % d’exactitude, légèrement supérieur aux outils comparables), ainsi que les meilleures stratégies concernant sa propagation.},
  abstract  = {Transition-based dependency parsing often uses deterministic techniques, where each parse step provides a single solution as the input to the next step. The same is true for the entire analysis chain which transforms raw text into a dependency graph, generally composed of four modules (sentence detection, tokenising, pos-tagging and parsing): each module provides only a single solution to the following module. However, some ambiguities cannot be resolved without taking the next level into consideration. In this article, we present Talismane, an open-source suite of tools providing a complete statistical parser of French. More specifically, we study the contribution of a beam search to syntax parsing. Our analysis allows us to conclude on the most appropriate beam width (enabling us to attain an accuracy of 88.5%, slightly higher than comparable tools), and on the best strategies concerning beam propagation from one level of analysis to the next.},
  motscles  = {Analyse syntaxique en dépendances, ambiguïtés, évaluation, beam search},
  keywords  = {Dependency parsing, ambiguities, evaluation, beam search},
}

@inproceedings{simon-gravier-sebillot:2013:TALN,
  author    = {Simon, Anca and Gravier, Guillaume and Sébillot, Pascale},
  title     = {Un modèle segmental probabiliste combinant cohésion lexicale et rupture lexicale pour la segmentation thématique},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {202--214},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-015},
  language  = {french},
  note      = {A probabilistic segment model combining lexical cohesion and disruption for topic segmentation},
  resume    = {L’identification d’une structure thématique dans des données textuelles quelconques est une tâche difficile. La plupart des techniques existantes reposent soit sur la maximisation d’une mesure de cohésion lexicale au sein d’un segment, soit sur la détection de ruptures lexicales. Nous proposons une nouvelle technique combinant ces deux critères de manière à obtenir le meilleur compromis entre cohésion et rupture. Nous définissons un nouveau modèle probabiliste, fondé sur l’approche proposée par Utiyama et Isahara (2001), en préservant les propriétés d’indépendance au domaine et de faible a priori de cette dernière. Des évaluations sont menées sur des textes écrits et sur des transcriptions automatiques de la parole à la télévision, transcriptions qui ne respectent pas les normes des textes écrits, ce qui accroît la difficulté. Les résultats expérimentaux obtenus démontrent la pertinence de la combinaison des critères de cohésion et de rupture.},
  abstract  = {Identifying topical structure in any text-like data is a challenging task. Most existing techniques rely either on maximizing a measure of the lexical cohesion or on detecting lexical disruptions. A novel method combining the two criteria so as to obtain the best trade-off between cohesion and disruption is proposed in this paper. A new statistical model is defined, based on the work of Isahara and Utiyama (2001), maintaining the properties of domain independence and limited a priori of the latter. Evaluations are performed both on written texts and on automatic transcripts of TV shows, the latter not respecting the norms of written texts, thus increasing the difficulty of the task. Experimental results demonstrate the relevance of combining lexical cohesion and disrupture.},
  motscles  = {segmentation thématique, cohésion lexicale, rupture de cohésion, journaux télévisés},
  keywords  = {topic segmentation, lexical cohesion, lexical disrupture, TV broadcast news},
}

@inproceedings{bourreau:2013:TALN,
  author    = {Bourreau, Pierre},
  title     = {Traitements d’ellipses : deux approches par les grammaires catégorielles abstraites},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {215--228},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-016},
  language  = {french},
  note      = {Treating ellipsis : two abstract categorial grammar perspectives},
  resume    = {L’étude de phénomènes d’ellipses dans les modèles de l’interface syntaxe-sémantique pose certains problèmes du fait que le matériel linguistique effacé au niveau phonologique est néanmoins présent au niveau sémantique. Tel est le cas d’une ellipse verbale ou d’une élision du sujet, par exemple, phénomènes qui interviennent lorsque deux phrases reliées par une conjonction partagent le même verbe, ou le même sujet. Nous proposons un traitement de ces phénomènes dans le formalisme des grammaires catégorielles abstraites selon un patron que nous intitulons extraction/instanciation et que nous implémentons de deux manières différentes dans les ACGs.},
  abstract  = {The treatment of ellipsis in models of the syntax-semantics interface is troublesome as the linguistic material removed in the phonologic interpretation is still necessary in the semantics. Examples are particular cases of coordination, especially the ones involving verbal phrase ellipsis or subject elision. We show a way to use abstract categorial grammars so as to implement a pattern we call extraction/instantiation in order to deal with some of these phenomena ; we exhibit two different constructions of this principle into ACGs.},
  motscles  = {ellipse, coordination, interface syntaxe-sémantique, grammaires catégorielles abstraites, grammaires d’arbres adjoints, grammaires IO d’arbres},
  keywords  = {ellipsis, coordination, syntax-semantics interface, abstract categorial grammars, tree-adjoining grammars, IO tree-grammars},
}

@inproceedings{blache:2013:TALN,
  author    = {Blache, Philippe},
  title     = {Chunks et activation : un modèle de facilitation du traitement linguistique},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {229--242},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-017},
  language  = {french},
  note      = {Chunks and the notion of activation : a facilitation model for sentence processing},
  resume    = {Nous proposons dans cet article d’intégrer la notion de chunk au sein d’une architecture globale de traitement de la phrase. Les chunks jouent un rôle important dans les théories cognitives comme ACT-R (Anderson et al., 2004) : il s’agit d’unités de traitement globales auxquelles il est possible d’accéder directement via des buffers en mémoire à court ou long terme. Ces chunks sont construits par une fonction d’activation (processus cognitif pouvant être quantifié) s’appuyant sur l’évaluation de leur relation au contexte. Nous proposons une interprétation de cette théorie appliquée à l’analyse syntaxique. Un mécanisme de construction des chunks est proposé. Nous développons pour cela une fonction d’activation tirant parti de la représentation de l’information linguistique sous forme de contraintes. Cette fonction permet de montrer en quoi les chunks sont faciles à construire et comment leur existence facilite le traitement de la phrase. Plusieurs exemples sont proposés, illustrant cette hypothèse de facilitation.},
  abstract  = {We propose in this paper to integrate the notion of chunk within a global architecture for sentence processing. Chunks play an important role in cognitive theories such as ACT-R cite Anderson04 : they constitute global processing units which can be accessed directly via short or long term memory buffers. Chunks are built on the basis of an activation function evaluating their relationship to the context. We propose an interpretation of this theory applied to parsing. A construction mechanism is proposed, based on an adapted version of the activation function which takes advantage of the representation of linguistic information in terms of constraints. This feature allows to show how chunks are easy to build and how they can facilitate treatment. Several examples are given, illustrating this hypothesis of facilitation.},
  motscles  = {Chunks, ACT-R, activation, mémoire, parsing, traitement de la phrase, expérimentation},
  keywords  = {Chunks, ACT-R, activation, memory, parsing, sentence processing, experimentation},
}

@inproceedings{hazem-morin:2013:TALN,
  author    = {Hazem, Amir and Morin, Emmanuel},
  title     = {Extraction de lexiques bilingues à partir de corpus comparables par combinaison de représentations contextuelles},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {243--256},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-018},
  language  = {french},
  note      = {Bilingual Lexicon Extraction from Comparable Corpora by Combining Contextual Representations},
  resume    = {La caractérisation du contexte des mots constitue le coeur de la plupart des méthodes d’extraction de lexiques bilingues à partir de corpus comparables. Dans cet article, nous revisitons dans un premier temps les deux principales stratégies de représentation contextuelle, à savoir celle par fenêtre ou sac de mots et celle par relations de dépendances syntaxiques. Dans un second temps, nous proposons deux nouvelles approches qui exploitent ces deux représentations de manière conjointe. Nos expériences montrent une amélioration significative des résultats sur deux corpus de langue de spécialité.},
  abstract  = {Words context characterisation constitute the heart of most methods of bilingual lexicon extraction from comparable corpora. In this article, we first revisit the two main strategies of context representation, that is : the window-based and the syntactic based context representation. Secondly, we propose two new methods that exploit jointly these different representations . Our experiments show a significant improvement of the results obtained on two different domain specific comparable corpora.},
  motscles  = {Multilingualisme, corpus comparables, lexique bilingue, vecteurs de contexte, dépendances syntaxiques},
  keywords  = {Multilingualism, comparable corpora, bilingual lexicon, context vectors, syntactic dependencies},
}

@inproceedings{claveau-ncibi:2013:TALN,
  author    = {Claveau, Vincent and Ncibi, Abir},
  title     = {Découverte de connaissances dans les séquences par CRF non-supervisés},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {257--270},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-019},
  language  = {french},
  note      = {Unsupervised CRF for knowledge discovery},
  resume    = {Les tâches de découverte de connaissances ont pour but de faire émerger des groupes d’entités cohérents. Ils reposent le plus souvent sur du clustering, tout l’enjeu étant de définir une notion de similarité pertinentes entre ces entités. Dans cet article, nous proposons de détourner les champs aléatoires conditionnels (CRF), qui ont montré leur intérêt pour des tâches d’étiquetage supervisées, pour calculer indirectement ces similarités sur des séquences de textes. Pour cela, nous générons des problèmes d’étiquetage factices sur les données à traiter pour faire apparaître des régularités dans les étiquetages des entités. Nous décrivons comment ce cadre peut être mis en oeuvre et l’expérimentons sur deux tâches d’extraction d’informations. Les résultats obtenus démontrent l’intérêt de cette approche non-supervisée, qui ouvre de nombreuses pistes pour le calcul de similarités dans des espaces de représentations complexes de séquences.},
  abstract  = {Knowledge discovery aims at bringing out coherent groups of entities. They are usually based on clustering ; the challenge is then to define a notion of similarity between the relevant entities. In this paper, we propose to divert Conditional Random Fields (CRF), which have shown their interest in supervised labeling tasks, in order tocalculate indirectly the similarities among text sequences. Our approach consists in generate artificial labeling problems on the data to be processed to reveal regularities in the labeling of the entities. We describe how this framework can be implemented and experiment it on two information retrieval tasks. The results demonstrate the usefulness of this unsupervised approach, which opens many avenues for defining similarities for complex representations of sequential data.},
  motscles  = {Découverte de connaissances, CRF, clustering, apprentissage non-supervisé, extraction d’informations},
  keywords  = {Knowledge discovery, CRF, clustering, unsupervised machine learning, information extraction},
}

@inproceedings{gaillat:2013:TALN,
  author    = {Gaillat, Thomas},
  title     = {Annotation automatique d'un corpus d'apprenants d'anglais avec un jeu d'étiquettes modifié du Penn Treebank},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {271--284},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-020},
  language  = {french},
  note      = {Automatic tagging of a learner corpus of English with a modified version of the Penn Treebank tagset},
  resume    = {Cet article aborde la problématique de l'annotation automatique d'un corpus d'apprenants d'anglais. L'objectif est de montrer qu'il est possible d'utiliser un étiqueteur PoS pour annoter un corpus d'apprenants afin d'analyser les erreurs faites par les apprenants. Cependant, pour permettre une analyse suffisamment fine, des étiquettes fonctionnelles spécifiques aux phénomènes linguistiques à étudier sont insérées parmi celles de l'étiqueteur. Celuici est entraîné avec ce jeu d'étiquettes étendu sur un corpus de natifs avant d'être appliqué sur le corpus d'apprenants. Dans cette expérience, on s'intéresse aux usages erronés de this et that par les apprenants. On montre comment l'ajout d'une couche fonctionnelle sous forme de nouvelles étiquettes pour ces deux formes, permet de discriminer des usages variables chez les natifs et nonnatifs et, partant, d’identifier des schémas incorrects d'utilisation. Les étiquettes fonctionnelles éclairent sur le fonctionnement discursif.},
  abstract  = {This article covers the issue of automatic annotation of a learner corpus of English. The objective is to show that it is possible to PoStag the corpus with a tagger to prepare the ground for learner error analysis. However, in order to have a finegrain analysis, some functional tags for the study of specific linguistic points are inserted within the tagger's tagset. This tagger is trained on a native-English corpus with an extended tagset and the tagging is done on the learner corpus. This experiment focuses on the incorrect use of this and that by learners. We show how the insertion of a functional layer by way of new tags for the forms allows us to discriminate varying uses among natives and nonnatives. This opens the path to the identification of incorrect patterns of use. The functional tags cast a light on the way the discourse functions.},
  motscles  = {Apprentissage L2, corpus d'apprenants, analyse linguistique d'erreurs, étiquetage automatique, this, that},
  keywords  = {Second Language Acquisition, learner corpus, linguistic error analysis, automated tagging, this, that},
}

@inproceedings{sajous-hathout-calderone:2013:TALN,
  author    = {Sajous, Franck and Hathout, Nabil and Calderone, Basilio},
  title     = {GLÀFF, un Gros Lexique À tout Faire du Français},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {285--298},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-021},
  language  = {french},
  note      = {GLÀFF, a Large Versatile French Lexicon},
  resume    = {Cet article présente GLÀFF, un lexique du français à large couverture extrait du Wiktionnaire, le dictionnaire collaboratif en ligne. GLÀFF contient pour chaque entrée une description morphosyntaxique et une transcription phonémique. Il se distingue des autres lexiques existants principalement par sa taille, sa licence libre et la possibilité de le faire évoluer de façon constante. Nous décrivons ici comment nous l’avons construit, puis caractérisé en le comparant à différentes ressources connues. Cette comparaison montre que sa taille et sa qualité font de GLÀFF un candidat sérieux comme nouvelle ressource standard pour le TAL, la linguistique et la psycholinguistique.},
  abstract  = {This paper introduces GLÀFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLÀFF contains, for each entry, a morphosyntactic description and a phonetic transcription. It distinguishes itself from the other available lexicons mainly by its size, its potential for constant updating and its copylefted license that makes it available for use, modification and redistribution. We explain how we have built GLÀFF and compare it to other known resources. We show that its size and quality are strong assets that could allow GLÀFF to become a reference lexicon for NLP, linguistics and psycholinguistics.},
  motscles  = {Lexique morpho-phonologique, ressources lexicales libres, Wiktionnaire},
  keywords  = {Morpho-phonological lexicon, free lexical resources, French Wiktionary},
}

@inproceedings{abdulhay-kraif:2013:TALN,
  author    = {Abdul Hay, Authoul and Kraif, Olivier},
  title     = {Constitution d’une ressource sémantique arabe à partir de corpus multilingue aligné},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {299--312},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-022},
  language  = {french},
  note      = {The constitution of an Arabic semantic resource from a multilingual aligned corpus},
  resume    = {Cet article porte sur la mise en oeuvre etsur l'étudede techniques d'extraction de relations sémantiques à partir d'un corpus multilingue aligné, en vue de construire une ressource lexicale pour l’arabe. Ces relations sontextraites par transitivité de l'équivalence traductionnelle, deux lexèmes qui possèdent les mêmes équivalents dans une langue cible étant susceptibles de partager un même sens. A partir d’équivalences extraites d’un corpus multilingue aligné, nous tâchons d'extraire des "cliques", ou sous-graphes maximaux complets connexes, dont toutes les unités sont en interrelation, du fait d'une probable intersection sémantique. Ces cliques présentent l'intérêt de renseigner à la fois sur la synonymie et la polysémie des unités, et d'apporter une forme de désambiguïsation sémantique. Ensuite nous tâchons de relier ces cliques avec un lexique sémantique (de type Wordnet) afin d'évaluer la possibilité de récupérer pour les unités arabes des relations sémantiques définies pour des unités en d’autres langues (français, anglais ou espagnol). Les résultats sont encourageants, et montrent qu’avec des corpus adaptés ces relations pourraient permettrede construire automatiquement un réseau utile pour certaines applications de traitement de la langue arabe.},
  abstract  = {This paper aims at the implementation and evaluation of techniques for extracting semantic relations from a multilingual alignedcorpus, in order to build a lexical resource for Arabic language. We first extract translational equivalents froma multilingual aligned corpus. From these equivalences, we try to extract "cliques", which are maximum complete related sub-graphs, where all units are interrelated because of a probable semantic intersection. These cliques have the advantage of giving information on both the synonymy and polysemy of units, providing a kindof semantic disambiguation. Secondly, we attempt to link these cliques with a semantic lexicon (like WordNet) in order to assess the possibility of recovering, for the Arabicunits, a semantic relationships already defined for English, French or Spanish units. These relations would automatically build a semantic resource which would be useful for different applications of NLP, such as Question Answering systems, Machine Translation, alignment systems, Information Retrieval…etc.},
  motscles  = {Corpus multilingues alignés, désambigüisation sémantique, cliques, lexiques multilingues, réseaux sémantiques, traitement de l’arabe},
  keywords  = {Multilingual aligned corpus, semantic disambiguation, cliques, multilingual lexicons, word net, Arabic Language Processing},
}

@inproceedings{harastani-daille-morin:2013:TALN,
  author    = {Harastani, Rima and Daille, Béatrice and Morin, Emmanuel},
  title     = {Identification, alignement, et traductions des adjectifs relationnels en corpus comparables},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {313--326},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-023},
  language  = {french},
  note      = {Identification, Alignment, and Tranlsation of Relational Adjectives from Comparable Corpora},
  resume    = {Dans cet article, nous extrayons des adjectifs relationnels français et nous les alignons automatiquement avec les noms dont ils sont dérivés en utilisant un corpus monolingue. Les alignements adjectif-nom seront ensuite utilisés dans la traduction compositionelle des termes complexes de la forme [N AdjR] à partir d’un corpus comparable français-anglais. Un nouveau terme [N N0] (ex. cancer du poumon) sera obtenu en remplaçant l’adjectif relationnel Ad jR (ex. pulmonaire) dans [N AdjR] (ex. cancer pulmonaire) par le nom N0 (ex. poumon) avec lequel il est aligné. Si aucune traduction n’est proposée pour [N AdjR], nous considérons que ses traduction(s) sont équivalentes à celle(s) de sa paraphrase [N N0]. Nous expérimentons avec un corpus comparable dans le domaine de cancer du sein, et nous obtenons des alignements adjectif-nom qui aident à traduire des termes complexes de la forme [N AdjR] vers l’anglais avec une précision de 86 %.},
  abstract  = {In this paper, we extract French relational adjectives and automatically align them with the nouns they are derived from by using a monolingual corpus. The obtained adjective-noun alignments are then used in the compositional translation of compound nouns of the form [N ADJR] with a French-English comparable corpora. A new term [N N0] (eg, cancer du poumon) is obtained by replacing the relational adjective Ad jR (eg, pulmonaire) in [N AdjR] (eg, cancer pulmonaire) by its corresponding N0 (eg, poumon). If no translation(s) are obtained for [N AdjR], we consider the one(s) obtained for its paraphrase [N N0]. We experiment with a comparable corpora in the field of breast cancer, and we get adjective-noun alignments that help in translating French compound nouns of the form [N AdjR] to English with a precision of 86%.},
  motscles  = {Adjectifs relationnels, Corpus comparables, Méthode compositionnelle, Termes complexes},
  keywords  = {Relational adjectives, Comparable corpora, Compositional method, Complex terms},
}

@inproceedings{bouamor-semmar-zweigenbaum:2013:TALN,
  author    = {Bouamor, Dhouha and Semmar, Nasredine and Zweigenbaum, Pierre},
  title     = {Utilisation de la similarité sémantique pour l’extraction de lexiques bilingues à partir de corpus comparables},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {327--338},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-024},
  language  = {french},
  resume    = {Cet article présente une nouvelle méthode visant à améliorer les résultats de l’approche standard utilisée pour l’extraction de lexiques bilingues à partir de corpus comparables spécialisés. Nous tentons de résoudre le problème de la polysémie des mots dans les vecteurs de contexte par l’introduction d’un processus de désambiguïsation sémantique basé sur WordNet. Pour traduire les vecteurs de contexte, au lieu de considérer toutes les traductions proposées par le dictionnaire bilingue, nous n’utilisons que les mots caractérisant au mieux les contextes en langue cible. Les expériences menées sur deux corpus comparables spécialisés français-anglais (financier et médical) montrent que notre méthode améliore les résultats de l’approche standard plus particulièrement lorsque plusieurs mots du contexte sont ambigus.},
  abstract  = {This paper presents a new method that aims to improve the results of the standard approach used for bilingual lexicon extraction from specialized comparable corpora. We attempt to solve the problem of context vector word polysemy. Instead of using all the entries of the dictionary to translate a context vector, we only use the words of the lexicon that are more likely to give the best characterization of context vectors in the target language. On two specialised French-English comparable corpora, empirical experimental results show that our method improves the results obtained by the standard approach especially when many words are ambiguous.},
  motscles  = {lexique bilingue, corpus comparable spécialisé, désambiguïsation sémantique, WordNet},
  keywords  = {bilingual lexicon, specialized comparable corpora, semantic disambiguation, Word-Net},
}

@inproceedings{zarrouk-lafourcade-joubert:2013:TALN,
  author    = {Zarrouk, Manel and Lafourcade, Mathieu and Joubert, Alain},
  title     = {Inférences déductives et réconciliation dans un réseau lexico-sémantique},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {339--352},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-025},
  language  = {french},
  note      = {Inductive and deductive inferences in a Crowdsourced Lexical-Semantic Network},
  resume    = {La construction et la validation des réseaux lexico-sémantiques est un enjeu majeur en TAL. Indépendamment des stratégies de construction utilisées, inférer automatiquement de nouvelles relations à partir de celles déjà existantes est une approche possible pour améliorer la couverture et la qualité globale de la ressource. Dans ce contexte, le moteur d’inférences a pour but de formuler de nouvelles conclusions (c’est-à-dire des relations entre les termes) à partir de prémisses (des relations préexistantes). L’approche que nous proposons est basée sur une méthode de triangulation impliquant la transitivité sémantique avec un mécanisme de blocage pour éviter de proposer des relations douteuses. Les relations inférées sont proposées aux contributeurs pour être validées. Dans le cas d’invalidation, une stratégie de réconciliation est engagée pour identifier la cause de l’inférence erronée : une exception, une erreur dans les prémisses, ou une confusion d’usage causée par la polysémie.},
  abstract  = {In Computational Linguistics, validated lexical-semantic networks are crucial resources. Regardless the construction strategies used, automatically inferring new relations from already existing ones may improve coverage and global quality of the resource. In this context, an inference engine aims at producing new conclusions (i.e. potential relations) from premises (pre-existing relations). The approach we propose is based on a triangulation method involving the semantic transitivity with a blocking mechanism to avoid proposing dubious relations. Inferred relations are then proposed to contributors to be validated or rejected. In cas of invalidation, a reconciliation strategy is implemented to identify the cause of the erroneous inference : an exception, an error in the premises, or a confusion caused by polysemy.},
  motscles  = {inférence de relations, réconciliation, enrichissement, réseau lexical, peuplonomie},
  keywords  = {relation inferences, reconcialiation, enrichment, lexical network, crowdsourcing},
}

@inproceedings{wang-EtAl:2013:TALN,
  author    = {Wang, Wei and Besançon, Romaric and Ferret, Olivier and Grau, Brigitte},
  title     = {Regroupement sémantique de relations pour l’extraction d’information non supervisée},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {353--366},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-026},
  language  = {french},
  note      = {Semantic relation clustering for unsupervised information extraction},
  resume    = {Beaucoup des recherches menées en extraction d’information non supervisée se concentrent sur l’extraction des relations et peu de travaux proposent des méthodes pour organiser les relations extraites. Nous présentons dans cet article une méthode de clustering en deux étapes pour regrouper des relations sémantiquement équivalentes : la première étape regroupe des relations proches par leur expression tandis que la seconde fusionne les premiers clusters obtenus sur la base d’une mesure de similarité sémantique. Nos expériences montrent en particulier que les mesures distributionnelles permettent d’obtenir pour cette tâche de meilleurs résultats que les mesures utilisant WordNet. Nous montrons également qu’un clustering à deux niveaux permet non seulement de limiter le nombre de similarités sémantiques à calculer mais aussi d’améliorer la qualité des résultats du clustering.},
  abstract  = {Most studies in unsupervised information extraction concentrate on the relation extraction and few work has been proposed on the organization of the extracted relations. We present in this paper a two-step clustering procedure to group semantically equivalent relations : a first step clusters relations with similar expressions while a second step groups these first clusters into larger semantic clusters, using different semantic similarities. Our experiments show the stability of distributional similarities over WordNet-based similarities for semantic clustering. We also demonstrate that the use of a multi-level clustering not only reduces the calculations from all relation pairs to basic clusters pairs, but it also improves the clustering results.},
  motscles  = {Extraction d’Information Non Supervisée, Similarité Sémantique, Clustering},
  keywords  = {Unsupervised Information Extraction, Semantic Similarity, Relation Clustering},
}

@inproceedings{retore:2013:TALN,
  author    = {Retoré, Christian},
  title     = {Sémantique des déterminants dans un cadre richement typé},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {367--380},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-027},
  language  = {french},
  note      = {On the semantics of determiners in a rich type-theoretical framework},
  resume    = {La variation du sens des mots en contexte nous a conduit à enrichir le système de types utilisés dans notre analyse syntaxico-sémantique du français basé sur les grammaires catégorielles et la sémantique de Montague (ou la lambda-DRT). L’avantage majeur d’une telle sémantique profonde est de représenter le sens par des formules logiques aisément exploitables, par exemple par un moteur d’inférence. Déterminants et quantificateurs jouent un rôle fondamental dans la construction de ces formules, et il nous a fallu leur trouver des termes sémantiques adaptés à ce nouveau cadre. Nous proposons une solution inspirée des opérateurs epsilon et tau de Hilbert, éléments génériques qui s’apparentent à des fonctions de choix. Cette modélisation unifie le traitement des différents types de déterminants et de quantificateurs et autorise le liage dynamique des pronoms. Surtout, cette description calculable des déterminants s’intègre parfaitement à l’analyseur à large échelle du français Grail, tant en théorie qu’en pratique.},
  abstract  = {The variation of word meaning according to the context led us to enrich the type system of our syntactical and semantic analyser of French based on categorial grammars and Montague semantics (or lambda-DRT). The main advantage of a deep semantic analyse is too represent meaning by logical formulae that can be easily used e.g. for inferences. Determiners and quantifiers play a fundamental role in the construction of those formulae and we needed to provide them with semantic terms adapted to this new framework. We propose a solution inspired by the tau and epsilon operators of Hilbert, generic elements that resemble choice functions. This approach unifies the treatment of the different determiners and quantifiers and allows a dynamic binding of pronouns. Above all, this fully computational view of determiners fits in well within the wide coverage parser Grail, both from a theoretical and a practical viewpoint.},
  motscles  = {Analyse sémantique automatique, Sémantique formelle, Compositionnalité},
  keywords  = {Automated semantic analysis, Formal Semantics, Compositional Semantics},
}

@inproceedings{lecluze-EtAl:2013:TALN,
  author    = {Lecluze, Charlotte and Brixtel, Romain and Rigouste, Loïs and Giguet, Emmanuel and Clouard, Régis and Lejeune, Gaël and Constant, Patrick},
  title     = {Détection de zones parallèles à l’intérieur de multi-documents pour l’alignement multilingue},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {381--394},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-028},
  language  = {french},
  note      = {Parallel areas detection in multi-documents for multilingual alignment},
  resume    = {Cet article aborde une question centrale de l’alignement automatique, celle du diagnostic de parallélisme des documents à aligner. Les recherches en la matière se sont jusqu’alors concentrées sur l’analyse de documents parallèles par nature : corpus de textes réglementaires, documents techniques ou phrases isolées. Les phénomènes d’inversions et de suppressions/ajouts pouvant exister entre les différentes versions d’un document sont ainsi souvent ignorées. Nous proposons donc une méthode pour diagnostiquer en contexte des zones parallèles à l’intérieur des documents. Cette méthode permet la détection d’inversions ou de suppressions entre les documents à aligner. Elle repose sur l’affranchissement de la notion de mot et de phrase, ainsi que sur la prise en compte de la Mise en Forme Matérielle du texte (MFM). Sa mise en oeuvre est basée sur des similitudes de répartition de chaînes de caractères répétées dans les différents documents. Ces répartitions sont représentées sous forme de matrices et l’identification des zones parallèles est effectuée à l’aide de méthodes de traitement d’image.},
  abstract  = {This article broaches a central issue of the automatic alignment : diagnosing the parallelism of documents. Previous research was concentrated on the analysis of documents which are parallel by nature such as corpus of regulations, technical documents or simple sentences. Inversions and deletions/additions phenomena that may exist between different versions of a document has often been overlooked. To the contrary, we propose a method to diagnose in context the parallel areas allowing the detection of deletions or inversions between documents to align. This original method is based on the freeing from word and sentence as well as the consideration of the text formatting. The implementation is based on the detection of repeated character strings and the identification of parallel segments by image processing.},
  motscles  = {détection et alignement de zones, appariement de N-grammes de caractères, corpus de multidocuments},
  keywords  = {area detection and alignment, character N-grams matching, multidocuments corpora},
}

@inproceedings{hamdi-EtAl:2013:TALN,
  author    = {Hamdi, Ahmed and Boujelbane, Rahma and Habash, Nizar and Nasr, Alexis},
  title     = {Un système de traduction de verbes entre arabe standard et arabe dialectal par analyse morphologique profonde},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {395--406},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-029},
  language  = {french},
  note      = {Translating verbs between MSA and arabic dialects through deep morphological analysis},
  resume    = {Le développement d’outils de TAL pour les dialectes de l’arabe se heurte à l’absence de ressources pour ces derniers. Comme conséquence d’une situation de diglossie, il existe une variante de l’arabe, l’arabe moderne standard, pour laquelle de nombreuses ressources ont été développées et ont permis de construire des outils de traitement automatique de la langue. Etant donné la proximité des dialectes de l’arabe, le tunisien dans notre cas, avec l’arabe moderne standard, une voie consiste à réaliser une traduction surfacique du dialecte vers l’arabe moderne standard afin de pouvoir utiliser les outils existants pour l’arabe standard. Nous décrivons dans cet article une architecture pour une telle traduction et nous l’évaluons sur les verbes.},
  abstract  = {The developpment of NLP tools for dialects faces the severe problem of lack of resources for such dialects. In the case of diglossia, as in arabic, a variant of arabic, Modern Standard Arabic, exists, for which many resources have been developped which can be used to build NLP tools. Taking advantage of the closeness of MSA and dialects, one way to solve the problem consist in performing a surfacic translation of the dialect into MSA in order to use the tools developped for MSA. We describe in this paper an achitecture for such a translation and we evaluate it on arabic verbs.},
  motscles  = {dialectes, langues peu dotées, analyse morphologique, traitement automatique de l’arabe},
  keywords  = {dialects, Arabic NLP, morphological analysis},
}

@inproceedings{sagot-EtAl:2013:TALN,
  author    = {Sagot, Benoît and Nouvel, Damien and Mouilleron, Virginie and Baranes, Marion},
  title     = {Extension dynamique de lexiques morphologiques pour le français à partir d’un flux textuel},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {407--420},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-030},
  language  = {french},
  note      = {Dynamic extension of a French morphological lexicon based a text stream},
  resume    = {L’incomplétude lexicale est un problème récurrent lorsque l’on cherche à traiter le langage naturel dans sa variabilité. Effectivement, il semble aujourd’hui nécessaire de vérifier et compléter régulièrement les lexiques utilisés par les applications qui analysent d’importants volumes de textes. Ceci est plus particulièrement vrai pour les flux textuels en temps réel. Dans ce contexte, notre article présente des solutions dédiées au traitement des mots inconnus d’un lexique. Nous faisons une étude des néologismes (linguistique et sur corpus) et détaillons la mise en oeuvre de modules d’analyse dédiés à leur détection et à l’inférence d’informations (forme de citation, catégorie et classe flexionnelle) à leur sujet. Nous y montrons que nous sommes en mesure, grâce notamment à des modules d’analyse des dérivés et des composés, de proposer en temps réel des entrées pour ajout aux lexiques avec une bonne précision.},
  abstract  = {Lexical incompleteness is a recurring problem when dealing with natural language and its variability. It seems indeed necessary today to regularly validate and extend lexica used by tools processing large amounts of textual data. This is even more true when processing real-time text flows. In this context, our paper introduces techniques aimed at addressing words unknown to a lexicon. We first study neology (from a theoretic and corpus-based point of view) and describe the modules we have developed for detecting them and inferring information about them (lemma, category, inflectional class). We show that we are able, using among others modules for analyzing derived and compound neologisms, to generate lexical entries candidates in real-time and with a good precision.},
  motscles  = {Néologismes, analyse morphologique, lexiques dynamiques},
  keywords  = {Neologisms, Morphological Analysis, Dynamic Lexica},
}

@inproceedings{nouvel-EtAl:2013:TALN,
  author    = {Nouvel, Damien and Antoine, Jean-Yves and Friburger, Nathalie and Soulet, Arnaud},
  title     = {Fouille de règles d’annotation partielles pour la reconnaissance des entités nommées},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {421--434},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-031},
  language  = {french},
  note      = {Mining Partial Annotation Rules for Named Entity Recognition},
  resume    = {Ces dernières décennies, l’accroissement des volumes de données a rendu disponible une diversité toujours plus importante de types de contenus échangés (texte, image, audio, vidéo, SMS, tweet, données statistiques, spatiales, etc.). En conséquence, de nouvelles problématiques ont vu le jour, dont la recherche d’information au sein de données potentiellement bruitées. Dans cet article, nous nous penchons sur la reconnaissance d’entités nommées au sein de transcriptions (manuelles ou automatiques) d’émissions radiodiffusées et télévisuelles. À cet effet, nous mettons en oeuvre une approche originale par fouille de données afin d’extraire des motifs, que nous nommons règles d’annotation. Au sein d’un modèle, ces règles réalisent l’annotation automatique de transcriptions. Dans le cadre de la campagne d’évaluation Etape, nous mettons à l’épreuve le système implémenté, mXS, étudions les règles extraites et rapportons les performances du système. Il obtient de bonnes performances, en particulier lorsque les transcriptions sont bruitées.},
  abstract  = {During the last decades, the unremitting increase of numeric data available has led to a more and more urgent need for efficient solution of information retrieval (IR). This paper concerns a problematic of first importance for the IR on linguistic data : the recognition of named entities (NE) on speech transcripts issued from radio or TV broadcasts.We present an original approach for named entity recognition which is based on data mining techniques. More precisely, we propose to adapt hierarchical sequence mining techniques to extract automatically from annotated corpora intelligible rules of NE detection. This research was carried out in the framework of the Etape NER evaluation campaign, where mXS, our text-mining based system has shown good performances challenging the best symbolic or data-driven systems},
  motscles  = {Entités nommées, Fouille de données, Règles d’annotation},
  keywords  = {Named Entities, Data Mining, Annotation Rules},
}

@inproceedings{keskes-beanamara-hadrichbelguith:2013:TALN,
  author    = {Keskes, Iskandar and Beanamara, Farah and Hadrich Belguith, Lamia},
  title     = {Segmentation de textes arabes en unités discursives minimales},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {435--449},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-032},
  language  = {french},
  note      = {Segmenting Arabic Texts into Elementary Discourse Units},
  resume    = {La segmentation d’un texte en Unités Discursives Minimales (UDM) a pour but de découper le texte en segments qui ne se chevauchent pas. Ces segments sont ensuite reliés entre eux afin de construire la structure discursive d’un texte. La plupart des approches existantes utilisent une analyse syntaxique extensive. Malheureusement, certaines langues ne disposent pas d'analyseur syntaxique robuste. Dans cet article, nous étudions la faisabilité de la segmentation discursive de textes arabes en nous basant sur une approche d'apprentissage supervisée qui prédit les UDM et les UDM imbriqués. La performance de notre segmentation a été évaluée sur deux genres de corpus : des textes de livres de l’enseignement secondaire et des textes du corpus Arabic Treebank. Nous montrons que la combinaison de traits typographiques, morphologiques et lexicaux permet une bonne reconnaissance des bornes de segments. De plus, nous montrons que l'ajout de traits syntaxiques n’améliore pas les performances de notre segmentation.},
  abstract  = {Discourse segmentation aims at splitting texts into Elementary Discourse Units (EDUs) which are non-overlapping units that serve to build a discourse structure of a document. Current state of the art approaches in discourse segmentation make an extensive use of syntactic information. Unfortunately, some languages do not have any robust parser. In this paper, we investigate the feasibility of Arabic discourse segmentation using a supervised learning approach that predicts nested EDUs. The performance of our segmenter was assessed on two genres of corpora: elementary school textbooks that we build ourselves and documents extracted from the Arabic Treebank. We show that a combination of typographical, morphological and lexical features is sufficient to achieve good results in segment boundaries detection. In addition, we show that adding low-level syntactic features that are manually encoded in ATB does not enhance the performance of our segmenter.},
  motscles  = {Segmentation discursive, unité discursive minimale, langue arabe},
  keywords  = {Discourse segmentation, Elementary discourse units, Arabic language},
}

@inproceedings{lavergne-allauzen-yvon:2013:TALN,
  author    = {Lavergne, Thomas and Allauzen, Alexandre and Yvon, François},
  title     = {Un cadre d’apprentissage intégralement discriminant pour la traduction statistique},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {450--463},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-033},
  language  = {french},
  note      = {A fully discriminative training framework for Statistical Machine Translation},
  resume    = {Une faiblesse des systèmes de traduction statistiques est le caractère ad hoc du processus d’apprentissage, qui repose sur un empilement d’heuristiques et conduit à apprendre des paramètres dont la valeur est sous-optimale. Dans ce travail, nous reformulons la traduction automatique sous la forme familière de l’apprentissage d’un modèle probabiliste structuré utilisant une paramétrisation log-linéaire. Cette entreprise est rendue possible par le développement d’une implantation efficace qui permet en particulier de prendre en compte la présence de variables latentes dans le modèle. Notre approche est comparée, avec succès, avec une approche de l’état de l’art sur la tâche de traduction de données du BTEC pour le couple Français-Anglais.},
  abstract  = {A major pitfall of existing statistical machine translation systems is their lack of a proper training procedure. In fact, the phrase extraction and scoring processes that underlie the construction of the translation model typically rely on a chain of crude heuristics, a situation deemed problematic by many. In this paper, we recast machine translation in the familiar terms of a probabilistic structure learning problem, using a standard log-linear parameterization. The tractability of this enterprise is achieved through an efficient implementation that can take into account all the aspects of the underlying translation process through latent variables. We also address the reference reachability issue by using oracle decoding techniques. This approach is experimentally contrasted with a state-of-the-art system on the French-English BTEC translation task.},
  motscles  = {Traduction Automatique, Apprentissage Discriminant},
  keywords  = {Machine Translation, Discriminative Learning},
}

@inproceedings{ma-levy-nazarenko:2013:TALN,
  author    = {Ma, Yue and Lévy, François and Nazarenko, Adeline},
  title     = {Annotation sémantique pour des domaines spécialisés et des ontologies riches},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {464--478},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-034},
  language  = {french},
  note      = {Semantic Annotation in Specific Domains with rich Ontologies},
  resume    = {Explorer et maintenir une documentation technique est une tâche difficile pour laquelle on pourrait bénéficier d’un outillage efficace, à condition que les documents soient annotés sémantiquement. Les annotations doivent être riches, cohérentes, suffisamment spécialisées et s’appuyer sur un modèle sémantique explicite – habituellement une ontologie – qui modélise la sémantique du domaine cible. Il s’avère que les approches d’annotation traditionnelles donnent pour cette tâche des résultats limités. Nous proposons donc une nouvelle approche, l’annotation sémantique statistique basée sur les syntagmes, qui prédit les annotations sémantiques à partir d’un ensemble d’apprentissage réduit. Cette modélisation facilite l’annotation sémantique spécialisée au regard de modèles sémantiques de domaine arbitrairement riches. Nous l’évaluons à l’aide de plusieurs métriques et sur deux textes décrivant des réglementations métier. Notre approche obtient de bons résultats. En particulier, la F-mesure est de l’ordre de 91, 9% et 97, 6% pour la prédiction de l’étiquette et de la position avec différents paramètres. Cela suggère que les annotateurs humains peuvent être fortement aidés pour l’annotation sémantique dans des domaines spécifiques.},
  abstract  = {Technical documentations are generally difficult to explore and maintain. Powerful tools can help, but they require that the documents have been semantically annotated. The annotations must be sufficiently specialized, rich and consistent. They must rely on some explicit semantic model – usually an ontology – that represents the semantics of the target domain. We observed that traditional approaches have limited success on this task and we propose a novel approach, phrase-based statistical semantic annotation, for predicting semantic annotations from a limited training data set. Such a modeling makes the challenging problem, domain specific semantic annotation regarding arbitrarily rich semantic models, easily handled. Our approach achieved a good performance, with several evaluation metrics and on two different business regulatory texts. In particular, it obtained 91.9% and 97.65% F-measure in the label and position predictions with different settings. This suggests that human annotators can be highly supported in domain specific semantic annotation tasks.},
  motscles  = {Annotation sémantique, Ontologie de domaine, Annotation automatique, Analyse sémantique des textes, Méthodes statistiques},
  keywords  = {Semantic Annotation, Domain Ontology, Automatic annotation, Semantic Text Analysis, Statistical methods},
}

@inproceedings{foucault-rosset-adda:2013:TALN,
  author    = {Foucault, Nicolas and Rosset, Sophie and Adda, Gilles},
  title     = {Pré-segmentation de pages web et sélection de documents pertinents en Questions-Réponses},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {479--492},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-035},
  language  = {french},
  note      = {Web pages segmentation for document selection in Question Answering},
  resume    = {Dans cet article, nous présentons une méthode de segmentation de pages web en blocs de texte pour la sélection de documents pertinents en questions-réponses. La segmentation des documents se fait préalablement à leur indexation en plus du découpage des segments obtenus en passages au moment de l’extraction des réponses. L’extraction du contenu textuel des pages est faite à l’aide d’un extracteur maison. Nous avons testé deux méthodes de segmentation. L’une segmente les textes extraits des pages web uniformément en blocs de taille fixe, l’autre les segmente par TextTiling (Hearst, 1997) en blocs thématiques de taille variable. Les expériences menées sur un corpus de 500K pages web et un jeu de 309 questions factuelles en français, issus du projet Quaero (Quintard et al., 2010), montrent que la méthode employée tend à améliorer la précision globale (top-10) du système RITEL–QR (Rosset et al., 2008) dans sa tâche.},
  abstract  = {In this paper, we study two different kinds of web pages segmentation for document selection in question answering. The segmentation is applied prior to indexation in addition to the traditionnal passage retrieval step in question answering. In both cases, the segmentation is textual and processed once the web pages textual content has been extracted using our own extraction system. In the first case, a document is tilled homogeneously in text blocs of fixed size while in the second case the segmentation is based on the TextTiling algorithm (Hearst, 1997). Evaluation on 309 factoid questions and a collection of 500K French web pages, coming from the Quaero project (Quintard et al., 2010), showed that such approaches tend to support properly the RITEL–QR system (Rosset et al., 2008) in this task.},
  motscles  = {pages web, TextTiling, sélection de documents, questions-réponses, Quaero, Ritel, segmentation textuelle, segmentation thématique},
  keywords  = {web pages, TextTiling, document selection, question answering, Quaero, Ritel, textual segmentation, topic segmentation},
}

@inproceedings{ligozat-EtAl:2013:TALN,
  author    = {Ligozat, Anne-Laure and Grouin, Cyril and Garcia-Fernandez, Anne and Bernhard, Delphine},
  title     = {Approches à base de fréquences pour la simplification lexicale},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {493--506},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-long-036},
  language  = {french},
  note      = {Studying frequency-based approaches to process lexical simplification},
  resume    = {La simplification lexicale consiste à remplacer des mots ou des phrases par leur équivalent plus simple. Dans cet article, nous présentons trois modèles de simplification lexicale, fondés sur différents critères qui font qu’un mot est plus simple à lire et à comprendre qu’un autre. Nous avons testé différentes tailles de contextes autour du mot étudié : absence de contexte avec un modèle fondé sur des fréquences de termes dans un corpus d’anglais simplifié ; quelques mots de contexte au moyen de probabilités à base de n-grammes issus de données du web ; et le contexte étendu avec un modèle fondé sur les fréquences de cooccurrences.},
  abstract  = {Lexical simplification aims at replacing words or phrases by simpler equivalents. In this paper, we present three models for lexical simplification, focusing on the criteria that make one word simpler to read and understand than another. We tested different contexts of the considered word : no context, with a model based on word frequencies in a simplified English corpus ; a few words context, with n-grams probabilites on Web data, and an extended context, with a model based on co-occurrence frequencies.},
  motscles  = {simplification lexicale, fréquence lexicale, modèle de langue},
  keywords  = {lexical simplification, lexical frequency, language model},
}

@inproceedings{boudin:2013:TALN,
  author    = {Boudin, Florian},
  title     = {TALN Archives : une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {507--514},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-001},
  language  = {french},
  note      = {TALN Archives : a digital archive of French research articles in Natural Language Processing},
  resume    = {La recherche scientifique est un processus incrémental. La première étape à effectuer avant de débuter des travaux consiste à réaliser un état de l’art des méthodes existantes. La communauté francophone du Traitement Automatique de la Langue (TAL) produit de nombreuses publications scientifiques qui sont malheureusement dispersées sur différents sites et pour lesquelles aucune méta-donnée n’est disponible. Cet article présente la construction de TALN Archives, une archive numérique francophone des articles de recherche en TAL dont le but est d’offrir un accès simplifié aux différents travaux effectués dans notre domaine. Nous présentons également une analyse du réseau de collaboration construit à partir des méta-données que nous avons extraites et dévoilons l’identité du Kevin Bacon de TALN Archives, i.e. l’auteur le plus central dans le réseau de collaboration.},
  abstract  = {Scientific research is an incremental process. Reviewing the literature is the first step to do before starting a new research project. The French Natural Language Processing (NLP) community produces numerous scientific publications which are scattered across different sources and for which no metadata is available. This paper presents the construction of TALN Archives, a digital archive of French research articles whose aim is to provide efficient access to articles in the NLP field. We also present an analysis of the collaboration network constructed from the metadata and disclose the identity of the Kevin Bacon of the TALN Archives, i.e. the most central author in the collaboration network.},
  motscles  = {TALN Archives, archive numérique, articles scientifiques},
  keywords  = {TALN Archives, digital archive, scientific articles},
}

@inproceedings{marteau-menier:2013:TALN,
  author    = {Marteau, Pierre-Francois and Ménier, Gildas},
  title     = {Similarités induites par mesure de comparabilité : signification et utilité pour le clustering et l’alignement de textes comparables},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {515--522},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-002},
  language  = {french},
  note      = {Similarities induced by a comparability mapping : meaning and utility in the context of the clustering of comparable texts},
  resume    = {En présence de corpus comparables bilingues, nous sommes confrontés à des données qu’il est naturel de plonger dans deux espaces de représentation linguistique distincts, chacun éventuellement muni d’une mesure quantifiable de similarité (ou d’une distance). Dès lors que ces données bilingues sont comparables au sens d’une mesure de comparabilité également calculable (Li et Gaussier, 2010), nous pouvons établir une connexion entre ces deux espaces de représentation linguistique en exploitant une carte d’association pondérée ("mapping") appréhendée sous la forme d’un graphe bi-directionnel dit de comparabilité. Nous abordons dans cet article les conséquences conceptuelles et pratique d’une telle connexion similarité-comparabilité en développant un algorithme (Hit-ComSim) basé sur sur le principe de similarité induite par la topologie du graphe de comparabilité. Nous essayons de qualifier qualitativement l’intérêt de cet algorithme en considérant quelques expériences préliminaires de clustering de documents comparables bilingues (Français/Anglais) collectés sur des flux RSS.},
  abstract  = {In the presence of bilingual comparable corpora it is natural to embed the data in two distinct linguistic representation spaces in which a "computational" notion of similarity is potentially defined. As far as these bilingual data are comparable in the sense of a measure of comparability also computable (Li et Gaussier, 2010), we can establish a connection between these two areas of linguistic representation by exploiting a weighted mapping that can be represented in the form of a weighted bidirectional graph of comparability. We study in this paper the conceptual and practical consequences of such a similarity-comparability connection, while developing an algorithm (Hit-ComSim) based on the concept of similarities induced by the topology of the graph of comparability. We try to evaluate the benefit of this algorithm considering some preliminary categorization or clustering tasks of bilingual (English/French) documents collected from RSS feeds.},
  motscles  = {Graphe de comparabilité, Similarités induites, Documents comparables, Clustering},
  keywords  = {Comparability graph, Induced similarities, Comparable documents, Clustering},
}

@inproceedings{maurel-bouchoumarkhoff:2013:TALN,
  author    = {Maurel, Denis and Bouchou Markhoff, Béatrice},
  title     = {ProLMF version 1.2. Une ressource libre de noms propres avec des expansions contextuelles},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {523--530},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-003},
  language  = {french},
  note      = {ProLMF 1.2, Proper Names with their Expansions},
  resume    = {ProLMF est la version LMF de la base lexicale multilingue de noms propres Prolexbase. Disponible librement sur le site du CNRTL, la version 1.2 a été largement améliorée et augmentée par de nouvelles entrées en français, complétées par des expansions contextuelles, et par de petits lexiques en une huitaine de langues.},
  abstract  = {ProLMF is the LMF version of Prolexbase, a multilingual lexical database of Proper Names. It can be freely downloaded on the CNRTL Website. Version 1.2 had been widely improved and increased, with new French entries whose description includes contextual expansions, and eight small lexica for other languages.},
  motscles  = {ressource libre, base lexicale multilingue, noms propres, expansions contextuelles, schémas de contextualisation, relations sémantiques, alias, point de vue, Prolexbase},
  keywords  = {free resource, multilingual lexical database, Proper Names, context, semantic relations, alias, point of view, Prolexbase},
}

@inproceedings{lecouteux-besacier:2013:TALN,
  author    = {Lecouteux, Benjamin and Besacier, Laurent},
  title     = {Vers un décodage guidé pour la traduction automatique},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {531--538},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-004},
  language  = {french},
  note      = {Driven Decoding for machine translation},
  resume    = {Récemment, le paradigme du décodage guidé a montré un fort potentiel dans le cadre de la reconnaissance automatique de la parole. Le principe est de guider le processus de décodage via l’utilisation de transcriptions auxiliaires. Ce paradigme appliqué à la traduction automatique permet d’envisager de nombreuses applications telles que la combinaison de systèmes, la traduction multi-sources etc. Cet article présente une approche préliminaire de l’application de ce paradigme à la traduction automatique (TA). Nous proposons d’enrichir le modèle log-linéaire d’un système primaire de TA avec des mesures de distance relatives à des systèmes de TA auxiliaires. Les premiers résultats obtenus sur la tâche de traduction Français/Anglais issue de la campagne d’évaluation WMT 2011 montrent le potentiel du décodage guidé.},
  abstract  = {Recently, the concept of driven decoding (DD), has been sucessfully applied to the automatic speech recognition (speech-to-text) task : an auxiliary transcription guide the decoding process. There is a strong interest in applying this concept to statistical machine translation (SMT). This paper presents our approach on this topic. Our first attempt in driven decoding consists in adding several feature functions corresponding to the distance between the current hypothesis decoded and the auxiliary translations available. Experimental results done for a french-to-english machine translation task, in the framework of the WMT 2011 evaluation, show the potential of the DD approach proposed.},
  motscles  = {Décodage guidé, traduction automatique, combinaison de systèmes},
  keywords  = {Driven Decoding, machine translation, system combination},
}

@inproceedings{gerlach-EtAl:2013:TALN,
  author    = {Gerlach, Johanna and Porro, Victoria and Bouillon, Pierrette and Lehmann, Sabine},
  title     = {La La préédition avec des règles peu coûteuses, utile pour la TA statistique des forums ?},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {539--546},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-005},
  language  = {french},
  note      = {Can lightweight pre-editing rules improve statistical MT of forum content?},
  resume    = {Cet article s’intéresse à la traduction automatique statistique des forums, dans le cadre du projet européen ACCEPT (« Automated Community Content Editing Portal »). Nous montrons qu’il est possible d’écrire des règles de préédition peu coûteuses sur le plan des ressources linguistiques et applicables sans trop d’effort avec un impact très significatif sur la traduction automatique (TA) statistique, sans avoir à modifier le système de TA. Nous décrivons la méthodologie proposée pour écrire les règles de préédition et les évaluer, ainsi que les résultats obtenus par type de règles.},
  abstract  = {This paper focuses on the statistical machine translation (SMT) of forums within the context of the European Framework ACCEPT («Automated Community Content Editing Portal») project. We demonstrate that it is possible to write lightweight pre-editing rules that require few linguistic resources, are relatively easy to apply and have significant impact on SMT without any changes to the machine translation system. We describe methodologies for rule development and evaluation, and provide results obtained for different rule types.},
  motscles  = {préédition, langage contrôlé, traduction statistique, forums},
  keywords  = {pre-edition, controlled language, statistical machine translation, forums},
}

@inproceedings{hamon-gibet-boustila:2013:TALN,
  author    = {Hamon, Ludovic and Gibet, Sylvie and Boustila, Sabah},
  title     = {Édition interactive d’énoncés en langue des signes française dédiée aux avatars signeurs},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {547--554},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-006},
  language  = {french},
  note      = {Interactive editing of utterances in French sign language dedicated to signing avatars},
  resume    = {Les avatars signeurs en Langue des Signes Française (LSF) sont de plus en plus utilisés en tant qu’interface de communication à destination de la communauté sourde. L’un des critères d’acceptation de ces avatars est l’aspect naturel et réaliste des gestes produits. Par conséquent, des méthodes de synthèse de gestes ont été élaborées à l’aide de corpus de mouvements capturés et annotés provenant d’un signeur réel. Néanmoins, l’enrichissement d’un tel corpus, en faisant fi des séances de captures supplémentaires, demeure une problématique certaine. De plus, l’application automatique d’opérations sur ces mouvements (e.g. concaténation, mélange, etc.) ne garantit pas la consistance sémantique du geste résultant. Une alternative est d’insérer l’opérateur humain dans la boucle de construction des énoncés en LSF. Dans cette optique, cet article propose un premier système interactif d’édition de gestes en LSF, basé "données capturées" et dédié aux avatars signeurs.},
  abstract  = {Signing avatars dedicated to French Sign Language (LSF) are more and more used as a communication interface for the deaf community. One of the acceptation criteria of these avatars is the natural and realistic aspect of the constructed gestures. Consequently, gestures synthesis methods have been designed thanks to some corpus of captured and annotated motions, performed by a real signer. However, the enlarging of such a corpus, without requiring of some additional capture sessions, is a major issue. Furthermore, the automatic application of motion transformations (e.g. concatenation, blending, etc.) does not guarantee the semantic consistency of the resulting gesture. Another option is to insert the human operator in the utterance building loop. In this context, this paper provides a first interactive editing system of FSL gestures, based on captured motions and dedicated to signing avatars.},
  motscles  = {Langue des Signes Française, édition, geste, base de données sémantiques, signeur virtuel, interaction},
  keywords  = {French sign language, editing, gesture, semantic data base, virtual signer, interaction},
}

@inproceedings{muzerelle-EtAl:2013:TALN,
  author    = {Muzerelle, Judith and Lefeuvre, Anaïs and Antoine, Jean-Yves and Schang, Emmanuel and Maurel, Denis and Villaneau, Jeanne and Eshkol, Iris},
  title     = {ANCOR, premier corpus de français parlé d’envergure annoté en coréférence et distribué librement},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {555--563},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-007},
  language  = {french},
  note      = {ANCOR, the first large French speaking corpus of conversational speech annotated in coreference to be freely available},
  resume    = {Cet article présente la réalisation d’ANCOR, qui constitue par son envergure (453 000 mots) le premier corpus francophone annoté en anaphores et coréférences permettant le développement d’approches centrées sur les données pour la résolution des anaphores et autres traitements de la coréférence. L’annotation a été réalisée sur trois corpus de parole conversationnelle (Accueil_UBS, OTG et ESLO) qui le destinent plus particulièrement au traitement du langage parlé. En l’absence d’équivalent pour le langage écrit, il est toutefois susceptible d’intéresser l’ensemble de la communauté TAL. Par ailleurs, le schéma d’annotation retenu est suffisamment riche pour permettre des études en linguistique de corpus. Le corpus sera diffusé librement à la mi-2013 sous licence Creative Commons BY-NC-SA. Cet article se concentre sur sa mise en oeuvre et décrit brièvement quelques résultats obtenus sur la partie déjà annotée de la ressource.},
  abstract  = {This paper presents the first French spoken corpus annotated in coreference whose size (453,000 words) is sufficient to investigate the achievement of data oriented systems of coreference resolution. The annotation was conducted on three different corpora of conversational speech (Accueil_UBS, OTG, ESLO) but this resource can also be interesting for NLP researchers working on written language, considering the lack of a large written French corpus annotated in coreference. We followed a rich annotation scheme which enables also research motivated by linguistic considerations. This corpus will be freely available (Creative Commons BY-NC-SA) around mid-2013. The paper details the achievement of the resource as well as preliminary experiments conducted on the part of the corpus already annotated.},
  motscles  = {Corpus, annotation, coréférence, anaphore, parole conversationnelle},
  keywords  = {Corpus, annotation, coreference, anaphora, conversational speech},
}

@inproceedings{loginovaclouet-daille:2013:TALN,
  author    = {Loginova-Clouet, Elizaveta and Daille, Béatrice},
  title     = {Segmentation Multilingue des Mots Composés},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {564--571},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-008},
  language  = {french},
  note      = {Multilingual Compound Splitting},
  resume    = {La composition est un phénomène fréquent dans plusieurs langues, surtout dans des langues ayant une morphologie riche. Le traitement des mots composés est un défi pour les systèmes de TAL car pour la plupart, ils ne sont pas présents dans les lexiques. Dans cet article, nous présentons une méthode de segmentation des composés qui combine des caractéristiques indépendantes de la langue (mesure de similarité, données du corpus) avec des règles de transformation sur les frontières des composants spécifiques à une langue. Nos expériences de segmentation de termes composés allemands et russes montrent une exactitude jusqu’à 95 % pour l’allemand et jusqu’à 91 % pour le russe. Nous constatons que l’utilisation de corpus spécialisés relevant du même domaine que les composés améliore la qualité de segmentation.},
  abstract  = {Compounding is a common phenomenon for many languages, especially those with a rich morphology. Dealing with compounds is a challenge for natural language processing systems since all compounds can not be included in lexicons. In this paper, we present a compound splitting method combining language independent features (similarity measure, corpus data) and language dependent features (component transformation rules). We report on our experiments in splitting of German and Russian compound terms giving accuracy up to 95% for German and up to 91% for Russian language. We observe that the usage of a corpus of the same domain as compounds improves splitting quality.},
  motscles  = {segmentation des mots composés, outil multilingue, mesure de similarité, règles de transformation des composants, corpus spécialisés},
  keywords  = {compound splitting, multilingual tool, similarity measure, component transformation rules, specialized corpora},
}

@inproceedings{zhang-mangeot:2013:TALN,
  author    = {Zhang, Ying and Mangeot, Mathieu},
  title     = {Gestion des terminologies riches : l'exemple des acronymes},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {572--579},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-009},
  language  = {french},
  note      = {Complex terminologies management - the case of acronyms},
  resume    = {La gestion des terminologies pose encore des problèmes, en particulier pour des constructions complexes comme les acronymes. Dans cet article, nous proposons une solution en reliant plusieurs termes différents à un seul référent via les notions de pivot et de prolexème. Ces notions permettent par exemple de faire le lien entre plusieurs termes qui désignent un même et unique référent : Nations Unies, ONU, Organisation des Nations Unies et onusien. Il existe Jibiki, une plate-forme générique de gestion de bases lexicales permettant de gérer n'importe quel type de structure (macro et microstructure). Nous avons implémenté une nouvelle macrostructure de ProAxie dans la plate-forme Jibiki pour réaliser la gestion des acronymes.},
  abstract  = {Terminology management is still problematic, especially for complex constructions such as acronyms. In this paper, we propose a solution to connect several different terms with a single referent through using the concepts of pivot and prolexeme. These concepts allow for example to link several terms for the same referent: Nations Unies, ONU, Organisation des Nations Unies and onusien. Jibiki is a generic platform for lexical database management, allowing the representation of any type of structure (macro and microstructure). We have implemented a new macrostructure ProAxie in the Jibiki platform to achieve acronym management.},
  motscles  = {base lexicale multilingue, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolèxeme},
  keywords  = {multilingual lexical database, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolexeme},
}

@inproceedings{zampieri-gebre-diwersy:2013:TALN,
  author    = {Zampieri, Marcos and Gebre, Binyam Gebrekidan and Diwersy, Sascha},
  title     = {Ngrammes et Traits Morphosyntaxiques pour la Identification de Variétés de l’Espagnol},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {580--587},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-010},
  language  = {french},
  note      = {N-gram Language Models and POS Distribution for the Identification of Spanish Varieties},
  resume    = {Notre article présente expérimentations portant sur la classification supervisée de variétés nationales de l’espagnol. Outre les approches classiques, basées sur l’utilisation de ngrammes de caractères ou de mots, nous avons testé des modèles calculés selon des traits morphosyntaxiques, l’objectif étant de vérifier dans quelle mesure il est possible de parvenir à une classification automatique des variétés d’une langue en s’appuyant uniquement sur des descripteurs grammaticaux. Les calculs ont été effectués sur la base d’un corpus de textes journalistiques de quatre pays hispanophones (Espagne, Argentine, Mexique et Pérou).},
  abstract  = {This article presents supervised computational methods for the identification of Spanish varieties. The features used for this task were the classical character and word n-gram language models as well as POS and morphological information. The use of these features is to our knowledge new and we aim to explore the extent to which it is possible to identify language varieties solely based on grammatical differences. Four journalistic corpora from different countries were used in these experiments : Spain, Argentina, Mexico and Peru.},
  motscles  = {classification automatique, ngrammes, espagnol, variétés nationales},
  keywords  = {automatic classification, n-grams, Spanish, language varieties},
}

@inproceedings{fraisse-paroubek-francopoulo:2013:TALN,
  author    = {Fraisse, Amel and Paroubek, Patrick and Francopoulo, Gil},
  title     = {L’apport des Entités Nommées pour la classification des opinions minoritaires},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {588--595},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-011},
  language  = {french},
  note      = {Improving Minor Opinion Polarity Classification with Named Entity Analysis},
  resume    = {La majeure partie des travaux en fouille d’opinion et en analyse de sentiment concerne le classement des opinions majoritaires. Les méthodes d’apprentissage supervisé à base de ngrammes sont souvent employées. Elles ont l’inconvénient d’avoir un biais en faveur des opinions majoritaires si on les utilise de manière classique. En fait la présence d’un terme particulier, fortement associé à la cible de l’opinion dans un document peut parfois suffire à faire basculer le classement de ce document dans la classe de ceux qui expriment une opinion majoritaire sur la cible. C’est un phénomène positif pour l’exactitude globale du classifieur, mais les documents exprimant des opinions minoritaires sont souvent mal classés. Ce point est un problème dans le cas où l’on s’intéresse à la détection des signaux faibles (détection de rumeur) ou pour l’anticipation de renversement de tendance. Nous proposons dans cet article d’améliorer la classification des opinions minoritaires en prenant en compte les Entités Nommées dans le calcul de pondération destiné à corriger le biais en faveur des opinions majoritaires.},
  abstract  = {The main part of the work on opinion mining and sentiment analysis concerns polarity classification of majority opinions. Supervised machine learning with n-gram features is a common approach to polarity classification, which is often biased towards the majority of opinions about a given opinion target, when using this kind of approach with traditional settings. The presence of a specific term, strongly associated to the opinion target in a document, is often enough to tip the classifier decision toward the majority opinion class. This is actually a good thing for overall accuracy. Howeverm documents about the opinion taget, but expressing a polarity different from the majority one, get misclassified. It is a problem if we want to detect weak signals (rumor detection) or for anticipating opinion reversal trends. We propose in this paper to improve minor reviews polarity classification by taking into account Named Entity information in the computation of specific weighting scheme used for correcting the bias toward majority opinions.},
  motscles  = {Fouille d’opinions, Opinion minoritaires, Entités Nommées, Apprentissage, N-grammes, Pondération},
  keywords  = {Opinion Mining, Minor Opinion, Named Entities, Machine Learning, N-grams, Weighting Scheme},
}

@inproceedings{belenguix-zock:2013:TALN,
  author    = {Bel-Enguix, Gemma and Zock, Michael},
  title     = {Trouver les mots dans un simple réseau de co-occurrences},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {596--603},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-012},
  language  = {french},
  note      = {Lexical access via a simple co-occurrence network},
  resume    = {Au cours des deux dernières décennies des psychologues et des linguistes informaticiens ont essayé de modéliser l'accès lexical en construisant des simulations ou des ressources. Cependant, parmi ces chercheurs, pratiquement personne n'a vraiment cherché à améliorer la navigation dans des 'dictionnaires électroniques destinés aux producteurs de langue'. Pourtant, beaucoup de travaux ont été consacrés à l'étude du phénomène du mot sur le bout de la langue et à la construction de réseaux lexicaux. Par ailleurs, vu les progrès réalisés en neurosciences et dans le domaine des réseaux complexes, on pourrait être tenté de construire un simulacre du dictionnaire mental, ou, à défaut une ressource destinée aux producteurs de langue (écrivains, conférenciers). Nous sommes restreints en construisant un réseau de co-occurrences à partir des résumés de Wikipedia, le but étant de vérifier jusqu'où l'on pouvait pousser une telle ressource pour trouver un mot, sachant que la ressource ne contient pas de liens sémantiques, car le réseau est construit de manière automatique et à partir de textes non-annotés.},
  abstract  = {During the last two decades psychologists and computational linguists have attempted to tackle the problem of word access via computational resources, yet hardly none of them has seriously tried to support 'interactive' word finding. Yet, a lot of work has been done to understand the causes of the tip-of-the-tongue problem (TOT). Given the progress made in neuroscience, corpus linguistics, and graph theory (complex graphs), one may be tempted to emulate the mental lexicon, or to build a resource likely to help authors (speakers, writers) to overcome word-finding problems. Our goal here is much more limited. We try to identify good hints for finding a target word. To this end we have built a co-occurrence network on the basis of Wikipedia abstracts. Since the network is built automatically and from raw data, i.e. non-annotated text, it does not reveal the kind of relationship holding between the nodes. Despite this shortcoming we tried to see whether we can find a given word, or, to identify what is a good clue word.},
  motscles  = {accès lexical, anomie, mot sur le bout de la langue, réseaux lexicaux},
  keywords  = {lexical access, anomia, tip of the tongue (TOT), lexical networks},
}

@inproceedings{perrier:2013:TALN,
  author    = {Perrier, Guy},
  title     = {Analyse statique des interactions entre structures élémentaires d’une grammaire},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {604--611},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-013},
  language  = {french},
  note      = {Static Analysis of Interactions between Elementary Structures of a Grammar},
  resume    = {Nous nous intéressons ici à la construction semi-automatique de grammaires computationnelles et à leur utilisation pour l’analyse syntaxique. Nous considérons des grammaires lexicalisées dont les structures élémentaires sont des arbres, sous-spécifiés ou pas. Nous présentons un algorithme qui vise à prévoir l’ensemble des arbres élémentaires attachés aux mots qui peuvent s’intercaler entre deux mots donnés d’une phrase, dont on sait que les arbres élémentaires associées sont des compagnons, c’est-à-dire qu’ils interagiront nécessairement dans la composition syntaxique de la phrase.},
  abstract  = {We are interested in the semi-automatic construction of computational grammars and in their use for parsing. We consider lexicalized grammars with elementary structures which are trees, underspecified or not. We present an algorithm that aims at foreseeing all elementary trees attached at words which can come between two given words of a sentence, whose associated elementary trees are companions, that is, they will necessarily interact in the syntactic composition of the sentence.},
  motscles  = {grammaire lexicalisée, grammaire d’interaction, construction de grammaires},
  keywords  = {Lexicalized Grammar, Interaction Grammar, Grammar Construction},
}

@inproceedings{charton-gagnon-jeanlouis:2013:TALN,
  author    = {Charton, Eric and Gagnon, Michel and Jean-Louis, Ludovic},
  title     = {Influence des annotations sémantiques sur un système de détection de coréférence à base de perceptron multi-couches},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {612--619},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-014},
  language  = {french},
  note      = {Semantic annotation influence on coreference detection using perceptron approach},
  resume    = {La série de campagnes d’évaluation CoNLL-2011/2012 a permis de comparer diverses propositions d’architectures de systèmes de détection de co-références. Cet article décrit le système de résolution de coréférence Poly-co développé dans le cadre de la campagne d’évaluation CoNLL-2011 et évalue son potentiel d’amélioration en introduisant des propriétés sémantiques dans son modèle de détection. Notre système s’appuie sur un classifieur perceptron multi-couches. Nous décrivons les heuristiques utilisées pour la sélection des paires de mentions candidates, ainsi que l’approche de sélection des traits caractéristiques que nous avons utilisée lors de la campagne CoNLL-2011. Nous introduisons ensuite un trait sémantique complémentaire et évaluons son influence sur les performances du système.},
  abstract  = {The ConLL-2011/2012 evaluation campaign was dedicated to coreference detection systems. This paper presents the coreference resolution system Poly-co submitted to the closed track of the CoNLL-2011 Shared Task and evaluate is potential of evolution when it includes a semantic feature. Our system integrates a multilayer perceptron classifier in a pipeline approach. We describe the heuristic used to select the candidate coreference pairs that are fed to the network for training, and our feature selection method. We introduce a complementary semantic feature and evaluate the performances improvement.},
  motscles  = {Coréférence, Perceptron multi-couches},
  keywords  = {Coreference, Multilayer perceptron},
}

@inproceedings{sadat-mohamed:2013:TALN,
  author    = {Sadat, Fatiha and Mohamed, Emad},
  title     = {Traduction automatique statistique pour l’arabe-français améliorée par le prétraitement et l’analyse de la langue},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {620--627},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-015},
  language  = {french},
  note      = {Pre-processing and Language Analysis for Arabic to French Statistical Machine Translation},
  resume    = {Dans cet article, nous nous intéressons au prétraitement de la langue arabe comme langue source à des fins de traduction automatique statistique. Nous présentons une étude sur la traduction automatique statistique basée sur les syntagmes, pour la paire de langues arabe-français utilisant le décodeur Moses ainsi que d’autres outils de base. Les propriétés morphologiques et syntaxiques de la langue arabe sont complexes, ce qui rend cette langue difficile à maîtriser dans le domaine du TALN. Aussi, les performances d’un système de traduction statistique dépendent considérablement de la quantité et de la qualité des corpus d’apprentissage. Dans cette étude, nous montrons qu’un prétraitement basé sur les mots de la langue source (arabe) et l’introduction de quelques règles linguistiques par rapport à la syntaxe de la langue cible (français), permet d’obtenir des améliorations du score BLEU. Cette amélioration est réalisée sans augmenter la quantité des corpus d’apprentissage.},
  abstract  = {Arabic is a morphologically rich and complex language, which presents significant challenges for natural language processing and machine translation. In this paper, we describe an ongoing effort to build a competitive Arabic-French phrase–based machine translation system using the Moses decoder and other tools. The results show an increase in terms of BLEU score after introducing some pre-processing schemes for Arabic and applying additional language analysis rules in relation to the target language. The proposed approach is completed using pre-processing and language analysis rules without increasing the amount of training data.},
  motscles  = {Traduction automatique statistique, traduction arabe-français, pré-traitement de corpus, morphologie de l’Arabe},
  keywords  = {Statistical machine translation, Arabic-French translation, Corpus pre-processing, Arabic morphology},
}

@inproceedings{guillaume-fort:2013:TALN,
  author    = {Guillaume, Bruno and Fort, Karën},
  title     = {Expériences de formalisation d’un guide d’annotation : vers l’annotation agile assistée},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {628--635},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-016},
  language  = {french},
  note      = {Formalizing an annotation guide : some experiments towards assisted agile annotation},
  resume    = {Nous proposons dans cet article une méthodologie, qui s’inspire du développement agile et qui permettrait d’assister la préparation d’une campagne d’annotation. Le principe consiste à formaliser au maximum les instructions contenues dans le guide d’annotation afin de vérifier automatiquement si le corpus en construction est cohérent avec le guide en cours d’écriture. Pour exprimer la partie formelle du guide, nous utilisons la réécriture de graphes, qui permet de décrire par des motifs les constructions définies. Cette formalisation permet de repérer les constructions prévues par le guide et, par contraste, celles qui ne sont pas cohérentes avec le guide. En cas d’incohérence, un expert peut soit corriger l’annotation, soit mettre à jour le guide et relancer le processus.},
  abstract  = {This article presents a methodology, inspired from the agile development paradigm, that helps preparing an annotation campaign. The idea behind the methodology is to formalize as much as possible the instructions given in the guidelines, in order to automatically check the consistency of the corpus being annotated with the guidelines, as they are being written. To formalize the guidelines, we use a graph rewriting tool, that allows us to use a rich language to describe the instructions. This formalization allows us to spot the rightfully annotated constructions and, by contrast, those that are not consistent with the guidelines. In case of inconsistency, an expert can either correct the annotation or update the guidelines and rerun the process.},
  motscles  = {annotation, guide d’annotation, annotation agile, réécriture de graphes},
  keywords  = {annotation, annotation guide, agile annotation, graph rewriting},
}

@inproceedings{domingues-eshkoltaravella:2013:TALN,
  author    = {Dominguès, Catherine and Eshkol-Taravella, Iris},
  title     = {Repérer des toponymes dans des titres de cartes topographiques},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {636--642},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-017},
  language  = {french},
  note      = {Localizing toponyms in topographic map titles},
  resume    = {Les titres de cartes topographiques personnalisées composent un corpus spécifique caractérisé par des variations orthographiques et un nombre élevé de désignations de lieux. L'article présente le repérage des toponymes dans ces titres. Ce repérage est fondé sur l'utilisation de BDNyme, la base de données de toponymes géoréférencés de l'IGN, et sur une analyse de surface à l'aide de patrons. La méthode proposée élargit la définition du toponyme pour tenir compte de la nature du corpus et des données qu'il contient. Elle se décompose en trois étapes successives qui tirent parti du contexte extralinguistique de géoréférencement des toponymes et du contexte linguistique. Une quatrième étape qui ne retient pas le géoréférencement est aussi étudiée. Le balisage et le typage des toponymes permettent de mettre en avant d'une part la diversité des désignations de lieux et d'autre part leurs variations d'écriture. La méthode est évaluée (rappel, précision, F-mesure) et les erreurs analysées.},
  abstract  = {The titles of customized topographic maps constitute a specific corpus which is characterized by spelling variations and a very significant number of place names. This paper is about identifying toponyms in these titles. The toponym tracking is based on IGN's toponym data base as well as light parsing according to patterns. The method used broadens the definition of the toponym to include the nature of the corpus and the data in it. It consists of three successive stages where both the extralinguistic context - in this case georeferencing toponyms - and the linguistic context are taken into account. The fourth stage which is without georeferencing is examined too. Toponym tagging and typing allow to highlight toponym naming and spelling variations. The method has been assessed (recall, precision, F-measure) and the results analysed.},
  motscles  = {toponyme, information spatiale, écriture des toponymes, BDNyme, ressource lexicale},
  keywords  = {toponyme, spatial information, toponyme writing, BDNyme, lexical resource},
}

@inproceedings{zweigenbaum-tannier:2013:TALN,
  author    = {Zweigenbaum, Pierre and Tannier, Xavier},
  title     = {Extraction des relations temporelles entre événements médicaux dans des comptes rendus hospitaliers},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {643--650},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-018},
  language  = {french},
  note      = {Extraction of temporal relations between clinical events in clinical documents},
  resume    = {Le défi i2b2/VA 2012 était dédié à la détection de relations temporelles entre événements et expressions temporelles dans des comptes rendus hospitaliers en anglais. Les situations considérées étaient beaucoup plus variées que dans les défis TempEval. Nous avons donc axé notre travail sur un examen systématique de 57 situations différentes et de leur importance dans le corpus d’apprentissage en utilisant un oracle, et avons déterminé empiriquement le classifieur qui se comportait le mieux dans chaque situation, atteignant ainsi une F-mesure globale de 0,623.},
  abstract  = {The 2012 i2b2/VA challenge focused on the detection of temporal relations between events and temporal expressions in English clinical texts. The addressed situations were much more diverse than in the TempEval challenges. We thus focused on the systematic study of 57 distinct situations and their importance in the training corpus by using an oracle, and empirically determined the best performing classifier for each situation, thereby achieving a 0.623 F-measure.},
  motscles  = {extraction d’information, événements médicaux, relations temporelles, médecine},
  keywords  = {Information Extraction, Clinical Events, Temporal Relations, Medicine},
}

@inproceedings{tulechki-tanguy:2013:TALN,
  author    = {Tulechki, Nikola and Tanguy, Ludovic},
  title     = {Similarité de second ordre pour l’exploration de bases textuelles multilingues},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {651--658},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-019},
  language  = {french},
  note      = {Second order similarity for exploring multilingual textual databases},
  resume    = {Cet article décrit l’utilisation de la technique de similarité de second ordre pour l’identification de textes semblables au sein d’une base de rapports d’incidents aéronautiques mélangeant les langues française et anglaise. L’objectif du système est, pour un document donné, de retrouver des documents au contenu similaire quelle que soit leur langue. Nous utilisons un corpus bilingue aligné de rapports d’accidents aéronautiques pour construire des paires de pivots et indexons les documents avec des vecteurs de similarités, tels que chaque coordonnée correspond au score de similarité entre un document dans une langue donnée et la partie du pivot de la même langue. Nous évaluons les performances du système sur un volumineux corpus de rapports d’incidents aéronautiques pour lesquels nous disposons de traductions. Les résultats sont prometteurs et valident la technique.},
  abstract  = {This paper describes the use of second order similarities for identifying similar texts inside a corpus of aviation incident reports written in both French and English. We use a second bilingual corpus to construct pairs of reference documents and map each target document to a vector so each coordinate represents a similarity score between this document and the part of the reference corpus written in the same language. We evaluate the system using a large corpus of translated incident reports. The results are promising and validate the approach.},
  motscles  = {similarité de second ordre, multilingue, ESA},
  keywords  = {second order similarity, multilingual, ESA},
}

@inproceedings{chaumartin:2013:TALN,
  author    = {Chaumartin, François-Régis},
  title     = {Apprentissage d’une classification thématique générique et cross-langue à partir des catégories de la Wikipédia},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {659--666},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-020},
  language  = {french},
  note      = {Cross-lingual and generic text categorization},
  resume    = {La catégorisation de textes nécessite généralement un investissement important en amont, avec une adaptation de domaine. L’approche que nous proposons ici permet d’associer finement à un texte tout-venant écrit dans une langue donnée, un graphe de catégories de la Wikipédia dans cette langue. L’utilisation de l’index inter-langues de l’encyclopédie en ligne permet de plus d’obtenir un sous-ensemble de ce graphe dans la plupart des autres langues.},
  abstract  = {Text categorization usually requires a significant investment, which must often be associated to a field adaptation. The approach we propose here allows to finely associate a graph of Wikipedia categories to any text written in a given language. Moreover, the inter-lingual index of the online encyclopedia allows to get a subset of this graph in most other languages.},
  motscles  = {catégorisation, apprentissage, recherche d’information, Wikipédia, graphes},
  keywords  = {categorization, machine learning, information retrieval, Wikipedia, graphs},
}

@inproceedings{okinina-EtAl:2013:TALN,
  author    = {Okinina, Nadia and Nouvel, Damien and Friburger, Nathalie and Antoine, Jean-Yves},
  title     = {Apprentissage supervisé sur ressources encyclopédiques pour l’enrichissement d’un lexique de noms propres destiné à la reconnaissance des entités nommées},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {667--674},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-021},
  language  = {french},
  note      = {Supervised learning on encyclopaedic resources for the extension of a lexicon of proper names dedicated to the recognition of named entities},
  resume    = {Cet article présente une méthode hybride d’enrichissement d’un lexique de noms propres à partir de la base encyclopédique en ligne Wikipedia. Une des particularités de cette recherche est de viser l’enrichissement d’une ressource existante (Prolexbase) très contrôlée décrivant finement les noms propres. A la différence d’autres travaux destinés à la reconnaissance des entités nommées, notre objectif est donc de réaliser un enrichissement automatique de qualité. Notre approche repose sur l’utilisation en pipe-line de règles déterministes basées sur certaines informations DBpedia et d’une catégorisation supervisée à base de classifieur SVM. Nos résultats montrent qu’il est ainsi possible d’enrichir un lexique de noms propres avec une très bonne précision.},
  abstract  = {This paper concerns the automatic extension of a lexicon of proper names by means of a hybrid mining of Wikipedia. The specificity of this research is to focus on the quality of the added lexical entries, since the mining process is supposed to extend a controlled existing resource (Prolexbase). Our approach consists in the successive application of deterministic rules based on some specific information of the DBpedia and of a supervised classification with a SVM classifier. Our experiments show that it is possible to extend automatically such a lexicon without adding a perceptible noise to the resource.},
  motscles  = {reconnaissance des entités nommées, lexique de nom propre, enrichissement automatique de lexique, Wikipedia, règles, classification supervisée, machine à vecteurs de support, SVM},
  keywords  = {named entities recognition, proper names lexicon, automatic extension of lexicon, Wikipedia, rules, supervised classification, support vector machines, SVM},
}

@inproceedings{paroubek-asadullah-vilnat:2013:TALN,
  author    = {Paroubek, Patrick and Asadullah, Munshi and Vilnat, Anne},
  title     = {Convertir des analyses syntaxiques en dépendances vers les relations fonctionnelles PASSAGE},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {675--682},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-022},
  language  = {french},
  note      = {Converting dependencies for syntactic analysis of French into PASSAGE functional relations},
  resume    = {Nous présentons ici les premiers travaux concernant l’établissement d’une passerelle bidirectionnelle entre d’une, part les schémas d’annotation syntaxique en dépendances qui ont été définis pour convertir les annotations du French Treebank en arbres de dépendances de surface pour l’analyseur syntaxique Bonsai, et d’autre part le formalisme d’annotation PASSAGE développé initialement pour servir de support à des campagnes d’évaluation ouvertes en mode objectif quantitatif boîte-noire pour l’analyse syntaxique du français.},
  abstract  = {We present here a first attempt at building a bidrictionnal converter between, on the one hand the dependency based syntaxtic formalism which has been defined to map the French Treebank annotation onto surface dependency trees used by the Bonsai parser, on the other hand the PASSAGE formalism developped intially for French parsing quantitative black-box objective open evaluation campaigns.},
  motscles  = {Analyse Syntaxique, Corpus arboré, Dependances, DepFTB, ConLL, PASSAGE},
  keywords  = {Parsing, Treebank, Dependencies, DepFTB, ConLL, PASSAGE},
}

@inproceedings{loaiciga:2013:TALN,
  author    = {Loáiciga, Sharid},
  title     = {Résolution d’anaphores et traitement des pronoms en traduction automatique à base de règles},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {683--690},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-023},
  language  = {french},
  note      = {Anaphora Resolution for Machine Translation},
  resume    = {La traduction des pronoms est l’un des problèmes actuels majeurs en traduction automatique. Étant donné que les pronoms ne transmettent pas assez de contenu sémantique en euxmêmes, leur traitement automatique implique la résolution des anaphores. La recherche en résolution des anaphores s’intéresse à établir le lien entre les entités sans contenu lexical (potentiellement des syntagmes nominaux et pronoms) et leurs référents dans le texte. Dans cet article, nous mettons en oeuvre un premier prototype d’une méthode inspirée de la théorie du liage chomskyenne pour l’interprétation des pronoms dans le but d’améliorer la traduction des pronoms personnels entre l’espagnol et le français.},
  abstract  = {Pronoun translation is one of the current problems within Machine Translation. Since pronouns do not convey enough semantic content by themselves, pronoun processing requires anaphora resolution. Research in anaphora resolution is interested in establishing the link between entities (NPs and pronouns) and their antecedents in the text. In this article, we implement a prototype of a linguistic anaphora resolution method inspired from the Chomskyan Binding Theory in order to improve the translation of personal pronouns between Spanish and French.},
  motscles  = {Résolution d’anaphores, traduction automatique à base de règles, sujets nuls},
  keywords  = {Anaphora Resolution, Rule-based Machine Translation, nul subjects},
}

@inproceedings{cailliau-EtAl:2013:TALN,
  author    = {Cailliau, Frederik and Cavet, Ariane and De Groc, Clément and De Loupy, Claude},
  title     = {Lexiques de corpus comparables et recherche d’information multilingue},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {691--698},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-024},
  language  = {french},
  note      = {Lexicons from Comparable Corpora for Multilingual Information Retrieval},
  resume    = {Nous évaluons l’utilité de trois lexiques bilingues dans un cadre de recherche interlingue français vers anglais sur le corpus CLEF. Le premier correspond à un dictionnaire qui couvre le corpus, alors que les deux autres ont été construits automatiquement à partir des sous-ensembles français et anglais de CLEF, en les considérant comme des corpus comparables. L’un contient des mots simples, alors que le deuxième ne contient que des termes complexes. Les lexiques sont intégrés dans des interfaces différentes dont les performances de recherche interlingue sont évaluées par 5 utilisateurs sur 15 thèmes de recherche CLEF. Les meilleurs résultats sont obtenus en intégrant le lexique de mots simples généré à partir des corpus comparables dans une interface proposant les cinq « meilleures » traductions pour chaque mot de la requête.},
  abstract  = {We evaluate the utility of three bilingual lexicons for English-to-French crosslingual search on the CLEF corpus. The first one is a kind of dictionary whose content covers the corpus. The other two have been automatically built on the French and English subparts of the CLEF corpus, by considering them as comparable corpora. One is made of simple words, the other one of complex words. The lexicons are integrated in different interfaces whose crosslingual search performances are evaluated by 5 users on 15 topics of CLEF. The best results are given with the interface having the simple-words lexicon generated on comparable corpora and proposing 5 translations for each query term.},
  motscles  = {recherche d’information multilingue, corpus comparables, lexiques multilingues},
  keywords  = {multilingual information retrieval, comparable corpora, multilingual lexicons},
}

@inproceedings{suignard-kerroua:2013:TALN,
  author    = {Suignard, Philippe and Kerroua, Sofiane},
  title     = {Utilisation de contextes pour la correction automatique ou semi-automatique de réclamations clients},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {699--706},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-025},
  language  = {french},
  note      = {Using contexts for automatic or semi-automatic correction of customer complaints},
  resume    = {Cet article présente deux méthodes permettant de corriger des réclamations contenant des erreurs rédactionnelles, en s’appuyant sur le graphe des voisins orthographiques et contextuels. Ce graphe est constitué des formes ou mots trouvés dans un corpus d’apprentissage. Un lien entre deux formes traduit le fait que les deux formes se « ressemblent » et partagent des contextes similaires. La première méthode est semi-automatique et consiste à produire un dictionnaire de substitution à partir de ce graphe. La seconde méthode, plus ambitieuse, est entièrement automatisée. Elle s’appuie sur les contextes pour déterminer à quel mot correspond telle forme abrégée ou erronée. Les résultats ainsi obtenus permettent d’améliorer le processus déjà existant de constitution d’un dictionnaire de substitution mis en place au sein d’EDF.},
  abstract  = {This article presents two methods allowing correcting complaints containing spelling errors, by using the spelling and contextual neighbors' graph. This graph is made of forms or words found in a learning corpus. A link between two forms conveys the fact that the two forms ''look alike'' and share similar contexts. The first method is semi-automatic and consists in producing a substitutional dictionary from this graph. The second method, more ambitious, is fully automatic. It is based on contexts to determine to which word corresponds such abbreviated or erroneous form. The results thus obtained allow us to improve the existing process regarding the creation of a substitutional dictionary at EDF.},
  motscles  = {Correction automatique, analyse distributionnelle, graphe, contexte},
  keywords  = {Spelling correction, distributional analysis, graph, context},
}

@inproceedings{cabreradiego-torresmoreno-elbeze:2013:TALN,
  author    = {Cabrera-Diego, Luis Adrián and Torres-Moreno, Juan-Manuel and El-Bèze, Marc},
  title     = {SegCV : traitement efficace de CV avec analyse et correction d’erreurs},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {707--714},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-026},
  language  = {french},
  note      = {SegCV : Eficient parsing of résumés with analysis and correction of errors},
  resume    = {Le marché d’offres d’emploi et des candidatures sur Internet a connu, ces derniers temps, une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de textes libres) intraitables manuellement. Les CV sont dans des formats très divers : .pdf, .doc, .dvi, .ps, etc., ce qui peut provoquer des erreurs lors de la conversion en texte plein. Nous proposons SegCV, un système qui a pour but l’analyse automatique des CV des candidats. Dans cet article, nous présentons des algorithmes reposant sur une analyse de surface, afin de segmenter les CV de manière précise. Nous avons évalué la segmentation automatique selon des corpus de référence que nous avons constitués. Les expériences préliminaires réalisées sur une grande collection de CV en français avec correction du bruit montrent de bons résultats en précision, rappel et F-Score.},
  abstract  = {Over the last years, the online market of jobs and candidatures offers has reached an exponential growth. This has implied great amounts of information (mainly in a text free style) which cannot be processed manually. The résumés are in several formats : .pdf, .doc, .dvi, .ps, etc., that can provoque errors or noise during the conversion to plain text. We propose SegCV, a system that has as goal the automatic parsing of candidates’ résumés. In this article we present the algoritms, which are based over a surface analysis, to segment the résumés in an accurate way. We evaluated the automatic segmentation using a reference corpus that we have created. The preliminary experiments, done over a large collection of résumés in French with noise correction, show good results in precision, recall and F-score.},
  motscles  = {RI, Ressources humaines, traitement de CV, Modèle à base de règles},
  keywords  = {Information Retrieval, Human Resources, CV Parsing, Rules Model},
}

@inproceedings{cossu-torresmoreno-elbeze:2013:TALN,
  author    = {Cossu, Jean-Valère and Torres-Moreno, Juan-Manuel and El-Bèze, Marc},
  title     = {Recherche et utilisation d'entités nommées conceptuelles dans une tâche de catégorisation},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {715--722},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-027},
  language  = {french},
  note      = {Search and usage of named conceptual entities in a categorisazion task},
  resume    = {Les recherches présentées sont directement liées aux travaux menés pour résoudre les problèmes de catégorisation automatique de texte. Les mots porteurs d’opinions jouent un rôle important pour déterminer l’orientation du message. Mais il est essentiel de pouvoir identifier les cibles auxquelles ils se rapportent pour en contextualiser la portée. L’analyse peut également être menée dans l’autre sens, on cherchant dans le contexte d’une cible détectée les termes polarisés. Une première étape d’apprentissage depuis des données permet d'obtenir automatiquement les marqueurs de polarité les plus importants. A partir de cette base, nous cherchons les cibles qui apparaissent le plus fréquemment à proximité de ces marqueurs d'opinions. Ensuite, nous construisons un ensemble de couples (marqueur de polarité, cible) pour montrer qu’en s’appuyant sur ces couples, on arrive à expliquer plus finement les prises de positions tout en maintenant (voire améliorant) le niveau de performance du classifieur.},
  abstract  = {The researchs presented are part of a text automatic categorization task. Words bearing opinions play an important role in determining the overall direction of the message. But it is essential to identify the elements (targets) which they are intended to relativize the scope. The analysis can also be conducted in the reverse direction. When a target is detected we need to search polarized terms in the context. A first step in an automatic learning from data will allow us to obtain the most important polarity markers. From this basis, we look for targets that appear most frequently in the vicinity of these opinions markers. Then, we construct a set of pairs (polarity marker, target) to show that relying on these couples we can maintain (or improve) the performance of the classifier.},
  motscles  = {Fouille d’opinion, Marqueurs de polarité, Reconnaissance d’entités nommées},
  keywords  = {Opinion Mining, Named Entity Recognition},
}

@inproceedings{wisniewski-EtAl:2013:TALN,
  author    = {Wisniewski, Guillaume and Singh, Anil Kumar and Segal, Natalia and Yvon, François},
  title     = {Un corpus d’erreurs de traduction},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {723--730},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-028},
  language  = {french},
  note      = {A corpus of post-edited translations},
  resume    = {Avec le développement de la post-édition, de plus en plus de corpus contenant des corrections de traductions sont disponibles. Ce travail présente un corpus de corrections d’erreurs de traduction collecté dans le cadre du projet ANR/TRACE et illustre les différents types d’analyses auxquels il peut servir. Nous nous intéresserons notamment à la détection des erreurs fréquentes et à l’analyse de la variabilité des post-éditions.},
  abstract  = {More and more datasets of post-edited translations are being collected. These corpora have many applications, such as failure analysis of SMT systems and the development of quality estimation systems for SMT. This work presents a large corpus of post-edited translations that has been gathered during the ANR/TRACE project. Applications to the detection of frequent errors and to the analysis of the inter-rater agreement of hTER are also reported.},
  motscles  = {Traduction automatique, Analyse d’erreur, Post-édiition},
  keywords  = {Machine Translation, Failure Analysis, Post-edition},
}

@inproceedings{walhaellouze-jaoua-hadrichbelguith:2013:TALN,
  author    = {Walha Ellouze, Samira and Jaoua, Maher and Hadrich Belguith, Lamia},
  title     = {Une méthode d’évaluation des résumés basée sur la combinaison de métriques automatiques et de complexité textuelle},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {731--738},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-029},
  language  = {french},
  note      = {An evaluation summary method based on combination of automatic and textual complexity metrics},
  resume    = {Cet article présente une méthode automatique d’évaluation du contenu des résumés automatiques. La méthode proposée est basée sur une combinaison de caractéristiques englobant des scores de contenu et d’autres de complexité textuelle et ce en s’appuyant sur une technique d’apprentissage, à savoir la régression linéaire. L’objectif de cette combinaison consiste à prédire le score manuel PYRAMID à partir des caractéristiques utilisées. Afin d’évaluer la méthode présentée, nous nous sommes intéressés à deux niveaux de granularité d’évaluation : la première est qualifiée de Micro-évaluation et propose l’évaluation de chaque résumé, alors que la deuxième est une Macro-évaluation et s’applique au niveau de chaque système.},
  abstract  = {This article presents an automatic method for evaluating content summaries. The proposed method is based on a combination of features encompassing scores of content and others of textual complexity. This method relies on a learning technique namely the linear regression. The objective of this combination is to predict the PYRAMID score from used features. In order to evaluate the presented method, we are interested in two levels of granularity evaluation: the first is named Micro-evaluation and proposes an evaluation of each summary, while the second is called Macro-evaluation and it applies at the level of each system.},
  motscles  = {Evaluation intrinsèque, évaluation du contenu, résumé automatique, complexité textuelle, régression linéaire},
  keywords  = {Intrinsic evaluation, content evaluation, automatic summary, textual complexity, linear regression},
}

@inproceedings{bouchekif-damnati-charlet:2013:TALN,
  author    = {Bouchekif, Abdessalam and Damnati, Géraldine and Charlet, Delphine},
  title     = {Segmentation thématique : processus itératif de pondération intra-contenu},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {739--746},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-030},
  language  = {french},
  note      = {An iterative topic segmentation algorithm with intra-content term weighting},
  resume    = {Dans cet article, nous nous intéressons à la segmentation thématique d’émissions télévisées exploitant la cohésion lexicale. Le but est d’étudier une approche générique, reposant uniquement sur la transcription automatique sans aucune information externe ni aucune information structurelle sur le contenu traité. L’étude porte plus particulièrement sur le mécanisme de pondération des mots utilisés lors du calcul de la cohésion lexicale. Les poids TF-IDF sont estimés à partir du contenu lui-même, qui est considéré comme une collection de documents mono-thème. Nous proposons une approche itérative, intégrée à un algorithme de segmentation, visant à raffiner la partition du contenu en documents pour l’estimation de la pondération. La segmentation obtenue à une itération donnée fournit un ensemble de documents à partir desquels les poids TF-IDF sont ré-estimés pour la prochaine itération. Des expériences menées sur un corpus couvrant différents formats des journaux télévisés issus de 8 chaînes françaises montrent une amélioration du processus global de segmentation.},
  abstract  = {This paper deals with topic segmentation of TV Broadcasts using lexical cohesion. The aim is to propose a generic approach, only relying on the automatic speech transcription with no external nor a priori information on the TV content. The study focuses on a new weighting scheme for lexical cohesion computation. TF-IDF weights are estimated from the content itself which is considered as a collection of mono-thematic documents. We propose an iterative process, integrated to a segmentation algorithm, aiming to refine the partition of a content into documents in order to estimate the weights. Topic segmentation obtained at a given iteration provides a set of documents from which TF-IDF weights are re-estimated for the next iteration. An experiment on a rich corpus covering various formats of Broadcast News shows from 8 French TV channels improves the overall topic segmentation process.},
  motscles  = {Segmentation thématique, pondération TF-IDF, cohésion lexicale, TextTiling},
  keywords  = {Topic segmentation, TF-IDF weighting, lexical cohesion, TextTiling},
}

@inproceedings{panchenko-EtAl:2013:TALN,
  author    = {Panchenko, Alexander and Naets, Hubert and Brouwers, Laetitia and Romanov, Pavel and Fairon, Cédrick},
  title     = {Recherche et visualisation de mots sémantiquement liés},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {747--754},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-031},
  language  = {french},
  note      = {Search and Visualization of Semantically Related Words},
  resume    = {Nous présentons PatternSim, une nouvelle mesure de similarité sémantique qui repose d’une part sur des patrons lexico-syntaxiques appliqués à de très vastes corpus et d’autre part sur une formule de réordonnancement des candidats extraits. Le système, initialement développé pour l’anglais, a été adapté au français. Nous rendons compte de cette adaptation, nous en proposons une évaluation et décrivons l’usage de ce nouveau modèle dans la plateforme de consultation en ligne Serelex.},
  abstract  = {We present PatternSim, a new semantic similarity measure that relies on morpho-syntactic patterns applied to very large corpora and on a re-ranking formula that reorder extracted candidates. The system, originally developed for English, was adapted to French. We explain this adaptation, propose a first evaluation of it and we describe how this new model was used to build the Serelex online search platform.},
  motscles  = {Mesure de similarité sémantique, relations sémantiques},
  keywords  = {Semantic similarity measure, semantic relations},
}

@inproceedings{guilbaud-boitet-berment:2013:TALN,
  author    = {Guilbaud, Jean-Philippe and Boitet, Christian and Berment, Vincent},
  title     = {Un analyseur morphologique étendu de l'allemand traitant les formes verbales à particule séparée},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {755--763},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-032},
  language  = {french},
  note      = {An extended morphological analyzer of German handling verbal forms with separated separable particles},
  resume    = {Nous décrivons l’organisation et l'état courant de l’analyseur morphologique de l’allemand AMALD de grande taille couvrant (près de 103000 lemmes et 500000 formes fléchies simples, en croissance) développé dans le cadre du projet ANR-Émergence Traouiero. C’est le premier lemmatiseur de l’allemand capable de traiter non seulement les mots simples et les mots composés, mais aussi les verbes à particules séparables quand elles sont séparées, même par un grand nombre de mots (ex : Hier schlagen wir eine neue Methode für die morphologische Analyse vor).},
  abstract  = {We describe the organisation and the current state of the large-scale (nearly 103000 lemmas and 500000 simple inflected forms, growing) morphological analyzer AMALD developed in the framework of the ANR-Émergence Traouiero project. It is the first lemmatizer of German able to handle not only simple and compound words, but also verbs with separable particles when they are separated, even by many words (e.g. Hier schlagen wir eine neue Methode für die morphologische Analyse vor.).},
  motscles  = {analyse morphologique, lemmatisation, allemand, verbes à particule séparable},
  keywords  = {morphological analysis, lemmatization, German, verbs with separable particles},
}

@inproceedings{vincent-winterstein:2013:TALN,
  author    = {Vincent, Marc and Winterstein, Grégoire},
  title     = {Construction et exploitation d’un corpus français pour l’analyse de sentiment},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {764--771},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-033},
  language  = {french},
  note      = {Building and exploiting a French corpus for sentiment analysis},
  resume    = {Ce travail présente un corpus en français dédié à l’analyse de sentiment. Nous y décrivons la construction et l’organisation du corpus. Nous présentons ensuite les résultats de l’application de techniques d’apprentissage automatique pour la tâche de classification d’opinion (positive ou négative) véhiculée par un texte. Deux techniques sont utilisées : la régression logistique et la classification basée sur des Support Vector Machines (SVM). Nous mentionnons également l’intérêt d’appliquer une sélection de variables avant la classification (par régularisation par elastic net).},
  abstract  = {This work introduces a French corpus for sentiment analysis. We describe the construction and organization of the corpus. We then apply machine learning techniques to automatically predict whether a text is positive or negative (the opinion classification task). Two techniques are used : logistic regression and classification based on Support Vector Machines (SVM). Finally, we briefly evaluate the merits of applying feature selection algorithms to our models (via elastic net regularization).},
  motscles  = {Analyse de sentiments, Corpus, Classification, Apprentissage automatique, Sélection de variable},
  keywords  = {Sentiment Analysis, Corpus, Opinion Mining, Classification, Machine Learning, Variable Selection},
}

@inproceedings{nerima-wehrli:2013:TALN,
  author    = {Nerima, Luka and Wehrli, Éric},
  title     = {Résolution d'anaphores appliquée aux collocations: une évaluation préliminaire},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {772--778},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-034},
  language  = {french},
  note      = {Anaphora Resolution Applied to Collocation Identification: A Preliminary Evaluation},
  resume    = {Le traitement des collocations en analyse et en traduction est depuis de nombreuses années au centre de nos intérêts de recherche. L’analyseur Fips a été récemment enrichi d’un module de résolution d’anaphores. Dans cet article nous décrivons comment la résolution d’anaphores a été appliquée à l’identification des collocations et comment cela permet à l’analyseur de repérer une collocation même si un de ses termes a été pronominalisé. Nous décrivons aussi la méthodologie de l’évaluation, notamment la préparation des données pour le calcul du rappel. Dans la tâche d’identification des collocations pronominalisées, Fips montre des résultats très encourageants : la précision mesurée est de 98% alors que le rappel est proche de 50%. Dans cette évaluation nous nous intéressons aux collocations de type verbe-objet direct en conjonction avec les pronoms anaphoriques à la 3e personne. Le corpus utilisé est un corpus anglais d’environ dix millions de mots.},
  abstract  = {Collocation identification and collocation translation have been at the center of our research interests for several years. Recently, the Fips parser has been enriched by an anaphora resolution mechanism. This article discusses how anaphora resolution has been applied to the collocation identification task, and how it enables the parser to identify a collocation when one of its terms is pronominalized. We also describe the evaluation methodology, in particular the preparation of data for the calculation of the recall. In the task of pronominalized collocation identification, Fips shows encouraging results: the measured precision is 98% while recall approaches 50%. In this paper we focus on collocations of the type verb-direct object and on a widespread type of anaphora: the third personal pronouns. The corpus used is a corpus of approximately ten million English words.},
  motscles  = {Analyse, résolution d’anaphores, pronoms personnels, collocations, corpus},
  keywords  = {Parsing, anaphora resolution, personal pronoun, collocations, corpus},
}

@inproceedings{mondary-EtAl:2013:TALN,
  author    = {Mondary, Thibault and Nazarenko, Adeline and Zargayouna, Haïfa and Barreaux, Sabine},
  title     = {Aide à l’enrichissement d’un référentiel terminologique : propositions et expérimentations},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {779--786},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-court-035},
  language  = {french},
  note      = {Help enrich a terminological repository : proposals and experiments},
  resume    = {En s’appuyant sur une expérience d’enrichissement terminologique, cet article montre comment assister le travail d’acquisition terminologique et surmonter concrètement les deux difficultés qu’il présente : la masse de candidats-termes à considérer et la subjectivité des jugements terminologiques qui varient notamment en fonction du type de terminologie à produire. Nous proposons des stratégies simples pour filtrer a priori une partie du bruit des résultats des extracteurs et rendre ainsi la validation praticable pour des terminologues et nous démontrons leur efficacité sur un échantillon de candidats-termes proposés à la validation de deux spécialistes du domaine. Nous montrons également qu’en appliquant à une campagne de validation terminologique les mêmes principes méthodologiques que pour une campagne d’annotation, on peut contrôler la qualité des jugements de validation posés et de la terminologie qui en résulte.},
  abstract  = {Based on an experience of terminological enrichment, this paper shows how to support the work of terminological acquisition and overcome practical difficulties it presents, i.e. the mass of candidate terms to consider and the subjectivity of terminological judgments which depends on the type of terminology to produce. We propose simple strategies to filter a priori part of the noise from the results of term extractors so as to make the validation practicable for terminologists. We demonstrate their effectiveness on a sample of candidate terms proposed for the validation of two experts. We also show that by applying to term validation campaigns the methodological principles that have been proposed for corpus annotation campaigns, we can control the quality of validation judgments and of the resulting terminologies.},
  motscles  = {Acquisition terminologique, validation de candidats-termes, filtrage de termes, distance terminologique, vote, accord inter-juges},
  keywords  = {Terminology acquisition, term candidate validation, term filtering, terminological distance, vote, inter-judge agreement},
}

@inproceedings{lejeune-EtAl:2013:TALN,
  author    = {Lejeune, Gaël and Brixtel, Romain and Lecluze, Charlotte and Doucet, Antoine and Lucas, Nadine},
  title     = {DAnIEL : Veille épidémiologique multilingue parcimonieuse},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {787--788},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-001},
  language  = {french},
  note      = {DAnIEL, parsimonious yet high-coverage multilingual epidemic surveillance},
  resume    = {DAnIEL est un système multilingue de veille épidémiologique. DAnIEL permet de traiter un grand nombre de langues à faible coût grâce à une approche parcimonieuse en ressources.},
  abstract  = {DAnIEL is a multilingual epidemic surveillance system. DAnIEL relies on a parsimonious scheme making it possible to process new languages at small cost.},
  motscles  = {extraction d’information, recherche d’information, veille, multilinguisme, genre journalistique, grain caractère},
  keywords  = {information extraction, information retrieval, news surveillance, multilingualism, news genre, character-level analysis},
}

@inproceedings{kozlova-gontcharova-popova:2013:TALN,
  author    = {Kozlova, Elena and Gontcharova, Maria and Popova, Tatiana},
  title     = {Lexique multilingue dans le cadre du modèle Compreno développé ABBYY},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {789--790},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-002},
  language  = {french},
  note      = {Multilingual lexical database in the framework of COMPRENO linguistic model developed by ABBYY},
  resume    = {Le lexique multilingue basé sur une hiérarchie sémantique universelle fait partie du modèle linguistique Compreno destiné à plusieurs applications du TALN, y compris la traduction automatique et l’analyse sémantique et syntaxique. La ressource est propriétaire et n’est pas librement disponible.},
  abstract  = {The multilingual lexical database based on the universal semantic hierarchy is part of Compreno linguistic model. This model is meant for various NLP applications dealing with machine translation, semantic and syntactic analysis. The resource is private and is not freely available.},
  motscles  = {Lexique multilingue, hiérarchie sémantique universelle, traduction automatique},
  keywords  = {Multilingual lexical database, universal semantic hierarchy, machine translation},
}

@inproceedings{quintana:2013:TALN,
  author    = {Quintana, Manon},
  title     = {Inbenta Semantic Search Engine : un moteur de recherche sémantique inspiré de la Théorie Sens-Texte},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {791--792},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-003},
  language  = {french},
  note      = {Inbenta Semantic Search Engine: a semantic search engine inspired by the Meaning-Text Theory},
  resume    = {Avec la digitalisation massive de documents apparaît la nécessité de disposer de systèmes de recherche capables de s’adapter aux habitudes de recherche des utilisateurs et de leur permettre d’accéder à l’information rapidement et efficacement. INBENTA a ainsi créé un moteur de recherche intelligent appellé Inbenta Semantic Search Engine (ISSE). Les deux tâches principales de l’ISSE sont d’analyser les questions des utilisateurs et de trouver la réponse appropriée à la requête en effectuant une recherche dans une base de connaissances. Pour cela, la solution logicielle d’INBENTA se base sur la Théorie Sens-Texte qui se concentre sur le lexique et la sémantique.},
  abstract  = {The need to have search systems able to adapt themselves to the particular way users pose their questions so that they can get a quick and efficient access to information is increasingly relevant due to the huge digitalization of documents. To cope with this reality, INBENTA has developed an intelligent search engine, called Inbenta Semantic Search Engine (ISSE). ISSE's main two tasks are analysing users' queries and finding the most appropriate answer to those questions in a knowledge-base. To carry out these tasks, INBENTA's software solution relies upon the Meaning-Text Theory, which focusses on the lexicon and semantics.},
  motscles  = {Moteur de Recherche Sémantique, Théorie Sens-Texte, fonction lexicale},
  keywords  = {Semantic Search Engine, Meaning-Text Theory, lexical function},
}

@inproceedings{bouraoui-canitrot:2013:TALN,
  author    = {Bouraoui, Jean-Leon and Canitrot, Marc},
  title     = {FMO : un outil d’analyse automatique de l’opinion},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {793--794},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-004},
  language  = {french},
  note      = {FMO: a tool for automated opinion mining},
  resume    = {Nous décrivons notre prototype d’analyse automatique d’opinion. Celui-ci est basé sur un moteur d’analyse linguistique. Il permet de détecter finement les segments de texte porteurs d’opinions, de les extraire, et de leur attribuer une note selon la polarité qu’ils expriment. Nous présentons enfin les différentes perspectives que nous envisageons pour ce prototype.},
  abstract  = {We describe our prototype of automatic opinion mining. It is based on a linguistic analysis engine. It allows to subtly identifying the text phrases which bear some opinion, to extract them, and to give them a note according to the polarity that they express. Finally, we present the perspectives that we plan to carry out.},
  motscles  = {Analyse d’opinion, e-reputation, extraction d’information},
  keywords  = {Opinion mining, e-reputation, information extraction},
}

@inproceedings{seguela-laurent:2013:TALN,
  author    = {Séguéla, Patrick and Laurent, Dominique},
  title     = {Corriger, analyser et représenter le texte Synapse Développement},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {795--796},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-005},
  language  = {french},
  note      = {Checking, analysing and representing texts},
  resume    = {Synapse Développement souhaite échanger avec les conférenciers autour des technologies qu'elle commercialise : correction de textes et analyse sémantique. Plusieurs produits et démonstrateurs seront présentés, notre but étant d'instaurer un dialogue et de confronter notre approche du TAL, à base de méthodes symboliques et statistiques influencées par des contraintes de production, et celles utilisées par les chercheurs, industriels ou passionnés qui viendront à notre rencontre.},
  abstract  = {Synapse Développement would like to demonstrate its grammar checker and semantic analysis technologies to open exciting discussions with natural language specialists. We are particularly interested in discussing the scientific issues we have to face and solve according to our industrial needs.},
  motscles  = {Correction grammaticale, analyse syntaxique, analyse sémantique, analyse d'opinions},
  keywords  = {Grammar checker, POS tagging, semantic analysis, opinion mining},
}

@inproceedings{tannier-moriceau-leflem:2013:TALN,
  author    = {Tannier, Xavier and Moriceau, Véronique and Le Flem, Erwan},
  title     = {Une interface pour la validation et l’évaluation de chronologies thématiques},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {797--798},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-006},
  language  = {french},
  note      = {An Interface for Validating and Evaluating Thematic Timelines},
  resume    = {Cet article décrit une interface graphique de visualisation de chronologies événementielles construites automatiquement à partir de requêtes thématiques en utilisant un corpus de dépêches fourni par l’Agence France Pressse (AFP). Cette interface permet également la validation des chronologies par des journalistes qui peuvent ainsi les éditer et les modifier.},
  abstract  = {This demo paper presents a graphical interface for the visualization and evaluation of event timelines built automatically from a search query on a newswire article corpus provided by the Agence France Pressse (AFP). This interface also enables journalists to validate chronologies by editing and modifying them.},
  motscles  = {chronologie événementielle, évaluation, validation},
  keywords  = {event timeline, evaluation, validation},
}

@inproceedings{maurel-friburger:2013:TALN,
  author    = {Maurel, Denis and Friburger, Nathalie},
  title     = {CasSys Un système libre de cascades de transducteurs},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {799--800},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-007},
  language  = {french},
  note      = {CasSys, a free transducer cascade system},
  resume    = {CasSys est un système de création et de mise en oeuvre de cascades de transducteurs intégré à la plateforme Unitex. Nous présentons dans cette démonstration la nouvelle version implantée fin 2012. En particulier ont été ajoutées une interface plus conviviale et la possibilité d’itérer un même transducteur jusqu’à ce qu’il n’ait plus d’influence sur le texte. Un premier exemple concernera le traitement de texte avec une gestion complexe de balises XML et un deuxième présentera la cascade CasEN de reconnaissance des entités nommées.},
  abstract  = {CasSys is a free toolkit integrated in the Unitex platform to create and use transducer cascades. We are presenting the new version implemented at the end of 2012. The system interface has been improved and the Kleen star operation has been added: this operation allows applying the same transducer until it no longer produces changes in the text. The first example deals with complex XML text parsing and the second with CasEN, a free cascade for French Named Entity Recognition.},
  motscles  = {cascade de transducteurs, graphes Unitex, texte avec balises XML, reconnaissance d'entités nommées},
  keywords  = {transducer cascade, Unitex graphs, XML text, French Named Entity Recognition},
}

@inproceedings{wang-zhang:2013:TALN,
  author    = {Wang, Lingxiao and Zhang, Ying},
  title     = {iMAG : post-édition, évaluation de qualité de TA et production d'un corpus parallèle},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {801--802},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-008},
  language  = {french},
  note      = {iMAG : MT-postediting, translation quality evaluation and parallel corpus production},
  resume    = {Une passerelle interactive d’accès multilingue (iMAG) dédiée à un site Web S (iMAG-­‐S) est un bon outil pour rendre S accessible dans beaucoup de langues, immédiatement et sans responsabilité éditoriale. Les visiteurs de S ainsi que des post-­‐éditeurs et des modérateurs payés ou non contribuent à l’amélioration continue et incrémentale des segments textuels les plus importants, et éventuellement de tous. Dans cette approche, les pré-­‐traductions sont produites par un ou plusieurs systèmes de Traduction Automatique (TA) gratuits. Il y a deux effets de bord intéressants, obtenables sans coût additionnel : les iMAGs peuvent être utilisées pour produire des corpus parallèles de haute qualité, et pour mettre en place une évaluation permanente et finalisée de multiples systèmes de TA.},
  abstract  = {An interactive Multilingual Access Gateway (iMAG) dedicated to a web site S (iMAG-­‐S) is a good tool to make S accessible in many languages immediately and without editorial responsibility. Visitors of S as well as paid or unpaid post-­‐editors and moderators contribute to the continuous and incremental improvement of the most important textual segments, and eventually of all. In this approach, pre-­‐translations are produced by one or more free machine translation systems. There are two interesting side effects obtainable without any added cost: iMAGs can be used to produce high-­‐quality parallel corpora and to set up a permanent task-­‐based evaluation of multiple MT systems.},
  motscles  = {post-édition, évaluation de systèmes de TA, production d’un corpus parallèle},
  keywords  = {post-edition, evaluation of MT systems, production of parallel corpora},
}

@inproceedings{rouquet:2013:TALN,
  author    = {Rouquet, David},
  title     = {Technologies du Web Sémantique pour l'exploitation de données lexicales en réseau (Lexical Linked Data)},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {803--804},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-009},
  language  = {french},
  note      = {Semantic Web technologies for Lexical Linked Data management},
  resume    = {Nous présentons un système basé sur les technologies du Web Sémantique pour la gestion, le développement et l'exploitation de données lexicales en réseau (Lexical Linked Data, LLD).},
  abstract  = {We present a system based on Semantic Web technologies for Lexical Linked Data management.},
  motscles  = {Lexical Linked Data, Lexique Multilingue, Pivot, Axies, Sparql, Spin},
  keywords  = {Lexical Linked Data, Multilingual Lexicon, Pivot, Axies, Sparql, Spin},
}

@inproceedings{falaise:2013:TALN,
  author    = {Falaise, Achille},
  title     = {Adaptation de la plateforme corporale ScienQuest pour l'aide à la rédaction en langue seconde},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {805--806},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-010},
  language  = {french},
  note      = {Adaptation of the corpus platform ScienQuest for assistance to writing in a second language},
  resume    = {La plateforme ScienQuest fut initialement créée pour l'étude linguistique du positionnement et du raisonnement dans le corpus Scientext. Cette démonstration présente les modifications apportées à cette plateforme, pour en faire une base phraséologique adaptée à l'aide à la rédaction en langue seconde. Cette adaptation est utilisée dans le cadre de deux expérimentations en cours : l'aide à la rédaction en anglais pour les scientifiques, et l'aide à la rédaction académique en français pour les apprenants.},
  abstract  = {The ScienQuest platform was initially created for the linguistic study of positioning and reasoning in the Scientext corpus. This demonstration introduces modifications to this platform, transforming it into a phraseological database adapted for assistance to writing in a second language. This adaptation is used as part of two ongoing experiments: an assistance to writing in English for scientists, and an assistance to academic writing in French for learners.},
  motscles  = {Aide à la rédaction, langue seconde, ScienQuest, Scientext},
  keywords  = {Writing assistance, second language, ScienQuest, Scientext},
}

@inproceedings{penasaldarriaga-vintache-daille:2013:TALN,
  author    = {Peña Saldarriaga, Sebastián and Vintache, Damien and Daille, Béatrice},
  title     = {Démonstrateur Apopsis pour l’analyse des tweets},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {807--808},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-011},
  language  = {french},
  note      = {Apopsis Demonstrator for Tweet Analysis},
  resume    = {Le démonstrateur Apopsis permet de délimiter et de catégoriser les opinions émises sur les tweets en temps réel pour un sujet choisi par l’utilisateur au travers d’une interface web.},
  abstract  = {Apopsis web demonstrator detects and categorizes opinion expressions appearing in Twitter in real time thorigh a web interface.},
  motscles  = {fouille d’opinion, polarité, twitter},
  keywords  = {opinion mining, polarity, twitter},
}

@inproceedings{cailliau-cavet:2013:TALN,
  author    = {Cailliau, Frederik and Cavet, Ariane},
  title     = {L’analyse des sentiments au service des centres d’appels},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {809--811},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-012},
  language  = {french},
  note      = {Sentiment Analysis for Call-centers},
  resume    = {Les conversations téléphoniques qui contiennent du sentiment négatif sont particulièrement intéressantes pour les centres d’appels, aussi bien pour évaluer la perception d’un produit par les clients que pour améliorer la formation des télé-conseillers. Néanmoins, ces conversations sont peu nombreuses et difficiles à trouver dans la masse d’enregistrements. Nous présentons un module d’analyse des sentiments qui permet de visualiser le déroulement émotionnel des conversations. Il se greffe sur un moteur de recherche, ce qui permet de trouver rapidement les conversations problématiques grâce à l’ordonnancement par score de négativité.},
  abstract  = {Phone conversations in which negative sentiment is expressed are particularly interesting for call centers, both to evaluate the clients’ perception of a product and for the training of the agents. However, these conversations are scarce and hard to find in the mass of the recorded calls. We present a module for sentiment analysis that allows the user to visualize the emotional course of each conversation. In combination with a search engine, a user can rapidly find the problematic calls using the ranking by negativity score.},
  motscles  = {analyse des sentiments, conversations téléphoniques, recherche d’information, parole spontanée, parole conversationnelle},
  keywords  = {sentiment analysis, information retrieval, spontaneous speech, conversational speech},
}

@inproceedings{daille-harastani:2013:TALN,
  author    = {Daille, Béatrice and Harastani, Rima},
  title     = {TTC TermSuite alignement terminologique à partir de corpus comparables},
  booktitle = {Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2013},
  address   = {Les Sables d'Olonne, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {812--813},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2013/taln-2013-demo-013},
  language  = {french},
  note      = {TTC TermSuite - Terminological Alignment from Comparable Corpora},
  resume    = {TermSuite est outil libre multilingue réalisant une extraction terminologique monolingue et une extraction terminologique bilingue à partir de corpus comparables.},
  abstract  = {TermSuite is based on a UIMA framework and performs monolingual and bilingual term extraction from comparable corpora for a range of languages.},
  motscles  = {corpus comparable, extraction terminologique, alignement, UIMA},
  keywords  = {comparable corpora, terminology extraction, terminology alignment, UIMA},
}