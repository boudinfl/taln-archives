<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>RECITAL'2009</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>RECITAL'2009, 11e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</h1>
				<h2>Senlis (France), du 2009-06-24 au 2009-06-26</h2>
				<p>Président(s) : Thibault Mondary, Aurélien Bossard, Thierry Hamon</p>
				<p>Taux d'acceptation :
							papiers longs (80.0%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#long">Papiers longs</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="long">Papiers longs</h1>
			

					<div class="article">

						<b>Pierre Gotab</b>


						<br/>

							<i>Apprentissage automatique et Co-training</i> <br/>

						<a href="actes/recital-2009-long-001.pdf">recital-2009-long-001</a> 
						<a href="bibtex/recital-2009-long-001.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-001-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-001-key');">mots clés</a> <br/>

							<p id="recital-2009-long-001-abs" class="resume">
							<b>Résumé : </b> Dans le domaine de la classification supervisée et semi-supervisée, cet article présente un contexte favorable à l’application de méthodes statistiques de classification. Il montre l’application d’une stratégie alternative dans le cas où les données d’apprentissage sont insuffisantes, mais où de nombreuses données non étiquetées sont à notre disposition : le cotraining multi-classifieurs. Les deux vues indépendantes habituelles du co-training sont remplacées par deux classifieurs basés sur des techniques de classification différentes : icsiboost sur le boosting et LIBLINEAR sur de la régression logistique.
							</p>

							<p id="recital-2009-long-001-key" class="mots_cles">
							<b>Mots clés : </b> Apprentissage automatique, classification, co-training
							</p>

					</div>
					

					<div class="article">

						<b>Marianne Santaholma</b>


						<br/>

							<i>Comparing Speech Recognizers Derived from Mono- and Multilingual Grammars</i> <br/>

						<a href="actes/recital-2009-long-002.pdf">recital-2009-long-002</a> 
						<a href="bibtex/recital-2009-long-002.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-002-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-002-key');">mots clés</a> <br/>

							<p id="recital-2009-long-002-abs" class="resume">
							<b>Résumé : </b> Nous présentons une comparaison de la performance de deux types différents de reconnaisseurs pour le japonais et l’anglais basés sur les grammaires. L’un des systèmes est dérivé à partir de règles d’une grammaire monolingue et l&#39;autre de règles paramétrisées et multilingues. Ce dernier emploie, les mêmes règles de grammaire pour la création de modèles de langue nécessaires à la reconnaissance des langues typologiquement différentes. Nous avons effectué des expériences sur la reconnaissance dans les applications de dialogue de domaine limitée. Ces expériences montrent que les modèles de langue dérivés des règles multilingues de grammaire (1) traitent aussi bien l’un que l’autre les deux langues examinées, et (2) que leur performance est comparable à celle des reconnaisseurs dérivés de grammaires monolingues. Ceci suggère que le partage de grammaires entre langues typologiquement différentes pourrait être une solution pour rendre plus efficace le développement de systèmes de reconnaissance de la parole linguistiques.
							</p>

							<p id="recital-2009-long-002-key" class="mots_cles">
							<b>Mots clés : </b> Grammaire multilingue paramétrisé, reconnaissance de la parole
							</p>

					</div>
					

					<div class="article">

						<b>Clémentine Adam, François Morlane-Hondère</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Détection de la cohésion lexicale par voisinage distributionnel : application à la segmentation thématique</i> <br/>

						<a href="actes/recital-2009-long-003.pdf">recital-2009-long-003</a> 
						<a href="bibtex/recital-2009-long-003.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-003-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-003-key');">mots clés</a> <br/>

							<p id="recital-2009-long-003-abs" class="resume">
							<b>Résumé : </b> Cette étude s’insère dans le projet VOILADIS (VOIsinage Lexical pour l’Analyse du DIScours), qui a pour objectif d’exploiter des marques de cohésion lexicale pour mettre au jour des phénomènes discursifs. Notre propos est de montrer la pertinence d’une ressource, construite par l’analyse distributionnelle automatique d’un corpus, pour repérer les liens lexicaux dans les textes. Nous désignons par voisins les mots rapprochés par l’analyse distributionnelle sur la base des contextes syntaxiques qu’ils partagent au sein du corpus. Pour évaluer la pertinence de la ressource ainsi créée, nous abordons le problème du repérage des liens lexicaux à travers une application de TAL, la segmentation thématique. Nous discutons l’importance, pour cette tâche, de la ressource lexicale mobilixsée ; puis nous présentons la base de voisins distributionnels que nous utilisons ; enfin, nous montrons qu’elle permet, dans un système de segmentation thématique inspiré de (Hearst, 1997), des performances supérieures à celles obtenues avec une ressource traditionnelle.
							</p>

							<p id="recital-2009-long-003-key" class="mots_cles">
							<b>Mots clés : </b> Cohésion lexicale, ressources lexicales, analyse distributionnelle, segmentation thématique
							</p>

					</div>
					

					<div class="article">

						<b>Gaël Patin</b>


						<br/>

							<i>Extraction de lexique dans un corpus spécialisé en chinois contemporain</i> <br/>

						<a href="actes/recital-2009-long-004.pdf">recital-2009-long-004</a> 
						<a href="bibtex/recital-2009-long-004.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-004-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-004-key');">mots clés</a> <br/>

							<p id="recital-2009-long-004-abs" class="resume">
							<b>Résumé : </b> La constitution de ressources lexicales est une tâche cruciale pour l’amélioration des performances des systèmes de recherche d’information. Cet article présente une méthode d’extraction d’unités lexicales en chinois contemporain dans un corpus spécialisé non-annoté et non-segmenté. Cette méthode se base sur une construction incrémentale de l’unité lexicale orientée par une mesure d’association. Elle se distingue des travaux précédents par une approche linguistique non-supervisée assistée par les statistiques. Les résultats de l’extraction, évalués sur un échantillon aléatoire du corpus de travail, sont honorables avec des scores de précision et de rappel respectivement de 52,6 % et 53,7 %.
							</p>

							<p id="recital-2009-long-004-key" class="mots_cles">
							<b>Mots clés : </b> corpus spécialisé, unité lexicale, lexie, extraction de lexique, chinois
							</p>

					</div>
					

					<div class="article">

						<b>Claire Mouton</b>


						<br/>

							<i>Induction de sens de mots à partir de multiples espaces sémantiques</i> <br/>

						<a href="actes/recital-2009-long-005.pdf">recital-2009-long-005</a> 
						<a href="bibtex/recital-2009-long-005.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-005-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-005-key');">mots clés</a> <br/>

							<p id="recital-2009-long-005-abs" class="resume">
							<b>Résumé : </b> Les mots sont souvent porteurs de plusieurs sens. Pour traiter l’information correctement, un ordinateur doit être capable de décider quel sens d’un mot est employé à chacune de ses occurrences. Ce problème non parfaitement résolu a généré beaucoup de travaux sur la désambiguïsation du sens des mots (Word Sense Disambiguation) et dans la génération d’espaces sémantiques dont un des buts est de distinguer ces différents sens. Nous nous inspirons ici de deux méthodes existantes de détection automatique des différents usages et/ou sens des mots, pour les appliquer à des espaces sémantiques issus d’une analyse syntaxique effectuée sur un très grand nombre de pages web. Les adaptations et résultats présentés dans cet article se distinguent par le fait d’utiliser non plus une seule représentation mais une combinaison de multiples espaces de forte dimensionnalité. Ces multiples représentations étant en compétition entre elles, elles participent chacune par vote à l’induction des sens lors de la phase de clustering.
							</p>

							<p id="recital-2009-long-005-key" class="mots_cles">
							<b>Mots clés : </b> espace sémantique, réduction de dimensions, Locality Sensitive Hashing, induction de sens, clustering de mots, objets multi-représentés
							</p>

					</div>
					

					<div class="article">

						<b>Marion Potet</b>


						<br/>

							<i>Méta-moteur de traduction automatique : proposition d’une métrique pour le classement de traductions</i> <br/>

						<a href="actes/recital-2009-long-006.pdf">recital-2009-long-006</a> 
						<a href="bibtex/recital-2009-long-006.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-006-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-006-key');">mots clés</a> <br/>

							<p id="recital-2009-long-006-abs" class="resume">
							<b>Résumé : </b> Compte tenu de l’essor du Web et du développement des documents multilingues, le besoin de traductions &#34;à la volée&#34; est devenu une évidence. Cet article présente un système qui propose, pour une phrase donnée, non pas une unique traduction, mais une liste de N hypothèses de traductions en faisant appel à plusieurs moteurs de traduction pré-existants. Neufs moteurs de traduction automatique gratuits et disponibles sur leWeb ont été sélectionnés pour soumettre un texte à traduire et réceptionner sa traduction. Les traductions obtenues sont classées selon une métrique reposant sur l’utilisation d’un modèle de langage. Les expériences conduites ont montré que ce méta-moteur de traduction se révèle plus pertinent que l’utilisation d’un seul système de traduction.
							</p>

							<p id="recital-2009-long-006-key" class="mots_cles">
							<b>Mots clés : </b> traduction automatique, web, modèle de langage, méta-moteur de traduction
							</p>

					</div>
					

					<div class="article">

						<b>Thomas François</b>


						<br/>

							<i>Modèles statistiques pour l’estimation automatique de la difficulté de textes de FLE</i> <br/>

						<a href="actes/recital-2009-long-007.pdf">recital-2009-long-007</a> 
						<a href="bibtex/recital-2009-long-007.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-007-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-007-key');">mots clés</a> <br/>

							<p id="recital-2009-long-007-abs" class="resume">
							<b>Résumé : </b> La lecture constitue l’une des tâches essentielles dans l’apprentissage d’une langue étrangère. Toutefois, la découverte d’un texte portant sur un sujet précis et qui soit adapté au niveau de chaque apprenant est consommatrice de temps et pourrait être automatisée. Des expériences montrent que, pour l’anglais, l’utilisation de classifieurs statistiques permet d’estimer automatiquement la difficulté d’un texte. Dans cet article, nous proposons une méthodologie originale comparant, pour le français langue étrangère (FLE), diverses techniques de classification (la régression logistique, le bagging et le boosting) sur deux corpus d’entraînement. Il ressort de cette analyse comparative une légère supériorité de la régression logistique multinomiale.
							</p>

							<p id="recital-2009-long-007-key" class="mots_cles">
							<b>Mots clés : </b> lisibilité, régression logistique, bagging, boosting, modèle de langue
							</p>

					</div>
					

					<div class="article">

						<b>Florent Pompigne</b>


						<br/>

							<i>Modélisation des mouvements explicites dans les ACG avec le produit dépendant</i> <br/>

						<a href="actes/recital-2009-long-008.pdf">recital-2009-long-008</a> 
						<a href="bibtex/recital-2009-long-008.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-008-abs');">abstract</a>
							<a onclick="toggle('recital-2009-long-008-key');">mots clés</a> <br/>

							<p id="recital-2009-long-008-abs" class="abstract">
							<b>Abstract : </b> Abstract Categorial Grammars (ACG) is a grammatical framework based on linear lambda-calculus. As in Muskens’ Lambda Grammars, an abstract term in this kind of categorial grammar can be realized in different directions, such as syntactic and semantic ones. This structure provides autonomy for these different processings. ACG’s architecture is independent from the logic used and so the type system is easily extensible in order to deal better with some linguistic phenomena. We will first introduce ACGs and the dependent product construction. This paper will then be concerned with the issue of overt grammatical movements, in particular extraction constraints in relative propositions, and how several close frameworks deal with it. Last we will show how to capture this phenomenon in extended ACG.
							</p>

							<p id="recital-2009-long-008-key" class="mots_cles">
							<b>Mots clés : </b> syntaxe, grammaires catégorielles abstraites, types dépendant, mouvements explicites, extraction
							</p>

					</div>
					

					<div class="article">

						<b>Vanessa Andréani</b>


						<br/>

							<i>Normalisation des entités nommées : pour une approche mixte et orientée utilisateurs</i> <br/>

						<a href="actes/recital-2009-long-009.pdf">recital-2009-long-009</a> 
						<a href="bibtex/recital-2009-long-009.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-009-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-009-key');">mots clés</a> <br/>

							<p id="recital-2009-long-009-abs" class="resume">
							<b>Résumé : </b> La normalisation intervient dans de nombreux champs du traitement de l&#39;information. Elle permet d&#39;optimiser les performances des applications, telles que la recherche ou l&#39;extraction d&#39;information, et de rendre plus fiable la constitution de ressources langagières. La normalisation consiste à ramener toutes les variantes d&#39;un même terme ou d&#39;une entité nommée à une forme standard, et permet de limiter l&#39;impact de la variation linguistique. Notre travail porte sur la normalisation des entités nommées, pour laquelle nous avons mis en place un système complexe mêlant plusieurs approches. Nous en présentons ici une des composantes : une méthode endogène de délimitation et de validation de l’entité nommée normée, adaptée à des données multilingues. De plus, nous plaçons l&#39;utilisateur au centre du processus de normalisation, dans l&#39;objectif d&#39;obtenir des données parfaitement fiables et adaptées à ses besoins.
							</p>

							<p id="recital-2009-long-009-key" class="mots_cles">
							<b>Mots clés : </b> normalisation, entités nommées, traitement de l&#39;information, analyse de corpus, méthodes endogènes, système complexe
							</p>

					</div>
					

					<div class="article">

						<b>Eric Charton</b>


						<br/>

							<i>Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités nommées en contexte</i> <br/>

						<a href="actes/recital-2009-long-010.pdf">recital-2009-long-010</a> 
						<a href="bibtex/recital-2009-long-010.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-010-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-010-key');">mots clés</a> <br/>

							<p id="recital-2009-long-010-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une méthode de transformation de Wikipédia en ressource d’information externe pour détecter et désambiguïser des entités nommées, en milieu ouvert et sans apprentissage spécifique. Nous expliquons comment nous construisons notre système, puis nous utilisons cinq éditions linguistiques de Wikipédia afin d’enrichir son lexique. Pour finir nous réalisons une évaluation et comparons les performances du système avec et sans compléments lexicaux issus des informations inter-linguistiques, sur une tâche d’extraction d’entités nommées appliquée à un corpus d’articles journalistiques.
							</p>

							<p id="recital-2009-long-010-key" class="mots_cles">
							<b>Mots clés : </b> Etiquetage d’entités nommées, ressources sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>Rami Ayadi, Walid Jaoudi</b>


						<br/>

							<i>La distance intertextuelle pour la classification de textes en langue arabe</i> <br/>

						<a href="actes/recital-2009-long-011.pdf">recital-2009-long-011</a> 
						<a href="bibtex/recital-2009-long-011.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-011-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-011-key');">mots clés</a> <br/>

							<p id="recital-2009-long-011-abs" class="resume">
							<b>Résumé : </b> Nos travaux de recherche s’intéressent à l’application de la théorie de la distance intertextuelle sur la langue arabe en tant qu’outil pour la classification de textes. Cette théorie traite de la classification de textes selon des critères de statistique lexicale, se basant sur la notion de connexion lexicale. Notre objectif est d’intégrer cette théorie en tant qu’outil de classification de textes en langue arabe. Ceci nécessite l’intégration d’une métrique pour la classification de textes au niveau d’une base de corpus lemmatisés étiquetés et identifiés comme étant des références d’époques, de genre, de thèmes littéraires et d’auteurs et ceci afin de permettre la classification de textes anonymes.
							</p>

							<p id="recital-2009-long-011-key" class="mots_cles">
							<b>Mots clés : </b> Distance intertextuelle, arabe, classification, lemmatisation, corpus, statistique lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Sara Boutouhami</b>


						<br/>

							<i>Techniques argumentatives pour aider à générer des descriptions orientées d’un événement</i> <br/>

						<a href="actes/recital-2009-long-012.pdf">recital-2009-long-012</a> 
						<a href="bibtex/recital-2009-long-012.bib">bibtex</a> 
							<a onclick="toggle('recital-2009-long-012-abs');">résumé</a>
							<a onclick="toggle('recital-2009-long-012-key');">mots clés</a> <br/>

							<p id="recital-2009-long-012-abs" class="resume">
							<b>Résumé : </b> Les moyens et les formes stratégiques permettant la génération de descriptions textuelles argumentées d’une même réalité effective sont nombreux. La plupart des définitions proposées de l’argumentation partagent l’idée qu’argumenter c’est fournir les éléments en faveur d’une conclusion donnée. Or dans notre tâche qui consiste à générer des descriptions argumentées pour des accidents de la route, nous ne disposons pas uniquement d’éléments en faveur de la conclusion souhaitée mais aussi d’éléments qui vont à l’encontre de cette dernière et dont la présence est parfois obligatoire pour la compréhension de ces descriptions. Afin de remédier à ce problème, nous proposons des techniques de génération de descriptions argumentées qui présentent au mieux les éléments indésirables à l’aide de stratégies argumentatives.
							</p>

							<p id="recital-2009-long-012-key" class="mots_cles">
							<b>Mots clés : </b> Argumentation, Insinuation, Norme coutumières, Justification
							</p>

					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>