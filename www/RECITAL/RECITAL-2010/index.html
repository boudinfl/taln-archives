<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>RECITAL'2010</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>RECITAL'2010, 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</h1>
				<h2>Montréal (Canada), du 2010-07-19 au 2010-07-23</h2>
				<p>Président(s) : Alexandre Patry, Philippe Langlais, Aurélien Max</p>
				<p>Taux d'acceptation :
							papiers longs (68.8%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#long">Papiers longs</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="long">Papiers longs</h1>
			

					<div class="article">

						<b>Audrey Laroche</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Attribution d’auteur au moyen de modèles de langue et de modèles stylométriques</i> <br/>

						<a href="actes/recital-2010-long-001.pdf">recital-2010-long-001</a> 
						<a href="bibtex/recital-2010-long-001.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-001-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-001-key');">mots clés</a> <br/>

							<p id="recital-2010-long-001-abs" class="resume">
							<b>Résumé : </b> Dans une tâche consistant à trouver l’auteur (parmi 53) de chacun de 114 textes, nous analysons la performance de modèles de langue et de modèles stylométriques sous les angles du rappel et du nombre de paramètres. Le modèle de mots bigramme à lissage de Kneser-Ney modifié interpolé est le plus performant (75 % de bonnes réponses au premier rang). Parmi les modèles stylométriques, une combinaison de 7 paramètres liés aux parties du discours produit les meilleurs résultats (rappel de 25 % au premier rang). Dans les deux catégories de modèles, le rappel maximal n’est pas atteint lorsque le nombre de paramètres est le plus élevé.
							</p>

							<p id="recital-2010-long-001-key" class="mots_cles">
							<b>Mots clés : </b> Attribution d’auteur, modèle de langue, stylométrie, n-grammes, vecteurs de traits
							</p>

					</div>
					

					<div class="article">

						<b>Hyeran Lee, Philippe Gambette, Elsa Maillé, Constance Thuillier</b>


						<br/>

							<i>Densidées : calcul automatique de la densité des idées dans un corpus oral</i> <br/>

						<a href="actes/recital-2010-long-002.pdf">recital-2010-long-002</a> 
						<a href="bibtex/recital-2010-long-002.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-002-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-002-key');">mots clés</a> <br/>

							<p id="recital-2010-long-002-abs" class="resume">
							<b>Résumé : </b> La densité des idées, qui correspond au ratio entre le nombre de propositions sémantiques et le nombre de mots dans un texte reflète la qualité informative des propositions langagières d’un texte. L&#39;apparition de la maladie d&#39;Alzheimer a été reliée à une dégradation de la densité des idées, ce qui explique l&#39;intérêt pour un calcul automatique de cette mesure. Nous proposons une méthode basée sur un étiquetage morphosyntaxique et des règles d&#39;ajustement, inspirée du logiciel CPIDR. Cette méthode a été validée sur un corpus de quarante entretiens oraux transcrits et obtient de meilleurs résultats pour le français que CPIDR pour l’anglais. Elle est implémentée dans le logiciel libre Densidées disponible sur http://code.google.com/p/densidees.
							</p>

							<p id="recital-2010-long-002-key" class="mots_cles">
							<b>Mots clés : </b> densité des idées, analyse prédicative, étiquetage sémantique, psycholinguistique
							</p>

					</div>
					

					<div class="article">

						<b>Li-Chi Wu</b>


						<br/>

							<i>Outils de segmentation du chinois et textométrie</i> <br/>

						<a href="actes/recital-2010-long-003.pdf">recital-2010-long-003</a> 
						<a href="bibtex/recital-2010-long-003.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-003-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-003-key');">mots clés</a> <br/>

							<p id="recital-2010-long-003-abs" class="resume">
							<b>Résumé : </b> La segmentation en mots est une première étape possible dans le traitement automatique de la langue chinoise. Les systèmes de segmentation se sont beaucoup développés depuis le premier apparu dans les années 1980. Il n’existe cependant aucun outil standard aujourd’hui. L’objectif de ce travail est de faire une comparaison des différents outils de segmentation en s’appuyant sur une analyse statistique. Le but est de définir pour quel type de texte chacun d’eux est le plus performant. Quatre outils de segmentation et deux corpus avec des thèmes distincts ont été choisis pour cette étude. À l’aide des outils textométriques Lexico3 et mkAlign, nous avons centré notre analyse sur le nombre de syllabes du chinois. Les données quantitatives ont permis d’objectiver des différences entre les outils. Le système Hylanda s’avère performant dans la segmentation des termes spécialisés et le système Stanford est plus indiqué pour les textes généraux. L’étude de la comparaison des outils de segmentation montre le statut incontournable de l’analyse textométrique aujourd’hui, celle-ci permettant d’avoir accès rapidement à la recherche d’information.
							</p>

							<p id="recital-2010-long-003-key" class="mots_cles">
							<b>Mots clés : </b> Textométrie, comparaison des segmenteurs chinois, nombre de syllabes
							</p>

					</div>
					

					<div class="article">

						<b>Mani Ezzat</b>


						<br/>

							<i>Acquisition de grammaires locales pour l’extraction de relations entre entités nommées</i> <br/>

						<a href="actes/recital-2010-long-004.pdf">recital-2010-long-004</a> 
						<a href="bibtex/recital-2010-long-004.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-004-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-004-key');">mots clés</a> <br/>

							<p id="recital-2010-long-004-abs" class="resume">
							<b>Résumé : </b> La constitution de ressources linguistiques est une tâche cruciale pour les systèmes d’extraction d’information fondés sur une approche symbolique. Ces systèmes reposent en effet sur des grammaires utilisant des informations issues de dictionnaires électroniques ou de réseaux sémantiques afin de décrire un phénomène linguistique précis à rechercher dans les textes. La création et la révision manuelle de telles ressources sont des tâches longues et coûteuses en milieu industriel. Nous présentons ici un nouvel algorithme produisant une grammaire d’extraction de relations entre entités nommées, de manière semi-automatique à partir d’un petit ensemble de phrases représentatives. Dans un premier temps, le linguiste repère un jeu de phrases pertinentes à partir d’une analyse des cooccurrences d’entités repérées automatiquement. Cet échantillon n’a pas forcément une taille importante. Puis, un algorithme permet de produire une grammaire en généralisant progressivement les éléments lexicaux exprimant la relation entre entités. L’originalité de l’approche repose sur trois aspects : une représentation riche du document initial permettant des généralisations pertinentes, la collaboration étroite entre les aspects automatiques et l’apport du linguiste et sur la volonté de contrôler le processus en ayant toujours affaire à des données lisibles par un humain.
							</p>

							<p id="recital-2010-long-004-key" class="mots_cles">
							<b>Mots clés : </b> relation, entité nommée, grammaire
							</p>

					</div>
					

					<div class="article">

						<b>Houda Bouamor</b>


						<br/>

							<i>Construction d’un corpus de paraphrases d’énoncés par traduction multiple multilingue</i> <br/>

						<a href="actes/recital-2010-long-005.pdf">recital-2010-long-005</a> 
						<a href="bibtex/recital-2010-long-005.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-005-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-005-key');">mots clés</a> <br/>

							<p id="recital-2010-long-005-abs" class="resume">
							<b>Résumé : </b> Les corpus de paraphrases à large échelle sont importants dans de nombreuses applications de TAL. Dans cet article nous présentons une méthode visant à obtenir un corpus parallèle de paraphrases d’énoncés en français. Elle vise à collecter des traductions multiples proposées par des contributeurs volontaires francophones à partir de plusieurs langues européennes. Nous formulons l’hypothèse que deux traductions soumises indépendamment par deux participants conservent généralement le sens de la phrase d’origine, quelle que soit la langue à partir de laquelle la traduction est effectuée. L’analyse des résultats nous permet de discuter cette hypothèse.
							</p>

							<p id="recital-2010-long-005-key" class="mots_cles">
							<b>Mots clés : </b> corpus monolingue parallèle, paraphrases, traductions multiples
							</p>

					</div>
					

					<div class="article">

						<b>Adila Amaria Bouabdallah</b>


						<br/>

							<i>Ces noms qui cachent des événements : un premier repérage</i> <br/>

						<a href="actes/recital-2010-long-006.pdf">recital-2010-long-006</a> 
						<a href="bibtex/recital-2010-long-006.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-006-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-006-key');">mots clés</a> <br/>

							<p id="recital-2010-long-006-abs" class="resume">
							<b>Résumé : </b> La détection des informations temporelles est cruciale pour le traitement automatique des textes, qu’il s’agisse de modélisation linguistique, d’applications en compréhension du langage ou encore de tâches de recherche documentaire ou d’extraction d’informations. De nombreux travaux ont été dédiés à l’analyse temporelle des textes, et plus précisément l’annotation des expressions temporelles ou des événements sous leurs différentes formes : verbales, adjectivales ou nominales. Dans cet article, nous décrivons une méthode pour la détection des syntagmes nominaux dénotant des événements. Notre approche est basée sur l’implémentation d’un test linguistique simple proposé par les linguistes pour cette tâche. Nous avons expérimenté notre méthode sur deux corpus différents ; le premier est composé d’articles de presse et le second est beaucoup plus grand, utilisant une interface pour interroger automatiquement le moteur de recherche Yahoo. Les résultats obtenus ont montré que cette méthode se révèle plus pertinente pour un plus large corpus.
							</p>

							<p id="recital-2010-long-006-key" class="mots_cles">
							<b>Mots clés : </b> Repérage des événements nominaux, annotation temporelle
							</p>

					</div>
					

					<div class="article">

						<b>Baptiste Chardon</b>


						<br/>

							<i>Catégorisation automatique d&#39;adjectifs d&#39;opinion à partir d&#39;une ressource linguistique générique</i> <br/>

						<a href="actes/recital-2010-long-007.pdf">recital-2010-long-007</a> 
						<a href="bibtex/recital-2010-long-007.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-007-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-007-key');">mots clés</a> <br/>

							<p id="recital-2010-long-007-abs" class="resume">
							<b>Résumé : </b> Cet article décrit un processus d’annotation manuelle de textes d’opinion, basé sur un schéma fin d&#39;annotation indépendant de la langue et du corpus. Ensuite, à partir d&#39;une partie de ce schéma, une méthode de construction automatique d&#39;un lexique d&#39;opinion à partir d&#39;un analyseur syntaxique et d&#39;une ressource linguistique est décrite. Cette méthode consiste à construire un arbre de décision basé sur les classes de concepts de la ressource utilisée. Dans un premier temps, nous avons étudié la couverture du lexique d&#39;opinion obtenu par comparaison avec l’annotation manuelle effectuée sur un premier corpus de critiques de restaurants. La généricité de ce lexique a été mesurée en le comparant avec un second lexique, généré à partir d&#39;un corpus de commentaires de films. Dans un second temps, nous avons évalué l&#39;utilisabilité du lexique au travers d&#39;une tâche extrinsèque, la reconnaissance de la polarité de commentaires d&#39;internautes.
							</p>

							<p id="recital-2010-long-007-key" class="mots_cles">
							<b>Mots clés : </b> Analyse d&#39;opinion, Extension de lexique, Annotation d&#39;opinions
							</p>

					</div>
					

					<div class="article">

						<b>Mohamed Hédi Maâloul, Iskandar keskes</b>


						<br/>

							<i>Résumé automatique de documents arabes basé sur la technique RST</i> <br/>

						<a href="actes/recital-2010-long-008.pdf">recital-2010-long-008</a> 
						<a href="bibtex/recital-2010-long-008.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-008-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-008-key');">mots clés</a> <br/>

							<p id="recital-2010-long-008-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous nous intéressons au résumé automatique de textes arabes. Nous commençons par présenter une étude analytique réalisée sur un corpus de travail qui nous a permis de déduire, suite à des observations empiriques, un ensemble de relations et de frames (règles ou patrons) rhétoriques; ensuite nous présentons notre méthode de production de résumés pour les textes arabes. La méthode que nous proposons se base sur la Théorie de la Structure Rhétorique (RST) (Mann et al., 1988) et utilise des connaissances purement linguistiques. Le principe de notre proposition s’appuie sur trois piliers. Le premier pilier est le repérage des relations rhétoriques entres les différentes unités minimales du texte dont l’une possède le statut de noyau – segment de texte primordial pour la cohérence – et l’autre a le statut noyau ou satellite – segment optionnel. Le deuxième pilier est le dressage et la simplification de l’arbre RST. Le troisième pilier est la sélection des phrases noyaux formant le résumé final, qui tiennent en compte le type de relation rhétoriques choisi pour l’extrait.
							</p>

							<p id="recital-2010-long-008-key" class="mots_cles">
							<b>Mots clés : </b> Théorie de la Structure Rhétorique, Relations rhétoriques, Marqueurs linguistiques, Résumé automatique de textes arabes
							</p>

					</div>
					

					<div class="article">

						<b>Hee-Jin Ro</b>


						<br/>

							<i>Inférences aspecto-temporelles analysées avec la Logique Combinatoire</i> <br/>

						<a href="actes/recital-2010-long-009.pdf">recital-2010-long-009</a> 
						<a href="bibtex/recital-2010-long-009.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-009-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-009-key');">mots clés</a> <br/>

							<p id="recital-2010-long-009-abs" class="resume">
							<b>Résumé : </b> Ce travail s’inscrit dans une recherche centrée sur une approche de l’Intelligence Artificielle (IA) et de la linguistique computationnelle. Il permet d’intégrer différentes techniques formelles de la Logique Combinatoire avec des types (Curry) et sa programmation fonctionnelle (Haskell) avec une théorie énonciative du temps et de l’aspect. Nous proposons des calculs formels de valeurs aspectotemporelles (processus inaccompli présent, processus inaccompli passé, événement passé et étatrésultant présent) associées à des représentations de significations verbales sous forme de schèmes applicatifs.
							</p>

							<p id="recital-2010-long-009-key" class="mots_cles">
							<b>Mots clés : </b> Logique Combinatoire, Référentiel énonciatif, Schème sémantico-cognitif, Grammaire Applicative et Cognitive, Haskell
							</p>

					</div>
					

					<div class="article">

						<b>Selja Seppälä</b>


						<br/>

							<i>Automatiser la rédaction de définitions terminographiques : questions et traitements</i> <br/>

						<a href="actes/recital-2010-long-010.pdf">recital-2010-long-010</a> 
						<a href="bibtex/recital-2010-long-010.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-010-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-010-key');">mots clés</a> <br/>

							<p id="recital-2010-long-010-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une analyse manuelle de corpus de contextes conceptuels afin (i) de voir dans quelle mesure les méthodes de TALN existantes sont en principe adéquates pour automatiser la rédaction de définitions terminographiques, et (ii) de dégager des question précises dont la résolution permettrait d’automatiser davantage la production de définitions. Le but est de contribuer à la réflexion sur les enjeux de l’automatisation de cette tâche, en procédant à une série d’analyses qui nous mènent, étape par étape, à examiner l’adéquation des méthodes d’extraction de définitions et de contextes plus larges au travail terminographique de rédaction des définitions. De ces analyses émergent des questions précises relatives à la pertinence des informations extraites et à leur sélection. Des propositions de solutions et leurs implications pour le TALN sont examinées.
							</p>

							<p id="recital-2010-long-010-key" class="mots_cles">
							<b>Mots clés : </b> Terminologie, définitions terminographiques, sélection des traits, pertinence des traits, extraction de définitions, contextes conceptuels, traitement automatique des définitions.
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Trouvilliez</b>


						<br/>

							<i>Représentation vectorielle de textes courts d’opinions, Analyse de traitements sémantiques pour la fouille d’opinions par clustering</i> <br/>

						<a href="actes/recital-2010-long-011.pdf">recital-2010-long-011</a> 
						<a href="bibtex/recital-2010-long-011.bib">bibtex</a> 
							<a onclick="toggle('recital-2010-long-011-abs');">résumé</a>
							<a onclick="toggle('recital-2010-long-011-key');">mots clés</a> <br/>

							<p id="recital-2010-long-011-abs" class="resume">
							<b>Résumé : </b> Avec le développement d’internet et des sites d’échanges (forums, blogs, sondages en ligne, ...), l’exploitation de nouvelles sources d’informations dans le but d’en extraire des opinions sur des sujets précis (film, commerce,...) devient possible. Dans ce papier, nous présentons une approche de fouille d’opinions à partir de textes courts. Nous expliquons notamment en quoi notre choix d’utilisation de regroupements autour des idées exprimées nous a conduit à opter pour une représentation implicite telle que la représentation vectorielle. Nous voyons également les différents traitements sémantiques intégrés à notre chaîne de traitement (traitement de la négation, lemmatisation, stemmatisation, synonymie ou même polysémie des mots) et discutons leur impact sur la qualité des regroupements obtenus.
							</p>

							<p id="recital-2010-long-011-key" class="mots_cles">
							<b>Mots clés : </b> représentation des textes, représentation vectorielle, traitement de textes courts, regroupements d’opinions
							</p>

					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>