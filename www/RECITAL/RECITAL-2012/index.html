<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>RECITAL'2012</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>RECITAL'2012, 14e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</h1>
				<h2>Grenoble (France), du 2012-06-04 au 2012-06-08</h2>
				<p>Président(s) : Didier Schwab, Jorge-Mauricio Molina-Mejia</p>
				<p>Taux d'acceptation :
							papiers longs (66.7%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#long">Papiers longs</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="long">Papiers longs</h1>
			

					<div class="article">

						<b>Pierre Magistry</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Segmentation non supervisée : le cas du mandarin</i> <br/>

						<a href="actes/recital-2012-long-001.pdf">recital-2012-long-001</a> 
						<a href="bibtex/recital-2012-long-001.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-001-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-001-key');">mots clés</a> <br/>

							<p id="recital-2012-long-001-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons un système de segmentation non supervisée que nous évaluons sur des données en mandarin. Notre travail s’inspire de l’hypothèse de Harris (1955) et suit Kempe (1999) et Tanaka-Ishii (2005) en se basant sur la reformulation de l’hypothèse en termes de variation de l’entropie de branchement. Celle-ci se révèle être un bon indicateur des frontières des unités linguistiques. Nous améliorons le système de (Jin et Tanaka-Ishii, 2006) en ajoutant une étape de normalisation qui nous permet de reformuler la façon dont sont prises les décisions de segmentation en ayant recours à la programmation dynamique. Ceci nous permet de supprimer la plupart des seuils de leur modèle tout en obtenant de meilleurs résultats, qui se placent au niveau de l’état de l’art (Wang et al., 2011) avec un système plus simple que ces derniers. Nous présentons une évaluation des résultats sur plusieurs corpus diffusés pour le Chinese Word Segmentation bake-off II (Emerson, 2005) et détaillons la borne supérieure que l’on peut espérer atteindre avec une méthode non-supervisée. Pour cela nous utilisons ZPAR en apprentissage croisé (Zhang et Clark, 2010) comme suggéré dans (Huang et Zhao, 2007; Zhao et Kit, 2008)
							</p>

							<p id="recital-2012-long-001-key" class="mots_cles">
							<b>Mots clés : </b> Apprentissage non-supervisé, segmentation, chinois, mandarin
							</p>

					</div>
					

					<div class="article">

						<b>Matthias Tauveron</b>


						<br/>

							<i>Incrémentation lexicale dans les textes : une auto-organisation</i> <br/>

						<a href="actes/recital-2012-long-002.pdf">recital-2012-long-002</a> 
						<a href="bibtex/recital-2012-long-002.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-002-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-002-key');">mots clés</a> <br/>

							<p id="recital-2012-long-002-abs" class="resume">
							<b>Résumé : </b> Nous proposons une étude dynamique du lexique, en décrivant la manière dont il s’organise progressivement du début à la fin d’un texte. Pour ce faire, nous nous focalisons sur la co-occurrence généralisée, en formant un graphe qui représente tous les lemmes du texte et synthétise leurs relations mutuelles de co-occurrence. L’étude d’un corpus de 40 textes montre que ces relations évoluent d’une manière auto-organisée : la forme - et l’identité - du graphe de co-occurrence restent stables après une phase d’organisation terminée avant la 1ère moitié du texte. Ensuite, il n’évolue plus : les nouveaux mots et les nouvelles relations de co-occurrence s’inscrivent peu à peu dans le réseau, sans modifier la forme d’ensemble de la structure. La relation de co-occurrence généralisée dans un texte apparaît donc comme la construction rapide d’un système, qui est ensuite assez souple pour canaliser un flux d’information sans changer d’identité.
							</p>

							<p id="recital-2012-long-002-key" class="mots_cles">
							<b>Mots clés : </b> Texte, lexique, co-occurrence généralisée, auto-organisation
							</p>

					</div>
					

					<div class="article">

						<b>Alexander Panchenko</b>


						<br/>

							<i>Etude des mesures de similarité hétérogènes pour l’extraction de relations sémantiques</i> <br/>

						<a href="actes/recital-2012-long-003.pdf">recital-2012-long-003</a> 
						<a href="bibtex/recital-2012-long-003.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-003-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-003-key');">mots clés</a> <br/>

							<p id="recital-2012-long-003-abs" class="resume">
							<b>Résumé : </b> L’article évalue un éventail de mesures de similarité qui ont pour but de prédire les scores de similarité sémantique et les relations sémantiques qui s’établissent entre deux termes, et étudie les moyens de combiner ces mesures. Nous présentons une analyse comparative à grande échelle de 34 mesures basées sur des réseaux sémantiques, le Web, des corpus, ainsi que des définitions. L’article met en évidence les forces et les faiblesses de chaque approche en contexte de l’extraction de relations. Enfin, deux techniques de combinaison de mesures sont décrites et testées. Les résultats montrent que les mesures combinées sont plus performantes que toutes les mesures simples et aboutissent à une corrélation de 0,887 et une Precision(20) de 0,979.
							</p>

							<p id="recital-2012-long-003-key" class="mots_cles">
							<b>Mots clés : </b> Similarité sémantique, Relations sémantiques, Similarité distributionnelle
							</p>

					</div>
					

					<div class="article">

						<b>Luong Ngoc Quang</b>


						<br/>

							<i>Intégration de paramètres lexicaux, syntaxiques et issus du système de traduction automatique pour améliorer l’estimation des mesures de confiance au niveau des mots</i> <br/>

						<a href="actes/recital-2012-long-004.pdf">recital-2012-long-004</a> 
						<a href="bibtex/recital-2012-long-004.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-004-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-004-key');">mots clés</a> <br/>

							<p id="recital-2012-long-004-abs" class="resume">
							<b>Résumé : </b> L’estimation des mesures de confiance (MC) au niveau des mots consiste à prédire leur exactitude dans la phrase cible générée par un système de traduction automatique. Ceci permet d’estimer la fiabilité d&#39;une sortie de traduction et de filtrer les segments trop mal traduits pour une post-édition. Nous étudions l’impact sur le calcul des MC de différents paramètres : lexicaux, syntaxiques et issus du système de traduction. Nous présentons la méthode permettant de labelliser automatiquement nos corpus (mot correct ou incorrect), puis le classifieur à base de champs aléatoires conditionnels utilisé pour intégrer les différents paramètres et proposer une classification appropriée des mots. Nous avons effectué des expériences préliminaires, avec l’ensemble des paramètres, où nous mesurons la précision, le rappel et la F-mesure. Finalement nous comparons les résultats avec notre système de référence. Nous obtenons de bons résultats pour la classification des mots considérés comme corrects (F-mesure : 86.7%), et encourageants pour ceux estimés comme mal traduits (F-mesure : 36,8%).
							</p>

							<p id="recital-2012-long-004-key" class="mots_cles">
							<b>Mots clés : </b> Système de traduction automatique, mesure de confiance, estimation de la confiance, champs aléatoires conditionnels
							</p>

					</div>
					

					<div class="article">

						<b>Gabriel Bernier-Colborne</b>


						<br/>

							<i>Application d’un algorithme de traduction statistique à la normalisation de textos</i> <br/>

						<a href="actes/recital-2012-long-005.pdf">recital-2012-long-005</a> 
						<a href="bibtex/recital-2012-long-005.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-005-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-005-key');">mots clés</a> <br/>

							<p id="recital-2012-long-005-abs" class="resume">
							<b>Résumé : </b> Ce travail porte sur l’application d’une technique de traduction statistique au problème de la normalisation de textos. La méthode est basée sur l’algorithme de recherche vorace décrit dans (Langlais et al., 2007). Une première normalisation est générée, puis nous appliquons itérativement une fonction qui génère des nouvelles hypothèses à partir de la normalisation courante, et maximisons une fonction de score. Cette méthode fournit une réduction du taux d’erreurs moyen par phrase de 33 % sur le corpus de test, et une augmentation du score BLEU de plus de 30 %. Nous mettons l’accent sur les fonctions qui génèrent la normalisation initiale et sur les opérations permettant de générer des nouvelles hypothèses.
							</p>

							<p id="recital-2012-long-005-key" class="mots_cles">
							<b>Mots clés : </b> Traduction statistique, normalisation de textos, algorithme de recherche vorace, modèle de langue
							</p>

					</div>
					

					<div class="article">

						<b>Marion Baranes</b>


						<br/>

							<i>Vers la correction automatique de textes bruités: Architecture générale et détermination de la langue d’un mot inconnu</i> <br/>

						<a href="actes/recital-2012-long-006.pdf">recital-2012-long-006</a> 
						<a href="bibtex/recital-2012-long-006.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-006-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-006-key');">mots clés</a> <br/>

							<p id="recital-2012-long-006-abs" class="resume">
							<b>Résumé : </b> Dans ce papier, nous introduisons le problème que pose la correction orthographique sur des corpus de qualité très dégradée tels que les messages publiés sur les forums, les sites d’avis ou les réseaux sociaux. Nous proposons une première architecture de correction qui a pour objectif d’éviter au maximum la sur-correction. Nous présentons, par ailleurs l’implémentation et les résultats d’un des modules de ce système qui a pour but de détecter si un mot inconnu, dans une phrase de langue connue, est un mot qui appartient à cette langue ou non.
							</p>

							<p id="recital-2012-long-006-key" class="mots_cles">
							<b>Mots clés : </b> Correction automatique, détection de langue, données produite par l’utilisateur
							</p>

					</div>
					

					<div class="article">

						<b>Carlos Ramisch</b>


						<br/>

							<i>Une plate-forme générique et ouverte pour l’acquisition des expressions polylexicales</i> <br/>

						<a href="actes/recital-2012-long-007.pdf">recital-2012-long-007</a> 
						<a href="bibtex/recital-2012-long-007.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-007-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-007-key');">mots clés</a> <br/>

							<p id="recital-2012-long-007-abs" class="resume">
							<b>Résumé : </b> Cet article présente et évalue une plate-forme ouverte et flexible pour l’acquisition automatique d’expressions polylexicales (EPL) à partir des corpus monolingues. Nous commençons par une motivation pratique suivie d’une discussion théorique sur le comportement et les défis posés par les EPL dans les applications de TAL. Ensuite, nous décrivons les modules de notre plate-forme, leur enchaînement et les choix d’implémentation. L’évaluation de la plate-forme a été effectuée à travers une applications : la lexicographie assistée par ordinateur. Cette dernière peut bénéficier de l’acquisition d’EPL puisque les expressions acquises automatiquement à partir des corpus peuvent à la fois accélérer la création et améliorer la qualité et la couverture des ressources lexicales. Les résultats prometteurs encouragent une recherche plus approfondie sur la manière optimale d’intégrer le traitement des EPL dans de nombreuses applications de TAL, notamment dans les systèmes traduction automatique.
							</p>

							<p id="recital-2012-long-007-key" class="mots_cles">
							<b>Mots clés : </b> Expressions polylexicales, extraction lexicale, lexique, mesures d’association, corpus, lexicographie
							</p>

					</div>
					

					<div class="article">

						<b>Aurélie Merlo</b>


						<br/>

							<i>Système de prédiction de néologismes formels : le cas des N suffixés par -IER dénotant des artefacts</i> <br/>

						<a href="actes/recital-2012-long-008.pdf">recital-2012-long-008</a> 
						<a href="bibtex/recital-2012-long-008.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-008-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-008-key');">mots clés</a> <br/>

							<p id="recital-2012-long-008-abs" class="resume">
							<b>Résumé : </b> Nous présentons ici un système de prédiction de néologismes formels avec pour exemple la génération automatique de néologismes nominaux suffixés par -IER dénotant des artefacts (saladier, brassière, thonier). L’objectif de cet article est double. Il s’agira (i) de mettre en évidence les contraintes de la suffixation par -IER afin de les implémenter dans un système de génération morphologique puis (ii) de montrer qu’il est possible de prédire les néologismes formels. Ce système de prédiction permettrait ainsi de compléter automatiquement les lexiques pour le Traitement Automatique des Langues (TAL).
							</p>

							<p id="recital-2012-long-008-key" class="mots_cles">
							<b>Mots clés : </b> morphologie constructionnelle, néologie, génération morphologique, incomplétude lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Boris Karlov, Ophélie Lacroix</b>


						<br/>

							<i>Prémices d’une analyse syntaxique par transition pour des structures de dépendance non-projectives</i> <br/>

						<a href="actes/recital-2012-long-009.pdf">recital-2012-long-009</a> 
						<a href="bibtex/recital-2012-long-009.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-009-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-009-key');">mots clés</a> <br/>

							<p id="recital-2012-long-009-abs" class="resume">
							<b>Résumé : </b> L’article présente une extension de l’analyseur traditionnel en dépendances par transitions adapté aux dépendances discontinues et les premiers résultats de son entraînement sur un corpus de structures de dépendances de phrases en français. Les résultats des premières expérimentations vont servir de base pour le choix des traits des configurations de calcul bien adaptés aux dépendances discontinues pour améliorer l’apprentissage des dépendances tête.
							</p>

							<p id="recital-2012-long-009-key" class="mots_cles">
							<b>Mots clés : </b> analyse syntaxique par transitions, structure de dépendance non-projective, grammaire catégorielle de dépendance
							</p>

					</div>
					

					<div class="article">

						<b>Julie Belião</b>


						<br/>

							<i>Création d’un multi-arbre à partir d’un texte balisé : l’exemple de l’annotation d’un corpus d’oral spontané</i> <br/>

						<a href="actes/recital-2012-long-010.pdf">recital-2012-long-010</a> 
						<a href="bibtex/recital-2012-long-010.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-010-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-010-key');">mots clés</a> <br/>

							<p id="recital-2012-long-010-abs" class="resume">
							<b>Résumé : </b> Dans cette étude, nous nous intéressons au problème de l’analyse d’un corpus annoté de l’oral. Le système d’annotation considéré est celui introduit par l’équipe des syntacticiens du projet Rhapsodie. La principale problématique qui sous-tend un tel projet est que la base écrite sur laquelle on travaille est en réalité une transcription de l’oral, balisée par les annotateurs de manière à délimiter un ensemble de structures arborescentes. Un tel système introduit plusieurs structures, en particulier macro et micro-syntaxiques. Du fait de leur étroite imbrication, il s’est avéré difficile de les analyser de façon indépendante et donc de travailler sur l’aspect macro-syntaxique indépendamment de l’aspect micro-syntaxique. Cependant, peu d’études jusqu’à présent considèrent ces problèmes conjointement et de manière automatisée. Dans ce travail, nous présentons nos efforts en vue de produire un outil de parsing capable de rendre compte à la fois de l’information micro et macro-syntaxique du texte annoté. Pour ce faire, nous proposons une représentation partant de la notion de multi-arbre et nous montrons comment une telle structure peut être générée à partir de l’annotation et utilisée à des fins d’analyse.
							</p>

							<p id="recital-2012-long-010-key" class="mots_cles">
							<b>Mots clés : </b> Arbres syntaxiques, unité illocutoire, unités rectionnelles, micro-syntaxe, macrosyntaxe, entassement
							</p>

					</div>
					

					<div class="article">

						<b>Noémi Boubel</b>


						<br/>

							<i>Construction automatique d’un lexique de modifieurs de polarité</i> <br/>

						<a href="actes/recital-2012-long-011.pdf">recital-2012-long-011</a> 
						<a href="bibtex/recital-2012-long-011.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-011-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-011-key');">mots clés</a> <br/>

							<p id="recital-2012-long-011-abs" class="resume">
							<b>Résumé : </b> La recherche présentée 1 s’inscrit dans le domaine de la fouille d’opinion, domaine qui consiste principalement à déterminer la polarité d’un texte ou d’une phrase. Dans cette optique, le contexte autour d’un mot polarisé joue un rôle essentiel, car il peut modifier la polarité initiale de ce terme. Nous avons choisi d’approfondir cette question et de détecter précisément ces modifieurs de polarité. Une étude exploratoire, décrite dans des travaux antérieurs, nous a permis d’extraire automatiquement des adverbes qui jouent un rôle sur la polarité des adjectifs auxquels ils sont associés et de préciser leur impact. Nous avons ensuite amélioré le système d’extraction afin de construire automatiquement un lexique de structures lexico-syntaxiques modifiantes associées au type d’impact qu’elles ont sur un terme polarisé. Nous présentons ici le fonctionnement du système actuel ainsi que l’évaluation du lexique obtenu.
							</p>

							<p id="recital-2012-long-011-key" class="mots_cles">
							<b>Mots clés : </b> fouille d’opinion, modifieurs de valence affective, modifieurs de polarité
							</p>

					</div>
					

					<div class="article">

						<b>Ahmed Hamdi</b>


						<br/>

							<i>Apport de la diacritisation dans l’analyse morphosyntaxique de l’arabe</i> <br/>

						<a href="actes/recital-2012-long-012.pdf">recital-2012-long-012</a> 
						<a href="bibtex/recital-2012-long-012.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-012-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-012-key');">mots clés</a> <br/>

							<p id="recital-2012-long-012-abs" class="resume">
							<b>Résumé : </b> Ce travail s’inscrit dans le cadre de l’analyse morphologique et syntaxique automatique de la langue arabe. Nous nous intéressons au traitement de la diacritisation et à son apport pour l’analyse morphologique. En effet, la plupart des analyseurs morphologiques et des étiqueteurs morphosyntaxiques existants ignorent les diacritiques présents dans le texte à analyser et commettent des erreurs qui pourraient être évitées. Dans cet article, nous proposons une méthode qui prend en considération les diacritiques lors de l’analyse, et nous montrons que cette prise en compte permet de diminuer considérablement le taux d’erreur de l’analyse morphologique selon le taux de diacritiques du texte traité.
							</p>

							<p id="recital-2012-long-012-key" class="mots_cles">
							<b>Mots clés : </b> diacritisation, traitement automatique, analyse morphosyntaxique, langue arabe
							</p>

					</div>
					

					<div class="article">

						<b>Alexandre Baudrillart</b>


						<br/>

							<i>Extraction d’indicateurs de construction collective de connaissances dans la formation en ligne</i> <br/>

						<a href="actes/recital-2012-long-013.pdf">recital-2012-long-013</a> 
						<a href="bibtex/recital-2012-long-013.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-013-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-013-key');">mots clés</a> <br/>

							<p id="recital-2012-long-013-abs" class="resume">
							<b>Résumé : </b> Dans le cadre d’apprentissages humains assistés par des environnements informatiques, les techniques de TAL ne sont que rarement employées ou restreintes à des tâches ou des domaines spécifiques comme l’ALAO (Apprentissage de la Langue Assisté par Ordinateur) où elles sont omniprésentes mais ne concernent que certaines dimensions du TAL. Nous cherchons à explorer les possibilités ou les performances des techniques voire des méthodes de TAL pour des systèmes moins spécifiques dès lors qu’une dimension de réseau et de collectivité est présente. Plus particulièrement, notre objectif est d’obtenir des indicateurs sur la construction collective de connaissances, et ses modalités. Ce papier présente la problématique de notre thèse, son contexte, nos motivations ainsi que nos premières réflexions.
							</p>

							<p id="recital-2012-long-013-key" class="mots_cles">
							<b>Mots clés : </b> TAL, EIAH, formation en ligne, socio-constructivisme, acquisition des connaissances, apprentissage collaboratif en ligne
							</p>

					</div>
					

					<div class="article">

						<b>Andon Tchechmedjiev</b>


						<br/>

							<i>État de l’art : mesures de similarité sémantique locales et algorithmes globaux pour la désambiguïsation lexicale à base de connaissances</i> <br/>

						<a href="actes/recital-2012-long-014.pdf">recital-2012-long-014</a> 
						<a href="bibtex/recital-2012-long-014.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-014-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-014-key');">mots clés</a> <br/>

							<p id="recital-2012-long-014-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons les principales méthodes non supervisées à base de connaissances pour la désambiguïsation lexicale. Elles sont composées d’une part de mesures de similarité sémantique locales qui donnent une valeur de proximité entre deux sens de mots et, d’autre part, d’algorithmes globaux qui utilisent les mesures de similarité sémantique locales pour trouver les sens appropriés des mots selon le contexte à l’échelle de la phrase ou du texte.
							</p>

							<p id="recital-2012-long-014-key" class="mots_cles">
							<b>Mots clés : </b> désambiguïsation lexicale non-supervisée, mesures de similarité sémantique à base de connaissances, algorithmes globaux de propagation de mesures locales
							</p>

					</div>
					

					<div class="article">

						<b>Arnaud Kirsch</b>


						<br/>

							<i>Compression textuelle sur la base de règles issues d&#39;un corpus de sms</i> <br/>

						<a href="actes/recital-2012-long-015.pdf">recital-2012-long-015</a> 
						<a href="bibtex/recital-2012-long-015.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-015-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-015-key');">mots clés</a> <br/>

							<p id="recital-2012-long-015-abs" class="resume">
							<b>Résumé : </b> La présente recherche cherche à réduire la taille de messages textuels sur la base de techniques de compression observées, pour la plupart, dans un corpus de sms. Ce papier explique la méthodologie suivie pour établir des règles de contraction. Il présente ensuite les 33 règles retenues, et illustre les quatre niveaux de compression proposés par deux exemples concrets, produits automatiquement par un premier prototype. Le but de cette recherche n&#39;est donc pas de produire de &#34;l&#39;écrit-sms&#34;, mais d&#39;élaborer un procédé de compression capable de produire des textes courts et compréhensibles à partir de n&#39;importe quelle source textuelle en français. Le terme &#34;d&#39;essentialisation&#34; est proposé pour désigner cette approche de réduction textuelle.
							</p>

							<p id="recital-2012-long-015-key" class="mots_cles">
							<b>Mots clés : </b> résumé automatique, compression de texte, sms, lisibilité, essentialisation
							</p>

					</div>
					

					<div class="article">

						<b>Aurélie Joseph</b>


						<br/>

							<i>Pour un étiquetage automatique des séquences verbales figées : état de l’art et approche transformationnelle</i> <br/>

						<a href="actes/recital-2012-long-016.pdf">recital-2012-long-016</a> 
						<a href="bibtex/recital-2012-long-016.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-016-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-016-key');">mots clés</a> <br/>

							<p id="recital-2012-long-016-abs" class="resume">
							<b>Résumé : </b> Cet article présente une approche permettant de reconnaitre automatiquement dans un texte des séquences verbales figées (casser sa pipe, briser la glace, prendre en compte) à partir d’une ressource. Cette ressource décrit chaque séquence en termes de possibilités et de restrictions transformationnelles. En effet, les séquences figées ne le sont pas complètement et nécessitent une description exhaustive afin de ne pas extraire seulement les formes canoniques. Dans un premier temps nous aborderons les approches traditionnelles permettant d’extraire des séquences phraséologiques. Par la suite, nous expliquerons comment est constituée notre ressource et comment celle-ci est utilisée pour un traitement automatique.
							</p>

							<p id="recital-2012-long-016-key" class="mots_cles">
							<b>Mots clés : </b> séquences verbales figées, reconnaissance automatique, étiquetage, transformations linguistiques, ressources électroniques
							</p>

					</div>
					

					<div class="article">

						<b>Céline Battaïa</b>


						<br/>

							<i>L’analyse de l’émotion dans les forums de santé</i> <br/>

						<a href="actes/recital-2012-long-017.pdf">recital-2012-long-017</a> 
						<a href="bibtex/recital-2012-long-017.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-017-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-017-key');">mots clés</a> <br/>

							<p id="recital-2012-long-017-abs" class="resume">
							<b>Résumé : </b> Les travaux sur l’émotion dans les forums sont nombreux en Linguistique et Psychologie. L’objectif de cette contribution est de proposer une analyse de l’émotion dans les forums de santé selon l’angle des Sciences de l’Information et de la Communication mais également selon une approche interdisciplinaire. Il s’agira ici, d’étudier l’émotion comme un critère de pertinence lorsque des personnes malades effectuent des recherches dans les forums. Ce papier introduit la méthodologie utilisée en traitement automatique de la langue afin de répondre à cette interrogation. Ainsi, le travail présenté abordera l’exploitation d’un corpus de messages de forums, la catégorisation semi-supervisée et l’utilisation du logiciel NooJ pour traiter de manière automatique les données.
							</p>

							<p id="recital-2012-long-017-key" class="mots_cles">
							<b>Mots clés : </b> émotion, forum de santé, traitement automatique de la langue, désambiguïsation lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Driss Sadoun</b>


						<br/>

							<i>Peuplement d’une ontologie modélisant le comportement d’un environnement intelligent guidé par l’extraction d’instances de relations</i> <br/>

						<a href="actes/recital-2012-long-018.pdf">recital-2012-long-018</a> 
						<a href="bibtex/recital-2012-long-018.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-018-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-018-key');">mots clés</a> <br/>

							<p id="recital-2012-long-018-abs" class="resume">
							<b>Résumé : </b> Nous présentons une approche de peuplement d’ontologie dont le but est de modéliser le comportement de composants logiciels afin de faciliter le passage de descriptions d’exigences en langue naturelle à des spécifications formelles. L’ontologie que nous cherchons à peupler a été conçue à partir des connaissances du domaine de la domotique et est initialisée à partir d’une description de la configuration physique d’un environnement intelligent. Notre méthode est guidée par l’extraction d’instances de relations permettant par là-même d’extraire les instances de concepts liés par ces relations. Nous construisons des règles d’extraction à partir d’éléments issus de l’analyse syntaxique de descriptions de besoins utilisateurs et de ressources terminologiques associées aux concepts et relations de l’ontologie. Notre approche de peuplement se distingue par sa finalité qui n’est pas d’extraire toutes les instances décrivant un domaine mais d’extraire des instances pouvant participer sans conflit à un des multiples fonctionnements décrit par des utilisateurs.
							</p>

							<p id="recital-2012-long-018-key" class="mots_cles">
							<b>Mots clés : </b> extraction de relations, peuplement d’ontologie, représentation des connaissances
							</p>

					</div>
					

					<div class="article">

						<b>Franck Dernoncourt</b>


						<br/>

							<i>De l&#39;utilisation du dialogue naturel pour masquer les QCM au sein des jeux sérieux</i> <br/>

						<a href="actes/recital-2012-long-019.pdf">recital-2012-long-019</a> 
						<a href="bibtex/recital-2012-long-019.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-019-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-019-key');">mots clés</a> <br/>

							<p id="recital-2012-long-019-abs" class="resume">
							<b>Résumé : </b> Une des principales faiblesses des jeux sérieux à l&#39;heure actuelle est qu&#39;ils incorporent très souvent des questionnaires à choix multiple (QCM). Or, aucune étude n&#39;a démontré que les QCM sont capables d&#39;évaluer précisément le niveau de compréhension des apprenants. Au contraire, certaines études ont montré expérimentalement que permettre à l&#39;apprenant d&#39;entrer une phrase libre dans le programme au lieu de simplement cocher une réponse dans un QCM rend possible une évaluation beaucoup plus fine des compétences de l&#39;apprenant. Nous proposons donc de concevoir un agent conversationnel capable de comprendre des énoncés en langage naturel dans un cadre sémantique restreint, cadre correspondant au domaine de compétence testé chez l&#39;apprenant. Cette fonctionnalité est destinée à permettre un dialogue naturel avec l&#39;apprenant, en particulier dans le cadre des jeux sérieux. Une telle interaction en langage naturel a pour but de masquer les QCM sous-jacents. Cet article présente notre approche.
							</p>

							<p id="recital-2012-long-019-key" class="mots_cles">
							<b>Mots clés : </b> Agent conversationnel éducatif, intelligence artificielle, jeu sérieux, questionnaire à choix multiple, système d&#39;évaluation de réponses libres
							</p>

					</div>
					

					<div class="article">

						<b>Jihene Jmal</b>


						<br/>

							<i>ResTS : Système de Résumé Automatique des Textes d’Opinions basé sur Twitter et SentiWordNet</i> <br/>

						<a href="actes/recital-2012-long-020.pdf">recital-2012-long-020</a> 
						<a href="bibtex/recital-2012-long-020.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-020-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-020-key');">mots clés</a> <br/>

							<p id="recital-2012-long-020-abs" class="resume">
							<b>Résumé : </b> Comme le E-commerce est devenu de plus en plus populaire, le nombre de commentaires des internautes est en croissance constante. Les opinions sur le Web affectent nos choix et nos décisions. Il s’avère alors indispensable de traiter une quantité importante de critiques des clients afin de présenter à l’utilisateur l’information dont il a besoin dans la forme la plus appropriée. Dans cet article, nous présentons ResTS, un nouveau système de résumé automatique de textes d’opinions basé sur les caractéristiques des produits. Notre approche vise à transformer les critiques des utilisateurs en des scores qui mesurent le degré de satisfaction des clients pour un produit donné et pour chacune de ses caractéristiques. Ces scores sont compris entre 0 et 1 et peuvent être utilisés pour la prise de décision. Nous avons étudié les opinions véhiculées par les noms, les adjectifs, les verbes et les adverbes, contrairement aux recherches précédentes qui utilisent essentiellement les adjectifs. Les résultats expérimentaux préliminaires montrent que notre méthode est comparable aux méthodes classiques de résumé automatique basées sur les caractéristiques des produits.
							</p>

							<p id="recital-2012-long-020-key" class="mots_cles">
							<b>Mots clés : </b> Fouille d’opinion, Classification, Intensité de l’Opinion, Résumé de texte d’opinion, Popularité
							</p>

					</div>
					

					<div class="article">

						<b>Mathieu-Henri Falco</b>


						<br/>

							<i>Typologie des questions à réponses multiples pour un système de question-réponse</i> <br/>

						<a href="actes/recital-2012-long-021.pdf">recital-2012-long-021</a> 
						<a href="bibtex/recital-2012-long-021.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-021-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-021-key');">mots clés</a> <br/>

							<p id="recital-2012-long-021-abs" class="resume">
							<b>Résumé : </b> L’évaluation des systèmes de question-réponse lors des campagnes repose généralement sur la validité d’une réponse individuelle supportée par un passage (question factuelle) ou d’un groupe de réponses toutes contenues dans un même passage (questions listes). Ce cadre évaluatif empêche donc de fournir un ensemble de plusieurs réponses individuelles et ne permet également pas de fournir des réponses provenant de documents différents. Ce recoupement inter-documents peut être necessaire pour construire une réponse composée de plusieurs éléments afin d’être le plus complet possible. De plus une grande majorité de questions formulées au singulier et semblant n’attendre qu’une seule réponse se trouve être des questions possédant plusieurs réponses correctes. Nous présentons ici une typologie des questions à réponses multiples ainsi qu’un aperçu sur les problèmes posés à un système de question-réponse par ce type de question.
							</p>

							<p id="recital-2012-long-021-key" class="mots_cles">
							<b>Mots clés : </b> question-réponse, questions à réponses multiples, question liste
							</p>

					</div>
					

					<div class="article">

						<b>Mohamed Hatmi</b>


						<br/>

							<i>Adaptation d’un système de reconnaissance d’entités nommées pour le français à l’anglais à moindre coût</i> <br/>

						<a href="actes/recital-2012-long-022.pdf">recital-2012-long-022</a> 
						<a href="bibtex/recital-2012-long-022.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-022-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-022-key');">mots clés</a> <br/>

							<p id="recital-2012-long-022-abs" class="resume">
							<b>Résumé : </b> La portabilité entre les langues des systèmes de reconnaissance d’entités nommées est coûteuse en termes de temps et de connaissances linguistiques requises. L’adaptation des systèmes symboliques souffrent du coût de développement de nouveaux lexiques et de la mise à jour des règles contextuelles. D’un autre côté, l’adaptation des systèmes statistiques se heurtent au problème du coût de préparation d’un nouveau corpus d’apprentissage. Cet article étudie l’intérêt et le coût associé pour porter un système existant de reconnaissance d’entités nommées pour du texte bien formé vers une autre langue. Nous présentons une méthode peu coûteuse pour porter un système symbolique dédié au français vers l’anglais. Pour ce faire, nous avons d’une part traduit automatiquement l’ensemble des lexiques de mots déclencheurs au moyen d’un dictionnaire bilingue. D’autre part, nous avons manuellement modifié quelques règles de manière à respecter la syntaxe de la langue anglaise. Les résultats expérimentaux sont comparés à ceux obtenus avec un système de référence développé pour l’anglais.
							</p>

							<p id="recital-2012-long-022-key" class="mots_cles">
							<b>Mots clés : </b> Reconnaissance d’entités nommées, approche symbolique, portabilité entre les langues
							</p>

					</div>
					

					<div class="article">

						<b>Monia Ben Mlouka</b>


						<br/>

							<i>Analyse automatique de discours en langue des signes : Représentation et traitement de l’espace de signation</i> <br/>

						<a href="actes/recital-2012-long-023.pdf">recital-2012-long-023</a> 
						<a href="bibtex/recital-2012-long-023.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-023-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-023-key');">mots clés</a> <br/>

							<p id="recital-2012-long-023-abs" class="resume">
							<b>Résumé : </b> En langue des signes, l’espace est utilisé pour localiser et faire référence à certaines entités dont l’emplacement est important pour la compréhension du sens. Dans cet article, nous proposons une représentation informatique de l’espace de signation et les fonctions de création et d’accès associées, afin d’analyser les gestes manuels et non manuels qui contribuent à la localisation et au référencement des signes et de matérialiser leur effet. Nous proposons une approche bi-directionnelle qui se base sur l’analyse de données de capture de mouvement de discours en langue des signes dans le but de caractériser les événements de localisation et de référencement.
							</p>

							<p id="recital-2012-long-023-key" class="mots_cles">
							<b>Mots clés : </b> Langue des signes, Espace de signation, gestes de pointage, capture de mouvement, suivi du regard
							</p>

					</div>
					

					<div class="article">

						<b>Morgane Marchand</b>


						<br/>

							<i>État de l’art : l’influence du domaine sur la classification de l’opinion, Dis-moi de quoi tu parles, je te dirai ce que tu penses</i> <br/>

						<a href="actes/recital-2012-long-024.pdf">recital-2012-long-024</a> 
						<a href="bibtex/recital-2012-long-024.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-024-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-024-key');">mots clés</a> <br/>

							<p id="recital-2012-long-024-abs" class="resume">
							<b>Résumé : </b> L’intérêt pour la fouille d’opinion s’est développé en même temps que se sont répandus les blogs, forums et autres plate-formes où les internautes peuvent librement exprimer leur opinion. La très grande quantité de données disponibles oblige à avoir recours à des traitements automatiques de fouille d’opinion. Cependant, la manière dont les gens expriment leur avis change selon ce dont ils parlent. Les distributions des mots utilisés sont différentes d’un domaine à l’autre. Aussi, il est très difficile d’obtenir un classifieur d’opinion fonctionnant sur tous les domaines. De plus, on ne peut appliquer sans adaptation sur un domaine cible un classifieur entraîné sur un domaine source différent. L’objet de cet article est de recenser les moyens de résoudre ce problème difficile.
							</p>

							<p id="recital-2012-long-024-key" class="mots_cles">
							<b>Mots clés : </b> État de l’art, Fouille d’opinion, Multi-domaines, Cross-domaines
							</p>

					</div>
					

					<div class="article">

						<b>Mounira Manser</b>


						<br/>

							<i>État de l’art sur l’acquisition de relations sémantiques entre termes : contextualisation des relations de synonymie</i> <br/>

						<a href="actes/recital-2012-long-025.pdf">recital-2012-long-025</a> 
						<a href="bibtex/recital-2012-long-025.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-025-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-025-key');">mots clés</a> <br/>

							<p id="recital-2012-long-025-abs" class="resume">
							<b>Résumé : </b> L’accès au contenu des textes de spécialité est une tâche difficile à réaliser. Cela nécessite la définition de méthodes automatiques ou semi-automatiques pour identifier des relations sémantiques entre les termes que contiennent ces textes. Nous distinguons les approches de TAL permettant d’acquérir ces relations suivant deux types d’information : la structure interne des termes ou le contexte de ces termes en corpus. Afin d’améliorer la qualité des relations acquises et faciliter leur réutilisation en corpus, nous nous intéressons à la prise en compte du contexte dans une méthode d’acquisition de relations de synonymie basée sur l’utilisation de la structure interne des termes. Nous présentons les résultats d’une expérience préliminaire tenant compte de l’usage des termes dans un corpus biomédical en anglais. Nous donnons quelques pistes de travail pour définir des contraintes sémantiques sur les relations de synonymie acquises.
							</p>

							<p id="recital-2012-long-025-key" class="mots_cles">
							<b>Mots clés : </b> Acquisition de relations, Synonymie, Relations sémantiques, Terminologie, Domaine Biomédical, Corpus de spécialité
							</p>

					</div>
					

					<div class="article">

						<b>Noémie-Fleur Sandillon-Rezer</b>


						<br/>

							<i>Extraction de PCFG et analyse de phrases pré-typées</i> <br/>

						<a href="actes/recital-2012-long-026.pdf">recital-2012-long-026</a> 
						<a href="bibtex/recital-2012-long-026.bib">bibtex</a> 
							<a onclick="toggle('recital-2012-long-026-abs');">résumé</a>
							<a onclick="toggle('recital-2012-long-026-key');">mots clés</a> <br/>

							<p id="recital-2012-long-026-abs" class="resume">
							<b>Résumé : </b> Cet article explique la chaîne de traitement suivie pour extraire une grammaire PCFG à partir du corpus de Paris VII. Dans un premier temps cela nécessite de transformer les arbres syntaxiques du corpus en arbres de dérivation d’une grammaire AB, ce que nous effectuons en utilisant un transducteur d’arbres généralisé ; il faut ensuite extraire de ces arbres une PCFG. Le transducteur d’arbres généralisé est une variation des transducteurs d’arbres classiques et c’est l’extraction de la grammaire à partir des arbres de dérivation qui donnera l’aspect probabiliste à la grammaire. La PCFG extraite est utilisée via l’algorithme CYK pour l’analyse de phrases.
							</p>

							<p id="recital-2012-long-026-key" class="mots_cles">
							<b>Mots clés : </b> Extraction de grammaire, grammaire de Lambek, PCFG, transducteur d’arbre, algorithme CYK
							</p>

					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>