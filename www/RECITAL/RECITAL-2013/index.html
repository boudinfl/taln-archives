<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>RECITAL'2013</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>RECITAL'2013, 15e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</h1>
				<h2>Sables d'Olonne (France), du 2013-06-17 au 2012-06-21</h2>
				<p>Président(s) : Florian Boudin, Loïc Barrault</p>
				<p>Taux d'acceptation :
							articles longs (72.0%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#long">Articles longs</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="long">Articles longs</h1>
			

					<div class="article">

						<b>Dhouha Bouamor</b>


						<br/>

							<i>Acquisition de lexique bilingue d’expressions polylexicales: Une application à la traduction automatique statistique</i> <br/>

						<a href="actes/recital-2013-long-001.pdf">recital-2013-long-001</a> 
						<a href="bibtex/recital-2013-long-001.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-001-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-001-key');">mots clés</a> <br/>

							<p id="recital-2013-long-001-abs" class="resume">
							<b>Résumé : </b> Cet article décrit une méthode permettant d’acquérir un lexique bilingue d’expressions polylexicales (EPLS) à partir d’un corpus parallèle français-anglais. Nous identifions dans un premier temps les EPLS dans chaque partie du corpus parallèle. Ensuite, nous proposons un algorithme d’alignement assurant la mise en correspondance bilingue d’EPLS. Pour mesurer l’apport du lexique construit, une évaluation basée sur la tâche de Traduction Automatique Statistique (TAS) est menée. Nous étudions les performances de trois stratégies dynamiques et d’une stratégie statique pour intégrer le lexique bilingue d’expressions polylexicales dans un système de TAS. Les expériences menées dans ce cadre montrent que ces unités améliorent significativement la qualité de traduction.
							</p>

							<p id="recital-2013-long-001-key" class="mots_cles">
							<b>Mots clés : </b> Expression polylexicale, alignement bilingue, traduction automatique statistique
							</p>

					</div>
					

					<div class="article">

						<b>Guiyao Ke</b>


						<br/>

							<i>Quelques variations sur les mesures de comparabilité quantitatives et évaluations sur des corpus comparables Français-Anglais synthétiques</i> <br/>

						<a href="actes/recital-2013-long-002.pdf">recital-2013-long-002</a> 
						<a href="bibtex/recital-2013-long-002.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-002-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-002-key');">mots clés</a> <br/>

							<p id="recital-2013-long-002-abs" class="resume">
							<b>Résumé : </b> Dans la suite des travaux de (Li et Gaussier, 2010) nous abordons dans cet article l&#39;analyse d&#39;une famille de mesures quantitatives de comparabilité pour la construction ou l&#39;évaluation des corpus comparables. Après avoir rappelé la définition de la mesure de comparabilité proposée par (Li et Gaussier, 2010), nous développons quelques variantes de cette mesure basées principalement sur la prise en compte des fréquences d&#39;occurrences des entrées lexicales et du nombre de leurs traductions. Nous comparons leurs avantages et inconvénients respectifs dans le cadre d&#39;expérimentations basées sur la dégradation progressive du corpus parallèle Europarl par remplacement de blocs selon la méthodologie suivie par (Li et Gaussier, 2010). L&#39;impact sur ces mesures des taux de couverture des dictionnaires bilingues vis-à-vis des blocs considérés est également examiné.
							</p>

							<p id="recital-2013-long-002-key" class="mots_cles">
							<b>Mots clés : </b> Corpus comparables, Mesures de comparabilité, Évaluation
							</p>

					</div>
					

					<div class="article">

						<b>Noémie-Fleur Sandillon-Rezer</b>


						<br/>

							<i>Inférence grammaticale guidée par clustering</i> <br/>

						<a href="actes/recital-2013-long-003.pdf">recital-2013-long-003</a> 
						<a href="bibtex/recital-2013-long-003.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-003-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-003-key');">mots clés</a> <br/>

							<p id="recital-2013-long-003-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous nous focalisons sur la manière d’utiliser du clustering hiérarchique pour apprendre une grammaire AB à partir d’arbres de dérivation partiels. Nous décrirons brièvement les grammaires AB ainsi que les arbres de dérivation dont nous nous servons comme entrée pour l’algorithme, puis la manière dont nous extrayons les informations des corpus arborés pour l’étape de clustering. L’algorithme d’unification, dont le pivot est le cluster, sera décrit et les résultats analysés en détails.
							</p>

							<p id="recital-2013-long-003-key" class="mots_cles">
							<b>Mots clés : </b> grammaires catégorielles, clustering hiérarchique, inférence grammaticale
							</p>

					</div>
					

					<div class="article">

						<b>Aurélie Joseph</b>


						<br/>

							<i>Améliorer l’extraction et la description d’expressions polylexicales grâce aux règles transformationnelles</i> <br/>

						<a href="actes/recital-2013-long-004.pdf">recital-2013-long-004</a> 
						<a href="bibtex/recital-2013-long-004.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-004-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-004-key');">mots clés</a> <br/>

							<p id="recital-2013-long-004-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthodologie permettant d’extraire et de décrire des locutions verbales vis-à-vis de leur comportement transformationnel. Plusieurs objectifs sont ciblés : 1) extraire automatiquement les expressions phraséologiques et en particulier les expressions figées, 2) décrire linguistiquement le comportement des phraséologismes 3) comparer les méthodes statistiques et notre approche et enfin 4) montrer l’importance de ces expressions dans un outil de classification de textes.
							</p>

							<p id="recital-2013-long-004-key" class="mots_cles">
							<b>Mots clés : </b> expressions polylexicales, expressions figées, locution verbale, extraction, transformation, classification de textes
							</p>

					</div>
					

					<div class="article">

						<b>Manuela Yapomo</b>


						<br/>

							<i>Construction de corpus multilingues : état de l’art</i> <br/>

						<a href="actes/recital-2013-long-005.pdf">recital-2013-long-005</a> 
						<a href="bibtex/recital-2013-long-005.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-005-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-005-key');">mots clés</a> <br/>

							<p id="recital-2013-long-005-abs" class="resume">
							<b>Résumé : </b> Les corpus multilingues sont extensivement exploités dans plusieurs branches du traitement automatique des langues. Cet article présente une vue d’ensemble des travaux en construction automatique de ces corpus. Nous traitons ce sujet en donnant premièrement un aperçu de différentes perceptions de la comparabilité. Nous examinons ensuite les principales approches de calcul de similarité, de construction et d’évaluation développées dans le domaine. Nous observons que Le calcul de la similarité textuelle se fait généralement sur la base de statistiques de corpus, de la structure de ressources ontologiques ou de la combinaison de ces deux approches. Dans un cadre multilingue avec l’utilisation d’un dictionnaire multilingue ou d’un traducteur automatique, de nombreux problèmes apparaissent. L’exploitation d’une ressource ontologique multilingue semble être une solution. En classification, la problématique de l’ajout de documents à la base initiale sans affecter la qualité des clusters demeure ouverte.
							</p>

							<p id="recital-2013-long-005-key" class="mots_cles">
							<b>Mots clés : </b> corpus multilingues, comparabilité, similarité textuelle translingue, classification
							</p>

					</div>
					

					<div class="article">

						<b>Dhaou Ghoul</b>


						<br/>

							<i>Développement de ressources pour l’entrainement et l’utilisation de l’étiqueteur morphosyntaxique TreeTagger sur l’arabe</i> <br/>

						<a href="actes/recital-2013-long-006.pdf">recital-2013-long-006</a> 
						<a href="bibtex/recital-2013-long-006.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-006-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-006-key');">mots clés</a> <br/>

							<p id="recital-2013-long-006-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons les étapes du développement de ressources pour l’entraînement et l’utilisation d’un nouvel outil de l’étiquetage morphosyntaxique de la langue arabe. Nous avons mis en oeuvre un système basé sur l&#39;étiqueteur stochastique TreeTagger, réputé pour son efficacité et la généricité de son architecture. Pour ce faire, nous avons commencé par la constitution de notre corpus de travail. Celui-ci nous a d&#39;abord servi à réaliser l&#39;étape de segmentation lexicale. Dans un second temps, ce corpus a permis d&#39;effectuer l&#39;entrainement de TreeTagger, grâce à un premier étiquetage réalisé avec l&#39;étiqueteur ASVM 1.0, suivi d&#39;une phase de correction manuelle. Nous détaillons ainsi les prétraitements requis, et les différentes étapes de la phase d&#39;apprentissage avec cet outil. Nous terminons par une évaluation sommaire des résultats, à la fois qualitative et quantitative. Cette évaluation, bien que réalisée sur un corpus de test de taille modeste, montre que nos premiers résultats sont encourageants.
							</p>

							<p id="recital-2013-long-006-key" class="mots_cles">
							<b>Mots clés : </b> TALN, langue arabe, corpus d&#39;apprentissage, étiquetage morphosyntaxique, segmentation de l&#39;arabe, arbre de décision, lexique, jeux d’étiquette, TreeTagger, ASVM 1.0
							</p>

					</div>
					

					<div class="article">

						<b>Amel Ziani, Nabiha Azizi, Yamina Tlili Guiassa</b>


						<br/>

							<i>Détection de polarité d’opinions dans les forums en langue arabe par fusion de plusieurs SVM</i> <br/>

						<a href="actes/recital-2013-long-007.pdf">recital-2013-long-007</a> 
						<a href="bibtex/recital-2013-long-007.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-007-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-007-key');">mots clés</a> <br/>

							<p id="recital-2013-long-007-abs" class="resume">
							<b>Résumé : </b> Cet article décrit notre contribution sur la détection de polarité d’opinions en langue arabe par apprentissage supervisé. En effet le système proposé comprend trois phases: le prétraitement du corpus, l’extraction des caractéristiques et la classification. Pour la deuxième phase, nous utilisons vingt caractéristiques dont les principales sont l’émotivité, la réflexivité, l’adressage et la polarité. La phase de classification représente dans notre travail la combinaison des plusieurs classifieurs SVMs (Machine à Vecteur de Support) pour résoudre le problème multi classes. Nous avons donc analysés les deux stratégies de SVM multi classes qui sont : « un contre tous » et « un contre un » afin de comparer les résultats et améliorer la performance du système global.
							</p>

							<p id="recital-2013-long-007-key" class="mots_cles">
							<b>Mots clés : </b> Fouille d’opinions, apprentissage supervisé, Machine à Vecteur de Support (SVM), combinaison des classifieurs
							</p>

					</div>
					

					<div class="article">

						<b>Adrien Bougouin</b>


						<br/>

							<i>État de l’art des méthodes d’extraction automatique de termes-clés</i> <br/>

						<a href="actes/recital-2013-long-008.pdf">recital-2013-long-008</a> 
						<a href="bibtex/recital-2013-long-008.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-008-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-008-key');">mots clés</a> <br/>

							<p id="recital-2013-long-008-abs" class="resume">
							<b>Résumé : </b> Cet article présente les principales méthodes d’extraction automatique de termes-clés. La tâche d’extraction automatique de termes-clés consiste à analyser un document pour en extraire les expressions (phrasèmes) les plus représentatives de celui-ci. Les méthodes d’extraction automatique de termes-clés sont réparties en deux catégories : les méthodes supervisées et les méthodes non supervisées. Les méthodes supervisées réduisent la tâche d’extraction de termes-clés à une tâche de classification binaire (tous les phrasèmes sont classés parmi les termesclés ou les non termes-clés). Cette classification est possible grâce à une phase préliminaire d’apprentissage, phase qui n’est pas requise par les méthodes non-supervisées. Ces dernières utilisent des caractéristiques (traits) extraites du document analysé (et parfois d’une collection de documents de références) pour vérifier des propriétés permettant d’identifier ses termes-clés.
							</p>

							<p id="recital-2013-long-008-key" class="mots_cles">
							<b>Mots clés : </b> extraction de termes-clés, méthodes supervisées, méthodes non-supervisées, état de l’art
							</p>

					</div>
					

					<div class="article">

						<b>Ophélie Lacroix</b>


						<br/>

							<i>Influence de l’étiquetage syntaxique des têtes sur l’analyse en dépendances discontinues du français</i> <br/>

						<a href="actes/recital-2013-long-009.pdf">recital-2013-long-009</a> 
						<a href="bibtex/recital-2013-long-009.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-009-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-009-key');">mots clés</a> <br/>

							<p id="recital-2013-long-009-abs" class="resume">
							<b>Résumé : </b> Dans cet article nous souhaitons mettre en évidence l’utilité d’un étiquetage syntaxique appliqué en amont d’une analyse syntaxique en dépendances. Les règles de la grammaire catégorielle de dépendances du français utilisées pour l’analyse gèrent les dépendances discontinues et les relations syntaxiques à longue distance. Une telle méthode d’analyse génère un nombre conséquent de structures de dépendances et emploie un temps d’analyse trop important. Nous voulons alors montrer qu’une méthode locale d’étiquetage peut diminuer l’ampleur de ces difficultés et par la suite aider à résoudre le problème global de désambiguïsation d’analyse en dépendances. Nous adaptons alors une méthode d’étiquetage aux catégories de la grammaire catégorielle de dépendance. Nous obtenons ainsi une pré-sélection des têtes des dépendances permettant de réduire l’ambiguïté de l’analyse et de voir que les résultats locaux d’une telle méthode permettent de trouver des relations distantes de dépendances.
							</p>

							<p id="recital-2013-long-009-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique en dépendances discontinues, Étiquetage syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>Houda Saadane</b>


						<br/>

							<i>Une approche linguistique pour l&#39;extraction des connaissances dans un texte arabe</i> <br/>

						<a href="actes/recital-2013-long-010.pdf">recital-2013-long-010</a> 
						<a href="bibtex/recital-2013-long-010.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-010-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-010-key');">mots clés</a> <br/>

							<p id="recital-2013-long-010-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article un système d&#39;extraction de connaissances en arabe, fondé sur une analyse morphosyntaxique profonde. Ce système reconnaît les mots simples, les expressions idiomatiques, les mots composés et les entités nommées. L&#39;analyse identifie aussi les relations syntaxiques de dépendance et traite les formes passives et actives. L’extraction des connaissances est propre à l’application et utilise des règles d’extraction sémantiques qui s&#39;appuient sur le résultat de l&#39;analyse morphosyntaxique. A ce niveau, le type de certaines entités nommées peut être révisé. L&#39;extraction se base, dans nos expérimentations, sur une ontologie dans le domaine de la sécurité. Le RDF (Resource Description Framework) produit est ensuite traité pour regrouper les informations qui concernent un même événement ou une même entité nommée. Les informations ainsi extraites peuvent alors aider à appréhender les informations contenues dans un ensemble de textes, alimenter une base de connaissances, ou bien servir à  des outils de veille.
							</p>

							<p id="recital-2013-long-010-key" class="mots_cles">
							<b>Mots clés : </b> Analyse linguistique, fouille de textes, arabe, entités nommées, extraction d’informations, règles d’extraction, ontologie
							</p>

					</div>
					

					<div class="article">

						<b>Sylvain Hatier</b>


						<br/>

							<i>Extraction des mots simples du lexique scientifique transdisciplinaire dans les écrits de sciences humaines : une première expérimentation</i> <br/>

						<a href="actes/recital-2013-long-011.pdf">recital-2013-long-011</a> 
						<a href="bibtex/recital-2013-long-011.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-011-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-011-key');">mots clés</a> <br/>

							<p id="recital-2013-long-011-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article les premiers résultats de nos travaux sur l&#39;extraction de mots simples appartenant au lexique scientifique transdisciplinaire sur un corpus analysé morpho-syntaxiquement composé d&#39;articles de recherche en sciences humaines et sociales. La ressource générée sera utilisée lors de l&#39;indexation automatique de textes comme filtre d&#39;exclusion afin d&#39;isoler ce lexique de la terminologie. Nous comparons plusieurs méthodes d&#39;extraction et montrons qu&#39;un premier lexique de mots simples peut être dégagé et que la prise en compte des unités polylexicales ainsi que de la distribution seront nécessaires par la suite afin d&#39;extraire l&#39;ensemble de la phraséologie transdisciplinaire.
							</p>

							<p id="recital-2013-long-011-key" class="mots_cles">
							<b>Mots clés : </b> corpus, écrits scientifiques, lexique, phraséologie
							</p>

					</div>
					

					<div class="article">

						<b>Marie Dubremetz</b>


						<br/>

							<i>Vers une identification automatique du chiasme de mots</i> <br/>

						<a href="actes/recital-2013-long-012.pdf">recital-2013-long-012</a> 
						<a href="bibtex/recital-2013-long-012.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-012-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-012-key');">mots clés</a> <br/>

							<p id="recital-2013-long-012-abs" class="resume">
							<b>Résumé : </b> Cette recherche porte sur le chiasme de mots : figure de style jouant sur la réversion (ex. « Bonnet blanc, blanc bonnet »). Elle place le chiasme dans la problématique de sa reconnaissance automatique : qu’est-ce qui le définit et comment un ordinateur peut le trouver ? Nous apportons une description formelle du phénomène. Puis nous procédons à la constitution d’une liste d’exemples contextualisés qui nous sert au test des hypothèses. Nous montrons ainsi que l’ajout de contraintes formelles (contrôle de la ponctuation et omission des mots vides) pénalise très peu le rappel et augmente significativement la précision de la détection. Nous montrons aussi que la lemmatisation occasionne peu d’erreurs pour le travail d’extraction mais qu’il n’en est pas de même pour la racinisation. Enfin nous mettons en évidence que l’utilisation d’un thésaurus apporte quelques résultats pertinents.
							</p>

							<p id="recital-2013-long-012-key" class="mots_cles">
							<b>Mots clés : </b> chiasme, rhétorique, antimétabole, figure de style
							</p>

					</div>
					

					<div class="article">

						<b>Maxime Lefrançois</b>


						<br/>

							<i>Représentation des connaissances du DEC: Concepts fondamentaux du formalisme des Graphes d’Unités</i> <br/>

						<a href="actes/recital-2013-long-013.pdf">recital-2013-long-013</a> 
						<a href="bibtex/recital-2013-long-013.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-013-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-013-key');">mots clés</a> <br/>

							<p id="recital-2013-long-013-abs" class="resume">
							<b>Résumé : </b> Dans cet article nous nous intéressons au choix d’un formalisme de représentation des connaissances qui nous permette de représenter, manipuler, interroger et raisonner sur des connaissances linguistiques du Dictionnaire Explicatif et Combinatoire (DEC) de la Théorie Sens-Texte. Nous montrons que ni les formalismes du web sémantique ni le formalisme des Graphes conceptuels n’est adapté pour cela, et justifions l’introduction d’un nouveau formalisme dit des Graphes d’Unités. Nous introduisons la hiérarchie des Types d’Unités au coeur du formalisme, et présentons les Graphes d’Unités ainsi que la manière dont on peut les utiliser pour représenter certains aspects du DEC.
							</p>

							<p id="recital-2013-long-013-key" class="mots_cles">
							<b>Mots clés : </b> Représentation de Connaissances Linguistiques, Théorie Sens-Texte, Graphes d’Unités, Dictionnaire Explicatif et Combinatoire
							</p>

					</div>
					

					<div class="article">

						<b>Corentin Ribeyre</b>


						<br/>

							<i>Vers un système générique de réécriture de graphes pour l’enrichissement de structures syntaxiques</i> <br/>

						<a href="actes/recital-2013-long-014.pdf">recital-2013-long-014</a> 
						<a href="bibtex/recital-2013-long-014.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-014-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-014-key');">mots clés</a> <br/>

							<p id="recital-2013-long-014-abs" class="resume">
							<b>Résumé : </b> Ce travail présente une nouvelle approche pour injecter des dépendances profondes (sujet des verbes à contrôle, partage du sujet en cas d’ellipses, ...) dans un corpus arboré présentant un schéma d’annotation surfacique et projectif. Nous nous appuyons sur un système de réécriture de graphes utilisant des techniques de programmation par contraintes pour produire des règles génériques qui s’appliquent aux phrases du corpus. Par ailleurs, nous testons la généricité des règles en utilisant des sorties de trois analyseurs syntaxiques différents, afin d’évaluer la dégradation exacte de l’application des règles sur des analyses syntaxiques prédites.
							</p>

							<p id="recital-2013-long-014-key" class="mots_cles">
							<b>Mots clés : </b> réécriture de graphes, évaluation de shéma d’annotations, parsing, analyse en syntaxe profonde
							</p>

					</div>
					

					<div class="article">

						<b>Mohammad Nasiruddin</b>


						<br/>

							<i>État de l&#39;art de l&#39;induction de sens: une voie vers la désambiguïsation lexicale pour les langues peu dotées</i> <br/>

						<a href="actes/recital-2013-long-015.pdf">recital-2013-long-015</a> 
						<a href="bibtex/recital-2013-long-015.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-015-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-015-key');">mots clés</a> <br/>

							<p id="recital-2013-long-015-abs" class="resume">
							<b>Résumé : </b> La désambiguïsation lexicale, le processus qui consiste à automatiquement identifier le ou les sens possible d&#39;un mot polysémique dans un contexte donné, est une tâche fondamentale pour le Traitement Automatique des Langues (TAL). Le développement et l&#39;amélioration des techniques de désambiguïsation lexicale ouvrent de nombreuses perspectives prometteuses pour le TAL. En effet, cela pourrait conduire à un changement paradigmatique en permettant de réaliser un premier pas vers la compréhension des langues naturelles. En raison du manque de ressources langagières, il est parfois difficile d&#39;appliquer des techniques de désambiguïsation à des langues peu dotées. C&#39;est pourquoi, nous nous intéressons ici, à enquêter sur comment avoir un début de recherche sur la désambiguïsation lexicale pour les langues peu dotées, en particulier en exploitant des techniques d&#39;induction des sens de mots, ainsi que quelques suggestions de pistes intéressantes à explorer.
							</p>

							<p id="recital-2013-long-015-key" class="mots_cles">
							<b>Mots clés : </b> désambiguïsation lexicale, induction de sens, langues peu dotées, ressources langagières
							</p>

					</div>
					

					<div class="article">

						<b>Rahma Boujelbane</b>


						<br/>

							<i>Génération des corpus en dialecte tunisien pour la modélisation de langage d&#39;un système de reconnaissance</i> <br/>

						<a href="actes/recital-2013-long-016.pdf">recital-2013-long-016</a> 
						<a href="bibtex/recital-2013-long-016.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-016-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-016-key');">mots clés</a> <br/>

							<p id="recital-2013-long-016-abs" class="resume">
							<b>Résumé : </b> Ces derniers temps, vu la situation préoccupante du monde arabe, les dialectes arabes et notamment le dialecte tunisien est devenu de plus en plus utilisé dans les interviews, les journaux télévisés et les émissions de débats. Cependant, cette situation présente des conséquences négatives importantes pour le Traitement Automatique du Langage Naturel (TALN): depuis que les dialectes parlés ne sont pas officiellement écrits et n’ont pas d’orthographe standard, il est très coûteux d&#39;obtenir des corpus adéquats à utiliser pour des outils de TALN. Par conséquent, il n’existe pas des corpus parallèles entre l’Arabe Standard Moderne(ASM) et le Dialecte Tunisien (DT). Dans ce travail, nous proposons une méthode pour la création d’un lexique bilingue ASM–DT et un processus pour la génération automatique de corpus dialectaux. Ces ressources vont servir à la construction d’un modèle de langage pour les journaux télévisés tunisiens, afin de l’intégrer dans un Système de Reconnaissance Automatique de Parole (SRAP).
							</p>

							<p id="recital-2013-long-016-key" class="mots_cles">
							<b>Mots clés : </b> Dialecte Tunisien, lexique ASM-DT, TDT: Tunisian Dialect Translator
							</p>

					</div>
					

					<div class="article">

						<b>Simon Leva, Nicolas Faessel</b>


						<br/>

							<i>Détection automatique des sessions de recherche par similarité des résultats provenant d’une collection de documents externe</i> <br/>

						<a href="actes/recital-2013-long-017.pdf">recital-2013-long-017</a> 
						<a href="bibtex/recital-2013-long-017.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-017-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-017-key');">mots clés</a> <br/>

							<p id="recital-2013-long-017-abs" class="resume">
							<b>Résumé : </b> Les utilisateurs d’un système de recherche d’information mettent en oeuvre des comportements de recherche complexes tels que la reformulation de requête et la recherche multitâche afin de satisfaire leurs besoins d’information. Ces comportements de recherche peuvent être observés à travers des journaux de requêtes, et constituent des indices permettant une meilleure compréhension des besoins des utilisateurs. Dans cette perspective, il est nécessaire de regrouper au sein d’une même session de recherche les requêtes reliées à un même besoin d’information. Nous proposons une méthode de détection automatique des sessions exploitant la collection de documents WIKIPÉDIA, basée sur la similarité des résultats renvoyés par l’interrogation de cette collection afin d’évaluer la similarité entre les requêtes. Cette méthode obtient de meilleures performances que les approches temporelle et lexicale traditionnellement employées pour la détection de sessions séquentielles, et peut être appliquée à la détection de sessions imbriquées. Ces expérimentations ont été réalisées sur des données provenant du portail OpenEdition.
							</p>

							<p id="recital-2013-long-017-key" class="mots_cles">
							<b>Mots clés : </b> Recherche d’information, détection automatique de sessions de recherche, analyse de journal de requêtes
							</p>

					</div>
					

					<div class="article">

						<b>Zhen Wang</b>


						<br/>

							<i>Une approche mixte morpho-syntaxique et statistique pour la reconnaissance d&#39;entités nommées en langue chinoise</i> <br/>

						<a href="actes/recital-2013-long-018.pdf">recital-2013-long-018</a> 
						<a href="bibtex/recital-2013-long-018.bib">bibtex</a> 
							<a onclick="toggle('recital-2013-long-018-abs');">résumé</a>
							<a onclick="toggle('recital-2013-long-018-key');">mots clés</a> <br/>

							<p id="recital-2013-long-018-abs" class="resume">
							<b>Résumé : </b> Cet article présente une approche mixte, morpho-syntaxique et statistique, pour la reconnaissance d&#39;entités nommées en langue chinoise dans un système d&#39;extraction automatique d&#39;information. Le processus se divise principalement en trois étapes : la première génère des noms propres potentiels à l&#39;aide de règles morphologiques ; la deuxième utilise un modèle de langue afin de sélectionner le meilleur résultat ; la troisième effectue la reconnaissance d&#39;entités nommées grâce à une analyse syntaxique locale. Cette dernière permet une reconnaissance automatique d&#39;entités nommées plus pertinente et plus complète.
							</p>

							<p id="recital-2013-long-018-key" class="mots_cles">
							<b>Mots clés : </b> Reconnaissance de noms propres, Reconnaissance d&#39;entités nommées, Traitement automatique du chinois, Extraction d&#39;information, Analyse syntaxique
							</p>

					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>