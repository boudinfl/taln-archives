<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>TALN'2007</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>TALN'2007, 14ème conférence sur le Traitement Automatique des Langues Naturelles</h1>
				<h2>Toulouse (France), du 2007-06-05 au 2007-06-08</h2>
				<p>Président(s) : Nabil Hathout, Philippe Muller</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#long">Papiers longs</a></li>
				<li><a href="#poster">Posters</a></li>
				<li><a href="#démonstration">Démonstrations</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="long">Papiers longs</h1>
			

					<div class="article">

						<b>Maria Georgescul, Alexander Clarck, Susan Armstrong</b>


						<br/>

							<i>Exploiting structural meeting-specific features for topic segmentation</i> <br/>

						<a href="actes/taln-2007-long-001.pdf">taln-2007-long-001</a> 
						<a href="bibtex/taln-2007-long-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-001-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-001-key');">mots clés</a> <br/>

							<p id="taln-2007-long-001-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous traitons de la segmentation automatique des textes en épisodes thématiques non superposés et ayant une structure linéaire. Notre étude porte sur l’utilisation des traits lexicaux, acoustiques et syntaxiques et sur l’influence de ces traits sur la performance d’un système automatique de segmentation thématique. Nous appliquons notre approche, basée sur des machines à vecteurs support, à des transcriptions des dialogues multilocuteurs.
							</p>

							<p id="taln-2007-long-001-key" class="mots_cles">
							<b>Mots clés : </b> segmentation automatique en épisodes thématiques, machines à vecteurs support, dialogues multi-locuteurs
							</p>

					</div>
					

					<div class="article">

						<b>Silvia Fernández, Eric Sanjuan, Juan-Manuel Torres-Moreno</b>


						<br/>

							<i>Énergie textuelle de mémoires associatives</i> <br/>

						<a href="actes/taln-2007-long-002.pdf">taln-2007-long-002</a> 
						<a href="bibtex/taln-2007-long-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-002-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-002-key');">mots clés</a> <br/>

							<p id="taln-2007-long-002-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une approche de réseaux de neurones inspirée de la physique statistique de systèmes magnétiques pour étudier des problèmes fondamentaux du Traitement Automatique de la Langue Naturelle. L’algorithme modélise un document comme un système de neurones où l’on déduit l’énergie textuelle. Nous avons appliqué cette approche aux problèmes de résumé automatique et de détection de frontières thématiques. Les résultats sont très encourageants.
							</p>

							<p id="taln-2007-long-002-key" class="mots_cles">
							<b>Mots clés : </b> réseaux de neurones, réseaux de Hopfield, résumé, frontière thématiques
							</p>

					</div>
					

					<div class="article">

						<b>Mehdi Embarek, Olivier Ferret</b>


						<br/>

							<i>Une expérience d’extraction de relations sémantiques à partir de textes dans le domaine médical</i> <br/>

						<a href="actes/taln-2007-long-003.pdf">taln-2007-long-003</a> 
						<a href="bibtex/taln-2007-long-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-003-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-003-key');">mots clés</a> <br/>

							<p id="taln-2007-long-003-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une méthode permettant d’extraire à partir de textes des relations sémantiques dans le domaine médical en utilisant des patrons linguistiques. La première partie de cette méthode consiste à identifier les entités entre lesquelles les relations visées interviennent, en l’occurrence les maladies, les examens, les médicaments et les symptômes. La présence d’une des relations sémantiques visées dans les phrases contenant un couple de ces entités est ensuite validée par l’application de patrons linguistiques préalablement appris de manière automatique à partir d’un corpus annoté. Nous rendons compte de l’évaluation de cette méthode sur un corpus en Français pour quatre relations.
							</p>

							<p id="taln-2007-long-003-key" class="mots_cles">
							<b>Mots clés : </b> extraction de relations sémantiques, patrons lexico-syntaxiques, domaine médical
							</p>

					</div>
					

					<div class="article">

						<b>Davy Weissenbacher, Adeline Nazarenko</b>


						<br/>

							<i>Identifier les pronoms anaphoriques et trouver leurs antécédents : l’intérêt de la classification bayésienne</i> <br/>

						<a href="actes/taln-2007-long-004.pdf">taln-2007-long-004</a> 
						<a href="bibtex/taln-2007-long-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-004-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-004-key');">mots clés</a> <br/>

							<p id="taln-2007-long-004-abs" class="resume">
							<b>Résumé : </b> On oppose souvent en TAL les systèmes à base de connaissances linguistiques et ceux qui reposent sur des indices de surface. Chaque approche a ses limites et ses avantages. Nous proposons dans cet article une nouvelle approche qui repose sur les réseaux bayésiens et qui permet de combiner au sein d’une même représentation ces deux types d’informations hétérogènes et complémentaires. Nous justifions l’intérêt de notre approche en comparant les performances du réseau bayésien à celles des systèmes de l’état de l’art, sur un problème difficile du TAL, celui de la résolution d’anaphore.
							</p>

							<p id="taln-2007-long-004-key" class="mots_cles">
							<b>Mots clés : </b> réseaux bayésiens, résolution des anaphores, connaissance linguistique, indice de surface
							</p>

					</div>
					

					<div class="article">

						<b>Bruno Cartoni</b>


						<br/>

							<i>Régler les règles d’analyse morphologique</i> <br/>

						<a href="actes/taln-2007-long-005.pdf">taln-2007-long-005</a> 
						<a href="bibtex/taln-2007-long-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-005-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-005-key');">mots clés</a> <br/>

							<p id="taln-2007-long-005-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons différentes contraintes mécaniques et linguistiques applicables à des règles d’analyse des mots inconnus afin d’améliorer la performance d’un analyseur morphologique de l’italien. Pour mesurer l’impact de ces contraintes, nous présentons les résultats d’une évaluation de chaque contrainte qui prend en compte les gains et les pertes qu’elle engendre. Nous discutons ainsi de la nécessaire évaluation de chaque réglage apporté aux règles afin d’en déterminer la pertinence.
							</p>

							<p id="taln-2007-long-005-key" class="mots_cles">
							<b>Mots clés : </b> évaluation, analyse morphologique, mots inconnus, morphologie constructionnelle
							</p>

					</div>
					

					<div class="article">

						<b>François Barthélemy</b>


						<br/>

							<i>Structures de traits typées et morphologie à partitions</i> <br/>

						<a href="actes/taln-2007-long-006.pdf">taln-2007-long-006</a> 
						<a href="bibtex/taln-2007-long-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-006-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-006-key');">mots clés</a> <br/>

							<p id="taln-2007-long-006-abs" class="resume">
							<b>Résumé : </b> Les structures de traits typées sont une façon abstraite et agréable de représenter une information partielle. Dans cet article, nous montrons comment la combinaison de deux techniques relativement classiques permet de définir une variante de morphologie à deux niveaux intégrant harmonieusement des structures de traits et se compilant en une machine finie. La première de ces techniques est la compilation de structure de traits en expressions régulières, la seconde est la morphologie à partition. Nous illustrons au moyen de deux exemples l’expressivité d’un formalisme qui rapproche les grammaires à deux niveaux des grammaires d’unification.
							</p>

							<p id="taln-2007-long-006-key" class="mots_cles">
							<b>Mots clés : </b> morphologie à deux niveaux, transducteurs finis à états, structure de traits
							</p>

					</div>
					

					<div class="article">

						<b>Louise Deléger, Fiammetta Namer, Pierre Zweigenbaum</b>


						<br/>

							<i>Analyse morphosémantique des composés savants : transposition du français à l’anglais</i> <br/>

						<a href="actes/taln-2007-long-007.pdf">taln-2007-long-007</a> 
						<a href="bibtex/taln-2007-long-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-007-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-007-key');">mots clés</a> <br/>

							<p id="taln-2007-long-007-abs" class="resume">
							<b>Résumé : </b> La plupart des vocabulaires spécialisés comprennent une part importante de lexèmes morphologiquement complexes, construits à partir de racines grecques et latines, qu’on appelle « composés savants ». Une analyse morphosémantique permet de décomposer et de donner des définitions à ces lexèmes, et semble pouvoir être appliquée de façon similaire aux composés de plusieurs langues. Cet article présente l’adaptation d’un analyseur morphosémantique, initialement dédié au français (DériF), à l’analyse de composés savants médicaux anglais, illustrant ainsi la similarité de structure de ces composés dans des langues européennes proches. Nous exposons les principes de cette transposition et ses performances. L’analyseur a été testé sur un ensemble de 1299 lexèmes extraits de la terminologie médicale WHO-ART : 859 ont pu être décomposés et définis, dont 675 avec succès. Outre une simple transposition d’une langue à l’autre, la méthode montre la potentialité d’un système multilingue.
							</p>

							<p id="taln-2007-long-007-key" class="mots_cles">
							<b>Mots clés : </b> analyse morphosémantique, composition savante, terminologie médicale
							</p>

					</div>
					

					<div class="article">

						<b>Oana Frunza, Diana Inkpen</b>


						<br/>

							<i>A tool for detecting French-English cognates and false friends</i> <br/>

						<a href="actes/taln-2007-long-008.pdf">taln-2007-long-008</a> 
						<a href="bibtex/taln-2007-long-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-008-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-008-key');">mots clés</a> <br/>

							<p id="taln-2007-long-008-abs" class="resume">
							<b>Résumé : </b> Les congénères sont des mots qui ont au moins un sens en commun entre deux langues en plus d‘avoir une orthographie semblable. La reconnaissance de ce type de mots permet aux apprenants de langue seconde ou étrangère d‘enrichir plus rapidement leur vocabulaire et d‘améliorer leur compréhension écrite. Toutefois, les faux amis sont des paires de mots qui à l‘écrit ont des similarités, mais ils ont des significations différentes. Pour leur part, les congénères partiels sont des mots qui ont la même signification dans certains contextes dans chacune des deux langues. Cet article présente une méthode pour la classification automatique des paires des mots classées en congénères ou faux amis, en utilisant des mesures de similarité orthographiques et des méthodes d‘apprentissage automatique. Ainsi, nous construisons des listes complètes des congénères et des faux amis entre les deux langues. Nous désambiguisons les congénères partiels dans des contextes spécifiques. Nos méthodes sont évaluées pour le français et l‘anglais, mais elles seraient applicables à d‘autres paires des langues. Nous avons construit un outil qui prend ces listes et marque dans un texte français les mots qui ont des congénères ou des faux amis en anglais, dans le but d‘aider les apprenants en français langue seconde ou étrangère à améliorer leur compréhension écrite et à développer une meilleure rétention.
							</p>

							<p id="taln-2007-long-008-key" class="mots_cles">
							<b>Mots clés : </b> congénères, faux amis, congénères partiels, mesures de similarité orthographiques, apprentissage automatique, apprentissage des langues assisté par ordinateur
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Langlais, Alexandre Patry</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Enrichissement d’un lexique bilingue par analogie</i> <br/>

						<a href="actes/taln-2007-long-009.pdf">taln-2007-long-009</a> 
						<a href="bibtex/taln-2007-long-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-009-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-009-key');">mots clés</a> <br/>

							<p id="taln-2007-long-009-abs" class="resume">
							<b>Résumé : </b> La présence de mots inconnus dans les applications langagières représente un défi de taille bien connu auquel n’échappe pas la traduction automatique. Les systèmes professionnels de traduction offrent à cet effet à leurs utilisateurs la possibilité d’enrichir un lexique de base avec de nouvelles entrées. Récemment, Stroppa et Yvon (2005) démontraient l’intérêt du raisonnement par analogie pour l’analyse morphologique d’une langue. Dans cette étude, nous montrons que le raisonnement par analogie offre également une réponse adaptée au problème de la traduction d’entrées lexicales inconnues.
							</p>

							<p id="taln-2007-long-009-key" class="mots_cles">
							<b>Mots clés : </b> analogie formelle, enrichissement de lexiques bilingues, traduction automatique
							</p>

					</div>
					

					<div class="article">

						<b>Vincent Claveau</b>


						<br/>

							<i>Inférence de règles de réécriture pour la traduction de termes biomédicaux</i> <br/>

						<a href="actes/taln-2007-long-010.pdf">taln-2007-long-010</a> 
						<a href="bibtex/taln-2007-long-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-010-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-010-key');">mots clés</a> <br/>

							<p id="taln-2007-long-010-abs" class="resume">
							<b>Résumé : </b> Dans le domaine biomédical, le caractère multilingue de l’accès à l’information est un problème d’importance. Dans cet article nous présentons une technique originale permettant de traduire des termes simples du domaine biomédical de et vers de nombreuses langues. Cette technique entièrement automatique repose sur l’apprentissage de règles de réécriture à partir d’exemples et l’utilisation de modèles de langues. Les évaluations présentées sont menées sur différentes paires de langues (français-anglais, espagnol-portugais, tchèque-anglais, russe-anglais...). Elles montrent que cette approche est très efficace et offre des performances variables selon les langues mais très bonnes dans l’ensemble et nettement supérieures à celles disponibles dans l’état de l’art. Les taux de précision de traductions s’étagent ainsi de 57.5% pour la paire russe-anglais jusqu’à 85% pour la paire espagnol-portugais et la paire françaisanglais.
							</p>

							<p id="taln-2007-long-010-key" class="mots_cles">
							<b>Mots clés : </b> traduction artificielle, terminologie biomédicale, apprentissage artificiel, modèles de langue
							</p>

					</div>
					

					<div class="article">

						<b>Émilie Guimier De Neef, Arnaud Debeurme, Jungyeul Park</b>


						<br/>

							<i>TiLT correcteur de SMS : évaluation et bilan qualitatif</i> <br/>

						<a href="actes/taln-2007-long-011.pdf">taln-2007-long-011</a> 
						<a href="bibtex/taln-2007-long-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-011-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-011-key');">mots clés</a> <br/>

							<p id="taln-2007-long-011-abs" class="resume">
							<b>Résumé : </b> Nous présentons le logiciel TiLT pour la correction des SMS et évaluons ses performances sur le corpus de SMS du DELIC. L&#39;évaluation utilise la distance de Jaccard et la mesure BLEU. La présentation des résultats est suivie d&#39;une analyse qualitative du système et de ses limites.
							</p>

							<p id="taln-2007-long-011-key" class="mots_cles">
							<b>Mots clés : </b> SMS, SMS corpus, correction orthographique, TiLT, evaluation
							</p>

					</div>
					

					<div class="article">

						<b>Hong-Thai Nguyen, Christian Boitet</b>


						<br/>

							<i>Vers un méta-EDL complet, puis un EDL universel pour la TAO</i> <br/>

						<a href="actes/taln-2007-long-012.pdf">taln-2007-long-012</a> 
						<a href="bibtex/taln-2007-long-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-012-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-012-key');">mots clés</a> <br/>

							<p id="taln-2007-long-012-abs" class="resume">
							<b>Résumé : </b> Un “méta-EDL” (méta-Environnement de Développement Linguiciel) pour la TAO permet de piloter à distance un ou plusieurs EDL pour construire des systèmes de TAO hétérogènes. Partant de CASH, un méta-EDL dédié à Ariane-G5, et de WICALE 1.0, un premier méta-EDL générique mais aux fonctionnalités minimales, nous dégageons les problèmes liés à l’ajout de fonctionnalités riches comme l’édition et la navigation en local, et donnons une solution implémentée dans WICALE 2.0. Nous y intégrons maintenant une base lexicale pour les systèmes à « pivot lexical », comme UNL/U++. Un but à plus long terme est de passer d’un tel méta-EDL générique multifonctionnel à un EDL « universel », ce qui suppose la réingénierie des compilateurs et des moteurs des langages spécialisés pour la programmation linguistique (LSPL) supportés par les divers EDL.
							</p>

							<p id="taln-2007-long-012-key" class="mots_cles">
							<b>Mots clés : </b> génie linguiciel, langages spécialisés pour la programmation linguistique, LSPL, environnement de développement, EDL, TAO, systèmes distribués hétérogènes
							</p>

					</div>
					

					<div class="article">

						<b>Frederik Cailliau, Claude De Loupy</b>


						<br/>

							<i>Aides à la navigation dans un corpus de transcriptions d’oral</i> <br/>

						<a href="actes/taln-2007-long-013.pdf">taln-2007-long-013</a> 
						<a href="bibtex/taln-2007-long-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-013-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-013-key');">mots clés</a> <br/>

							<p id="taln-2007-long-013-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous évaluons les performances de fonctionnalités d’aide à la navigation dans un contexte de recherche dans un corpus audio. Nous montrons que les particularités de la transcription et, en particulier les erreurs, conduisent à une dégradation parfois importante des performances des outils d’analyse. Si la navigation par concepts reste dans des niveaux d’erreur acceptables, la reconnaissance des entités nommées, utilisée pour l’aide à la lecture, voit ses performances fortement baisser. Notre remise en doute de la portabilité de ces fonctions à un corpus oral est néanmoins atténuée par la nature même du corpus qui incite à considérer que toute méthodes permettant de réduire le temps d’accès à l’information est pertinente, même si les outils utilisés sont imparfaits.
							</p>

							<p id="taln-2007-long-013-key" class="mots_cles">
							<b>Mots clés : </b> évaluation, moteur de recherche, corpus oral
							</p>

					</div>
					

					<div class="article">

						<b>Marie-Laure Guénot</b>


						<br/>

							<i>Une grammaire du français pour une théorie descriptive et formelle de la langue</i> <br/>

						<a href="actes/taln-2007-long-014.pdf">taln-2007-long-014</a> 
						<a href="bibtex/taln-2007-long-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-014-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-014-key');">mots clés</a> <br/>

							<p id="taln-2007-long-014-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une grammaire du français qui fait l’objet d’un modèle basé sur des descriptions linguistiques de corpus (provenant notamment des travaux de l’Approche Pronominale) et représentée selon le formalisme des Grammaires de Propriétés. Elle constitue une proposition nouvelle parmi les grammaires formelles du français, participant à la mise en convergence de la variété des travaux de description linguistique, et de la diversité des possibilités de représentation formelle. Cette grammaire est mise à disposition publique sur le Centre de Ressources pour la Description de l’Oral en tant que ressource pour la représentation et l’analyse.
							</p>

							<p id="taln-2007-long-014-key" class="mots_cles">
							<b>Mots clés : </b> développement de grammaire, ressource pour le TAL, grammaire du français, syntaxe, linguistique formelle, linguistique descriptive, grammaires de propriétés (GP)
							</p>

					</div>
					

					<div class="article">

						<b>Alexandre Dikovsky</b>


						<br/>

							<i>Architecture compositionnelle pour les dépendances croisées</i> <br/>

						<a href="actes/taln-2007-long-015.pdf">taln-2007-long-015</a> 
						<a href="bibtex/taln-2007-long-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-015-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-015-key');">mots clés</a> <br/>

							<p id="taln-2007-long-015-abs" class="resume">
							<b>Résumé : </b> L’article présente les principes généraux sous-jacent aux grammaires catégorielles de dépendances : une classe de grammaires de types récemment proposée pour une description compositionnelle et uniforme des dépendances continues et discontinues. Ces grammaires très expressives et analysées en temps polynomial, adoptent naturellement l’architecture multimodale et expriment les dépendances croisées illimitées.
							</p>

							<p id="taln-2007-long-015-key" class="mots_cles">
							<b>Mots clés : </b> grammaires catégorielles de dépendances, grammaires multimodales, analyseur syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>Claire Gardent, Yannick Parmentier</b>


						<br/>

							<i>SemTAG, une architecture pour le développement et l’utilisation de grammaires d’arbres adjoints à portée sémantique</i> <br/>

						<a href="actes/taln-2007-long-016.pdf">taln-2007-long-016</a> 
						<a href="bibtex/taln-2007-long-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-016-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-016-key');">mots clés</a> <br/>

							<p id="taln-2007-long-016-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une architecture logicielle libre et ouverte pour le développement de grammaires d’arbres adjoints à portée sémantique. Cette architecture utilise un compilateur de métagrammaires afin de faciliter l’extension et la maintenance de la grammaire, et intègre un module de construction sémantique permettant de vérifier la couverture aussi bien syntaxique que sémantique de la grammaire. Ce module utilise un analyseur syntaxique tabulaire généré automatiquement à partir de la grammaire par le système DyALog. Nous présentons également les résultats de l’évaluation d’une grammaire du français développée au moyen de cette architecture.
							</p>

							<p id="taln-2007-long-016-key" class="mots_cles">
							<b>Mots clés : </b> analyseur syntaxique, grammaires d’arbres adjoints, construction sémantique, architecture logicielle
							</p>

					</div>
					

					<div class="article">

						<b>Fabienne Venant</b>


						<br/>

							<i>Utiliser des classes de sélection distributionnelle pour désambiguïser les adjectifs</i> <br/>

						<a href="actes/taln-2007-long-017.pdf">taln-2007-long-017</a> 
						<a href="bibtex/taln-2007-long-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-017-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-017-key');">mots clés</a> <br/>

							<p id="taln-2007-long-017-abs" class="resume">
							<b>Résumé : </b> La désambiguïsation lexicale présente un intérêt considérable pour un nombre important d’applications, en traitement automatique des langues comme en recherche d&#39;information. Nous proposons un modèle d’un genre nouveau, fondé sur la théorie de la construction dynamique du sens (Victorri et Fuchs, 1996). Ce modèle donne une place centrale à la polysémie et propose une représentation géométrique du sens. Nous présentons ici une application de ce modèle à la désambiguïsation automatique des adjectifs. La méthode utilisée s&#39;appuie sur une pré-désambiguïsation du nom régissant l&#39;adjectif, par le biais de classes de sélection distributionnelle. Elle permet aussi de prendre en compte les positions relatives du nom et de l&#39;adjectif (postpostion ou antéposition) dans le calcul du sens.
							</p>

							<p id="taln-2007-long-017-key" class="mots_cles">
							<b>Mots clés : </b> traitement automatique des langues, désambiguïsation, sémantique, polysémie adjectivale, construction dynamique du sens, synonymie, classes distributionnelles, corpus, espace sémantique, espace distributionnel
							</p>

					</div>
					

					<div class="article">

						<b>Véronique Malaisé, Luit Gazendam, Hennie Brugman</b>


						<br/>

							<i>Disambiguating automatic semantic annotation based on a thesaurus structure</i> <br/>

						<a href="actes/taln-2007-long-018.pdf">taln-2007-long-018</a> 
						<a href="bibtex/taln-2007-long-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-018-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-018-key');">mots clés</a> <br/>

							<p id="taln-2007-long-018-abs" class="resume">
							<b>Résumé : </b> La relation voir/employé pour d’un thesaurus est souvent plus complexe que la (para-)synonymie recommandée par l’ISO-2788, standard décrivant le contenu de ces vocabulaires contrôlés. Le fait qu’un non descripteur puisse renvoyer à plusieurs descripteurs (seuls les descripteurs sont pertinents dans le cadre de l’indexation contrôlée) fait que cette relation est complexe à utiliser dans un contexte d’annotation automatique : elle génère des cas d’ambiguité. Dans ce papier, nous présentons CARROT, un algorithme que nous avons mis au point pour classer les résultats de notre chaîne de traitements pour l’Extraction d’Information, et son utilisation dans le cadre de la sélection du descripteur pertinent lorsque plusieurs choix sont possibles. Cette sélection s’adresse à des documentalistes, dans le but de simplifier et d’accélérer leur travail, et se base sur la structure de leur thesaurus. Nous arrivons à un succès de 95 % dans nos suggestions ; nous discutons ces résultats et présentons des perspectives à cette expérimentation.
							</p>

							<p id="taln-2007-long-018-key" class="mots_cles">
							<b>Mots clés : </b> désambiguisation sémantique, algorithme de classement, annotation automatique
							</p>

					</div>
					

					<div class="article">

						<b>Marianna Apidianaki</b>


						<br/>

							<i>Repérage de sens et désambiguïsation dans un contexte bilingue</i> <br/>

						<a href="actes/taln-2007-long-019.pdf">taln-2007-long-019</a> 
						<a href="bibtex/taln-2007-long-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-019-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-019-key');">mots clés</a> <br/>

							<p id="taln-2007-long-019-abs" class="resume">
							<b>Résumé : </b> Les besoins de désambiguïsation varient dans les différentes applications du Traitement Automatique des Langues (TAL). Dans cet article, nous proposons une méthode de désambiguïsation lexicale opératoire dans un contexte bilingue et, par conséquent, adéquate pour la désambiguïsation au sein d’applications relatives à la traduction. Il s’agit d’une méthode contextuelle, qui combine des informations de cooccurrence avec des informations traductionnelles venant d’un bitexte. L’objectif est l’établissement de correspondances de traduction au niveau sémantique entre les mots de deux langues. Cette méthode étend les conséquences de l’hypothèse contextuelle du sens dans un contexte bilingue, tout en admettant l’existence d’une relation de similarité sémantique entre les mots de deux langues en relation de traduction. La modélisation de ces correspondances de granularité fine permet la désambiguïsation lexicale de nouvelles occurrences des mots polysémiques de la langue source ainsi que la prédiction de la traduction la plus adéquate pour ces occurrences.
							</p>

							<p id="taln-2007-long-019-key" class="mots_cles">
							<b>Mots clés : </b> désambiguïsation contextuelle, similarité sémantique, substituabilité, traduction
							</p>

					</div>
					

					<div class="article">

						<b>Karën Fort, Bruno Guillaume</b>


						<br/>

							<i>PrepLex : un lexique des prépositions du français pour l’analyse syntaxique</i> <br/>

						<a href="actes/taln-2007-long-020.pdf">taln-2007-long-020</a> 
						<a href="bibtex/taln-2007-long-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-020-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-020-key');">mots clés</a> <br/>

							<p id="taln-2007-long-020-abs" class="resume">
							<b>Résumé : </b> PrepLex est un lexique des prépositions du français. Il contient les informations utiles à des systèmes d’analyse syntaxique. Il a été construit en comparant puis fusionnant différentes sources d’informations lexicales disponibles. Ce lexique met également en évidence les prépositions ou classes de prépositions qui apparaissent dans la définition des cadres de sous-catégorisation des ressources lexicales qui décrivent la valence des verbes.
							</p>

							<p id="taln-2007-long-020-key" class="mots_cles">
							<b>Mots clés : </b> prépositions, lexique, analyse syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>Laurence Danlos, Benoît Sagot</b>


						<br/>

							<i>Comparaison du Lexique-Grammaire des verbes pleins et de DICOVALENCE : vers une intégration dans le Lefff</i> <br/>

						<a href="actes/taln-2007-long-021.pdf">taln-2007-long-021</a> 
						<a href="bibtex/taln-2007-long-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-021-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-021-key');">mots clés</a> <br/>

							<p id="taln-2007-long-021-abs" class="resume">
							<b>Résumé : </b> Cet article compare le Lexique-Grammaire des verbes pleins et DICOVALENCE, deux ressources lexicales syntaxiques pour le français développées par des linguistes depuis de nombreuses années. Nous étudions en particulier les divergences et les empiètements des modèles lexicaux sous-jacents. Puis nous présentons le Lefff , lexique syntaxique à grande échelle pour le TAL, et son propre modèle lexical. Nous montrons que ce modèle est à même d’intégrer les informations lexicales présentes dans le Lexique-Grammaire et dans DICOVALENCE. Nous présentons les résultats des premiers travaux effectués en ce sens, avec pour objectif à terme la constitution d’un lexique syntaxique de référence pour le TAL.
							</p>

							<p id="taln-2007-long-021-key" class="mots_cles">
							<b>Mots clés : </b> lexique syntaxique, Lexique-Grammaire, DICOVALENCE, Lefff
							</p>

					</div>
					

					<div class="article">

						<b>Pierre-André Buvet, Emmanuel Cartier, Fabrice Issac, Salah Mejri</b>


						<br/>

							<i>Dictionnaires électroniques et étiquetage syntactico-sémantique</i> <br/>

						<a href="actes/taln-2007-long-022.pdf">taln-2007-long-022</a> 
						<a href="bibtex/taln-2007-long-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-022-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-022-key');">mots clés</a> <br/>

							<p id="taln-2007-long-022-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article le prototype d’un système d’étiquetage syntactico-sémantique des mots qui utilise comme principales ressources linguistiques différents dictionnaires du laboratoire Lexiques, Dictionnaires, Informatique (LDI). Dans un premier temps, nous mentionnons des travaux sur le même sujet. Dans un deuxième temps, nous faisons la présentation générale du système. Dans un troisième temps, nous exposons les principales caractéristiques des dictionnaires syntactico-sémantiques utilisés. Dans un quatrième temps, nous détaillons un exemple de traitement.
							</p>

							<p id="taln-2007-long-022-key" class="mots_cles">
							<b>Mots clés : </b> étiqueteur sémantique, dictionnaire, LMF, XML, XPATH
							</p>

					</div>
					

					<div class="article">

						<b>Chiraz Ben Othmane Zribi, Hanène Mejri, Mohamed Ben Ahmed</b>


						<br/>

							<i>Un analyseur hybride pour la détection et la correction des erreurs cachées sémantiques en langue arabe</i> <br/>

						<a href="actes/taln-2007-long-023.pdf">taln-2007-long-023</a> 
						<a href="bibtex/taln-2007-long-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-023-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-023-key');">mots clés</a> <br/>

							<p id="taln-2007-long-023-abs" class="resume">
							<b>Résumé : </b> Cet article s’intéresse au problème de la détection et de la correction des erreurs cachées sémantiques dans les textes arabes. Ce sont des erreurs orthographiques produisant des mots lexicalement valides mais invalides sémantiquement. Nous commençons par décrire le type d’erreur sémantique auquel nous nous intéressons. Nous exposons par la suite l’approche adoptée qui se base sur la combinaison de plusieurs méthodes, tout en décrivant chacune de ces méthodes. Puis, nous évoquons le contexte du travail qui nous a mené au choix de l’architecture multi-agent pour l’implémentation de notre système. Nous présentons et commentons vers la fin les résultats de l’évaluation dudit système.
							</p>

							<p id="taln-2007-long-023-key" class="mots_cles">
							<b>Mots clés : </b> erreur cachée, erreur sémantique, détection, correction, système multi-agent, langue arabe
							</p>

					</div>
					

					<div class="article">

						<b>Alexandre Denis, Frédéric Béchet, Matthieu Quignard</b>


						<br/>

							<i>Résolution de la référence dans des dialogues homme-machine : évaluation sur corpus de deux approches symbolique et probabiliste</i> <br/>

						<a href="actes/taln-2007-long-024.pdf">taln-2007-long-024</a> 
						<a href="bibtex/taln-2007-long-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-024-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-024-key');">mots clés</a> <br/>

							<p id="taln-2007-long-024-abs" class="resume">
							<b>Résumé : </b> Cet article décrit deux approches, l’une numérique, l’autre symbolique, traitant le problème de la résolution de la référence dans un cadre de dialogue homme-machine. L’analyse des résultats obtenus sur le corpus MEDIA montre la complémentarité des deux systèmes développés : robustesse aux erreurs et hypothèses multiples pour l’approche numérique ; modélisation de phénomènes complexes et interprétation complète pour l’approche symbolique.
							</p>

							<p id="taln-2007-long-024-key" class="mots_cles">
							<b>Mots clés : </b> dialogue homme-machine, résolution de la référence, évaluation, compréhension dans le dialogue
							</p>

					</div>
					

					<div class="article">

						<b>Sebastian Padó, Guillaume Pitel</b>


						<br/>

							<i>Annotation précise du français en sémantique de rôles par projection cross-linguistique</i> <br/>

						<a href="actes/taln-2007-long-025.pdf">taln-2007-long-025</a> 
						<a href="bibtex/taln-2007-long-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-025-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-025-key');">mots clés</a> <br/>

							<p id="taln-2007-long-025-abs" class="resume">
							<b>Résumé : </b> Dans le paradigme FrameNet, cet article aborde le problème de l’annotation précise et automatique de rôles sémantiques dans une langue sans lexique FrameNet existant. Nous évaluons la méthode proposée par Padó et Lapata (2005, 2006), fondée sur la projection de rôles et appliquée initialement à la paire anglais-allemand. Nous testons sa généralisabilité du point de vue (a) des langues, en l&#39;appliquant à la paire (anglais-français) et (b) de la qualité de la source, en utilisant une annotation automatique du côté anglais. Les expériences montrent des résultats à la hauteur de ceux obtenus pour l&#39;allemand, nous permettant de conclure que cette approche présente un grand potentiel pour réduire la quantité de travail nécessaire à la création de telles ressources dans de nombreuses langues.
							</p>

							<p id="taln-2007-long-025-key" class="mots_cles">
							<b>Mots clés : </b> multilingue, FrameNet, annotation sémantique automatique, sémantique lexicale, projection d’annotation de rôles, rôles sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>Simon Charest, Éric Brunelle, Jean Fontaine, Bertrand Pelletier</b>


						<br/>

							<i>Élaboration automatique d’un dictionnaire de cooccurrences grand public</i> <br/>

						<a href="actes/taln-2007-long-026.pdf">taln-2007-long-026</a> 
						<a href="bibtex/taln-2007-long-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-026-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-026-key');">mots clés</a> <br/>

							<p id="taln-2007-long-026-abs" class="resume">
							<b>Résumé : </b> Antidote RX, un logiciel d’aide à la rédaction grand public, comporte un nouveau dictionnaire de 800 000 cooccurrences, élaboré essentiellement automatiquement. Nous l’avons créé par l’analyse syntaxique détaillée d’un vaste corpus et par la sélection automatique des cooccurrences les plus pertinentes à l’aide d’un test statistique, le rapport de vraisemblance. Chaque cooccurrence est illustrée par des exemples de phrases également tirés du corpus automatiquement. Les cooccurrences et les exemples extraits ont été révisés par des linguistes. Nous examinons les choix d’interface que nous avons faits pour présenter ces données complexes à un public non spécialisé. Enfin, nous montrons comment nous avons intégré les cooccurrences au correcteur d’Antidote pour améliorer ses performances.
							</p>

							<p id="taln-2007-long-026-key" class="mots_cles">
							<b>Mots clés : </b> antidote, cooccurrences, collocations, corpus, analyseur, correcteur
							</p>

					</div>
					

					<div class="article">

						<b>Didier Schwab, Lim Lian Tze, Mathieu Lafourcade</b>


						<br/>

							<i>Les vecteurs conceptuels, un outil complémentaire aux réseaux lexicaux</i> <br/>

						<a href="actes/taln-2007-long-027.pdf">taln-2007-long-027</a> 
						<a href="bibtex/taln-2007-long-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-027-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-027-key');">mots clés</a> <br/>

							<p id="taln-2007-long-027-abs" class="resume">
							<b>Résumé : </b> Fréquemment utilisés dans le Traitement Automatique des Langues Naturelles, les réseaux lexicaux font aujourd’hui l’objet de nombreuses recherches. La plupart d’entre eux, et en particulier le plus célèbre WordNet, souffrent du manque d’informations syntagmatiques mais aussi d’informations thématiques (« problème du tennis »). Cet article présente les vecteurs conceptuels qui permettent de représenter les idées contenues dans un segment textuel quelconque et permettent d’obtenir une vision continue des thématiques utilisées grâce aux distances calculables entre eux. Nous montrons leurs caractéristiques et en quoi ils sont complémentaires des réseaux lexico-sémantiques. Nous illustrons ce propos par l’enrichissement des données de WordNet par des vecteurs conceptuels construits par émergence.
							</p>

							<p id="taln-2007-long-027-key" class="mots_cles">
							<b>Mots clés : </b> WordNet, vecteurs conceptuels, informations lexicales, informations thématiques
							</p>

					</div>
					

					<div class="article">

						<b>Julien Bourdaillet, Jean-Gabriel Ganascia</b>


						<br/>

							<i>Alignements monolingues avec déplacements</i> <br/>

						<a href="actes/taln-2007-long-028.pdf">taln-2007-long-028</a> 
						<a href="bibtex/taln-2007-long-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-028-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-028-key');">mots clés</a> <br/>

							<p id="taln-2007-long-028-abs" class="resume">
							<b>Résumé : </b> Ce travail présente une application d’alignement monolingue qui répond à une problématique posée par la critique génétique textuelle, une école d’études littéraires qui s’intéresse à la genèse textuelle en comparant les différentes versions d’une oeuvre. Ceci nécessite l’identification des déplacements, cependant, le problème devient ainsi NP-complet. Notre algorithme heuristique est basé sur la reconnaissance des homologies entre séquences de caractères. Nous présentons une validation expérimentale et montrons que notre logiciel obtient de bons résultats ; il permet notamment l’alignement de livres entiers.
							</p>

							<p id="taln-2007-long-028-key" class="mots_cles">
							<b>Mots clés : </b> alignement monolingue, distance d’édition avec déplacements, critique génétique textuelle
							</p>

					</div>
					

					<div class="article">

						<b>Lionel Nicolas, Jacques Farré, Éric Villemonte De La Clergerie</b>


						<br/>

							<i>Confondre le coupable : corrections d’un lexique suggérées par une grammaire</i> <br/>

						<a href="actes/taln-2007-long-029.pdf">taln-2007-long-029</a> 
						<a href="bibtex/taln-2007-long-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-029-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-029-key');">mots clés</a> <br/>

							<p id="taln-2007-long-029-abs" class="resume">
							<b>Résumé : </b> Le succès de l’analyse syntaxique d’une phrase dépend de la qualité de la grammaire sous-jacente mais aussi de celle du lexique utilisé. Une première étape dans l’amélioration des lexiques consiste à identifier les entrées lexicales potentiellement erronées, par exemple en utilisant des techniques de fouilles d’erreurs sur corpus (Sagot &amp; Villemonte de La Clergerie, 2006). Nous explorons ici l’étape suivante : la suggestion de corrections pour les entrées identifiées. Cet objectif est atteint au travers de réanalyses des phrases rejetées à l’étape précédente, après modification des informations portées par les entrées suspectées. Un calcul statistique sur les nouveaux résultats permet ensuite de mettre en valeur les corrections les plus pertinentes.
							</p>

							<p id="taln-2007-long-029-key" class="mots_cles">
							<b>Mots clés : </b> analyse syntaxique, lexique, apprentissage, correction
							</p>

					</div>
					

					<div class="article">

						<b>Sylvain Pogodalla</b>


						<br/>

							<i>Ambiguïté de portée et approche fonctionnelle des grammaires d’arbres adjoints</i> <br/>

						<a href="actes/taln-2007-long-030.pdf">taln-2007-long-030</a> 
						<a href="bibtex/taln-2007-long-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-030-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-030-key');">mots clés</a> <br/>

							<p id="taln-2007-long-030-abs" class="resume">
							<b>Résumé : </b> En s’appuyant sur la notion d’arbre de dérivation des Grammaires d’Arbres Adjoints (TAG), cet article propose deux objectifs : d’une part rendre l’interface entre syntaxe et sémantique indépendante du langage de représentation sémantique utilisé, et d’autre part offrir un noyau qui permette le traitement sémantique des ambiguïtés de portée de quantificateurs sans utiliser de langage de représentation sous-spécifiée.
							</p>

							<p id="taln-2007-long-030-key" class="mots_cles">
							<b>Mots clés : </b> interface syntaxe et sémantique, sémantique formelle, grammaires d’arbres adjoints, grammaires catégorielles
							</p>

					</div>
					

					<div class="article">

						<b>Ingrid Falk, Gil Francopoulo, Claire Gardent</b>


						<br/>

							<i>Évaluer SYNLEX</i> <br/>

						<a href="actes/taln-2007-long-031.pdf">taln-2007-long-031</a> 
						<a href="bibtex/taln-2007-long-031.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-031-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-031-key');">mots clés</a> <br/>

							<p id="taln-2007-long-031-abs" class="resume">
							<b>Résumé : </b> SYNLEX est un lexique syntaxique extrait semi-automatiquement des tables du LADL. Comme les autres lexiques syntaxiques du français disponibles et utilisables pour le TAL (LEFFF, DICOVALENCE), il est incomplet et n’a pas fait l’objet d’une évaluation permettant de déterminer son rappel et sa précision par rapport à un lexique de référence. Nous présentons une approche qui permet de combler au moins partiellement ces lacunes. L’approche s’appuie sur les méthodes mises au point en acquisition automatique de lexique. Un lexique syntaxique distinct de SYNLEX est acquis à partir d’un corpus de 82 millions de mots puis utilisé pour valider et compléter SYNLEX. Le rappel et la précision de cette version améliorée de SYNLEX sont ensuite calculés par rapport à un lexique de référence extrait de DICOVALENCE.
							</p>

							<p id="taln-2007-long-031-key" class="mots_cles">
							<b>Mots clés : </b> lexique syntaxique, évaluation
							</p>

					</div>
					

					<div class="article">

						<b>Fathi Debili, Zied Ben Tahar, Emna Souissi</b>


						<br/>

							<i>Analyse automatique vs analyse interactive : un cercle vertueux pour la voyellation, l’étiquetage et la lemmatisation de l’arabe</i> <br/>

						<a href="actes/taln-2007-long-032.pdf">taln-2007-long-032</a> 
						<a href="bibtex/taln-2007-long-032.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-032-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-032-key');">mots clés</a> <br/>

							<p id="taln-2007-long-032-abs" class="resume">
							<b>Résumé : </b> Comment produire de façon massive des textes annotés dans des conditions d’efficacité, de reproductibilité et de coût optimales ? Plutôt que de corriger les sorties d’analyse automatique moyennant des outils d’éditions éventuellement dédiés, ainsi qu’il estcommunément préconisé, nous proposons de recourir à des outils d’analyse interactive où la correction manuelle est au fur et à mesure prise en compte par l’analyse automatique. Posant le problème de l’évaluation de ces outils interactifs et du rendement de leur ergonomie linguistique, et proposant pour cela une métrique fondée sur le calcul du coût qu’exigent ces corrections exprimé en nombre de manipulations (frappe au clavier, clic de souris, etc.), nous montrons, au travers d’un protocole expérimental simple orienté vers la voyellation, l’étiquetage et la lemmatisation de l’arabe, que paradoxalement, les meilleures performances interactives d’un système ne sont pas toujours corrélées à ses meilleures performances automatiques. Autrement dit, que le comportement linguistique automatique le plus performant n’est pas toujours celui qui assure, dès lors qu’il y a contributions manuelles, le meilleur rendement interactif.
							</p>

							<p id="taln-2007-long-032-key" class="mots_cles">
							<b>Mots clés : </b> analyse automatique vs interactive, annotation séquentielle, parallèle, voyellation, lemmatisation, étiquetage de l’arabe, métrique pour l’évaluation de l’analyse interactive
							</p>

					</div>
					

					<div class="article">

						<b>Jonas Granfeldt, Pierre Nugues</b>


						<br/>

							<i>Évaluation des stades de développement en français langue étrangère</i> <br/>

						<a href="actes/taln-2007-long-033.pdf">taln-2007-long-033</a> 
						<a href="bibtex/taln-2007-long-033.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-033-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-033-key');">mots clés</a> <br/>

							<p id="taln-2007-long-033-abs" class="resume">
							<b>Résumé : </b> Cet article décrit un système pour définir et évaluer les stades de développement en français langue étrangère. L’évaluation de tels stades correspond à l’identification de la fréquence de certains phénomènes lexicaux et grammaticaux dans la production des apprenants et comment ces fréquences changent en fonction du temps. Les problèmes à résoudre dans cette démarche sont triples : identifier les attributs les plus révélateurs, décider des points de séparation entre les stades et évaluer le degré d’efficacité des attributs et de la classification dans son ensemble. Le système traite ces trois problèmes. Il se compose d’un analyseur morphosyntaxique, appelé Direkt Profil, auquel nous avons relié un module d’apprentissage automatique. Dans cet article, nous décrivons les idées qui ont conduit au développement du système et son intérêt. Nous présentons ensuite le corpus que nous avons utilisé pour développer notre analyseur morphosyntaxique. Enfin, nous présentons les résultats sensiblement améliorés des classificateurs comparé aux travaux précédents (Granfeldt et al., 2006). Nous présentons également une méthode de sélection de paramètres afin d’identifier les attributs grammaticaux les plus appropriés.
							</p>

							<p id="taln-2007-long-033-key" class="mots_cles">
							<b>Mots clés : </b> analyseur morphosyntaxique, apprentissage automatique, acquisition des langues
							</p>

					</div>
					

					<div class="article">

						<b>Delphine Bernhard</b>


						<br/>

							<i>Apprentissage non supervisé de familles morphologiques par classification ascendante hiérarchique</i> <br/>

						<a href="actes/taln-2007-long-034.pdf">taln-2007-long-034</a> 
						<a href="bibtex/taln-2007-long-034.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-034-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-034-key');">mots clés</a> <br/>

							<p id="taln-2007-long-034-abs" class="resume">
							<b>Résumé : </b> Cet article présente un système d’acquisition de familles morphologiques qui procède par apprentissage non supervisé à partir de listes de mots extraites de corpus de textes. L’approche consiste à former des familles par groupements successifs, similairement aux méthodes de classification ascendante hiérarchique. Les critères de regroupement reposent sur la similarité graphique des mots ainsi que sur des listes de préfixes et de paires de suffixes acquises automatiquement à partir des corpus traités. Les résultats obtenus pour des corpus de textes de spécialité en français et en anglais sont évalués à l’aide de la base CELEX et de listes de référence construites manuellement. L’évaluation démontre les bonnes performances du système, indépendamment de la langue, et ce malgré la technicité et la complexité morphologique du vocabulaire traité.
							</p>

							<p id="taln-2007-long-034-key" class="mots_cles">
							<b>Mots clés : </b> familles morphologiques, classification, apprentissage non supervisé
							</p>

					</div>
					

					<div class="article">

						<b>Catherine Recanati, Nicoleta Rogovschi</b>


						<br/>

							<i>Enchaînements verbaux – étude sur le temps et l&#39;aspect utilisant des techniques d’apprentissage non supervisé</i> <br/>

						<a href="actes/taln-2007-long-035.pdf">taln-2007-long-035</a> 
						<a href="bibtex/taln-2007-long-035.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-035-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-035-key');">mots clés</a> <br/>

							<p id="taln-2007-long-035-abs" class="resume">
							<b>Résumé : </b> L’apprentissage non supervisé permet la découverte de catégories initialement inconnues. Les techniques actuelles permettent d&#39;explorer des séquences de phénomènes alors qu&#39;on a tendance à se focaliser sur l&#39;analyse de phénomènes isolés ou sur la relation entre deux phénomènes. Elles offrent ainsi de précieux outils pour l&#39;analyse de données organisées en séquences, et en particulier, pour la découverte de structures textuelles. Nous présentons ici les résultats d’une première tentative de les utiliser pour inspecter les suites de verbes provenant de phrases de récits d’accident de la route. Les verbes étaient encodés comme paires (cat, temps), où cat représente la catégorie aspectuelle d’un verbe, et temps son temps grammatical. L’analyse, basée sur une approche originale, a fourni une classification des enchaînements de deux verbes successifs en quatre groupes permettant de segmenter les textes. Nous donnons ici une interprétation de ces groupes à partir de statistiques sur des annotations sémantiques indépendantes.
							</p>

							<p id="taln-2007-long-035-key" class="mots_cles">
							<b>Mots clés : </b> temps, aspect, sémantique, apprentissage non supervisé, fouille de données
							</p>

					</div>
					

					<div class="article">

						<b>Laurence Danlos</b>


						<br/>

							<i>D-STAG : un formalisme pour le discours basé sur les TAG synchrones</i> <br/>

						<a href="actes/taln-2007-long-036.pdf">taln-2007-long-036</a> 
						<a href="bibtex/taln-2007-long-036.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-036-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-036-key');">mots clés</a> <br/>

							<p id="taln-2007-long-036-abs" class="resume">
							<b>Résumé : </b> Nous proposons D-STAG, un formalisme pour le discours qui utilise les TAG synchrones. Les analyses sémantiques produites par D-STAG sont des structures de discours hiérarchiques annotées de relations de discours coordonnantes ou subordonnantes. Elles sont compatibles avec les structures de discours produites tant en RST qu’en SDRT. Les relations de discours coordonnantes et subordonnantes sont modélisées respectivement par les opérations de substitution et d’adjonction introduites en TAG.
							</p>

							<p id="taln-2007-long-036-key" class="mots_cles">
							<b>Mots clés : </b> discours, grammaires d’arbres adjoints (synchrones), interface syntaxe/sémantique
							</p>

					</div>
					

					<div class="article">

						<b>Violeta Seretan, Éric Wehrli</b>


						<br/>

							<i>Collocation translation based on sentence alignment and parsing</i> <br/>

						<a href="actes/taln-2007-long-037.pdf">taln-2007-long-037</a> 
						<a href="bibtex/taln-2007-long-037.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-037-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-037-key');">mots clés</a> <br/>

							<p id="taln-2007-long-037-abs" class="resume">
							<b>Résumé : </b> Bien que de nombreux efforts aient été déployés pour extraire des collocations à partir de corpus de textes, seule une minorité de travaux se préoccupent aussi de rendre le résultat de l’extraction prêt à être utilisé dans les applications TAL qui pourraient en bénéficier, telles que la traduction automatique. Cet article décrit une méthode précise d’identification de la traduction des collocations dans un corpus parallèle, qui présente les avantages suivants : elle peut traiter des collocation flexibles (et pas seulement figées) ; elle a besoin de ressources limitées et d’un pouvoir de calcul raisonnable (pas d’alignement complet, pas d’entraînement) ; elle peut être appliquée à plusieurs paires des langues et fonctionne même en l’absence de dictionnaires bilingues. La méthode est basée sur l’information syntaxique provenant du parseur multilingue Fips. L’évaluation effectuée sur 4000 collocations de type verbe-objet correspondant à plusieurs paires de langues a montré une précision moyenne de 89.8% et une couverture satisfaisante (70.9%). Ces résultats sont supérieurs à ceux enregistrés dans l’évaluation d’autres méthodes de traduction de collocations.
							</p>

							<p id="taln-2007-long-037-key" class="mots_cles">
							<b>Mots clés : </b> traduction de collocations, extraction de collocations, parsing, alignement de textes
							</p>

					</div>
					

					<div class="article">

						<b>Nasredine Semmar, Christian Fluhr</b>


						<br/>

							<i>Utilisation d’une approche basée sur la recherche cross-lingue d’information pour l’alignement de phrases à partir de textes bilingues Arabe-Français</i> <br/>

						<a href="actes/taln-2007-long-038.pdf">taln-2007-long-038</a> 
						<a href="bibtex/taln-2007-long-038.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-long-038-abs');">résumé</a>
							<a onclick="toggle('taln-2007-long-038-key');">mots clés</a> <br/>

							<p id="taln-2007-long-038-abs" class="resume">
							<b>Résumé : </b> L’alignement de phrases à partir de textes bilingues consiste à reconnaître les phrases qui sont traductions les unes des autres. Cet article présente une nouvelle approche pour aligner les phrases d’un corpus parallèle. Cette approche est basée sur la recherche crosslingue d’information et consiste à construire une base de données des phrases du texte cible et considérer chaque phrase du texte source comme une requête à cette base. La recherche crosslingue utilise un analyseur linguistique et un moteur de recherche. L’analyseur linguistique traite aussi bien les documents à indexer que les requêtes et produit un ensemble de lemmes normalisés, un ensemble d’entités nommées et un ensemble de mots composés avec leurs étiquettes morpho-syntaxiques. Le moteur de recherche construit les fichiers inversés des documents en se basant sur leur analyse linguistique et retrouve les documents pertinents à partir de leur indexes. L’aligneur de phrases a été évalué sur un corpus parallèle Arabe-Français et les résultats obtenus montrent que 97% des phrases ont été correctement alignées.
							</p>

							<p id="taln-2007-long-038-key" class="mots_cles">
							<b>Mots clés : </b> alignement de phrases, corpus parallèle, recherche cross-lingue d’information
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="poster">Posters</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Laurent Audibert</b>


						<br/>

							<i>Désambiguïsation lexicale automatique : sélection automatique d’indices</i> <br/>

						<a href="actes/taln-2007-poster-001.pdf">taln-2007-poster-001</a> 
						<a href="bibtex/taln-2007-poster-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-001-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-001-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-001-abs" class="resume">
							<b>Résumé : </b> Nous exposons dans cet article une expérience de sélection automatique des indices du contexte pour la désambiguïsation lexicale automatique. Notre point de vue est qu’il est plus judicieux de privilégier la pertinence des indices du contexte plutôt que la sophistication des algorithmes de désambiguïsation utilisés. La sélection automatique des indices par le biais d’un algorithme génétique améliore significativement les résultats obtenus dans nos expériences précédentes tout en confortant des observations que nous avions faites sur la nature et la répartition des indices les plus pertinents.
							</p>

							<p id="taln-2007-poster-001-key" class="mots_cles">
							<b>Mots clés : </b> désambiguïsation lexicale automatique, corpus sémantiquement étiqueté, cooccurrences, sélection d’indices, algorithmes génétiques
							</p>

					</div>
					

					<div class="article">

						<b>Delphine Battistelli, Marie Chagnoux</b>


						<br/>

							<i>Représenter la dynamique énonciative et modale de textes</i> <br/>

						<a href="actes/taln-2007-poster-002.pdf">taln-2007-poster-002</a> 
						<a href="bibtex/taln-2007-poster-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-002-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-002-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-002-abs" class="resume">
							<b>Résumé : </b> Nous proposons d’exposer ici une méthodologie d’analyse et de représentation d’une des composantes de la structuration des textes, celle liée à la notion de prise en charge énonciative. Nous mettons l’accent sur la structure hiérarchisée des segments textuels qui en résulte ; nous la représentons d’une part sous forme d’arbre et d’autre part sous forme de graphe. Ce dernier permet d’appréhender la dynamique énonciative et modale de textes comme un cheminement qui s’opère entre différents niveaux de discours dans un texte au fur et à mesure de sa lecture syntagmatique.
							</p>

							<p id="taln-2007-poster-002-key" class="mots_cles">
							<b>Mots clés : </b> linguistique textuelle, énonciation, représentation sémantique
							</p>

					</div>
					

					<div class="article">

						<b>Olivier Blanc, Matthieu Constant, Patrick Watrin</b>


						<br/>

							<i>Segmentation en super-chunks</i> <br/>

						<a href="actes/taln-2007-poster-003.pdf">taln-2007-poster-003</a> 
						<a href="bibtex/taln-2007-poster-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-003-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-003-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-003-abs" class="resume">
							<b>Résumé : </b> Depuis l’analyseur développé par Harris à la fin des années 50, les unités polylexicales ont peu à peu été intégrées aux analyseurs syntaxiques. Cependant, pour la plupart, elles sont encore restreintes aux mots composés qui sont plus stables et moins nombreux. Toutefois, la langue est remplie d’expressions semi-figées qui forment également des unités sémantiques : les expressions adverbiales et les collocations. De même que pour les mots composés traditionnels, l’identification de ces structures limite la complexité combinatoire induite par l’ambiguïté lexicale. Dans cet article, nous détaillons une expérience qui intègre ces notions dans un processus de segmentation en super-chunks, préalable à l’analyse syntaxique. Nous montrons que notre chunker, développé pour le français, atteint une précision et un rappel de 92,9 % et 98,7 %, respectivement. Par ailleurs, les unités polylexicales réalisent 36,6 % des attachements internes aux constituants nominaux et prépositionnels.
							</p>

							<p id="taln-2007-poster-003-key" class="mots_cles">
							<b>Mots clés : </b> chunker, super-chunks, analyse syntaxique, patrons lexico-syntaxiques
							</p>

					</div>
					

					<div class="article">

						<b>Narjès Boufaden, Truong Le Hoang, Pierre Dumouchel</b>


						<br/>

							<i>Détection et prédiction de la satisfaction des usagers dans les dialogues Personne-Machine</i> <br/>

						<a href="actes/taln-2007-poster-004.pdf">taln-2007-poster-004</a> 
						<a href="bibtex/taln-2007-poster-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-004-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-004-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-004-abs" class="resume">
							<b>Résumé : </b> Nous étudions le rôle des entités nommées et marques discursives de rétroaction pour la tâche de classification et prédiction de la satisfaction usager à partir de dialogues. Les expériences menées sur 1027 dialogues Personne-Machine dans le domaine des agences de voyage montrent que les entités nommées et les marques discursives n’améliorent pas de manière significative le taux de classification des dialogues. Par contre, elles permettent une meilleure prédiction de la satisfaction usager à partir des premiers tours de parole usager.
							</p>

							<p id="taln-2007-poster-004-key" class="mots_cles">
							<b>Mots clés : </b> prédiction de la satisfaction usager, classification des dialogues Personne-Machine
							</p>

					</div>
					

					<div class="article">

						<b>Pierrette Bouillon, Manny Rayner, Marianne Starlander, Marianne Santaholma</b>


						<br/>

							<i>Les ellipses dans un système de traduction automatique de la parole</i> <br/>

						<a href="actes/taln-2007-poster-005.pdf">taln-2007-poster-005</a> 
						<a href="bibtex/taln-2007-poster-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-005-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-005-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-005-abs" class="resume">
							<b>Résumé : </b> Dans tout dialogue, les phrases elliptiques sont très nombreuses. Dans cet article, nous évaluons leur impact sur la reconnaissance et la traduction dans le système de traduction automatique de la parole MedSLT. La résolution des ellipses y est effectuée par une méthode robuste et portable, empruntée aux systèmes de dialogue homme-machine. Cette dernière exploite une représentation sémantique plate et combine des techniques linguistiques (pour construire la représentation) et basées sur les exemples (pour apprendre sur la base d’un corpus ce qu’est une ellipse bien formée dans un sous-domaine donné et comment la résoudre).
							</p>

							<p id="taln-2007-poster-005-key" class="mots_cles">
							<b>Mots clés : </b> traduction automatique de la parole, reconnaissance de la parole, ellipses, évaluation, traitement du dialogue, modèle du language fondé sur les grammaire
							</p>

					</div>
					

					<div class="article">

						<b>Nathalie Camelin, Frédéric Béchet, Géraldine Damnati, Renato De Mori</b>


						<br/>

							<i>Analyse automatique de sondages téléphoniques d’opinion</i> <br/>

						<a href="actes/taln-2007-poster-006.pdf">taln-2007-poster-006</a> 
						<a href="bibtex/taln-2007-poster-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-006-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-006-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-006-abs" class="resume">
							<b>Résumé : </b> Cette étude présente la problématique de l’analyse automatique de sondages téléphoniques d’opinion. Cette analyse se fait en deux étapes : tout d’abord extraire des messages oraux les expressions subjectives relatives aux opinions de utilisateurs sur une dimension particulière (efficacité, accueil, etc.) ; puis sélectionner les messages fiables, selon un ensemble de mesures de confiance, et estimer la distribution des diverses opinions sur le corpus de test. Le but est d’estimer une distribution aussi proche que possible de la distribution de référence. Cette étude est menée sur un corpus de messages provenant de vrais utilisateurs fournis par France Télécom R&amp;D.
							</p>

							<p id="taln-2007-poster-006-key" class="mots_cles">
							<b>Mots clés : </b> détection d’opinions, classification automatique, reconnaissance automatique de la parole, champs conditionnels aléatoires
							</p>

					</div>
					

					<div class="article">

						<b>Claire Gardent, Éric Kow</b>


						<br/>

							<i>Une réalisateur de surface basé sur une grammaire réversible</i> <br/>

						<a href="actes/taln-2007-poster-007.pdf">taln-2007-poster-007</a> 
						<a href="bibtex/taln-2007-poster-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-007-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-007-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-007-abs" class="resume">
							<b>Résumé : </b> En génération, un réalisateur de surface a pour fonction de produire, à partir d’une représentation conceptuelle donnée, une phrase grammaticale. Les réalisateur existants soit utilisent une grammaire réversible et des méthodes statistiques pour déterminer parmi l’ensemble des sorties produites la plus plausible ; soit utilisent des grammaires spécialisées pour la génération et des méthodes symboliques pour déterminer la paraphrase la plus appropriée à un contexte de génération donné. Dans cet article, nous présentons GENI, un réalisateur de surface basé sur une grammaire d’arbres adjoints pour le français qui réconcilie les deux approches en combinant une grammaire réversible avec une sélection symbolique des paraphrases.
							</p>

							<p id="taln-2007-poster-007-key" class="mots_cles">
							<b>Mots clés : </b> réalisation de surface, grammaire d’arbres adjoints, réversibilité
							</p>

					</div>
					

					<div class="article">

						<b>Laurent Gillard, Patrice Bellot, Marc El-Bèze</b>


						<br/>

							<i>Analyse des échecs d’une approche pour traiter les questions définitoires soumises à un système de questions/réponses</i> <br/>

						<a href="actes/taln-2007-poster-008.pdf">taln-2007-poster-008</a> 
						<a href="bibtex/taln-2007-poster-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-008-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-008-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-008-abs" class="resume">
							<b>Résumé : </b> Cet article revient sur le type particulier des questions définitoires étudiées dans le cadre des campagnes d’évaluation des systèmes de Questions/Réponses. Nous présentons l’approche développée suite à notre participation à la campagne EQueR et son évaluation lors de QA@CLEF 2006. La réponse proposée est la plus représentative des expressions présentes en apposition avec l’objet à définir, sa sélection est faite depuis des indices dérivés de ces appositions. Environ 80% de bonnes réponses sont trouvées sur les questions définitoires des volets francophones de CLEF. Les cas d’erreurs rencontrés sont analysés et discutés en détail.
							</p>

							<p id="taln-2007-poster-008-key" class="mots_cles">
							<b>Mots clés : </b> système de questions/réponses, questions définitoires
							</p>

					</div>
					

					<div class="article">

						<b>Lorraine Goeuriot, Natalia Grabar, Béatrice Daille</b>


						<br/>

							<i>Caractérisation des discours scientifiques et vulgarisés en français, japonais et russe</i> <br/>

						<a href="actes/taln-2007-poster-009.pdf">taln-2007-poster-009</a> 
						<a href="bibtex/taln-2007-poster-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-009-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-009-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-009-abs" class="resume">
							<b>Résumé : </b> L’objectif principal de notre travail consiste à étudier la notion de comparabilité des corpus, et nous abordons cette question dans un contexte monolingue en cherchant à distinguer les documents scientifiques et vulgarisés. Nous travaillons séparément sur des corpus composés de documents du domaine médical dans trois langues à forte distance linguistique (le français, le japonais et le russe). Dans notre approche, les documents sont caractérisés dans chaque langue selon leur thématique et une typologie discursive qui se situe à trois niveaux de l’analyse des documents : structurel, modal et lexical. Le typage des documents est implémenté avec deux algorithmes d’apprentissage (SVMlight et C4.5). L’évaluation des résultats montre que la typologie discursive proposée est portable d’une langue à l’autre car elle permet en effet de distinguer les deux discours. Nous constatons néanmoins des performances très variées selon les langues, les algorithmes et les types de caractéristiques discursives.
							</p>

							<p id="taln-2007-poster-009-key" class="mots_cles">
							<b>Mots clés : </b> linguistique des corpus, corpus comparable, algorithmes d’apprentissage, analyse stylistique, degré de comparabilité
							</p>

					</div>
					

					<div class="article">

						<b>Thierry Hamon, Julien Derivière, Adeline Nazarenko</b>


						<br/>

							<i>OGMIOS : une plate-forme d’annotation linguistique de collection de documents issus du Web</i> <br/>

						<a href="actes/taln-2007-poster-010.pdf">taln-2007-poster-010</a> 
						<a href="bibtex/taln-2007-poster-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-010-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-010-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-010-abs" class="resume">
							<b>Résumé : </b> L’un des objectifs du projet ALVIS est d’intégrer des informations linguistiques dans des moteurs de recherche spécialisés. Dans ce contexte, nous avons conçu une plate-forme d’enrichissement linguistique de documents issus du Web, OGMIOS, exploitant des outils de TAL existants. Les documents peuvent être en français ou en anglais. Cette architecture est distribuée, afin de répondre aux contraintes liées aux traitements de gros volumes de textes, et adaptable, pour permettre l’analyse de sous-langages. La plate-forme est développée en Perl et disponible sous forme de modules CPAN. C’est une structure modulaire dans lequel il est possible d’intégrer de nouvelles ressources ou de nouveaux outils de TAL. On peut ainsi définir des configuration différentes pour différents domaines et types de collections. Cette plateforme robuste permet d’analyser en masse des données issus du web qui sont par essence très hétérogènes. Nous avons évalué les performances de la plateforme sur plusieurs collections de documents. En distribuant les traitements sur vingt machines, une collection de 55 329 documents du domaine de la biologie (106 millions de mots) a été annotée en 35 heures tandis qu’une collection de 48 422 dépêches relatives aux moteurs de recherche (14 millions de mots) a été annotée en 3 heures et 15 minutes.
							</p>

							<p id="taln-2007-poster-010-key" class="mots_cles">
							<b>Mots clés : </b> plateforme d’annotation linguistique, passage à l’échelle, robustesse
							</p>

					</div>
					

					<div class="article">

						<b>Sébastien Haton, Jean-Marie Pierrel</b>


						<br/>

							<i>Les Lexiques-Miroirs. Du dictionnaire bilingue au graphe multilingue</i> <br/>

						<a href="actes/taln-2007-poster-011.pdf">taln-2007-poster-011</a> 
						<a href="bibtex/taln-2007-poster-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-011-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-011-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-011-abs" class="resume">
							<b>Résumé : </b> On observe dans les dictionnaires bilingues une forte asymétrie entre les deux parties d’un même dictionnaire et l’existence de traductions et d’informations « cachées », i.e. pas directement visibles à l’entrée du mot à traduire. Nous proposons une méthodologie de récupération des données cachées ainsi que la « symétrisation » du dictionnaire grâce à un traitement automatique. L’étude d’un certain nombre de verbes et de leurs traductions en plusieurs langues a conduit à l’intégration de toutes les données, visibles ou cachées, au sein d’une base de données unique et multilingue. L’exploitation de la base de données a été rendue possible par l’écriture d’un algorithme de création de graphe synonymique qui lie dans un même espace les mots de langues différentes. Le programme qui en découle permettra de générer des dictionnaires paramétrables directement à partir du graphe.
							</p>

							<p id="taln-2007-poster-011-key" class="mots_cles">
							<b>Mots clés : </b> dictionnaires bilingues, traduction automatique, graphe multilingue, algorithme, polysémie verbale, dissymétrie lexicographique
							</p>

					</div>
					

					<div class="article">

						<b>Sylvain Kahane</b>


						<br/>

							<i>Traduction, restructurations syntaxiques et grammaires de correspondance</i> <br/>

						<a href="actes/taln-2007-poster-012.pdf">taln-2007-poster-012</a> 
						<a href="bibtex/taln-2007-poster-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-012-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-012-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-012-abs" class="resume">
							<b>Résumé : </b> Cet article présente une nouvelle formalisation du modèle de traduction par transfert de la Théorie Sens-Texte. Notre modélisation utilise les grammaires de correspondance polarisées et fait une stricte séparation entre les modèles monolingues, un lexique bilingue minimal et des règles de restructuration universelles, directement associées aux fonctions lexicales syntaxiques.
							</p>

							<p id="taln-2007-poster-012-key" class="mots_cles">
							<b>Mots clés : </b> traduction automatique, paraphrase, restructuration syntaxique, TST (Théorie Sens-Texte), grammaire de dépendance, fonction lexicale, lexique bilingue, GUP (Grammaire d’Unification Polarisée), grammaire de correspondance, grammaires synchrones
							</p>

					</div>
					

					<div class="article">

						<b>Aïda Khemakhem, Bilel Gargouri, Abdelhamid Abdelwahed, Gil Francopoulo</b>


						<br/>

							<i>Modélisation des paradigmes de flexion des verbes arabes selon la norme LMF - ISO 24613</i> <br/>

						<a href="actes/taln-2007-poster-013.pdf">taln-2007-poster-013</a> 
						<a href="bibtex/taln-2007-poster-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-013-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-013-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-013-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous spécifions les paradigmes de flexion des verbes arabes en respectant la version 9 de LMF (Lexical Markup Framework), future norme ISO 24613 qui traite de la standardisation des bases lexicales. La spécification de ces paradigmes se fonde sur une combinaison des racines et des schèmes. En particulier, nous mettons en relief les terminaisons de racines sensibles aux ajouts de suffixes et ce, afin de couvrir les situations non considérées dans les travaux existants. L’élaboration des paradigmes de flexion verbale que nous proposons est une description en intension d&#39;ArabicLDB (Arabic Lexical DataBase) qui est une base lexicale normalisée pour la langue arabe. Nos travaux sont illustrés par la réalisation d’un conjugueur des verbes arabes à partir d&#39;ArabicLDB.
							</p>

							<p id="taln-2007-poster-013-key" class="mots_cles">
							<b>Mots clés : </b> langue arabe, paradigmes de flexion verbale, base lexicale, norme ISO 24613, LMF, lexical markup framework, conjugueur des verbes arabes
							</p>

					</div>
					

					<div class="article">

						<b>Olivier Kraif, Claude Ponton</b>


						<br/>

							<i>Du bruit, du silence et des ambiguïtés : que faire du TAL pour l&#39;apprentissage des langues ?</i> <br/>

						<a href="actes/taln-2007-poster-014.pdf">taln-2007-poster-014</a> 
						<a href="bibtex/taln-2007-poster-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-014-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-014-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-014-abs" class="resume">
							<b>Résumé : </b> Nous proposons une nouvelle approche pour l’intégration du TAL dans les systèmes d’apprentissage des langues assisté par ordinateur (ALAO), la stratégie « moinsdisante ». Cette approche tire profit des technologies élémentaires mais fiables du TAL et insiste sur la nécessité de traitements modulaires et déclaratifs afin de faciliter la portabilité et la prise en main didactique des systèmes. Basé sur cette approche, ExoGen est un premier prototype pour la génération automatique d’activités lacunaires ou de lecture d’exemples. Il intègre un module de repérage et de description des réponses des apprenants fondé sur la comparaison entre réponse attendue et réponse donnée. L’analyse des différences graphiques, orthographiques et morphosyntaxiques permet un diagnostic des erreurs de type fautes d’orthographe, confusions, problèmes d’accord, de conjugaison, etc. La première évaluation d’ExoGen sur un extrait du corpus d’apprenants FRIDA produit des résultats prometteurs pour le développement de cette approche « moins-disante », et permet d&#39;envisager un modèle d&#39;analyse performant et généralisable à une grande variété d&#39;activités.
							</p>

							<p id="taln-2007-poster-014-key" class="mots_cles">
							<b>Mots clés : </b> ALAO, apprentissage des langues, diagnostic d&#39;erreur, feed-back d&#39;erreur
							</p>

					</div>
					

					<div class="article">

						<b>Anna Kupsc</b>


						<br/>

							<i>Extraction automatique de cadres de sous-catégorisation verbale pour le français à partir d’un corpus arboré</i> <br/>

						<a href="actes/taln-2007-poster-015.pdf">taln-2007-poster-015</a> 
						<a href="bibtex/taln-2007-poster-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-015-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-015-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-015-abs" class="resume">
							<b>Résumé : </b> Nous présentons une expérience d’extraction automatique des cadres de souscatégorisation pour 1362 verbes français. Nous exploitons un corpus journalistique richement annoté de 15 000 phrases dont nous extrayons 12 510 occurrences verbales. Nous évaluons dans un premier temps l’extraction des cadres basée sur la fonction des arguments, ce qui nous fournit 39 cadres différents avec une moyenne de 1.54 cadres par lemme. Ensuite, nous adoptons une approche mixte (fonction et catégorie syntaxique) qui nous fournit dans un premier temps 925 cadres différents, avec une moyenne de 3.44 cadres par lemme. Plusieurs méthodes de factorisation, neutralisant en particulier les variantes de réalisation avec le passif ou les pronoms clitiques, sont ensuite appliquées et nous permettent d’aboutir à 235 cadres différents avec une moyenne de 1.94 cadres par verbe. Nous comparons brièvement nos résultats avec les travaux existants pour le français et pour l’anglais.
							</p>

							<p id="taln-2007-poster-015-key" class="mots_cles">
							<b>Mots clés : </b> français, corpus arboré, sous-catégorisation verbale, lexique-grammaire
							</p>

					</div>
					

					<div class="article">

						<b>François Lareau</b>


						<br/>

							<i>Vers une formalisation des décompositions sémantiques dans la Grammaire d’Unification Sens-Texte</i> <br/>

						<a href="actes/taln-2007-poster-016.pdf">taln-2007-poster-016</a> 
						<a href="bibtex/taln-2007-poster-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-016-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-016-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-016-abs" class="resume">
							<b>Résumé : </b> Nous proposons une formalisation de la décomposition du sens dans le cadre de la Grammaire d’Unification Sens-Texte. Cette formalisation vise une meilleure intégration des décompositions sémantiques dans un modèle global de la langue. Elle repose sur un jeu de saturation de polarités qui permet de contrôler la construction des représentations décomposées ainsi que leur mise en correspondance avec des arbres syntaxiques qui les expriment. Le formalisme proposé est illustré ici dans une perspective de synthèse, mais il s’applique également en analyse.
							</p>

							<p id="taln-2007-poster-016-key" class="mots_cles">
							<b>Mots clés : </b> Grammaire d’Unification Sens-Texte, Théorie Sens-Texte, sémantique, représentation du sens, paraphrasage
							</p>

					</div>
					

					<div class="article">

						<b>Anne-Laure Ligozat, Brigitte Grau, Isabelle Robba, Anne Vilnat</b>


						<br/>

							<i>Systèmes de questions-réponses : vers la validation automatique des réponses</i> <br/>

						<a href="actes/taln-2007-poster-017.pdf">taln-2007-poster-017</a> 
						<a href="bibtex/taln-2007-poster-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-017-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-017-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-017-abs" class="resume">
							<b>Résumé : </b> Les systèmes de questions-réponses (SQR) ont pour but de trouver une information précise extraite d’une grande collection de documents comme le Web. Afin de pouvoir comparer les différentes stratégies possibles pour trouver une telle information, il est important d’évaluer ces systèmes. L’objectif d’une tâche de validation de réponses est d’estimer si une réponse donnée par un SQR est correcte ou non, en fonction du passage de texte donné comme justification. En 2006, nous avons participé à une tâche de validation de réponses, et dans cet article nous présentons la stratégie que nous avons utilisée. Celle-ci est fondée sur notre propre système de questions-réponses. Le principe est de comparer nos réponses avec les réponses à valider. Nous présentons les résultats obtenus et montrons les extensions possibles. À partir de quelques exemples, nous soulignons les difficultés que pose cette tâche.
							</p>

							<p id="taln-2007-poster-017-key" class="mots_cles">
							<b>Mots clés : </b> systèmes de questions-réponses, validation de réponses
							</p>

					</div>
					

					<div class="article">

						<b>Huei-Chi Lin, Max Silberztein</b>


						<br/>

							<i>Ressources lexicales chinoises pour le TALN</i> <br/>

						<a href="actes/taln-2007-poster-018.pdf">taln-2007-poster-018</a> 
						<a href="bibtex/taln-2007-poster-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-018-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-018-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-018-abs" class="resume">
							<b>Résumé : </b> Nous voulons traiter des textes chinois automatiquement ; pour ce faire, nous formalisons le vocabulaire chinois, en utilisant principalement des dictionnaires et des grammaires morphologiques et syntaxiques formalisés avec le logiciel NooJ. Nous présentons ici les critères linguistiques qui nous ont permis de construire dictionnaires et grammaires, sachant que l’application envisagée (linguistique de corpus) nous impose certaines contraintes dans la formalisation des unités de la langue, en particulier des composés.
							</p>

							<p id="taln-2007-poster-018-key" class="mots_cles">
							<b>Mots clés : </b> ressources linguistiques pour le chinois, linguistique de corpus, NooJ
							</p>

					</div>
					

					<div class="article">

						<b>Sinikka Loikkanen</b>


						<br/>

							<i>Étiquetage morpho-syntaxique de textes kabyles</i> <br/>

						<a href="actes/taln-2007-poster-019.pdf">taln-2007-poster-019</a> 
						<a href="bibtex/taln-2007-poster-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-019-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-019-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-019-abs" class="resume">
							<b>Résumé : </b> Cet article présente la construction d’un étiqueteur morpho-syntaxique développé pour annoter un corpus de textes kabyles (1 million de mots). Au sein de notre projet, un étiqueteur morpho-syntaxique a été développé et implémenté. Ceci inclut un analyseur morphologique ainsi que l’ensemble de règles de désambiguïsation qui se basent sur l’approche supervisée à base de règles. Pour effectuer le marquage, un jeu d’étiquettes morpho-syntaxiques pour le kabyle est proposé. Les résultats préliminaires sont très encourageants. Nous obtenons un taux d’étiquetage réussi autour de 97 % des textes en prose.
							</p>

							<p id="taln-2007-poster-019-key" class="mots_cles">
							<b>Mots clés : </b> Étiquetage morpho-syntaxique, corpus de textes, langue kabyle, berbère
							</p>

					</div>
					

					<div class="article">

						<b>Athina Michou</b>


						<br/>

							<i>Analyse syntaxique et traitement automatique du syntagme nominal grec moderne</i> <br/>

						<a href="actes/taln-2007-poster-020.pdf">taln-2007-poster-020</a> 
						<a href="bibtex/taln-2007-poster-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-020-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-020-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-020-abs" class="resume">
							<b>Résumé : </b> Cet article décrit le traitement automatique du syntagme nominal en grec moderne par le modèle d’analyse syntaxique multilingue Fips. L’analyse syntaxique linguistique est focalisée sur les points principaux du DP grec : l’accord entre les constituants fléchis, l’ordre flexible des constituants, la cliticisation sur les noms et le phénomène de la polydéfinitude. Il est montré comment ces phénomènes sont traités et implémentés dans le cadre de l’analyseur syntaxique FipsGreek, qui met en oeuvre un formalisme inspiré de la grammaire générative chomskyenne.
							</p>

							<p id="taln-2007-poster-020-key" class="mots_cles">
							<b>Mots clés : </b> analyseur grec, analyse morphosyntaxique, syntagme nominal, grec moderne
							</p>

					</div>
					

					<div class="article">

						<b>Erwan Moreau</b>


						<br/>

							<i>Apprentissage symbolique de grammaires et traitement automatique des langues</i> <br/>

						<a href="actes/taln-2007-poster-021.pdf">taln-2007-poster-021</a> 
						<a href="bibtex/taln-2007-poster-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-021-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-021-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-021-abs" class="resume">
							<b>Résumé : </b> Le modèle de Gold formalise le processus d’apprentissage d’un langage. Nous présentons dans cet article les avantages et inconvénients de ce cadre théorique contraignant, dans la perspective d’applications en TAL. Nous décrivons brièvement les récentes avancées dans ce domaine, qui soulèvent selon nous certaines questions importantes.
							</p>

							<p id="taln-2007-poster-021-key" class="mots_cles">
							<b>Mots clés : </b> apprentissage symbolique, modèle de Gold, grammaires catégorielles
							</p>

					</div>
					

					<div class="article">

						<b>Yayoi Nakamura-Delloye</b>


						<br/>

							<i>Méthodes d’alignement des propositions : un défi aux traductions croisées</i> <br/>

						<a href="actes/taln-2007-poster-022.pdf">taln-2007-poster-022</a> 
						<a href="bibtex/taln-2007-poster-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-022-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-022-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-022-abs" class="resume">
							<b>Résumé : </b> Le présent article décrit deux méthodes d’alignement des propositions : l’une basée sur les méthodes d’appariement des graphes et une autre inspirée de la classification ascendante hiérarchique (CAH). Les deux méthodes sont caractérisées par leur capacité d’alignement des traductions croisées, ce qui était impossible pour beaucoup de méthodes classiques d’alignement des phrases. Contrairement aux résultats obtenus avec l’approche spectrale qui nous paraissent non satisfaisants, l’alignement basé sur la méthode de classification ascendante hiérarchique est prometteur dans la mesure où cette technique supporte bien les traductions croisées.
							</p>

							<p id="taln-2007-poster-022-key" class="mots_cles">
							<b>Mots clés : </b> alignement des corpus parallèles, appariement de graphes, classification ascendante hiérarchique, proposition syntaxique, mémoire de traduction, linguistique contrastive
							</p>

					</div>
					

					<div class="article">

						<b>Fiammetta Namer, Pierrette Bouillon, Evelyne Jacquey</b>


						<br/>

							<i>Un Lexique Génératif de référence pour le français</i> <br/>

						<a href="actes/taln-2007-poster-023.pdf">taln-2007-poster-023</a> 
						<a href="bibtex/taln-2007-poster-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-023-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-023-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-023-abs" class="resume">
							<b>Résumé : </b> Cet article propose une approche originale visant la construction d’un lexique sémantique de référence sur le français. Sa principale caractéristique est de pouvoir s’appuyer sur les propriétés morphologiques des lexèmes. La méthode combine en effet des résultats d’analyse morphologique (Namer, 2002;2003), à partir de ressources lexicales de grande taille (nomenclatures du TLF) et des méthodologies d’acquisition d’information lexicale déjà éprouvées (Namer 2005; Sébillot 2002). Le format de représentation choisi, dans le cadre du Lexique Génératif, se distingue par ses propriétés d’expressivité et d’économie. Cette approche permet donc d’envisager la construction d’un lexique de référence sur le français caractérisé par une forte homogénéité tout en garantissant une couverture large, tant du point de vue de la nomenclature que du point de vue des contenus sémantiques. Une première validation de la méthode fournit une projection quantitative et qualitative des résultats attendus.
							</p>

							<p id="taln-2007-poster-023-key" class="mots_cles">
							<b>Mots clés : </b> acquisition lexicale, lexique de référence du français, modèle du lexique génératif, morphologie constructionnelle, corpus, sémantique
							</p>

					</div>
					

					<div class="article">

						<b>Patrick Paroubek, Anne Vilnat, Isabelle Robba, Christelle Ayache</b>


						<br/>

							<i>Les résultats de la campagne EASY d’évaluation des analyseurs syntaxiques du français</i> <br/>

						<a href="actes/taln-2007-poster-024.pdf">taln-2007-poster-024</a> 
						<a href="bibtex/taln-2007-poster-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-024-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-024-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-024-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons les résultats de la campagne d’évaluation EASY des analyseurs syntaxiques du français. EASY a été la toute première campagne d’évaluation comparative des analyseurs syntaxiques du français en mode boîte noire utilisant des mesures objectives quantitatives. EASY fait partie du programme TECHNOLANGUE du Ministère délégué à la Recherche et à l’Éducation, avec le soutien du ministère de délégué à l’industrie et du ministère de la culture et de la communication. Nous exposons tout d’abord la position de la campagne par rapport aux autres projets d’évaluation en analyse syntaxique, puis nous présentos son déroulement, et donnons les résultats des 15 analyseurs participants en fonction des différents types de corpus et des différentes annotations (constituants et relations). Nous proposons ensuite un ensemble de leçons à tirer de cette campagne, en particulier à propos du protocole d’évaluation, de la définition de la segmentation en unités linguistiques, du formalisme et des activités d’annotation, des critères de qualité des données, des annotations et des résultats, et finalement de la notion de référence en analyse syntaxique. Nous concluons en présentant comment les résultats d’EASY se prolongent dans le projet PASSAGE (ANR-06-MDCA-013) qui vient de débuter et dont l’objectif est d’étiqueter un grand corpus par plusieurs analyseurs en les combinant selon des paramètres issus de l’évaluation.
							</p>

							<p id="taln-2007-poster-024-key" class="mots_cles">
							<b>Mots clés : </b> analyseur syntaxique, évaluation, français
							</p>

					</div>
					

					<div class="article">

						<b>Holger Schwenk, Daniel Déchelotte, Hélène Bonneau-Maynard, Alexandre Allauzen</b>


						<br/>

							<i>Modèles statistiques enrichis par la syntaxe pour la traduction automatique</i> <br/>

						<a href="actes/taln-2007-poster-025.pdf">taln-2007-poster-025</a> 
						<a href="bibtex/taln-2007-poster-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-025-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-025-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-025-abs" class="resume">
							<b>Résumé : </b> La traduction automatique statistique par séquences de mots est une voie prometteuse. Nous présentons dans cet article deux évolutions complémentaires. La première permet une modélisation de la langue cible dans un espace continu. La seconde intègre des catégories morpho-syntaxiques aux unités manipulées par le modèle de traduction. Ces deux approches sont évaluées sur la tâche Tc-Star. Les résultats les plus intéressants sont obtenus par la combinaison de ces deux méthodes.
							</p>

							<p id="taln-2007-poster-025-key" class="mots_cles">
							<b>Mots clés : </b> traduction automatique, approche statistique, modélisation linguistique dans un espace continu, analyse morpho-syntaxique, désambiguïsation lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Laurianne Sitbon, Patrice Bellot, Philippe Blache</b>


						<br/>

							<i>Traitements phrastiques phonétiques pour la réécriture de phrases dysorthographiées</i> <br/>

						<a href="actes/taln-2007-poster-026.pdf">taln-2007-poster-026</a> 
						<a href="bibtex/taln-2007-poster-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-026-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-026-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-026-abs" class="resume">
							<b>Résumé : </b> Cet article décrit une méthode qui combine des hypothèses graphémiques et phonétiques au niveau de la phrase, à l’aide d’une réprésentation en automates à états finis et d’un modèle de langage, pour la réécriture de phrases tapées au clavier par des dysorthographiques. La particularité des écrits dysorthographiés qui empêche les correcteurs orthographiques d’être efficaces pour cette tâche est une segmentation en mots parfois incorrecte. La réécriture diffère de la correction en ce sens que les phrases réécrites ne sont pas à destination de l’utilisateur mais d’un système automatique, tel qu’un moteur de recherche. De ce fait l’évaluation est conduite sur des versions filtrées et lemmatisées des phrases. Le taux d’erreurs mots moyen passe de 51 % à 20 % avec notre méthode, et est de 0 % sur 43 % des phrases testées.
							</p>

							<p id="taln-2007-poster-026-key" class="mots_cles">
							<b>Mots clés : </b> réécriture de phrases, dyslexie, automates, correction orthographique
							</p>

					</div>
					

					<div class="article">

						<b>Grégory Smits, Christine Chardenon</b>


						<br/>

							<i>Vers une méthodologie générique de contrôle basée sur la combinaison de sources de jugement</i> <br/>

						<a href="actes/taln-2007-poster-027.pdf">taln-2007-poster-027</a> 
						<a href="bibtex/taln-2007-poster-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-027-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-027-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-027-abs" class="resume">
							<b>Résumé : </b> Le contrôle des hypothèses concurrentes générées par les différents modules qui peuvent intervenir dans des processus de TALN reste un enjeu important malgré de nombreuses avancées en terme de robustesse. Nous présentons dans cet article une méthodologie générique de contrôle exploitant des techniques issues de l’aide multicritère à la décision. À partir de l’ensemble des critères de comparaison disponibles et la formalisation des préférences d’un expert, l’approche proposée évalue la pertinence relative des différents objets linguistiques générés et conduit à la mise en place d’une action de contrôle appropriée telle que le filtrage, le classement, le tri ou la propagation.
							</p>

							<p id="taln-2007-poster-027-key" class="mots_cles">
							<b>Mots clés : </b> méthodologie de contrôle, aide multicritère à la décision, apprentissage automatique de métriques
							</p>

					</div>
					

					<div class="article">

						<b>Agnès Tutin</b>


						<br/>

							<i>Traitement sémantique par analyse distributionnelle des noms transdisciplinaires des écrits scientifiques</i> <br/>

						<a href="actes/taln-2007-poster-028.pdf">taln-2007-poster-028</a> 
						<a href="bibtex/taln-2007-poster-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-028-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-028-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-028-abs" class="resume">
							<b>Résumé : </b> Dans cette étude sur le lexique transdisciplinaire des écrits scientifiques, nous souhaitons évaluer dans quelle mesure les méthodes distributionnelles de TAL peuvent faciliter la tâche du linguiste dans le traitement sémantique de ce lexique. Après avoir défini le champ lexical et les corpus exploités, nous testons plusieurs méthodes basées sur des dépendances syntaxiques et observons les proximités sémantiques et les classes établies. L’hypothèse que certaines relations syntaxiques - en particulier les relations de sous-catégorisation – sont plus appropriées pour établir des classements sémantiques n’apparaît qu’en partie vérifiée. Si les relations de sous-catégorisation génèrent des proximités sémantiques entre les mots de meilleure qualité, cela ne semble pas le cas pour la classification par voisinage.
							</p>

							<p id="taln-2007-poster-028-key" class="mots_cles">
							<b>Mots clés : </b> corpus, écrits scientifiques, classes sémantiques, analyse distributionnelle
							</p>

					</div>
					

					<div class="article">

						<b>Jeanne Villaneau</b>


						<br/>

							<i>Une expérience de compréhension en contexte de dialogue avec le système LOGUS, approche logique de la compréhension de la langue orale</i> <br/>

						<a href="actes/taln-2007-poster-029.pdf">taln-2007-poster-029</a> 
						<a href="bibtex/taln-2007-poster-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-029-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-029-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-029-abs" class="resume">
							<b>Résumé : </b> LOGUS est un système de compréhension de la langue orale dans le cadre d’un dialogue homme-machine finalisé. Il est la mise en oeuvre d’une approche logique qui utilise différents formalismes afin d’obtenir un système robuste mais néanmoins relativement extensible. Cet article décrit essentiellement l’étape de compréhension en contexte de dialogue implémentée sur LOGUS, développée et testée à partir d’un corpus de réservation hôtelière enregistré et annoté lors des travaux du groupe MEDIA du projet technolangue. Il décrit également les différentes interrogations et conclusions que peut susciter une telle expérience et les résultats obtenus par le système dans la résolution des références. Concernant l’approche elle-même, cette expérience semble montrer que le formalisme adopté pour la représentation sémantique des énoncés est bien adapté à la compréhension en contexte.
							</p>

							<p id="taln-2007-poster-029-key" class="mots_cles">
							<b>Mots clés : </b> compréhension automatique de la parole, résolution des références, dialogue oral homme-machine
							</p>

					</div>
					

					<div class="article">

						<b>Anis Zouaghi, Mounir Zrigui, Mohamed Ben Ahmed</b>


						<br/>

							<i>Évaluation des performances d’un modèle de langage stochastique pour la compréhension de la parole arabe spontanée</i> <br/>

						<a href="actes/taln-2007-poster-030.pdf">taln-2007-poster-030</a> 
						<a href="bibtex/taln-2007-poster-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-poster-030-abs');">résumé</a>
							<a onclick="toggle('taln-2007-poster-030-key');">mots clés</a> <br/>

							<p id="taln-2007-poster-030-abs" class="resume">
							<b>Résumé : </b> Les modèles de Markov cachés (HMM : Hidden Markov Models) (Baum et al., 1970), sont très utilisés en reconnaissance de la parole et depuis quelques années en compréhension de la parole spontanée latine telle que le français ou l’anglais. Dans cet article, nous proposons d’utiliser et d’évaluer la performance de ce type de modèle pour l’interprétation sémantique de la parole arabe spontanée. Les résultats obtenus sont satisfaisants, nous avons atteint un taux d’erreur de l’ordre de 9,9% en employant un HMM à un seul niveau, avec des probabilités tri_grammes de transitions.
							</p>

							<p id="taln-2007-poster-030-key" class="mots_cles">
							<b>Mots clés : </b> analyse sémantique, modèle de langage stochastique, contexte pertinent, information mutuelle moyenne, parole arabe spontanée
							</p>

					</div>
					

					

					

					

					

				<h1 id="démonstration">Démonstrations</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Éric Brunelle, Simon Charest</b>


						<br/>

							<i>Présentation du logiciel Antidote RX</i> <br/>

						<a href="actes/taln-2007-demo-001.pdf">taln-2007-demo-001</a> 
						<a href="bibtex/taln-2007-demo-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-demo-001-abs');">résumé</a>

							<p id="taln-2007-demo-001-abs" class="resume">
							<b>Résumé : </b> Antidote RX est la sixième édition d’Antidote, un logiciel d’aide à la rédaction développé et commercialisé par la société Druide informatique. Antidote RX comporte un correcteur grammatical avancé, dix dictionnaires de consultation et dix guides linguistiques. Il fonctionne sous les systèmes d’exploitation Windows, Mac OS X et Linux.
							</p>


					</div>
					

					<div class="article">

						<b>Dominique Laurent, Sophie Nègre, Patrick Séguéla</b>


						<br/>

							<i>Logiciel Cordial</i> <br/>

						<a href="actes/taln-2007-demo-002.pdf">taln-2007-demo-002</a> 
						<a href="bibtex/taln-2007-demo-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-demo-002-abs');">résumé</a>

							<p id="taln-2007-demo-002-abs" class="resume">
							<b>Résumé : </b> Cordial est un correcteur efficace et discret enrichi d&#39;un grand nombre de fonctions d&#39;aide à la rédaction et d&#39;analyse de documents. Très riche avec ces multiples dictionnaires et souvent pertinent dans ses propositions, Cordial est un compagnon précieux qui vous permet d&#39;assurer la qualité de vos écrits. La version 2007 de Cordial s&#39;intègre dans un vaste éventail de logiciels comme les traitements de texte (Word, Open Office, Word Perfect...), clients de messagerie (Outlook, Notes, Thunderbird, webmails...) ou navigateurs (Explorer, Mozilla).
							</p>


					</div>
					

					<div class="article">

						<b>Elliott Macklovitch, Guy Lapalme</b>


						<br/>

							<i>TransCheck : un vérificateur automatique de traductions</i> <br/>

						<a href="actes/taln-2007-demo-003.pdf">taln-2007-demo-003</a> 
						<a href="bibtex/taln-2007-demo-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-demo-003-abs');">résumé</a>
							<a onclick="toggle('taln-2007-demo-003-key');">mots clés</a> <br/>

							<p id="taln-2007-demo-003-abs" class="resume">
							<b>Résumé : </b> Nous offrirons une démonstration de la dernière version de TransCheck, un vérificateur automatique de traductions que le RALI est en train de développer. TransCheck prend en entrée deux textes, un texte source dans une langue et sa traduction dans une autre, les aligne au niveau de la phrase et ensuite vérifie les régions alignées pour s’assurer de la présence de certains équivalents obligatoires (p. ex. la terminologie normalisée) et de l’absence de certaines interdictions de traduction (p. ex. des interférences de la langue source). Ainsi, TransCheck se veut un nouveau type d’outil d’aide à la traduction qui pourra à réduire le fardeau de la révision et diminuer le coût du contrôle de la qualité.
							</p>

							<p id="taln-2007-demo-003-key" class="mots_cles">
							<b>Mots clés : </b> traduction assistée par ordinateur, vérification automatique de traductions, révision de traduction
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Marie Pierrel, Etienne Petitjean</b>


						<br/>

							<i>Le CNRTL, Centre National de Ressources Textuelles et Lexicales, un outil de mutualisation de ressources linguistiques</i> <br/>

						<a href="actes/taln-2007-demo-004.pdf">taln-2007-demo-004</a> 
						<a href="bibtex/taln-2007-demo-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2007-demo-004-abs');">résumé</a>

							<p id="taln-2007-demo-004-abs" class="resume">
							<b>Résumé : </b> Créé en 2005 à l’initiative du Centre National de la Recherche Scientifique, le CNRTL propose une plate-forme unifiée pour l’accès aux ressources et documents électroniques destinés à l’étude et l’analyse de la langue française. Les services du CNRTL comprennent le recensement, la documentation (métadonnées), la normalisation, l’archivage, l’enrichissement et la diffusion des ressources. La pérennité du service et des données est garantie par le soutien institutionnel du CNRS, l’adossement à un laboratoire de recherche en linguistique et informatique du CNRS et de Nancy Université (ATILF – Analyse et Traitement Informatique de la Langue Française), ainsi que l’intégration dans le réseau européen CLARIN (common language resources and technology infrastructure european).
							</p>


					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>