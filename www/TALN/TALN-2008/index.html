<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>TALN'2008</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>TALN'2008, 15ème conférence sur le Traitement Automatique des Langues Naturelles</h1>
				<h2>Avignon (France), du 2008-06-09 au 2008-06-13</h2>
				<p>Président(s) : Frédéric Bechet, Jean-Francois Bonastre</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#long">Papiers longs</a></li>
				<li><a href="#court">Papiers courts</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="long">Papiers longs</h1>
			

					<div class="article">

						<b>Amanda Rocha-Chaves, Lucia-Helena Machado-Rino</b>


						<br/>

							<i>The Mitkov algorithm for anaphora resolution in Portuguese</i> <br/>

						<a href="actes/taln-2008-long-001.pdf">taln-2008-long-001</a> 
						<a href="bibtex/taln-2008-long-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-001-abs');">abstract</a>
							<a onclick="toggle('taln-2008-long-001-key');">keywords</a> <br/>

							<p id="taln-2008-long-001-abs" class="abstract">
							<b>Abstract : </b> This paper reports on the use of the Mitkov´s algorithm for pronoun resolution in texts written in Brazilian Portuguese. Third person pronouns are the only ones focused upon here, with noun phrases as antecedents. A system for anaphora resolution in Brazilian Portuguese texts was built that embeds most of the Mitkov’s features. Some of his resolution factors were directly incorporated into the system; others had to be slightly modified for language adequacy. The resulting approach was intrinsically evaluated on hand-annotated corpora. It was also compared to Lappin &amp; Leass’s algorithm for pronoun resolution, also customized to Portuguese. Success rate was the evaluation measure used in both experiments. The results of both evaluations are discussed here.
							</p>

							<p id="taln-2008-long-001-key" class="keywords">
							<b>Keywords : </b> Pronoun resolution, anaphora resolution
							</p>

					</div>
					

					<div class="article">

						<b>Paul Bédaride, Claire Gardent</b>


						<br/>

							<i>Réécriture et Détection d’Implication Textuelle</i> <br/>

						<a href="actes/taln-2008-long-002.pdf">taln-2008-long-002</a> 
						<a href="bibtex/taln-2008-long-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-002-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-002-key');">mots clés</a> <br/>

							<p id="taln-2008-long-002-abs" class="resume">
							<b>Résumé : </b> Nous présentons un système de normalisation de la variation syntaxique qui permet de mieux reconnaître la relation d’implication textuelle entre deux phrases. Le système est évalué sur une suite de tests comportant 2 520 paires test et les résultats montrent un gain en précision par rapport à un système de base variant entre 29.8 et 78.5 points la complexité des cas considérés.
							</p>

							<p id="taln-2008-long-002-key" class="mots_cles">
							<b>Mots clés : </b> Normalisation syntaxique, Détection d’implication textuelle, Réécriture de graphe
							</p>

					</div>
					

					<div class="article">

						<b>Delphine Battistelli, Javier Couto, Jean-Luc Minel, Sylviane Schwer</b>


						<br/>

							<i>Représentation algébrique des expressions calendaires et vue calendaire d’un texte</i> <br/>

						<a href="actes/taln-2008-long-003.pdf">taln-2008-long-003</a> 
						<a href="bibtex/taln-2008-long-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-003-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-003-key');">mots clés</a> <br/>

							<p id="taln-2008-long-003-abs" class="resume">
							<b>Résumé : </b> Cet article aborde l’étude des expressions temporelles qui font référence directement à des unités de temps relatives aux divisions courantes des calendriers, que nous qualifions d’expressions calendaires (EC). Nous proposons une modélisation de ces expressions en définissant une algèbre d’opérateurs qui sont liés aux classes de marqueurs linguistiques qui apparaissent dans les EC. A partir de notre modélisation, une vue calendaire est construite dans la plate-forme de visualisation et navigation textuelle NaviTexte, visant le support à la lecture de textes. Enfin, nous concluons sur les perspectives offertes par le développement d’une première application de navigation temporelle.
							</p>

							<p id="taln-2008-long-003-key" class="mots_cles">
							<b>Mots clés : </b> expressions temporelles calendaires, modélisation algébrique, visualisation
							</p>

					</div>
					

					<div class="article">

						<b>Gabriel Parent, Michel Gagnon, Philippe Muller</b>


						<br/>

							<i>Annotation d’expressions temporelles et d’événements en français</i> <br/>

						<a href="actes/taln-2008-long-004.pdf">taln-2008-long-004</a> 
						<a href="bibtex/taln-2008-long-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-004-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-004-key');">mots clés</a> <br/>

							<p id="taln-2008-long-004-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous proposons une méthode pour identifier, dans un texte en français, l’ensemble des expressions adverbiales de localisation temporelle, ainsi que tous les verbes, noms et adjectifs dénotant une éventualité (événement ou état). Cette méthode, en plus d’identifier ces expressions, extrait certaines informations sémantiques : la valeur de la localisation temporelle selon la norme TimeML et le type des éventualités. Pour les expressions adverbiales de localisation temporelle, nous utilisons une cascade d’automates, alors que pour l’identification des événements et états nous avons recours à une analyse complète de la phrase. Nos résultats sont proches de travaux comparables sur l’anglais, en l’absence d’évaluation quantitative similaire sur le français.
							</p>

							<p id="taln-2008-long-004-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’informations temporelle, TimeML
							</p>

					</div>
					

					<div class="article">

						<b>Stéphane Huet, Guillaume Gravier, Pascale Sébillot</b>


						<br/>

							<i>Un modèle multi-sources pour la segmentation en sujets de journaux radiophoniques</i> <br/>

						<a href="actes/taln-2008-long-005.pdf">taln-2008-long-005</a> 
						<a href="bibtex/taln-2008-long-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-005-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-005-key');">mots clés</a> <br/>

							<p id="taln-2008-long-005-abs" class="resume">
							<b>Résumé : </b> Nous présentons une méthode de segmentation de journaux radiophoniques en sujets, basée sur la prise en compte d’indices lexicaux, syntaxiques et acoustiques. Partant d’un modèle statistique existant de segmentation thématique, exploitant la notion de cohésion lexicale, nous étendons le formalisme pour y inclure des informations d’ordre syntaxique et acoustique. Les résultats expérimentaux montrent que le seul modèle de cohésion lexicale ne suffit pas pour le type de documents étudié en raison de la taille variable des segments et de l’absence d’un lien direct entre segment et thème. L’utilisation d’informations syntaxiques et acoustiques permet une amélioration substantielle de la segmentation obtenue.
							</p>

							<p id="taln-2008-long-005-key" class="mots_cles">
							<b>Mots clés : </b> segmentation en sujets, corpus oraux, cohésion lexicale, indices acoustiques, indices syntaxiques
							</p>

					</div>
					

					<div class="article">

						<b>Cédric Vidrequin, Juan-Manuel Torres-Moreno, Jean-Jacques Schneider, Marc El-Bèze</b>


						<br/>

							<i>Extraction automatique d&#39;informations à partir de micro-textes non structurés</i> <br/>

						<a href="actes/taln-2008-long-006.pdf">taln-2008-long-006</a> 
						<a href="bibtex/taln-2008-long-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-006-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-006-key');">mots clés</a> <br/>

							<p id="taln-2008-long-006-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article une méthode d&#39;extraction automatique d&#39;informations sur des textes de très petite taille, faiblement structurés. Nous travaillons sur des textes dont la rédaction n&#39;est pas normalisée, avec très peu de mots pour caractériser chaque information. Les textes ne contiennent pas ou très peu de phrases. Il s&#39;agit le plus souvent de morceaux de phrases ou d&#39;expressions composées de quelques mots. Nous comparons plusieurs méthodes d&#39;extraction, dont certaines sont entièrement automatiques. D&#39;autres utilisent en partie une connaissance du domaine que nous voulons réduite au minimum, de façon à minimiser le travail manuel en amont. Enfin, nous présentons nos résultats qui dépassent ce dont il est fait état dans la littérature, avec une précision équivalente et un rappel supérieur.
							</p>

							<p id="taln-2008-long-006-key" class="mots_cles">
							<b>Mots clés : </b> extraction automatique, micro-texte, texte non structuré, petites annonces
							</p>

					</div>
					

					<div class="article">

						<b>Laurent Gillard, Patrice Bellot, Marc El-Bèze</b>


						<br/>

							<i>Quelles combinaisons de scores et de critères numériques pour un système de Questions/Réponses ?</i> <br/>

						<a href="actes/taln-2008-long-007.pdf">taln-2008-long-007</a> 
						<a href="bibtex/taln-2008-long-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-007-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-007-key');">mots clés</a> <br/>

							<p id="taln-2008-long-007-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une discussion sur la combinaison de différents scores et critères numériques pour la sélection finale d’une réponse dans la partie en charge des questions factuelles du système de Questions/Réponses développé au LIA. Ces scores et critères numériques sont dérivés de ceux obtenus en sortie de deux composants cruciaux pour notre système : celui de sélection des passages susceptibles de contenir une réponse et celui d’extraction et de sélection d’une réponse. Ils sont étudiés au regard de leur expressivité. Des comparaisons sont faites avec des approches de sélection de passages mettant en oeuvre des scores conventionnels en recherche d’information. Parallèlement, l’influence de la taille des contextes (en nombre de phrases) est évaluée. Cela permet de mettre en évidence que le choix de passages constitués de trois phrases autour d’une réponse candidate, avec une sélection des réponses basée sur une combinaison entre un score de passage de type Lucene ou Cosine et d’un score de compacité apparaît comme un compromis intéressant.
							</p>

							<p id="taln-2008-long-007-key" class="mots_cles">
							<b>Mots clés : </b> Système de Questions/Réponses, compacité, densité, combinaison de scores
							</p>

					</div>
					

					<div class="article">

						<b>Vladimir Popescu, Jean Caelen</b>


						<br/>

							<i>Contrôle rhétorique de la génération des connecteurs concessifs en dialogue homme-machine</i> <br/>

						<a href="actes/taln-2008-long-008.pdf">taln-2008-long-008</a> 
						<a href="bibtex/taln-2008-long-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-008-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-008-key');">mots clés</a> <br/>

							<p id="taln-2008-long-008-abs" class="resume">
							<b>Résumé : </b> Les connecteurs discursifs ont on rôle important dans l’interprétation des discours (dialogiques ou pas), donc lorsqu’il s’agit de produire des énoncés, le choix des mots qui relient les énoncés (par exemple, en dialogue oral) s’avère essentiel pour assurer la compréhension des visées illocutoires des locuteurs. En linguistique computationnelle, le problème a été abordé surtout au niveau de l’interprétation des discours monologiques, tandis que pour le dialogue, les recherches se sont limitées en général à établir une correspondance quasiment biunivoque entre relations rhétoriques et connecteurs. Dans ce papier nous proposons un mécanisme pour guider la génération des connecteurs concessifs en dialogue, à la fois du point de vue discursif et sémantique ; chaque connecteur considéré sera contraint par un ensemble de conditions qui prennent en compte la cohérence du discours et la pertinence sémantique de chaque mot concerné. Les contraintes discursives, exprimées dans un formalisme dérivé de la SDRT (« Segmented Discourse Representation Theory ») seront plongées dans des contraintes sémantiques sur les connecteurs, proposées par l’école genevoise (Moeschler), pour enfin évaluer la cohérence du discours résultant de l’emploi de ces connecteurs.
							</p>

							<p id="taln-2008-long-008-key" class="mots_cles">
							<b>Mots clés : </b> Dialogue homme-machine, cohérence discursive, connecteurs concessifs, sémantique, pragmatique
							</p>

					</div>
					

					<div class="article">

						<b>Alexandre Denis, Matthieu Quignard</b>


						<br/>

							<i>Modélisation du principe d’ancrage pour la robustesse des systèmes de dialogue homme-machine finalisés</i> <br/>

						<a href="actes/taln-2008-long-009.pdf">taln-2008-long-009</a> 
						<a href="bibtex/taln-2008-long-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-009-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-009-key');">mots clés</a> <br/>

							<p id="taln-2008-long-009-abs" class="resume">
							<b>Résumé : </b> Cet article présente une modélisation du principe d’ancrage (grounding) pour la robustesse des systèmes de dialogue finalisés. Ce principe, décrit dans (Clark &amp; Schaefer, 1989), suggère que les participants à un dialogue fournissent des preuves de compréhension afin d’atteindre la compréhension mutuelle. Nous explicitons une définition computationnelle du principe d’ancrage fondée sur des jugements de compréhension qui, contrairement à d’autres modèles, conserve une motivation pour l’expression de la compréhension. Nous déroulons enfin le processus d’ancrage sur un exemple tiré de l’implémentation du modèle.
							</p>

							<p id="taln-2008-long-009-key" class="mots_cles">
							<b>Mots clés : </b> dialogue homme-machine, robustesse, ancrage, compréhension mutuelle
							</p>

					</div>
					

					<div class="article">

						<b>Silvia Fernández, Eric Sanjuan, Juan-Manuel Torres-Moreno</b>


						<br/>

							<i>Enertex : un système basé sur l’énergie textuelle</i> <br/>

						<a href="actes/taln-2008-long-010.pdf">taln-2008-long-010</a> 
						<a href="bibtex/taln-2008-long-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-010-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-010-key');">mots clés</a> <br/>

							<p id="taln-2008-long-010-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons des applications du système Enertex au Traitement Automatique de la Langue Naturelle. Enertex est basé sur l’énergie textuelle, une approche par réseaux de neurones inspirée de la physique statistique des systèmes magnétiques. Nous avons appliqué cette approche aux problèmes du résumé automatique multi-documents et de la détection de frontières thématiques. Les résultats, en trois langues : anglais, espagnol et français, sont très encourageants.
							</p>

							<p id="taln-2008-long-010-key" class="mots_cles">
							<b>Mots clés : </b> Énergie textuelle, Réseaux de neurones, Modèle de Hopfield, Résumé automatique, Frontières thématiques
							</p>

					</div>
					

					<div class="article">

						<b>Fatma Kallel Jaoua, Lamia Hadrich Belguith, Maher Jaoua, Abdelmajid Ben Hamadou</b>


						<br/>

							<i>Intégration d’une étape de pré-filtrage et d’une fonction multiobjectif en vue d’améliorer le système ExtraNews de résumé de documents multiples</i> <br/>

						<a href="actes/taln-2008-long-011.pdf">taln-2008-long-011</a> 
						<a href="bibtex/taln-2008-long-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-011-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-011-key');">mots clés</a> <br/>

							<p id="taln-2008-long-011-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons les améliorations que nous avons apportées au système ExtraNews de résumé automatique de documents multiples. Ce système se base sur l’utilisation d’un algorithme génétique qui permet de combiner les phrases des documents sources pour former les extraits, qui seront croisés et mutés pour générer de nouveaux extraits. La multiplicité des critères de sélection d’extraits nous a inspiré une première amélioration qui consiste à utiliser une technique d’optimisation multi-objectif en vue d’évaluer ces extraits. La deuxième amélioration consiste à intégrer une étape de pré-filtrage de phrases qui a pour objectif la réduction du nombre des phrases des textes sources en entrée. Une évaluation des améliorations apportées à notre système est réalisée sur les corpus de DUC’04 et DUC’07.
							</p>

							<p id="taln-2008-long-011-key" class="mots_cles">
							<b>Mots clés : </b> Résumé automatique, pré-filtrage de phrases, optimisation multi-objectif, algorithme génétique
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Langlais, Alexandre Patry, Fabrizio Gotti</b>


						<br/>

							<i>Recherche locale pour la traduction statistique à base de segments</i> <br/>

						<a href="actes/taln-2008-long-012.pdf">taln-2008-long-012</a> 
						<a href="bibtex/taln-2008-long-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-012-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-012-key');">mots clés</a> <br/>

							<p id="taln-2008-long-012-abs" class="resume">
							<b>Résumé : </b> Dans cette étude, nous nous intéressons à des algorithmes de recherche locale pour la traduction statistique à base de segments (phrase-based machine translation). Les algorithmes que nous étudions s’appuient sur une formulation complète d’un état dans l’espace de recherche contrairement aux décodeurs couramment utilisés qui explorent l’espace des préfixes des traductions possibles. Nous montrons que la recherche locale seule, permet de produire des traductions proches en qualité de celles fournies par les décodeurs usuels, en un temps nettement inférieur et à un coût mémoire constant. Nous montrons également sur plusieurs directions de traduction qu’elle permet d’améliorer de manière significative les traductions produites par le système à l’état de l’art Pharaoh (Koehn, 2004).
							</p>

							<p id="taln-2008-long-012-key" class="mots_cles">
							<b>Mots clés : </b> Traduction statistique, recherche locale, post-traitement
							</p>

					</div>
					

					<div class="article">

						<b>Catherine Kobus, François Yvon, Géraldine Damnati</b>


						<br/>

							<i>Transcrire les SMS comme on reconnaît la parole</i> <br/>

						<a href="actes/taln-2008-long-013.pdf">taln-2008-long-013</a> 
						<a href="bibtex/taln-2008-long-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-013-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-013-key');">mots clés</a> <br/>

							<p id="taln-2008-long-013-abs" class="resume">
							<b>Résumé : </b> Cet article présente une architecture inspirée des systèmes de reconnaissance vocale pour effectuer une normalisation orthographique de messages en « langage SMS ». Nous décrivons notre système de base, ainsi que diverses évolutions de ce système, qui permettent d’améliorer sensiblement la qualité des normalisations produites.
							</p>

							<p id="taln-2008-long-013-key" class="mots_cles">
							<b>Mots clés : </b> SMS, décodage phonétique, modèles de langage, transducteurs finis
							</p>

					</div>
					

					<div class="article">

						<b>Laura Kallmeyer, Yannick Parmentier</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Convertir des grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres (TT-MCTAG) en grammaires à concaténation d’intervalles (RCG)</i> <br/>

						<a href="actes/taln-2008-long-014.pdf">taln-2008-long-014</a> 
						<a href="bibtex/taln-2008-long-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-014-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-014-key');">mots clés</a> <br/>

							<p id="taln-2008-long-014-abs" class="resume">
							<b>Résumé : </b> Cet article étudie la relation entre les grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres (TT-MCTAG), un formalisme utilisé en linguistique informatique, et les grammaires à concaténation d’intervalles (RCG). Les RCGs sont connues pour décrire exactement la classe PTIME, il a en outre été démontré que les RCGs « simples » sont même équivalentes aux systèmes de réécriture hors-contextes linéaires (LCFRS), en d’autres termes, elles sont légèrement sensibles au contexte. TT-MCTAG a été proposé pour modéliser les langages à ordre des mots libre. En général ces langages sont NP-complets. Dans cet article, nous définissons une contrainte additionnelle sur les dérivations autorisées par le formalisme TT-MCTAG. Nous montrons ensuite comment cette forme restreinte de TT-MCTAG peut être convertie en une RCG simple équivalente. Le résultat est intéressant pour des raisons théoriques (puisqu’il montre que la forme restreinte de TT-MCTAG est légèrement sensible au contexte), mais également pour des raisons pratiques (la transformation proposée ici a été utilisée pour implanter un analyseur pour TT-MCTAG).
							</p>

							<p id="taln-2008-long-014-key" class="mots_cles">
							<b>Mots clés : </b> Grammaires d’arbres adjoints à composantesmultiples, grammaires à concaténation d’intervalles, légère sensibilité au contexte
							</p>

					</div>
					

					<div class="article">

						<b>Piet Mertens</b>


						<br/>

							<i>Factorisation des contraintes syntaxiques dans un analyseur de dépendance</i> <br/>

						<a href="actes/taln-2008-long-015.pdf">taln-2008-long-015</a> 
						<a href="bibtex/taln-2008-long-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-015-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-015-key');">mots clés</a> <br/>

							<p id="taln-2008-long-015-abs" class="resume">
							<b>Résumé : </b> Cet article décrit un analyseur syntaxique pour grammaires de dépendance lexicalisées. Le formalisme syntaxique se caractérise par une factorisation des contraintes syntaxiques qui se manifeste dans la séparation entre dépendance et ordre linéaire, la spécification fonctionnelle (plutôt que syntagmatique) des dépendants, la distinction entre dépendants valenciels (la sous-catégorisation) et non valenciels (les circonstants) et la saturation progressive des arbres. Ceci résulte en une formulation concise de la grammaire à un niveau très abstrait et l’élimination de la reduplication redondante des informations due aux réalisations alternatives des dépendants ou à leur ordre. Les arbres élémentaires (obtenus à partir des formes dans l’entrée) et dérivés sont combinés entre eux par adjonction d’un arbre dépendant saturé à un arbre régissant, moyennant l’unification des noeuds et des relations. La dérivation est réalisée grâce à un analyseur chart bi-directionnel.
							</p>

							<p id="taln-2008-long-015-key" class="mots_cles">
							<b>Mots clés : </b> Analyseur syntaxique, dépendance
							</p>

					</div>
					

					<div class="article">

						<b>Pascal Vaillant</b>


						<br/>

							<i>Grammaires factorisées pour des dialectes apparentés</i> <br/>

						<a href="actes/taln-2008-long-016.pdf">taln-2008-long-016</a> 
						<a href="bibtex/taln-2008-long-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-016-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-016-key');">mots clés</a> <br/>

							<p id="taln-2008-long-016-abs" class="resume">
							<b>Résumé : </b> Pour la formalisation du lexique et de la grammaire de dialectes étroitement apparentés, il peut se révéler utile de factoriser une partie du travail de modélisation. Les soussystèmes linguistiques isomorphes dans les différents dialectes peuvent alors faire l’objet d’une description commune, les différences étant spécifiées par ailleurs. Cette démarche aboutit à un modèle de grammaire à couches : le noyau est commun à la famille de dialectes, et une couche superficielle détermine les caractéristiques de chacun. Nous appliquons ce procédé à la famille des langues créoles à base lexicale française de l’aire américano-caraïbe.
							</p>

							<p id="taln-2008-long-016-key" class="mots_cles">
							<b>Mots clés : </b> TAG, modélisation, grammaire, variation dialectale
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Crabbé, Marie Candito</b>


						<br/>

							<i>Expériences d’analyse syntaxique statistique du français</i> <br/>

						<a href="actes/taln-2008-long-017.pdf">taln-2008-long-017</a> 
						<a href="bibtex/taln-2008-long-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-017-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-017-key');">mots clés</a> <br/>

							<p id="taln-2008-long-017-abs" class="resume">
							<b>Résumé : </b> Nous montrons qu’il est possible d’obtenir une analyse syntaxique statistique satisfaisante pour le français sur du corpus journalistique, à partir des données issues du French Treebank du laboratoire LLF, à l’aide d’un algorithme d’analyse non lexicalisé.
							</p>

							<p id="taln-2008-long-017-key" class="mots_cles">
							<b>Mots clés : </b> Analyseur syntaxique statistique, Analyse syntaxique non lexicalisée, Analyse du français
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Sagot, Darja Fišer</b>


						<br/>

							<i>Construction d’un wordnet libre du français à partir de ressources multilingues</i> <br/>

						<a href="actes/taln-2008-long-018.pdf">taln-2008-long-018</a> 
						<a href="bibtex/taln-2008-long-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-018-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-018-key');">mots clés</a> <br/>

							<p id="taln-2008-long-018-abs" class="resume">
							<b>Résumé : </b> Cet article décrit la construction d’un Wordnet Libre du Français (WOLF) à partir du Princeton WordNet et de diverses ressources multilingues. Les lexèmes polysémiques ont été traités au moyen d’une approche reposant sur l’alignement en mots d’un corpus parallèle en cinq langues. Le lexique multilingue extrait a été désambiguïsé sémantiquement à l’aide des wordnets des langues concernées. Par ailleurs, une approche bilingue a été suffisante pour construire de nouvelles entrées à partir des lexèmes monosémiques. Nous avons pour cela extrait des lexiques bilingues à partir deWikipédia et de thésaurus. Le wordnet obtenu a été évalué par rapport au wordnet français issu du projet EuroWordNet. Les résultats sont encourageants, et des applications sont d’ores et déjà envisagées.
							</p>

							<p id="taln-2008-long-018-key" class="mots_cles">
							<b>Mots clés : </b> Wordnet, corpus alignés, Wikipédia, sémantique lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Mathieu Lafourcade, Alain Joubert</b>


						<br/>

							<i>Détermination des sens d’usage dans un réseau lexical construit à l’aide d’un jeu en ligne</i> <br/>

						<a href="actes/taln-2008-long-019.pdf">taln-2008-long-019</a> 
						<a href="bibtex/taln-2008-long-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-019-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-019-key');">mots clés</a> <br/>

							<p id="taln-2008-long-019-abs" class="resume">
							<b>Résumé : </b> Les informations lexicales, indispensables pour les tâches réalisées en TALN, sont difficiles à collecter. En effet, effectuée manuellement, cette tâche nécessite la compétence d’experts et la durée nécessaire peut être prohibitive, alors que réalisée automatiquement, les résultats peuvent être biaisés par les corpus de textes retenus. L’approche présentée ici consiste à faire participer un grand nombre de personnes à un projet contributif en leur proposant une application ludique accessible sur le web. A partir d’une base de termes préexistante, ce sont ainsi les joueurs qui vont construire le réseau lexical, en fournissant des associations qui ne sont validées que si elles sont proposées par au moins une paire d’utilisateurs. De plus, ces relations typées sont pondérées en fonction du nombre de paires d’utilisateurs qui les ont proposées. Enfin, nous abordons la question de la détermination des différents sens d’usage d’un terme, en analysant les relations entre ce terme et ses voisins immédiats dans le réseau lexical, avant de présenter brièvement la réalisation et les premiers résultats obtenus.
							</p>

							<p id="taln-2008-long-019-key" class="mots_cles">
							<b>Mots clés : </b> Traitement Automatique du Langage Naturel, réseau lexical, relations typées pondérées, sens d’usage d’un terme, jeu en ligne
							</p>

					</div>
					

					<div class="article">

						<b>Feten Baccar, Aïda Khemakhem, Bilel Gargouri, Kais Haddar, Abdelmajid Ben Hamadou</b>


						<br/>

							<i>Modélisation normalisée LMF des dictionnaires électroniques éditoriaux de l’arabe</i> <br/>

						<a href="actes/taln-2008-long-020.pdf">taln-2008-long-020</a> 
						<a href="bibtex/taln-2008-long-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-020-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-020-key');">mots clés</a> <br/>

							<p id="taln-2008-long-020-abs" class="resume">
							<b>Résumé : </b> Le présent papier s’intéresse à l’élaboration des dictionnaires électroniques arabes à usage éditorial. Il propose un modèle unifié et normalisé de ces dictionnaires en se référant à la future norme LMF (Lexical Markup Framework) ISO 24613. Ce modèle permet de construire des dictionnaires extensibles, sur lesquels on peut réaliser, grâce à une structuration fine et standard, des fonctions de consultation génériques adaptées aux besoins des utilisateurs. La mise en oeuvre du modèle proposé est testée sur des dictionnaires existants de la langue arabe en utilisant, pour la consultation, le système ADIQTO (Arabic DIctionary Query TOols) que nous avons développé pour l’interrogation générique des dictionnaires normalisés de l’arabe.
							</p>

							<p id="taln-2008-long-020-key" class="mots_cles">
							<b>Mots clés : </b> Dictionnaire électronique Arabe, usage éditorial, modèle normalisé, LMF, interrogation générique
							</p>

					</div>
					

					<div class="article">

						<b>Lucie Barque, François-Régis Chaumartin</b>


						<br/>

							<i>La polysémie régulière dans WordNet</i> <br/>

						<a href="actes/taln-2008-long-021.pdf">taln-2008-long-021</a> 
						<a href="bibtex/taln-2008-long-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-021-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-021-key');">mots clés</a> <br/>

							<p id="taln-2008-long-021-abs" class="resume">
							<b>Résumé : </b> Cette étude propose une analyse et une modélisation des relations de polysémie dans le lexique électronique anglais WordNet. Elle exploite pour cela la hiérarchie des concepts (représentés par des synsets), et la définition associée à chacun de ces concepts. Le résultat est constitué d&#39;un ensemble de règles qui nous ont permis d&#39;identifier d’une façon largement automatisée, avec une précision voisine de 91%, plus de 2100 paires de synsets liés par une relation de polysémie régulière. Notre méthode permet aussi une désambiguïsation lexicale partielle des mots de la définition associée à ces synsets.
							</p>

							<p id="taln-2008-long-021-key" class="mots_cles">
							<b>Mots clés : </b> polysémie régulière, métaphore, métonymie, WordNet, désambiguïsation lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Caroline Lavecchia, Kamel Smaïli, David Langlois</b>


						<br/>

							<i>Une alternative aux modèles de traduction statistique d’IBM: Les triggers inter-langues</i> <br/>

						<a href="actes/taln-2008-long-022.pdf">taln-2008-long-022</a> 
						<a href="bibtex/taln-2008-long-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-022-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-022-key');">mots clés</a> <br/>

							<p id="taln-2008-long-022-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une nouvelle approche pour la traduction automatique fondée sur les triggers inter-langues. Dans un premier temps, nous expliquons le concept de triggers inter-langues ainsi que la façon dont ils sont déterminés. Nous présentons ensuite les différentes expérimentations qui ont été menées à partir de ces triggers afin de les intégrer au mieux dans un processus complet de traduction automatique. Pour cela, nous construisons à partir des triggers inter-langues des tables de traduction suivant différentes méthodes. Nous comparons par la suite notre système de traduction fondé sur les triggers interlangues à un système état de l’art reposant sur le modèle 3 d’IBM (Brown &amp; al., 1993). Les tests menés ont montré que les traductions automatiques générées par notre système améliorent le score BLEU (Papineni &amp; al., 2001) de 2, 4% comparé à celles produites par le système état de l’art.
							</p>

							<p id="taln-2008-long-022-key" class="mots_cles">
							<b>Mots clés : </b> Traduction Automatique Statistique, Triggers Inter-Langues, Information Mutuelle, Corpus parallèle, Décodage
							</p>

					</div>
					

					<div class="article">

						<b>Aurélien Max</b>


						<br/>

							<i>Génération de reformulations locales par pivot pour l’aide à la révision</i> <br/>

						<a href="actes/taln-2008-long-023.pdf">taln-2008-long-023</a> 
						<a href="bibtex/taln-2008-long-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-023-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-023-key');">mots clés</a> <br/>

							<p id="taln-2008-long-023-abs" class="resume">
							<b>Résumé : </b> Cet article présente une approche pour obtenir des paraphrases pour de courts segments de texte qui peuvent aider un rédacteur à reformuler localement des textes. La ressource principale utilisée est une table d’alignements bilingues de segments d’un système de traduction automatique statistique. Un segment marqué par le rédacteur est tout d’abord traduit dans une langue pivot avant d’être traduit à nouveau dans la langue d’origine, ce qui est permis par la nature même de la ressource bilingue utilisée sans avoir recours à un processus de traduction complet. Le cadre proposé permet l’intégration et la combinaison de différents modèles d’estimation de la qualité des paraphrases. Des modèles linguistiques tentant de prendre en compte des caractéristiques des paraphrases de courts segments de textes sont proposés, et une évaluation est décrite et ses résultats analysés. Les domaines d’application possibles incluent, outre l’aide à la reformulation, le résumé et la réécriture des textes pour répondre à des conventions ou à des préférences stylistiques. L’approche est critiquée et des perspectives d’amélioration sont proposées.
							</p>

							<p id="taln-2008-long-023-key" class="mots_cles">
							<b>Mots clés : </b> Paraphrase, Traduction Automatique Statistique basée sur les segments, Aide à la rédaction
							</p>

					</div>
					

					<div class="article">

						<b>Christian Boitet</b>


						<br/>

							<i>Les architectures linguistiques et computationnelles en traduction automatique sont indépendantes</i> <br/>

						<a href="actes/taln-2008-long-024.pdf">taln-2008-long-024</a> 
						<a href="bibtex/taln-2008-long-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-024-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-024-key');">mots clés</a> <br/>

							<p id="taln-2008-long-024-abs" class="resume">
							<b>Résumé : </b> Contrairement à une idée répandue, les architectures linguistiques et computationnelles des systèmes de traduction automatique sont indépendantes. Les premières concernent le choix des représentations intermédiaires, les secondes le type d&#39;algorithme, de programmation et de ressources utilisés. Il est ainsi possible d&#39;utiliser des méthodes de calcul « expertes » ou « empiriques » pour construire diverses phases ou modules de systèmes d&#39;architectures linguistiques variées. Nous terminons en donnant quelques éléments pour le choix de ces architectures en fonction des situations traductionnelles et des ressources disponibles, en termes de dictionnaires, de corpus, et de compétences humaines.
							</p>

							<p id="taln-2008-long-024-key" class="mots_cles">
							<b>Mots clés : </b> Traduction Automatique, TA, TAO, architecture linguistique, architecture computationnelle, TA experte, TA par règles, TA empirique, TA statistique, TA par l&#39;exemple
							</p>

					</div>
					

					<div class="article">

						<b>Caroline Brun, Caroline Hagège</b>


						<br/>

							<i>Vérification sémantique pour l’annotation d’entités nommées</i> <br/>

						<a href="actes/taln-2008-long-025.pdf">taln-2008-long-025</a> 
						<a href="bibtex/taln-2008-long-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-025-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-025-key');">mots clés</a> <br/>

							<p id="taln-2008-long-025-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous proposons une méthode visant à corriger et à associer dynamiquement de nouveaux types sémantiques dans le cadre de systèmes de détection automatique d’entités nommées (EN). Après la détection des entités nommées et aussi de manière plus générale des noms propres dans les textes, une vérification de compatibilité de types sémantiques est effectuée non seulement pour confirmer ou corriger les résultats obtenus par le système de détection d’EN, mais aussi pour associer de nouveaux types non couverts par le système de détection d’EN. Cette vérification est effectuée en utilisant l’information syntaxique associée aux EN par un système d’analyse syntaxique robuste et en confrontant ces résultats avec la ressource sémantique WordNet. Les résultats du système de détection d’EN sont alors considérablement enrichis, ainsi que les étiquettes sémantiques associées aux EN, ce qui est particulièrement utile pour l’adaptation de systèmes de détection d’EN à de nouveaux domaines.
							</p>

							<p id="taln-2008-long-025-key" class="mots_cles">
							<b>Mots clés : </b> Entités nommées, Analyse syntaxique robuste, Types sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>Thomas Girault</b>


						<br/>

							<i>Exploitation de treillis de Galois en désambiguïsation non supervisée d’entités nommées</i> <br/>

						<a href="actes/taln-2008-long-026.pdf">taln-2008-long-026</a> 
						<a href="bibtex/taln-2008-long-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-026-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-026-key');">mots clés</a> <br/>

							<p id="taln-2008-long-026-abs" class="resume">
							<b>Résumé : </b> Nous présentons une méthode non supervisée de désambiguïsation d’entités nommées, basée sur l’exploitation des treillis de Galois. Nous réalisons une analyse de concepts formels à partir de relations entre des entités nommées et leurs contextes syntaxiques extraits d’un corpus d’apprentissage. Le treillis de Galois résultant fournit des concepts qui sont utilisés comme des étiquettes pour annoter les entités nommées et leurs contextes dans un corpus de test. Une évaluation en cascade montre qu’un système d’apprentissage supervisé améliore la classification des entités nommées lorsqu’il s’appuie sur l’annotation réalisée par notre système de désambiguïsation non supervisée.
							</p>

							<p id="taln-2008-long-026-key" class="mots_cles">
							<b>Mots clés : </b> Désambiguïsation non supervisée, treillis de Galois, entités nommées
							</p>

					</div>
					

					<div class="article">

						<b>Caroline Brun, Maud Ehrmann, Guillaume Jacquet</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Résolution de Métonymie des Entités Nommées : proposition d’une méthode hybride</i> <br/>

						<a href="actes/taln-2008-long-027.pdf">taln-2008-long-027</a> 
						<a href="bibtex/taln-2008-long-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-027-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-027-key');">mots clés</a> <br/>

							<p id="taln-2008-long-027-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous décrivons la méthode que nous avons développée pour la résolution de métonymie des entités nommées dans le cadre de la compétition SemEval 2007. Afin de résoudre les métonymies sur les noms de lieux et noms d’organisation, tel que requis pour cette tâche, nous avons mis au point un système hybride basé sur l’utilisation d’un analyseur syntaxique robuste combiné avec une méthode d’analyse distributionnelle. Nous décrivons cette méthode ainsi que les résultats obtenus par le système dans le cadre de la compétition SemEval 2007.
							</p>

							<p id="taln-2008-long-027-key" class="mots_cles">
							<b>Mots clés : </b> Entités Nommées, métonymie, méthode hybride, analyse syntaxique robuste, approche distributionnelle
							</p>

					</div>
					

					<div class="article">

						<b>Tatiana El-Khoury</b>


						<br/>

							<i>Etude de la corrélation entre morphosyntaxe et sémantique dans une perspective d’étiquetage automatique de textes médicaux arabes</i> <br/>

						<a href="actes/taln-2008-long-028.pdf">taln-2008-long-028</a> 
						<a href="bibtex/taln-2008-long-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-028-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-028-key');">mots clés</a> <br/>

							<p id="taln-2008-long-028-abs" class="resume">
							<b>Résumé : </b> Cet article se propose d’étudier les relations sémantiques reliant base et expansion au sein des termes médicaux arabes de type « N+N », particulièrement ceux dont la base est un déverbal. En étudiant les relations sémantiques établies par une base déverbale, ce travail tente d’attirer l’attention sur l’interpénétration du sémantique et du morphosyntaxique ; il montre que, dans une large mesure, la structure morphosyntaxique de la base détermine l’éventail des possibilités relationnelles. La découverte de régularités dans le comportement de la base déverbale permet de prédire le type de relations que peut établir cette base avec son expansion pavant ainsi la voie à un traitement automatique et un travail d’étiquetage sémantique des textes médicaux arabes.
							</p>

							<p id="taln-2008-long-028-key" class="mots_cles">
							<b>Mots clés : </b> étiquetage automatique, terminologie médicale arabe, morphosyntaxe, sémantique
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Blache, Stéphane Rauzy</b>


						<br/>

							<i>Influence de la qualité de l’étiquetage sur le chunking : une corrélation dépendant de la taille des chunks</i> <br/>

						<a href="actes/taln-2008-long-029.pdf">taln-2008-long-029</a> 
						<a href="bibtex/taln-2008-long-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-029-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-029-key');">mots clés</a> <br/>

							<p id="taln-2008-long-029-abs" class="resume">
							<b>Résumé : </b> Nous montrons dans cet article qu’il existe une corrélation étroite existant entre la qualité de l’étiquetage morpho-syntaxique et les performances des chunkers. Cette corrélation devient linéaire lorsque la taille des chunks est limitée. Nous appuyons notre démonstration sur la base d’une expérimentation conduite suite à la campagne d’évaluation Passage 2007 (de la Clergerie et al., 2008). Nous analysons pour cela les comportements de deux analyseurs ayant participé à cette campagne. L’interprétation des résultats montre que la tâche de chunking, lorsqu’elle vise des chunks courts, peut être assimilée à une tâche de “super-étiquetage”.
							</p>

							<p id="taln-2008-long-029-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique, étiquetage morphosyntaxique, analyseur stochastique, analyseur symbolique superficiel, chunker
							</p>

					</div>
					

					<div class="article">

						<b>Aurélie Névéol, Vincent Claveau</b>


						<br/>

							<i>Apprentissage artificiel de règles d’indexation pour MEDLINE</i> <br/>

						<a href="actes/taln-2008-long-030.pdf">taln-2008-long-030</a> 
						<a href="bibtex/taln-2008-long-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-long-030-abs');">résumé</a>
							<a onclick="toggle('taln-2008-long-030-key');">mots clés</a> <br/>

							<p id="taln-2008-long-030-abs" class="resume">
							<b>Résumé : </b> L’indexation est une composante importante de tout système de recherche d’information. Dans MEDLINE, la base documentaire de référence pour la littérature du domaine biomédical, le contenu des articles référencés est indexé à l’aide de descripteurs issus du thésaurus MeSH. Avec l’augmentation constante de publications à indexer pour maintenir la base à jour, le besoin d’outils automatiques se fait pressant pour les indexeurs. Dans cet article, nous décrivons l’utilisation et l’adaptation de la Programmation Logique Inductive (PLI) pour découvrir des règles d’indexation permettant de générer automatiquement des recommandations d’indexation pour MEDLINE. Les résultats obtenus par cette approche originale sont très satisfaisants comparés à ceux obtenus à l’aide de règles manuelles lorsque celles-ci existent. Ainsi, les jeux de règles obtenus par PLI devraient être prochainement intégrés au système produisant les recommandations d’indexation automatique pour MEDLINE.
							</p>

							<p id="taln-2008-long-030-key" class="mots_cles">
							<b>Mots clés : </b> Analyse et Indexation/méthodes, Medical Subject Headings, Apprentissage Artificiel, Programmation Logique Inductive
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="court">Papiers courts</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Yayoi Nakamura-Delloye</b>


						<br/>

							<i>Y a-t-il une véritable équivalence entre les propositions syntaxiques du français et du japonais ?</i> <br/>

						<a href="actes/taln-2008-court-001.pdf">taln-2008-court-001</a> 
						<a href="bibtex/taln-2008-court-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-001-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-001-key');">mots clés</a> <br/>

							<p id="taln-2008-court-001-abs" class="resume">
							<b>Résumé : </b> La présente contribution part de nos constats réalisés à partir des résultats d’évaluation de notre système d’alignement des propositions de textes français-japonais. La présence importante de structures fondamentalement difficiles à aligner et les résultats peu satisfaisants de différentes méthodes de mise en correspondance des mots nous ont finalement amenés à remettre en cause l’existence même d’équivalence au niveau des propositions syntaxiques entre le français et le japonais. Afin de compenser les défauts que nous avons découverts, nous proposons des opérations permettant de restaurer l’équivalence des propositions alignées et d’améliorer la qualité des corpus alignés.
							</p>

							<p id="taln-2008-court-001-key" class="mots_cles">
							<b>Mots clés : </b> Alignement, proposition syntaxique, études contrastives français-japonais, similarité lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Sylvain Schmitz, Joseph Le Roux</b>


						<br/>

							<i>Calculs d’unification sur les arbres de dérivation TAG</i> <br/>

						<a href="actes/taln-2008-court-002.pdf">taln-2008-court-002</a> 
						<a href="bibtex/taln-2008-court-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-002-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-002-key');">mots clés</a> <br/>

							<p id="taln-2008-court-002-abs" class="resume">
							<b>Résumé : </b> Nous définissons un formalisme, les grammaires rationnelles d’arbres avec traits, et une traduction des grammaires d’arbres adjoints avec traits vers ce nouveau formalisme. Cette traduction préserve les structures de dérivation de la grammaire d’origine en tenant compte de l’unification de traits. La construction peut être appliquée aux réalisateurs de surface qui se fondent sur les arbres de dérivation.
							</p>

							<p id="taln-2008-court-002-key" class="mots_cles">
							<b>Mots clés : </b> Unification, grammaire d’arbres adjoints, arbre de dérivation, grammaire rationnelle d’arbres
							</p>

					</div>
					

					<div class="article">

						<b>Alexandre Labadié, Violaine Prince</b>


						<br/>

							<i>Comparaison de méthodes lexicales et syntaxico-sémantiques dans la segmentation thématique de texte non supervisée</i> <br/>

						<a href="actes/taln-2008-court-003.pdf">taln-2008-court-003</a> 
						<a href="bibtex/taln-2008-court-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-003-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-003-key');">mots clés</a> <br/>

							<p id="taln-2008-court-003-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthode basée sur des calculs de distance et une analyse sémantique et syntaxique pour la segmentation thématique de texte. Pour évaluer cette méthode nous la comparons à un un algorithme lexical très connu : c99. Nous testons les deux méthodes sur un corpus de discours politique français et comparons les résultats. Les deux conclusions qui ressortent de notre expérience sont que les approches sont complémentaires et que les protocoles d’évaluation actuels sont inadaptés.
							</p>

							<p id="taln-2008-court-003-key" class="mots_cles">
							<b>Mots clés : </b> Méthodes d’évaluation, segmentation de texte, segmentation thématique
							</p>

					</div>
					

					<div class="article">

						<b>Jérôme Lehuen</b>


						<br/>

							<i>Un modèle de langage pour le DHM : la Grammaire Sémantique Réversible</i> <br/>

						<a href="actes/taln-2008-court-004.pdf">taln-2008-court-004</a> 
						<a href="bibtex/taln-2008-court-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-004-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-004-key');">mots clés</a> <br/>

							<p id="taln-2008-court-004-abs" class="resume">
							<b>Résumé : </b> Cet article propose un modèle de langage dédié au dialogue homme-machine, ainsi que des algorithmes d’analyse et de génération. L’originalité de notre approche est de faire reposer l’analyse et la génération sur les mêmes connaissances, essentiellement sémantiques. Celles-ci sont structurées sous la forme d’une bibliothèque de concepts, et de formes d’usage associées aux concepts. Les algorithmes, quant à eux, sont fondés sur un double principe de correspondance entre des offres et des attentes, et d’un calcul heuristique de score.
							</p>

							<p id="taln-2008-court-004-key" class="mots_cles">
							<b>Mots clés : </b> Grammaire Sémantique, Réversibilité, Analyse, Génération, Dialogue
							</p>

					</div>
					

					<div class="article">

						<b>Maxime Amblard, Johannes Heinecke, Estelle Maillebuau</b>


						<br/>

							<i>Discourse Representation Theory et graphes sémantiques : formalisation sémantique en contexte industriel</i> <br/>

						<a href="actes/taln-2008-court-005.pdf">taln-2008-court-005</a> 
						<a href="bibtex/taln-2008-court-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-005-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-005-key');">mots clés</a> <br/>

							<p id="taln-2008-court-005-abs" class="resume">
							<b>Résumé : </b> Ces travaux présentent une extension des représentations formelles pour la sémantique, de l’outil de traitement automatique des langues de Orange Labs1. Nous abordons ici uniquement des questions relatives à la construction des représentations sémantiques, dans le cadre de l’analyse linguistique. Afin d’obtenir des représentations plus fines de la structure argumentale des énoncés, nous incluons des concepts issus de la DRT dans le système de représentation basé sur les graphes sémantiques afin de rendre compte de la notion de portée.
							</p>

							<p id="taln-2008-court-005-key" class="mots_cles">
							<b>Mots clés : </b> Modèle sémantique, analyse syntaxique en dépendance, DRT
							</p>

					</div>
					

					<div class="article">

						<b>Karën Fort, Bruno Guillaume</b>


						<br/>

							<i>Sylva : plate-forme de validation multi-niveaux de lexiques</i> <br/>

						<a href="actes/taln-2008-court-006.pdf">taln-2008-court-006</a> 
						<a href="bibtex/taln-2008-court-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-006-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-006-key');">mots clés</a> <br/>

							<p id="taln-2008-court-006-abs" class="resume">
							<b>Résumé : </b> La production de lexiques est une activité indispensable mais complexe, qui nécessite, quelle que soit la méthode de création utilisée (acquisition automatique ou manuelle), une validation humaine. Nous proposons dans ce but une plate-forme Web librement disponible, appelée Sylva (Systematic lexicon validator). Cette plate-forme a pour caractéristiques principales de permettre une validation multi-niveaux (par des validateurs, puis un expert) et une traçabilité de la ressource. La tâche de l’expert(e) linguiste en est allégée puisqu’il ne lui reste à considérer que les données sur lesquelles il n’y a pas d’accord inter-validateurs.
							</p>

							<p id="taln-2008-court-006-key" class="mots_cles">
							<b>Mots clés : </b> Lexiques, plate-forme de validation, cadres de sous-catégorisation
							</p>

					</div>
					

					<div class="article">

						<b>Rémy Kessler, Juan-Manuel Torres-Moreno, Marc El-Bèze</b>


						<br/>

							<i>E-Gen : Profilage automatique de candidatures</i> <br/>

						<a href="actes/taln-2008-court-007.pdf">taln-2008-court-007</a> 
						<a href="bibtex/taln-2008-court-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-007-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-007-key');">mots clés</a> <br/>

							<p id="taln-2008-court-007-abs" class="resume">
							<b>Résumé : </b> La croissance exponentielle de l’Internet a permis le développement de sites d’offres d’emploi en ligne. Le système E-Gen (Traitement automatique d’offres d’emploi) a pour but de permettre l’analyse et la catégorisation d’offres d’emploi ainsi qu’une analyse et classification des réponses des candidats (Lettre de motivation et CV). Nous présentons les travaux réalisés afin de résoudre la seconde partie : on utilise une représentation vectorielle de texte pour effectuer une classification des pièces jointes contenus dans le mail à l’aide de SVM. Par la suite, une évaluation de la candidature est effectuée à l’aide de différents classifieurs (SVM et n-grammes de mots).
							</p>

							<p id="taln-2008-court-007-key" class="mots_cles">
							<b>Mots clés : </b> Classification de textes, Modèle probabiliste, Ressources humaines, Offres d’emploi
							</p>

					</div>
					

					<div class="article">

						<b>François Barthélemy</b>


						<br/>

							<i>Typage, produit cartésien et unités d’analyse pour les modèles à états finis</i> <br/>

						<a href="actes/taln-2008-court-008.pdf">taln-2008-court-008</a> 
						<a href="bibtex/taln-2008-court-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-008-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-008-key');">mots clés</a> <br/>

							<p id="taln-2008-court-008-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons un nouveau langage permettant d’écrire des relations rationnelles compilées en automates finis. Les deux caractéristiques innovantes de ce langage sont de pourvoir décrire des relations à plusieurs niveaux, pas nécessairement deux et d’utiliser diverses unités d’analyse pour exprimer les liens entre niveaux. Cela permet d’aligner de façon fine des représentations multiples.
							</p>

							<p id="taln-2008-court-008-key" class="mots_cles">
							<b>Mots clés : </b> Machine finie à états, morphologie à deux niveau
							</p>

					</div>
					

					<div class="article">

						<b>Frédéric Landragin</b>


						<br/>

							<i>Vers l’évaluation de systèmes de dialogue homme-machine : de l’oral au multimodal</i> <br/>

						<a href="actes/taln-2008-court-009.pdf">taln-2008-court-009</a> 
						<a href="bibtex/taln-2008-court-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-009-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-009-key');">mots clés</a> <br/>

							<p id="taln-2008-court-009-abs" class="resume">
							<b>Résumé : </b> L’évaluation pour le dialogue homme-machine ne se caractérise pas par l’efficacité, l’objectivité et le consensus que l’on observe dans d’autres domaines du traitement automatique des langues. Les systèmes de dialogue oraux et multimodaux restent cantonnés à des domaines applicatifs restreints, ce qui rend difficiles les évaluations comparatives ou normées. De plus, les avancées technologiques constantes rendent vite obsolètes les paradigmes d’évaluation et ont pour conséquence une multiplication de ceux-ci. Des solutions restent ainsi à trouver pour améliorer les méthodes existantes et permettre des diagnostics plus automatisés des systèmes. Cet article se veut un ensemble de réflexions autour de l’évaluation de la multimodalité dans les systèmes à forte composante linguistique. Des extensions des paradigmes existants sont proposées, en particulier DQR/DCR, sachant que certains sont mieux adaptés que d’autres au dialogue multimodal. Des conclusions et perspectives sont tirées sur l’avenir de l’évaluation pour le dialogue homme-machine.
							</p>

							<p id="taln-2008-court-009-key" class="mots_cles">
							<b>Mots clés : </b> Dialogue finalisé, multimodalité, évaluation pour le dialogue hommemachine, paradigme d’évaluation, test utilisateur, diagnostic, paraphrase multimodale
							</p>

					</div>
					

					<div class="article">

						<b>Nuria Gala, Véronique Rey</b>


						<br/>

							<i>POLYMOTS : une base de données de constructions dérivationnelles en français à partir de radicaux phonologiques</i> <br/>

						<a href="actes/taln-2008-court-010.pdf">taln-2008-court-010</a> 
						<a href="bibtex/taln-2008-court-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-010-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-010-key');">mots clés</a> <br/>

							<p id="taln-2008-court-010-abs" class="resume">
							<b>Résumé : </b> Cet article présente POLYMOTS, une base de données lexicale contenant huit mille mots communs en français. L’originalité de l’approche proposée tient à l&#39;analyse des mots. En effet, à la différence d’autres bases lexicales représentant la morphologie dérivationnelle des mots à partir d’affixes, ici l’idée a été d’isoler un radical commun à un ensemble de mots d’une même famille. Nous avons donc analysé les formes des mots et, par comparaison phonologique (forme phonique comparable) et morphologique (continuité de sens), nous avons regroupé les mots par familles, selon le type de radical phonologique. L’article présente les fonctionnalités de la base et inclut une discussion sur les applications et les perspectives d’une telle ressource.
							</p>

							<p id="taln-2008-court-010-key" class="mots_cles">
							<b>Mots clés : </b> ressource lexicale, morphologie dérivationnelle, traitement automatique des familles de mots
							</p>

					</div>
					

					<div class="article">

						<b>Bruno Cartoni</b>


						<br/>

							<i>Mesure de l’alternance entre préfixes pour la génération en traduction automatique</i> <br/>

						<a href="actes/taln-2008-court-011.pdf">taln-2008-court-011</a> 
						<a href="bibtex/taln-2008-court-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-011-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-011-key');">mots clés</a> <br/>

							<p id="taln-2008-court-011-abs" class="resume">
							<b>Résumé : </b> La génération de néologismes construits pose des problèmes dans un système de traduction automatique, notamment au moment de la sélection du préfixe dans les formations préfixées, quand certains préfixes paraissent pouvoir alterner. Nous proposons une étude « extensive », qui vise à rechercher dans de larges ressources textuelles (l’Internet) des formes préfixées générées automatiquement, dans le but d’individualiser les paramètres qui favorisent l’un des préfixes ou qui, au contraire, permettent cette alternance. La volatilité de cette ressource textuelle nécessite certaines précautions dans la méthodologie de décompte des données extraites.
							</p>

							<p id="taln-2008-court-011-key" class="mots_cles">
							<b>Mots clés : </b> morphologie, traduction automatique, génération, néologisme, études empiriques
							</p>

					</div>
					

					<div class="article">

						<b>Abdenour Mokrane, Nathalie Friburger, Jean-Yves Antoine</b>


						<br/>

							<i>Cascades de transducteurs pour le chunking de la parole conversationnelle : l’utilisation de la plateforme CasSys dans le projet EPAC</i> <br/>

						<a href="actes/taln-2008-court-012.pdf">taln-2008-court-012</a> 
						<a href="bibtex/taln-2008-court-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-012-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-012-key');">mots clés</a> <br/>

							<p id="taln-2008-court-012-abs" class="resume">
							<b>Résumé : </b> Cet article présente l’utilisation de la plate-forme CasSys pour la segmentation de la parole conversationnelle (chunking) à l’aide de cascades de transducteurs Unitex. Le système que nous présentons est utilisé dans le cadre du projet ANR EPAC. Ce projet a pour objectif l’indexation et l’annotation automatique de grands flux de parole issus d’émissions télévisées ou radiophoniques. Cet article présente tout d’abord l’adaptation à ce type de données d’un système antérieur de chunking (Romus) qui avait été développé pour le dialogue oral homme-machine. Il décrit ensuite les principaux problèmes qui se posent à l’analyse : traitement des disfluences de l’oral spontané, mais également gestion des erreurs dues aux étapes antérieures de reconnaissance de la parole et d’étiquetage morphosyntaxique.
							</p>

							<p id="taln-2008-court-012-key" class="mots_cles">
							<b>Mots clés : </b> Traitement Automatique du Langage Parlé (TALP), segmentation, chunks, parole conversationnelle, transducteurs, Unitex
							</p>

					</div>
					

					<div class="article">

						<b>Aurélien Bossard, Thierry Poibeau</b>


						<br/>

							<i>Regroupement automatique de documents en classes événementielles</i> <br/>

						<a href="actes/taln-2008-court-013.pdf">taln-2008-court-013</a> 
						<a href="bibtex/taln-2008-court-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-013-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-013-key');">mots clés</a> <br/>

							<p id="taln-2008-court-013-abs" class="resume">
							<b>Résumé : </b> Cet article porte sur le regroupement automatique de documents sur une base événementielle. Après avoir précisé la notion d’événement, nous nous intéressons à la représentation des documents d’un corpus de dépêches, puis à une approche d’apprentissage pour réaliser les regroupements de manière non supervisée fondée sur k-means. Enfin, nous évaluons le système de regroupement de documents sur un corpus de taille réduite et nous discutons de l’évaluation quantitative de ce type de tâche.
							</p>

							<p id="taln-2008-court-013-key" class="mots_cles">
							<b>Mots clés : </b> Regroupement de documents, Suivi d’événement
							</p>

					</div>
					

					<div class="article">

						<b>Mary Hearne, Sylwia Ozdowska, John Tinsley</b>


						<br/>

							<i>Comparing Constituency and Dependency Representations for SMT Phrase-Extraction</i> <br/>

						<a href="actes/taln-2008-court-014.pdf">taln-2008-court-014</a> 
						<a href="bibtex/taln-2008-court-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-014-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-014-key');">mots clés</a> <br/>

							<p id="taln-2008-court-014-abs" class="resume">
							<b>Résumé : </b> Nous évaluons le recours à des techniques de traduction à base de segments syntaxiquement motivés, seules ou en combinaison avec des techniques à base de segments non motivés, et nous comparons les apports respectifs de l’analyse en constituants et de l’analyse en dépendances dans ce cadre. À partir d’un corpus parallèle Anglais–Français, nous construisons automatiquement deux corpus d’entraînement arborés, en constituants et en dépendances, alignés au niveau sous-phrastique et en extrayons des correspondances bilingues entre mots et syntagmes motivées syntaxiquement. Nous mesurons automatiquement la qualité de la traduction obtenue par un système à base de segments. Les résultats montrent que la combinaison des correspondances bilingues non motivées et motivées sur le plan syntaxique améliore la qualité de la traduction quel que soit le type d’analyse considéré. Par ailleurs, le gain en qualité est plus important avec le recours à l’analyse en dépendances au regard des constituants.
							</p>

							<p id="taln-2008-court-014-key" class="mots_cles">
							<b>Mots clés : </b> Traduction statistique à base de segments, annotation en constituants, annotation en dépendances, corpus parallèles arborés alignés au niveau sousphrastique
							</p>

					</div>
					

					<div class="article">

						<b>Fabien Poulard, Thierry Waszak, Nicolas Hernandez, Patrice Bellot</b>


						<br/>

							<i>Repérage de citations, classification des styles de discours rapporté et identification des constituants citationnels en écrits journalistiques</i> <br/>

						<a href="actes/taln-2008-court-015.pdf">taln-2008-court-015</a> 
						<a href="bibtex/taln-2008-court-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-015-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-015-key');">mots clés</a> <br/>

							<p id="taln-2008-court-015-abs" class="resume">
							<b>Résumé : </b> Dans le contexte de la recherche de plagiat, le repérage de citations et de ses constituants est primordial puisqu’il peut amener à évaluer le caractère licite ou illicite d’une reprise (source citée ou non). Nous proposons ici une comparaison de méthodes automatiques pour le repérage de ces informations et rapportons une évaluation quantitative de celles-ci. Un corpus d’écrits journalistiques français a été manuellement annoté pour nous servir de base d’apprentissage et de test.
							</p>

							<p id="taln-2008-court-015-key" class="mots_cles">
							<b>Mots clés : </b> détection de citations, classification des styles de discours rapporté, identification du locuteur, techniques par apprentissage et base de règles, écrits journalistiques
							</p>

					</div>
					

					<div class="article">

						<b>Frédéric Landragin</b>


						<br/>

							<i>Vers l’identification et le traitement des actes de dialogue composites</i> <br/>

						<a href="actes/taln-2008-court-016.pdf">taln-2008-court-016</a> 
						<a href="bibtex/taln-2008-court-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-016-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-016-key');">mots clés</a> <br/>

							<p id="taln-2008-court-016-abs" class="resume">
							<b>Résumé : </b> Il peut être difficile d’attribuer une seule valeur illocutoire à un énoncé dans un dialogue. En premier lieu, un énoncé peut comporter plusieurs segments de discours ayant chacun leur valeur illocutoire spécifique. De plus, un seul segment peut s’analyser en tant qu’acte de langage composite, regroupant par exemple la formulation d’une question et l’émission simultanée d’une information. Enfin, la structure du dialogue en termes d’échanges et de séquences peut être déterminante dans l’identification de l’acte, et peut également apporter une valeur illocutoire supplémentaire, comme celle de clore la séquence en cours. Dans le but de déterminer la réaction face à un tel acte de dialogue composite, nous présentons une approche théorique pour l’analyse des actes de dialogue en fonction du contexte de tâche et des connaissances des interlocuteurs. Nous illustrons sur un exemple nos choix de segmentation et d’identification des actes composites, et nous présentons les grandes lignes d’une stratégie pour déterminer la réaction qui semble être la plus pertinente.
							</p>

							<p id="taln-2008-court-016-key" class="mots_cles">
							<b>Mots clés : </b> Actes de langage complexes, structure du dialogue, terrain commun
							</p>

					</div>
					

					<div class="article">

						<b>Manal El Zant, Jean Royauté, Michel Roux</b>


						<br/>

							<i>Représentation évènementielle des déplacements dans des dépêches épidémiologiques</i> <br/>

						<a href="actes/taln-2008-court-017.pdf">taln-2008-court-017</a> 
						<a href="bibtex/taln-2008-court-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-017-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-017-key');">mots clés</a> <br/>

							<p id="taln-2008-court-017-abs" class="resume">
							<b>Résumé : </b> La représentation évènementielle des déplacements de personnes dans des dépêches épidémiologiques est d’une grande importance pour une compréhension détaillée du sens de ces dépêches. La dissémination des composants d’une telle représentation dans les dépêches rend difficile l’accès à leurs contenus. Ce papier décrit un système d’extraction d’information utilisant des cascades de transducteurs à nombre d’états fini qui ont permis la réalisation de trois tâches : la reconnaissance des entités nommées, l’annotation et la représentation des composants ainsi que la représentation des structures évènementielles. Nous avons obtenu une moyenne de rappel de 80, 93% pour la reconnaissance des entités nommées et de 97, 88% pour la représentation des composants. Ensuite, nous avons effectué un travail de normalisation de cette représentation par la résolution de certaines anaphores pronominales. Nous avons obtenu une valeur moyenne de précision de 81, 72% pour cette résolution.
							</p>

							<p id="taln-2008-court-017-key" class="mots_cles">
							<b>Mots clés : </b> Sous-langage, représentation évènementielle, extraction d’information, structure prédicative, structure predicate-arguments
							</p>

					</div>
					

					<div class="article">

						<b>Éric Wehrli, Luka Nerima</b>


						<br/>

							<i>Traduction multilingue : le projet MulTra</i> <br/>

						<a href="actes/taln-2008-court-018.pdf">taln-2008-court-018</a> 
						<a href="bibtex/taln-2008-court-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-018-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-018-key');">mots clés</a> <br/>

							<p id="taln-2008-court-018-abs" class="resume">
							<b>Résumé : </b> L’augmentation rapide des échanges et des communications pluriculturels, en particulier sur internet, intensifie les besoins d’outils multilingues y compris de traduction. Cet article décrit un projet en cours au LATL pour le développement d’un système de traduction multilingue basé sur un modèle linguistique abstrait et largement générique, ainsi que sur un modèle logiciel basé sur la notion d’objet. Les langues envisagées dans la première phase de ce projet sont l’allemand, le français, l’italien, l’espagnol et l’anglais.
							</p>

							<p id="taln-2008-court-018-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique multilingue, approche par objets, génération de lexiques bilingues
							</p>

					</div>
					

					<div class="article">

						<b>Erwan Moreau, François Yvon, Olivier Cappé</b>


						<br/>

							<i>Appariement d’entités nommées coréférentes : combinaisons de mesures de similarité par apprentissage supervisé</i> <br/>

						<a href="actes/taln-2008-court-019.pdf">taln-2008-court-019</a> 
						<a href="bibtex/taln-2008-court-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-019-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-019-key');">mots clés</a> <br/>

							<p id="taln-2008-court-019-abs" class="resume">
							<b>Résumé : </b> L’appariement d’entités nommées consiste à regrouper les différentes formes sous lesquelles apparaît une entité. Pour cela, des mesures de similarité textuelle sont généralement utilisées. Nous proposons de combiner plusieurs mesures afin d’améliorer les performances de la tâche d’appariement. À l’aide d’expériences menées sur deux corpus, nous montrons la pertinence de l’apprentissage supervisé dans ce but, particulièrement avec l’algorithme C4.5.
							</p>

							<p id="taln-2008-court-019-key" class="mots_cles">
							<b>Mots clés : </b> Entités nommées, Appariement, Mesures de similarité textuelle, Apprentissage supervisé
							</p>

					</div>
					

					<div class="article">

						<b>Renaud Marlet</b>


						<br/>

							<i>Un sens logique pour les graphes sémantiques</i> <br/>

						<a href="actes/taln-2008-court-020.pdf">taln-2008-court-020</a> 
						<a href="bibtex/taln-2008-court-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-020-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-020-key');">mots clés</a> <br/>

							<p id="taln-2008-court-020-abs" class="resume">
							<b>Résumé : </b> Nous discutons du sens des graphes sémantiques, notamment de ceux utilisés en Théorie Sens-Texte. Nous leur donnons un sens précis, éventuellement sous-spécifié, grâce à une traduction simple vers une formule de Minimal Recursion Semantics qui couvre les cas de prédications multiples sur plusieurs entités, de prédication d’ordre supérieur et de modalités.
							</p>

							<p id="taln-2008-court-020-key" class="mots_cles">
							<b>Mots clés : </b> Graphe sémantique, logique, quantification, Théorie Sens-Texte (TST)
							</p>

					</div>
					

					<div class="article">

						<b>Marie-Jean Meurs, Frédéric Duvert, Frédéric Béchet, Fabrice Lefèvre, Renato De Mori</b>


						<br/>

							<i>Annotation en Frames Sémantiques du corpus de dialogue MEDIA</i> <br/>

						<a href="actes/taln-2008-court-021.pdf">taln-2008-court-021</a> 
						<a href="bibtex/taln-2008-court-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-021-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-021-key');">mots clés</a> <br/>

							<p id="taln-2008-court-021-abs" class="resume">
							<b>Résumé : </b> Cet article présente un formalisme de représentation des connaissances qui a été utilisé pour fournir des annotations sémantiques de haut niveau pour le corpus de dialogue oral MEDIA. Ces annotations en structures sémantiques, basées sur le paradigme FrameNet, sont obtenues de manière incrémentale et partiellement automatisée. Nous décrivons le processus d’interprétation automatique qui permet d’obtenir des compositions sémantiques et de générer des hypothèses de frames par inférence. Le corpus MEDIA est un corpus de dialogues en langue française dont les tours de parole de l’utilisateur ont été manuellement transcrits et annotés (niveaux mots et constituants sémantiques de base). Le processus proposé utilise ces niveaux pour produire une annotation de haut niveau en frames sémantiques. La base de connaissances développée (définitions des frames et règles de composition) est présentée, ainsi que les résultats de l’annotation automatique.
							</p>

							<p id="taln-2008-court-021-key" class="mots_cles">
							<b>Mots clés : </b> compréhension automatique de la parole, système de dialogue oral, frames sémantiques, décodage conceptuel, annotation sémantique, inférence sémantique
							</p>

					</div>
					

					<div class="article">

						<b>Ramzi Abbès, Malek Boualem</b>


						<br/>

							<i>Dissymétrie entre l&#39;indexation des documents et le traitement des requêtes pour la recherche d’information en langue arabe</i> <br/>

						<a href="actes/taln-2008-court-022.pdf">taln-2008-court-022</a> 
						<a href="bibtex/taln-2008-court-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2008-court-022-abs');">résumé</a>
							<a onclick="toggle('taln-2008-court-022-key');">mots clés</a> <br/>

							<p id="taln-2008-court-022-abs" class="resume">
							<b>Résumé : </b> Les moteurs de recherches sur le web produisent des résultats comparables et assez satisfaisants pour la recherche de documents écrits en caractères latins. Cependant, ils présentent de sérieuses lacunes dès que l&#39;ont s&#39;intéresse à des langues peu dotées ou des langues sémitiques comme l&#39;arabe. Dans cet article nous présentons une étude analytique et qualitative de la recherche d’information en langue arabe en mettant l&#39;accent sur l&#39;insuffisance des outils de recherche actuels, souvent mal adaptés aux spécificités de la langue arabe. Pour argumenter notre analyse, nous présentons des résultats issus d’observations et de tests autour de certains phénomènes linguistiques de l’arabe écrit. Pour la validation des ces observations, nous avons testé essentiellement le moteur de recherche Google.
							</p>

							<p id="taln-2008-court-022-key" class="mots_cles">
							<b>Mots clés : </b> recherche d’information, langue arabe, indexation, lemmatisation, Google
							</p>

					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>