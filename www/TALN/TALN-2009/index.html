<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>TALN'2009</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>TALN'2009, 16ème conférence sur le Traitement Automatique des Langues Naturelles</h1>
				<h2>Senlis (France), du 2009-06-24 au 2009-06-26</h2>
				<p>Président(s) : Adeline Nazarenko, Thierry Poibeau</p>
				<p>Taux d'acceptation :
							papiers longs (26.9%)
							prise de position (50.0%)
							papiers courts (42.6%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#long">Papiers longs</a></li>
				<li><a href="#position">Prise de position</a></li>
				<li><a href="#court">Papiers courts</a></li>
				<li><a href="#démonstration">Démonstrations</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="long">Papiers longs</h1>
			

					<div class="article">

						<b>Nabil Hathout</b>


						<br/>

							<i>Acquisition morphologique à partir d’un dictionnaire informatisé</i> <br/>

						<a href="actes/taln-2009-long-001.pdf">taln-2009-long-001</a> 
						<a href="bibtex/taln-2009-long-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-001-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-001-key');">mots clés</a> <br/>

							<p id="taln-2009-long-001-abs" class="resume">
							<b>Résumé : </b> L’article propose un modèle linguistique et informatique permettant de faire émerger la structure morphologique dérivationnelle du lexique à partir des régularités sémantiques et formelles des mots qu’il contient. Ce modèle est radicalement lexématique. La structure morphologique est constituée par les relations que chaque mot entretient avec les autres unités du lexique et notamment avec les mots de sa famille morphologique et de sa série dérivationnelle. Ces relations forment des paradigmes analogiques. La modélisation a été testée sur le lexique du français en utilisant le dictionnaire informatisé TLFi.
							</p>

							<p id="taln-2009-long-001-key" class="mots_cles">
							<b>Mots clés : </b> Morphologie dérivationnelle, morphologie lexématique, similarité morphologique, analogie formelle
							</p>

					</div>
					

					<div class="article">

						<b>Joseph Le Roux</b>


						<br/>

							<i>Analyse déductive pour les grammaires d’interaction</i> <br/>

						<a href="actes/taln-2009-long-002.pdf">taln-2009-long-002</a> 
						<a href="bibtex/taln-2009-long-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-002-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-002-key');">mots clés</a> <br/>

							<p id="taln-2009-long-002-abs" class="resume">
							<b>Résumé : </b> Nous proposons un algorithme d’analyse pour les grammaires d’interaction qui utilise le cadre formel de l’analyse déductive. Cette approche donne un point de vue nouveau sur ce problème puisque les méthodes précédentes réduisaient ce dernier à la réécriture de graphes et utilisaient des techniques de résolution de contraintes. D’autre part, cette présentation permet de décrire le processus de manière standard et d’exhiber les sources d’indéterminisme qui rendent ce problème difficile.
							</p>

							<p id="taln-2009-long-002-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique, grammaires d’interaction
							</p>

					</div>
					

					<div class="article">

						<b>Alexis Nasr, Frédéric Béchet</b>


						<br/>

							<i>Analyse syntaxique en dépendances de l’oral spontané</i> <br/>

						<a href="actes/taln-2009-long-003.pdf">taln-2009-long-003</a> 
						<a href="bibtex/taln-2009-long-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-003-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-003-key');">mots clés</a> <br/>

							<p id="taln-2009-long-003-abs" class="resume">
							<b>Résumé : </b> Cet article décrit un modèle d’analyse syntaxique de l’oral spontané axé sur la reconnaissance de cadres valenciels verbaux. Le modèle d’analyse se décompose en deux étapes : une étape générique, basée sur des ressources génériques du français et une étape de réordonnancement des solutions de l’analyseur réalisé par un modèle spécifique à une application. Le modèle est évalué sur le corpus MEDIA.
							</p>

							<p id="taln-2009-long-003-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique, reconnaissance automatique de la parole
							</p>

					</div>
					

					<div class="article">

						<b>Marie Candito, Benoît Crabbé, Pascal Denis, François Guérin</b>


						<br/>

							<i>Analyse syntaxique du français : des constituants aux dépendances</i> <br/>

						<a href="actes/taln-2009-long-004.pdf">taln-2009-long-004</a> 
						<a href="bibtex/taln-2009-long-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-004-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-004-key');">mots clés</a> <br/>

							<p id="taln-2009-long-004-abs" class="resume">
							<b>Résumé : </b> Cet article présente une technique d’analyse syntaxique statistique à la fois en constituants et en dépendances. L’analyse procède en ajoutant des étiquettes fonctionnelles aux sorties d’un analyseur en constituants, entraîné sur le French Treebank, pour permettre l’extraction de dépendances typées. D’une part, nous spécifions d’un point de vue formel et linguistique les structures de dépendances à produire, ainsi que la procédure de conversion du corpus en constituants (le French Treebank) vers un corpus cible annoté en dépendances, et partiellement validé. D’autre part, nous décrivons l’approche algorithmique qui permet de réaliser automatiquement le typage des dépendances. En particulier, nous nous focalisons sur les méthodes d’apprentissage discriminantes d’étiquetage en fonctions grammaticales.
							</p>

							<p id="taln-2009-long-004-key" class="mots_cles">
							<b>Mots clés : </b> Analyseur syntaxique statistique, analyse en constituants/dépendances, étiquetage en fonctions grammaticales
							</p>

					</div>
					

					<div class="article">

						<b>Erwan Moreau, Isabelle Tellier, Antonio Balvet, Grégoire Laurence, Antoine Rozenknop, Thierry Poibeau</b>


						<br/>

							<i>Annotation fonctionnelle de corpus arborés avec des Champs Aléatoires Conditionnels</i> <br/>

						<a href="actes/taln-2009-long-005.pdf">taln-2009-long-005</a> 
						<a href="bibtex/taln-2009-long-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-005-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-005-key');">mots clés</a> <br/>

							<p id="taln-2009-long-005-abs" class="resume">
							<b>Résumé : </b> L’objectif de cet article est d’évaluer dans quelle mesure les “fonctions syntaxiques” qui figurent dans une partie du corpus arboré de Paris 7 sont apprenables à partir d’exemples. La technique d’apprentissage automatique employée pour cela fait appel aux “Champs Aléatoires Conditionnels” (Conditional Random Fields ou CRF), dans une variante adaptée à l’annotation d’arbres. Les expériences menées sont décrites en détail et analysées. Moyennant un bon paramétrage, elles atteignent une F1-mesure de plus de 80%.
							</p>

							<p id="taln-2009-long-005-key" class="mots_cles">
							<b>Mots clés : </b> fonctions syntaxiques, Conditional Random Fields, corpus arborés
							</p>

					</div>
					

					<div class="article">

						<b>Emmanuel Morin</b>


						<br/>

							<i>Apport d’un corpus comparable déséquilibré à l’extraction de lexiques bilingues</i> <br/>

						<a href="actes/taln-2009-long-006.pdf">taln-2009-long-006</a> 
						<a href="bibtex/taln-2009-long-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-006-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-006-key');">mots clés</a> <br/>

							<p id="taln-2009-long-006-abs" class="resume">
							<b>Résumé : </b> Les principaux travaux en extraction de lexiques bilingues à partir de corpus comparables reposent sur l’hypothèse implicite que ces corpus sont équilibrés. Cependant, les différentes méthodes computationnelles associées sont relativement insensibles à la taille de chaque partie du corpus. Dans ce contexte, nous étudions l’influence que peut avoir un corpus comparable déséquilibré sur la qualité des terminologies bilingues extraites à travers différentes expériences. Nos résultats montrent que sous certaines conditions l’utilisation d’un corpus comparable déséquilibré peut engendrer un gain significatif dans la qualité des lexiques extraits.
							</p>

							<p id="taln-2009-long-006-key" class="mots_cles">
							<b>Mots clés : </b> Multilinguisme, corpus comparable, extraction de lexiques bilingues
							</p>

					</div>
					

					<div class="article">

						<b>Eric Charton, Juan-Manuel Torres-Moreno</b>


						<br/>

							<i>Classification d’un contenu encyclopédique en vue d’un étiquetage par entités nommées</i> <br/>

						<a href="actes/taln-2009-long-007.pdf">taln-2009-long-007</a> 
						<a href="bibtex/taln-2009-long-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-007-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-007-key');">mots clés</a> <br/>

							<p id="taln-2009-long-007-abs" class="resume">
							<b>Résumé : </b> On utilise souvent des ressources lexicales externes pour améliorer les performances des systèmes d’étiquetage d’entités nommées. Les contenus de ces ressources lexicales peuvent être variés : liste de noms propres, de lieux, de marques. On note cependant que la disponibilité de corpus encyclopédiques exhaustifs et ouverts de grande taille tels que Worldnet ou Wikipedia, a fait émerger de nombreuses propositions spécifiques d’exploitation de ces contenus par des systèmes d’étiquetage. Un problème demeure néanmoins ouvert avec ces ressources : celui de l’adaptation de leur taxonomie interne, complexe et composée de dizaines de milliers catégories, aux exigences particulières de l’étiquetage des entités nommées. Pour ces dernières, au plus de quelques centaines de classes sémantiques sont requises. Dans cet article nous explorons cette difficulté et proposons un système complet de transformation d’un arbre taxonomique encyclopédique en une système à classe sémantiques adapté à l’étiquetage d’entités nommées.
							</p>

							<p id="taln-2009-long-007-key" class="mots_cles">
							<b>Mots clés : </b> Etiquetage, Entités nommées, classification, taxonomie
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Langlais</b>


						<br/>

							<i>Étude quantitative de liens entre l’analogie formelle et la morphologie constructionnelle</i> <br/>

						<a href="actes/taln-2009-long-008.pdf">taln-2009-long-008</a> 
						<a href="bibtex/taln-2009-long-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-008-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-008-key');">mots clés</a> <br/>

							<p id="taln-2009-long-008-abs" class="resume">
							<b>Résumé : </b> Plusieurs travaux ont récemment étudié l’apport de l’apprentissage analogique dans des applications du traitement automatique des langues comme la traduction automatique, ou la recherche d’information. Il est souvent admis que les relations analogiques de forme entre les mots capturent des informations de nature morphologique. Le but de cette étude est de présenter une analyse des points de rencontre entre l’analyse morphologique et les analogies de forme. C’est à notre connaissance la première étude de ce type portant sur des corpus de grande taille et sur plusieurs langues. Bien que notre étude ne soit pas dédiée à une tâche particulière du traitement des langues, nous montrons cependant que le principe d’analogie permet de segmenter des mots en morphèmes avec une bonne précision.
							</p>

							<p id="taln-2009-long-008-key" class="mots_cles">
							<b>Mots clés : </b> Apprentissage analogique, analogie formelle, analyse morphologique
							</p>

					</div>
					

					<div class="article">

						<b>Thi-Ngoc-Diep Do, Viet-Bac Le, Brigitte Bigi, Laurent Besacier, Eric Castelli</b>


						<br/>

							<i>Exploitation d’un corpus bilingue pour la création d’un système de traduction probabiliste Vietnamien - Français</i> <br/>

						<a href="actes/taln-2009-long-009.pdf">taln-2009-long-009</a> 
						<a href="bibtex/taln-2009-long-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-009-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-009-key');">mots clés</a> <br/>

							<p id="taln-2009-long-009-abs" class="resume">
							<b>Résumé : </b> Cet article présente nos premiers travaux en vue de la construction d’un système de traduction probabiliste pour le couple de langue vietnamien-français. La langue vietnamienne étant considérée comme une langue peu dotée, une des difficultés réside dans la constitution des corpus parallèles, indispensable à l’apprentissage des modèles. Nous nous concentrons sur la constitution d’un grand corpus parallèle vietnamien-français. La méthode d’identification automatique des paires de documents parallèles fondée sur la date de publication, les mots spéciaux et les scores d’alignements des phrases est appliquée. Cet article présente également la construction d’un premier système de traduction automatique probabiliste vietnamienfrançais et français-vietnamien à partir de ce corpus et discute l’opportunité d’utiliser des unités lexicales ou sous-lexicales pour le vietnamien (syllabes, mots, ou leurs combinaisons). Les performances du système sont encourageantes et se comparent avantageusement à celles du système de Google.
							</p>

							<p id="taln-2009-long-009-key" class="mots_cles">
							<b>Mots clés : </b> traduction probabiliste, corpus bilingue, alignement de documents, table de traduction
							</p>

					</div>
					

					<div class="article">

						<b>Emmanuel Prochasson, Emmanuel Morin</b>


						<br/>

							<i>Influence des points d’ancrage pour l’extraction lexicale bilingue à partir de corpus comparables spécialisés</i> <br/>

						<a href="actes/taln-2009-long-010.pdf">taln-2009-long-010</a> 
						<a href="bibtex/taln-2009-long-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-010-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-010-key');">mots clés</a> <br/>

							<p id="taln-2009-long-010-abs" class="resume">
							<b>Résumé : </b> L’extraction de lexiques bilingues à partir de corpus comparables affiche de bonnes performances pour des corpus volumineux mais chute fortement pour des corpus d’une taille plus modeste. Pour pallier cette faiblesse, nous proposons une nouvelle contribution au processus d’alignement lexical à partir de corpus comparables spécialisés qui vise à renforcer la significativité des contextes lexicaux en s’appuyant sur le vocabulaire spécialisé du domaine étudié. Les expériences que nous avons réalisées en ce sens montrent qu’une meilleure prise en compte du vocabulaire spécialisé permet d’améliorer la qualité des lexiques extraits.
							</p>

							<p id="taln-2009-long-010-key" class="mots_cles">
							<b>Mots clés : </b> Corpus comparable, extraction de lexiques bilingues, points d’ancrage
							</p>

					</div>
					

					<div class="article">

						<b>Stéphane Huet, Julien Bourdaillet, Philippe Langlais</b>


						<br/>

							<i>Intégration de l’alignement de mots dans le concordancier bilingue TransSearch</i> <br/>

						<a href="actes/taln-2009-long-011.pdf">taln-2009-long-011</a> 
						<a href="bibtex/taln-2009-long-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-011-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-011-key');">mots clés</a> <br/>

							<p id="taln-2009-long-011-abs" class="resume">
							<b>Résumé : </b> Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Dans cet article, nous présentons nos travaux menés dans le but d’améliorer le concordancier bilingue TransSearch. Ce service, accessible sur le Web, repose principalement sur un alignement au niveau des phrases. Dans cette étude, nous discutons et évaluons l’intégration d’un alignement statistique au niveau des mots. Nous présentons deux nouvelles problématiques essentielles au succès de notre nouveau prototype : la détection des traductions erronées et le regroupement des variantes de traduction similaires.
							</p>

							<p id="taln-2009-long-011-key" class="mots_cles">
							<b>Mots clés : </b> alignement au niveau des mots, concordancier bilingue, traduction automatique
							</p>

					</div>
					

					<div class="article">

						<b>Agata Jackiewicz, Thierry Charnois, Stéphane Ferrari</b>


						<br/>

							<i>Jugements d&#39;évaluation et constituants périphériques</i> <br/>

						<a href="actes/taln-2009-long-012.pdf">taln-2009-long-012</a> 
						<a href="bibtex/taln-2009-long-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-012-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-012-key');">mots clés</a> <br/>

							<p id="taln-2009-long-012-abs" class="resume">
							<b>Résumé : </b> L’article présente une étude portant sur des constituants détachés à valeur axiologique. Dans un premier temps, une analyse linguistique sur corpus met en évidence un ensemble de patrons caractéristiques du phénomène. Ensuite, une expérimentation informatique est proposée sur un corpus de plus grande taille afin de permettre l’observation des patrons en vue d’un retour sur le modèle linguistique. Ce travail s’inscrit dans un projet mené à l’interface de la linguistique et du TAL, qui se donne pour but d’enrichir, d’adapter au français et de formaliser le modèle général Appraisal de l’évaluation dans la langue.
							</p>

							<p id="taln-2009-long-012-key" class="mots_cles">
							<b>Mots clés : </b> jugement d’évaluation, constituants extra-prédicatifs, constructions et lexiques subjectifs, Appraisal, implémentation informatique, portraits et biographies dans la presse de spécialité et la presse d’information
							</p>

					</div>
					

					<div class="article">

						<b>François Portet, Albert Gatt, Jim Hunter, Ehud Reiter, Somayajulu Sripada</b>


						<br/>

							<i>Le projet BabyTalk : génération de texte à partir de données hétérogènes pour la prise de décision en unité néonatale</i> <br/>

						<a href="actes/taln-2009-long-013.pdf">taln-2009-long-013</a> 
						<a href="bibtex/taln-2009-long-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-013-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-013-key');">mots clés</a> <br/>

							<p id="taln-2009-long-013-abs" class="resume">
							<b>Résumé : </b> Notre société génère une masse d’information toujours croissante, que ce soit en médecine, en météorologie, etc. La méthode la plus employée pour analyser ces données est de les résumer sous forme graphique. Cependant, il a été démontré qu&#39;un résumé textuel est aussi un mode de présentation efficace. L&#39;objectif du prototype BT-45, développé dans le cadre du projet Babytalk, est de générer des résumés de 45 minutes de signaux physiologiques continus et d&#39;événements temporels discrets en unité néonatale de soins intensifs (NICU). L&#39;article présente l&#39;aspect génération de texte de ce prototype. Une expérimentation clinique a montré que les résumés humains améliorent la prise de décision par rapport à l&#39;approche graphique, tandis que les textes de BT-45 donnent des résultats similaires à l’approche graphique. Une analyse a identifié certaines des limitations de BT-45 mais en dépit de cellesci, notre travail montre qu&#39;il est possible de produire automatiquement des résumés textuels efficaces de données complexes.
							</p>

							<p id="taln-2009-long-013-key" class="mots_cles">
							<b>Mots clés : </b> Traitement automatique des langues naturelles, Génération de texte, Analyse de données, Unité de soins intensifs, Systèmes d&#39;aide à la décision
							</p>

					</div>
					

					<div class="article">

						<b>Bruno Cartoni</b>


						<br/>

							<i>Les adjectifs relationnels dans les lexiques informatisés : formalisation et exploitation dans un contexte multilingue</i> <br/>

						<a href="actes/taln-2009-long-014.pdf">taln-2009-long-014</a> 
						<a href="bibtex/taln-2009-long-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-014-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-014-key');">mots clés</a> <br/>

							<p id="taln-2009-long-014-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous nous intéressons aux adjectifs dits relationnels et à leur statut en traitement automatique des langues naturelles (TALN). Nous montrons qu’ils constituent une « sous-classe » d’adjectifs rarement explicitée et donc rarement représentée dans les lexiques sur lesquels reposent les applications du TALN, alors qu’ils jouent un rôle important dans de nombreuses applications. Leur formation morphologique est source d’importantes divergences entre différentes langues, et c’est pourquoi ces adjectifs sont un véritable défi pour les applications informatiques multilingues. Dans une partie plus pratique, nous proposons une formalisation de ces adjectifs permettant de rendre compte de leurs liens avec leur base nominale. Nous tentons d’extraire ces informations dans les lexiques informatisés existants, puis nous les exploitons pour traduire les adjectifs relationnels préfixés de l’italien en français.
							</p>

							<p id="taln-2009-long-014-key" class="mots_cles">
							<b>Mots clés : </b> Adjectifs relationnels, ressources lexicales, morphologie constructionnelle
							</p>

					</div>
					

					<div class="article">

						<b>Marc Plantevit, Thierry Charnois</b>


						<br/>

							<i>Motifs séquentiels pour l’extraction d’information : illustration sur le problème de la détection d’interactions entre gènes</i> <br/>

						<a href="actes/taln-2009-long-015.pdf">taln-2009-long-015</a> 
						<a href="bibtex/taln-2009-long-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-015-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-015-key');">mots clés</a> <br/>

							<p id="taln-2009-long-015-abs" class="resume">
							<b>Résumé : </b> Face à la prolifération des publications en biologie et médecine (plus de 18 millions de publications actuellement recensées dans PubMed), l’extraction d’information automatique est devenue un enjeu crucial. Il existe de nombreux travaux dans le domaine du traitement de la langue appliquée à la biomédecine (&#34;BioNLP&#34;). Ces travaux se distribuent en deux grandes tendances. La première est fondée sur les méthodes d’apprentissage automatique de type numérique qui donnent de bons résultats mais ont un fonctionnement de type &#34;boite noire&#34;. La deuxième tendance est celle du TALN à base d’analyses (lexicales, syntaxiques, voire sémantiques ou discursives) coûteuses en temps de développement des ressources nécessaires (lexiques, grammaires, etc.). Nous proposons dans cet article une approche basée sur la découverte de motifs séquentiels pour apprendre automatiquement les ressources linguistiques, en l’occurrence les patrons linguistiques qui permettent l’extraction de l’information dans les textes. Plusieurs aspects méritent d’être soulignés : cette approche permet de s’affranchir de l’analyse syntaxique de la phrase, elle ne nécessite pas de ressources en dehors du corpus d’apprentissage et elle ne demande que très peu d’intervention manuelle. Nous illustrons l’approche sur le problème de la détection d’interactions entre gènes et donnons les résultats obtenus sur des corpus biologiques qui montrent l’intérêt de ce type d’approche.
							</p>

							<p id="taln-2009-long-015-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’information, fouille de textes, motifs séquentiels, interactions entre gènes
							</p>

					</div>
					

					<div class="article">

						<b>Aurélien Max, Rafik Maklhoufi, Philippe Langlais</b>


						<br/>

							<i>Prise en compte de dépendances syntaxiques pour la traduction contextuelle de segments</i> <br/>

						<a href="actes/taln-2009-long-016.pdf">taln-2009-long-016</a> 
						<a href="bibtex/taln-2009-long-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-016-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-016-key');">mots clés</a> <br/>

							<p id="taln-2009-long-016-abs" class="resume">
							<b>Résumé : </b> Dans un système standard de traduction statistique basé sur les segments, le score attribué aux différentes traductions d’un segment ne dépend pas du contexte dans lequel il apparaît. Plusieurs travaux récents tendent à montrer l’intérêt de prendre en compte le contexte source lors de la traduction, mais ces études portent sur des systèmes traduisant vers l’anglais, une langue faiblement fléchie. Dans cet article, nous décrivons nos expériences sur la prise en compte du contexte source dans un système statistique traduisant de l’anglais vers le français, basé sur l’approche proposée par Stroppa et al. (2007). Nous étudions l’impact de différents types d’indices capturant l’information contextuelle, dont des dépendances syntaxiques typées. Si les mesures automatiques d’évaluation de la qualité d’une traduction ne révèlent pas de gains significatifs de notre système par rapport à un système à l’état de l’art ne faisant pas usage du contexte, une évaluation manuelle conduite sur 100 phrases choisies aléatoirement est en faveur de notre système. Cette évaluation fait également ressortir que la prise en compte de certaines dépendances syntaxiques est bénéfique à notre système.
							</p>

							<p id="taln-2009-long-016-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique statistique, contexte source, dépendances syntaxiques
							</p>

					</div>
					

					<div class="article">

						<b>Maud Ehrmann, Caroline Hagège</b>


						<br/>

							<i>Proposition de caractérisation et de typage des expressions temporelles en contexte</i> <br/>

						<a href="actes/taln-2009-long-017.pdf">taln-2009-long-017</a> 
						<a href="bibtex/taln-2009-long-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-017-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-017-key');">mots clés</a> <br/>

							<p id="taln-2009-long-017-abs" class="resume">
							<b>Résumé : </b> Nous assistons actuellement en TAL à un regain d’intérêt pour le traitement de la temporalité véhiculée par les textes. Dans cet article, nous présentons une proposition de caractérisation et de typage des expressions temporelles tenant compte des travaux effectués dans ce domaine tout en cherchant à pallier les manques et incomplétudes de certains de ces travaux. Nous explicitons comment nous nous situons par rapport à l’existant et les raisons pour lesquelles parfois nous nous en démarquons. Le typage que nous définissons met en évidence de réelles différences dans l’interprétation et le mode de résolution référentielle d’expressions qui, en surface, paraissent similaires ou identiques. Nous proposons un ensemble des critères objectifs et linguistiquement motivés permettant de reconnaître, de segmenter et de typer ces expressions. Nous verrons que cela ne peut se réaliser sans considérer les procès auxquels ces expressions sont associées et un contexte parfois éloigné.
							</p>

							<p id="taln-2009-long-017-key" class="mots_cles">
							<b>Mots clés : </b> Temporalité, typage et caractérisation des expressions temporelles
							</p>

					</div>
					

					<div class="article">

						<b>Yves Bestgen</b>


						<br/>

							<i>Quel indice pour mesurer l&#39;efficacité en segmentation de textes?</i> <br/>

						<a href="actes/taln-2009-long-018.pdf">taln-2009-long-018</a> 
						<a href="bibtex/taln-2009-long-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-018-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-018-key');">mots clés</a> <br/>

							<p id="taln-2009-long-018-abs" class="resume">
							<b>Résumé : </b> L&#39;évaluation de l&#39;efficacité d&#39;algorithmes de segmentation thématique est généralement effectuée en quantifiant le degré d&#39;accord entre une segmentation hypothétique et une segmentation de référence. Les indices classiques de précision et de rappel étant peu adaptés à ce domaine, WindowDiff (Pevzner, Hearst, 2002) s&#39;est imposé comme l&#39;indice de référence. Une analyse de cet indice montre toutefois qu&#39;il présente plusieurs limitations. L&#39;objectif de ce rapport est d&#39;évaluer un indice proposé par Bookstein, Kulyukin et Raita (2002), la distance de Hamming généralisée, qui est susceptible de remédier à celles-ci. Les analyses montrent que celui-ci conserve tous les avantages de WindowDiff sans les limitations. De plus, contrairement à WindowDiff, il présente une interprétation simple puisqu&#39;il correspond à une vraie distance entre les deux segmentations à comparer.
							</p>

							<p id="taln-2009-long-018-key" class="mots_cles">
							<b>Mots clés : </b> Segmentation thématique, évaluation, distance de Hamming généralisée, WindowDiff
							</p>

					</div>
					

					<div class="article">

						<b>Marion Laignelet, François Rioult</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Repérer automatiquement les segments obsolescents à l’aide d’indices sémantiques et discursifs</i> <br/>

						<a href="actes/taln-2009-long-019.pdf">taln-2009-long-019</a> 
						<a href="bibtex/taln-2009-long-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-019-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-019-key');">mots clés</a> <br/>

							<p id="taln-2009-long-019-abs" class="resume">
							<b>Résumé : </b> Cet article vise la description et le repérage automatique des segments d’obsolescence dans les documents de type encyclopédique. Nous supposons que des indices sémantiques et discursifs peuvent permettre le repérage de tels segments. Pour ce faire, nous travaillons sur un corpus annoté manuellement par des experts sur lequel nous projetons des indices repérés automatiquement. Les techniques statistiques de base ne permettent pas d’expliquer ce phénomène complexe. Nous proposons l’utilisation de techniques de fouille de données pour le caractériser et nous évaluons le pouvoir prédictif de nos indices. Nous montrons, à l’aide de techniques de classification supervisée et de calcul de l’aire sous la courbe ROC, que nos hypothèses sont pertinentes.
							</p>

							<p id="taln-2009-long-019-key" class="mots_cles">
							<b>Mots clés : </b> repérage automatique de l’obsolescence, indices sémantiques et discursifs, textes encyclopédiques, classification supervisée, aire sous la courbe ROC
							</p>

					</div>
					

					<div class="article">

						<b>Michel Généreux, Aurélien Bossard</b>


						<br/>

							<i>Résumé automatique de textes d’opinions</i> <br/>

						<a href="actes/taln-2009-long-020.pdf">taln-2009-long-020</a> 
						<a href="bibtex/taln-2009-long-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-020-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-020-key');">mots clés</a> <br/>

							<p id="taln-2009-long-020-abs" class="resume">
							<b>Résumé : </b> Le traitement des langues fait face à une demande croissante en matière d’analyse de textes véhiculant des critiques ou des opinions. Nous présentons ici un système de résumé automatique tourné vers l’analyse d’articles postés sur des blogues, où sont exprimées à la fois des informations factuelles et des prises de position sur les faits considérés. Nous montrons qu’une approche classique à base de traits de surface est tout à fait efficace dans ce cadre. Le système est évalué à travers une participation à la campagne d’évaluation internationale TAC (Text Analysis Conference) où notre système a réalisé des performances satisfaisantes.
							</p>

							<p id="taln-2009-long-020-key" class="mots_cles">
							<b>Mots clés : </b> résumé automatique, analyse de textes subjectifs, évaluation automatique
							</p>

					</div>
					

					<div class="article">

						<b>Ingrid Falk, Claire Gardent, Evelyne Jacquey, Fabienne Venant</b>


						<br/>

							<i>Sens, synonymes et définitions</i> <br/>

						<a href="actes/taln-2009-long-021.pdf">taln-2009-long-021</a> 
						<a href="bibtex/taln-2009-long-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-021-abs');">résumé</a>

							<p id="taln-2009-long-021-abs" class="resume">
							<b>Résumé : </b> Cet article décrit une méthodologie visant la réalisation d’une ressource sémantique en français centrée sur la synonymie. De manière complémentaire aux travaux existants, la méthode proposée n’a pas seulement pour objectif d’établir des liens de synonymie entre lexèmes, mais également d’apparier les sens possibles d’un lexème avec les ensembles de synonymes appropriés. En pratique, les sens possibles des lexèmes proviennent des définitions du TLFi et les synonymes de cinq dictionnaires accessibles à l’ATILF. Pour évaluer la méthode d’appariement entre sens d’un lexème et ensemble de synonymes, une ressource de référence a été réalisée pour 27 verbes du français par quatre lexicographes qui ont spécifié manuellement l’association entre verbe, sens (définition TLFi) et ensemble de synonymes. Relativement à ce standard étalon, la méthode d’appariement affiche une F-mesure de 0.706 lorsque l’ensemble des paramètres est pris en compte, notamment la distinction pronominal / non-pronominal pour les verbes du français et de 0.602 sans cette distinction.
							</p>


					</div>
					

					<div class="article">

						<b>Étienne Ailloud, Manfred Klenner</b>


						<br/>

							<i>Vers des contraintes plus linguistiques en résolution de coréférences</i> <br/>

						<a href="actes/taln-2009-long-022.pdf">taln-2009-long-022</a> 
						<a href="bibtex/taln-2009-long-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-022-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-022-key');">mots clés</a> <br/>

							<p id="taln-2009-long-022-abs" class="resume">
							<b>Résumé : </b> Nous proposons un modèle filtrant de résolution de coréférences basé sur les notions de transitivité et d’exclusivité linguistique. À partir de l’hypothèse générale que les chaînes de coréférence demeurent cohérentes tout au long d’un texte, notre modèle assure le respect de certaines contraintes linguistiques (via des filtres) quant à la coréférence, ce qui améliore la résolution globale. Le filtrage a lieu à différentes étapes de l’approche standard (c-à-d. par apprentissage automatique), y compris avant l’apprentissage et avant la classification, accélérant et améliorant ce processus.
							</p>

							<p id="taln-2009-long-022-key" class="mots_cles">
							<b>Mots clés : </b> Résolution de coréférences, apprentissage automatique, linguistique informatique par contraintes
							</p>

					</div>
					

					<div class="article">

						<b>Lionel Nicolas, Benoît Sagot, Miguel A. Molinero, Jacques Farré, Éric de La Clergerie</b>


						<br/>

							<i>Trouver et confondre les coupables : un processus sophistiqué de correction de lexique</i> <br/>

						<a href="actes/taln-2009-long-023.pdf">taln-2009-long-023</a> 
						<a href="bibtex/taln-2009-long-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-023-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-023-key');">mots clés</a> <br/>

							<p id="taln-2009-long-023-abs" class="resume">
							<b>Résumé : </b> La couverture d’un analyseur syntaxique dépend avant tout de la grammaire et du lexique sur lequel il repose. Le développement d’un lexique complet et précis est une tâche ardue et de longue haleine, surtout lorsque le lexique atteint un certain niveau de qualité et de couverture. Dans cet article, nous présentons un processus capable de détecter automatiquement les entrées manquantes ou incomplètes d’un lexique, et de suggérer des corrections pour ces entrées. La détection se réalise au moyen de deux techniques reposant soit sur un modèle statistique, soit sur les informations fournies par un étiqueteur syntaxique. Les hypothèses de corrections pour les entrées lexicales détectées sont générées en étudiant les modifications qui permettent d’améliorer le taux d’analyse des phrases dans lesquelles ces entrées apparaissent. Le processus global met en oeuvre plusieurs techniques utilisant divers outils tels que des étiqueteurs et des analyseurs syntaxiques ou des classifieurs d’entropie. Son application au Lefff , un lexique morphologique et syntaxique à large couverture du français, nous a déjà permis de réaliser des améliorations notables.
							</p>

							<p id="taln-2009-long-023-key" class="mots_cles">
							<b>Mots clés : </b> Acquisition et correction lexicale, lexique à large couverture, fouille d’erreurs, étiqueteur syntaxique, classifieur d’entropie, analyseur syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>François Trouilleux</b>


						<br/>

							<i>Un analyseur de surface non déterministe pour le français</i> <br/>

						<a href="actes/taln-2009-long-024.pdf">taln-2009-long-024</a> 
						<a href="bibtex/taln-2009-long-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-024-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-024-key');">mots clés</a> <br/>

							<p id="taln-2009-long-024-abs" class="resume">
							<b>Résumé : </b> Les analyseurs syntaxiques de surface à base de règles se caractérisent par un processus en deux temps : désambiguïsation lexicale, puis reconnaissance de patrons. Considérant que ces deux étapes introduisent une certaine redondance dans la description linguistique et une dilution des heuristiques dans les différents processus, nous proposons de définir un analyseur de surface qui fonctionne sur une entrée non désambiguïsée et produise l’ensemble des analyses possibles en termes de syntagmes noyau (chunks). L’analyseur, implanté avec NooJ, repose sur la définition de patrons étendus qui annotent des séquences de syntagmes noyau. Les résultats obtenus sur un corpus de développement d’environ 22 500 mots, avec un rappel proche de 100 %, montrent la faisabilité de l’approche et signalent quelques points d’ambiguïté à étudier plus particulièrement pour améliorer la précision.
							</p>

							<p id="taln-2009-long-024-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique de surface, automates à états finis, déterminisme, désambiguïsation
							</p>

					</div>
					

					<div class="article">

						<b>Aurélien Bossard</b>


						<br/>

							<i>Une approche mixte-statistique et structurelle - pour le résumé automatique de dépêches</i> <br/>

						<a href="actes/taln-2009-long-025.pdf">taln-2009-long-025</a> 
						<a href="bibtex/taln-2009-long-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-025-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-025-key');">mots clés</a> <br/>

							<p id="taln-2009-long-025-abs" class="resume">
							<b>Résumé : </b> Les techniques de résumé automatique multi-documents par extraction ont récemment évolué vers des méthodes statistiques pour la sélection des phrases à extraire. Dans cet article, nous présentons un système conforme à l’« état de l’art » — CBSEAS — que nous avons développé pour les tâches Opinion (résumés d’opinions issues de blogs) et Update (résumés de dépêches et mise à jour du résumé à partir de nouvelles dépêches sur le même événement) de la campagne d’évaluation TAC 2008, et montrons l’intérêt d’analyses structurelles et linguistiques des documents à résumer. Nous présentons également notre étude sur la structure des dépêches et l’impact de son intégration à CBSEAS.
							</p>

							<p id="taln-2009-long-025-key" class="mots_cles">
							<b>Mots clés : </b> Résumé automatique, structure de documents
							</p>

					</div>
					

					<div class="article">

						<b>Caroline Brun, Nicolas Dessaigne, Maud Ehrmann, Baptiste Gaillard, Sylvie Guillemin-Lanne, Guillaume Jacquet, Aaron Kaplan, Marianna Kucharski, Claude Martineau, Aurélie Migeotte, Takuya Nakamura, Stavroula Voyatzi</b>


						<br/>

							<i>Une expérience de fusion pour l’annotation d&#39;entités nommées</i> <br/>

						<a href="actes/taln-2009-long-026.pdf">taln-2009-long-026</a> 
						<a href="bibtex/taln-2009-long-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-026-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-026-key');">mots clés</a> <br/>

							<p id="taln-2009-long-026-abs" class="resume">
							<b>Résumé : </b> Nous présentons une expérience de fusion d’annotations d’entités nommées provenant de différents annotateurs. Ce travail a été réalisé dans le cadre du projet Infom@gic, projet visant à l’intégration et à la validation d’applications opérationnelles autour de l’ingénierie des connaissances et de l’analyse de l’information, et soutenu par le pôle de compétitivité Cap Digital « Image, MultiMédia et Vie Numérique ». Nous décrivons tout d’abord les quatre annotateurs d’entités nommées à l’origine de cette expérience. Chacun d’entre eux fournit des annotations d’entités conformes à une norme développée dans le cadre du projet Infom@gic. L’algorithme de fusion des annotations est ensuite présenté ; il permet de gérer la compatibilité entre annotations et de mettre en évidence les conflits, et ainsi de fournir des informations plus fiables. Nous concluons en présentant et interprétant les résultats de la fusion, obtenus sur un corpus de référence annoté manuellement.
							</p>

							<p id="taln-2009-long-026-key" class="mots_cles">
							<b>Mots clés : </b> Entités nommées, fusion d’annotations, UIMA
							</p>

					</div>
					

					<div class="article">

						<b>Stéphanie Léon</b>


						<br/>

							<i>Un système modulaire d’acquisition automatique de traductions à partir du Web</i> <br/>

						<a href="actes/taln-2009-long-027.pdf">taln-2009-long-027</a> 
						<a href="bibtex/taln-2009-long-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-027-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-027-key');">mots clés</a> <br/>

							<p id="taln-2009-long-027-abs" class="resume">
							<b>Résumé : </b> Nous présentons une méthode de Traduction Automatique d’Unités Lexicales Complexes (ULC) pour la construction de ressources bilingues français/anglais, basée sur un système modulaire qui prend en compte les propriétés linguistiques des unités sources (compositionnalité, polysémie, etc.). Notre système exploite les différentes « facettes » du Web multilingue pour valider des traductions candidates ou acquérir de nouvelles traductions. Après avoir collecté une base d’ULC en français à partir d’un corpus de pages Web, nous passons par trois phases de traduction qui s’appliquent à un cas linguistique, avec une méthode adaptée : les traductions compositionnelles non polysémiques, les traductions compositionnelles polysémiques et les traductions non compositionnelles et/ou inconnues. Notre évaluation sur un vaste échantillon d’ULC montre que l’exploitation du Web pour la traduction et la prise en compte des propriétés linguistiques au sein d’un système modulaire permet une acquisition automatique de traductions avec une excellente précision.
							</p>

							<p id="taln-2009-long-027-key" class="mots_cles">
							<b>Mots clés : </b> Traduction Automatique, Unités Lexicales Complexes, Désambiguïsation lexicale, World Wide Web, Terminologie
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Blache</b>


						<br/>

							<i>Des relations d’alignement pour décrire l’interaction des domaines linguistiques : vers des Grammaires Multimodales</i> <br/>

						<a href="actes/taln-2009-long-028.pdf">taln-2009-long-028</a> 
						<a href="bibtex/taln-2009-long-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-028-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-028-key');">mots clés</a> <br/>

							<p id="taln-2009-long-028-abs" class="resume">
							<b>Résumé : </b> Un des problèmes majeurs de la linguistique aujourd’hui réside dans la prise en compte de phénomènes relevant de domaines et de modalités différentes. Dans la littérature, la réponse consiste à représenter les relations pouvant exister entre ces domaines de façon externe, en termes de relation de structure à structure, s’appuyant donc sur une description distincte de chaque domaine ou chaque modalité. Nous proposons dans cet article une approche différente permettant représenter ces phénomènes dans un cadre formel unique, permettant de rendre compte au sein d’une même grammaire tous les phénomènes concernés. Cette représentation précise de l’interaction entre domaines et modalités s’appuie sur la définition de relations d’alignement.
							</p>

							<p id="taln-2009-long-028-key" class="mots_cles">
							<b>Mots clés : </b> Multimodalité, interaction entre domaines, grammaire, corpus multimodaux
							</p>

					</div>
					

					<div class="article">

						<b>Karën Fort, Maud Ehrmann, Adeline Nazarenko</b>


						<br/>

							<i>Vers une méthodologie d’annotation des entités nommées en corpus ?</i> <br/>

						<a href="actes/taln-2009-long-029.pdf">taln-2009-long-029</a> 
						<a href="bibtex/taln-2009-long-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-long-029-abs');">résumé</a>
							<a onclick="toggle('taln-2009-long-029-key');">mots clés</a> <br/>

							<p id="taln-2009-long-029-abs" class="resume">
							<b>Résumé : </b> La tâche, aujourd’hui considérée comme fondamentale, de reconnaissance d’entités nommées, présente des difficultés spécifiques en matière d’annotation. Nous les précisons ici, en les illustrant par des expériences d’annotation manuelle dans le domaine de la microbiologie. Ces problèmes nous amènent à reposer la question fondamentale de ce que les annotateurs doivent annoter et surtout, pour quoi faire. Nous identifions pour cela les applications nécessitant l’extraction d’entités nommées et, en fonction des besoins de ces applications, nous proposons de définir sémantiquement les éléments à annoter. Nous présentons ensuite un certain nombre de recommandations méthodologiques permettant d’assurer un cadre d’annotation cohérent et évaluable.
							</p>

							<p id="taln-2009-long-029-key" class="mots_cles">
							<b>Mots clés : </b> annotation, reconnaissance d’entités nommées
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="position">Prise de position</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Sylwia Ozdowska</b>


						<br/>

							<i>Données bilingues pour la TAS français-anglais : impact de la langue source et direction de traduction originales sur la qualité de la traduction</i> <br/>

						<a href="actes/taln-2009-position-001.pdf">taln-2009-position-001</a> 
						<a href="bibtex/taln-2009-position-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-position-001-abs');">résumé</a>
							<a onclick="toggle('taln-2009-position-001-key');">mots clés</a> <br/>

							<p id="taln-2009-position-001-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous prenons position par rapport à la question de la qualité des données bilingues destinées à la traduction automatique statistique en terme de langue source et direction de traduction originales à l’égard d’une tâche de traduction français-anglais. Nous montrons que l’entraînement sur un corpus contenant des textes qui ont été à l’origine traduits du français vers l’anglais améliore la qualité de la traduction. Inversement, l’entraînement sur un corpus contenant exclusivement des textes dont la langue source originale n’est ni le français ni l’anglais dégrade la traduction.
							</p>

							<p id="taln-2009-position-001-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique statistique, corpus bilingue, direction de la traduction, langue source, langue cible
							</p>

					</div>
					

					<div class="article">

						<b>Marianna Apidianaki</b>


						<br/>

							<i>La place de la désambiguïsation lexicale dans la Traduction Automatique Statistique</i> <br/>

						<a href="actes/taln-2009-position-002.pdf">taln-2009-position-002</a> 
						<a href="bibtex/taln-2009-position-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-position-002-abs');">résumé</a>
							<a onclick="toggle('taln-2009-position-002-key');">mots clés</a> <br/>

							<p id="taln-2009-position-002-abs" class="resume">
							<b>Résumé : </b> L’étape de la désambiguïsation lexicale est souvent esquivée dans les systèmes de Traduction Automatique Statistique (Statistical Machine Translation (SMT)) car considérée comme non nécessaire à la sélection de traductions correctes. Le débat autour de cette nécessité est actuellement assez vif. Dans cet article, nous présentons les principales positions sur le sujet. Nous analysons les avantages et les inconvénients de la conception actuelle de la désambiguïsation dans le cadre de la SMT, d’après laquelle les sens des mots correspondent à leurs traductions dans des corpus parallèles. Ensuite, nous présentons des arguments en faveur d’une analyse plus poussée des informations sémantiques induites à partir de corpus parallèles et nous expliquons comment les résultats d’une telle analyse pourraient être exploités pour une évaluation plus flexible et concluante de l’impact de la désambiguïsation dans la SMT.
							</p>

							<p id="taln-2009-position-002-key" class="mots_cles">
							<b>Mots clés : </b> Désambiguïsation lexicale, Traduction Automatique Statistique, sélection lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Marianne Laurent, Ghislain Putois, Philippe Bretier, Thierry Moudenc</b>


						<br/>

							<i>Nouveau paradigme d’évaluation des systèmes de dialogue homme-machine</i> <br/>

						<a href="actes/taln-2009-position-003.pdf">taln-2009-position-003</a> 
						<a href="bibtex/taln-2009-position-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-position-003-abs');">résumé</a>
							<a onclick="toggle('taln-2009-position-003-key');">mots clés</a> <br/>

							<p id="taln-2009-position-003-abs" class="resume">
							<b>Résumé : </b> L’évaluation des systèmes de dialogue homme-machine est un problème difficile et pour lequel ni les objectifs ni les solutions proposées ne font aujourd’hui l’unanimité. Les approches ergonomiques traditionnelles soumettent le système de dialogue au regard critique de l’utilisateur et tente d’en capter l’expression, mais l’absence d’un cadre objectivable des usages de ces utilisateurs empêche une comparaison entre systèmes différents, ou entre évolutions d’un même système. Nous proposons d’inverser cette vision et de mesurer le comportement de l’utilisateur au regard du système de dialogue. Aussi, au lieu d’évaluer l’adéquation du système à ses utilisateurs, nous mesurons l’adéquation des utilisateurs au système. Ce changement de paradigme permet un changement de référentiel qui n’est plus les usages des utilisateurs mais le cadre du système. Puisque le système est complètement défini, ce paradigme permet des approches quantitatives et donc des évaluations comparatives de systèmes.
							</p>

							<p id="taln-2009-position-003-key" class="mots_cles">
							<b>Mots clés : </b> Évaluation, Dialogue
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="court">Papiers courts</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Djamé Seddah, Marie Candito, Benoît Crabbé</b>


						<br/>

							<i>Adaptation de parsers statistiques lexicalisés pour le français : Une évaluation complète sur corpus arborés</i> <br/>

						<a href="actes/taln-2009-court-001.pdf">taln-2009-court-001</a> 
						<a href="bibtex/taln-2009-court-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-001-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-001-key');">mots clés</a> <br/>

							<p id="taln-2009-court-001-abs" class="resume">
							<b>Résumé : </b> Cet article présente les résultats d’une évaluation exhaustive des principaux analyseurs syntaxiques probabilistes dit “lexicalisés” initialement conçus pour l’anglais, adaptés pour le français et évalués sur le CORPUS ARBORÉ DU FRANÇAIS (Abeillé et al., 2003) et le MODIFIED FRENCH TREEBANK (Schluter &amp; van Genabith, 2007). Confirmant les résultats de (Crabbé &amp; Candito, 2008), nous montrons que les modèles lexicalisés, à travers les modèles de Charniak (Charniak, 2000), ceux de Collins (Collins, 1999) et le modèle des TIG Stochastiques (Chiang, 2000), présentent des performances moindres face à un analyseur PCFG à Annotation Latente (Petrov et al., 2006). De plus, nous montrons que le choix d’un jeu d’annotations issus de tel ou tel treebank oriente fortement les résultats d’évaluations tant en constituance qu’en dépendance non typée. Comparés à (Schluter &amp; van Genabith, 2008; Arun &amp; Keller, 2005), tous nos résultats sont state-of-the-art et infirment l’hypothèse d’une difficulté particulière qu’aurait le français en terme d’analyse syntaxique probabiliste et de sources de données.
							</p>

							<p id="taln-2009-court-001-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique probabiliste, corpus arborés, évaluation, analyse du français
							</p>

					</div>
					

					<div class="article">

						<b>Fiammetta Namer</b>


						<br/>

							<i>Analyse automatique des noms déverbaux composés : pourquoi et comment faire interagir analogie et système de règles</i> <br/>

						<a href="actes/taln-2009-court-002.pdf">taln-2009-court-002</a> 
						<a href="bibtex/taln-2009-court-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-002-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-002-key');">mots clés</a> <br/>

							<p id="taln-2009-court-002-abs" class="resume">
							<b>Résumé : </b> Cet article aborde deux problèmes d’analyse morpho-sémantique du lexique : (1) attribuer automatiquement une définition à des noms et verbes morphologiquement construits inconnus des dictionnaires mais présents dans les textes ; (2) proposer une analyse combinant règles et analogie, deux techniques généralement contradictoires. Les noms analysés sont apparemment suffixés et composés (HYDROMASSAGE). La plupart d’entre eux, massivement attestés dans les documents (journaux, Internet) sont absents des dictionnaires. Ils sont souvent reliés à des verbes (HYDROMASSER) également néologiques. Le nombre de ces noms et verbes est estimé à 5.400. L’analyse proposée leur attribue une définition par rapport à leur base, et enrichit un lexique de référence pour le TALN au moyen de cette base, si elle est néologique. L’implémentation des contraintes linguistiques qui régissent ces formations est reproductible dans d’autres langues européennes où sont rencontrés les mêmes types de données dont l’analyse reflète le même raisonnement que pour le français.
							</p>

							<p id="taln-2009-court-002-key" class="mots_cles">
							<b>Mots clés : </b> Analyse morphologique, Annotation sémantique, Composition savante, Noms déverbaux, Règles, Analogie
							</p>

					</div>
					

					<div class="article">

						<b>Jonathan Marchand, Bruno Guillaume, Guy Perrier</b>


						<br/>

							<i>Analyse en dépendances à l’aide des grammaires d’interaction</i> <br/>

						<a href="actes/taln-2009-court-003.pdf">taln-2009-court-003</a> 
						<a href="bibtex/taln-2009-court-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-003-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-003-key');">mots clés</a> <br/>

							<p id="taln-2009-court-003-abs" class="resume">
							<b>Résumé : </b> Cet article propose une méthode pour extraire une analyse en dépendances d’un énoncé à partir de son analyse en constituants avec les grammaires d’interaction. Les grammaires d’interaction sont un formalisme grammatical qui exprime l’interaction entre les mots à l’aide d’un système de polarités. Le mécanisme de composition syntaxique est régi par la saturation des polarités. Les interactions s’effectuent entre les constituants, mais les grammaires étant lexicalisées, ces interactions peuvent se traduire sur les mots. La saturation des polarités lors de l’analyse syntaxique d’un énoncé permet d’extraire des relations de dépendances entre les mots, chaque dépendance étant réalisée par une saturation. Les structures de dépendances ainsi obtenues peuvent être vues comme un raffinement de l’analyse habituellement effectuée sous forme d’arbre de dépendance. Plus généralement, ce travail apporte un éclairage nouveau sur les liens entre analyse en constituants et analyse en dépendances.
							</p>

							<p id="taln-2009-court-003-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique, grammaires de dépendances, grammaires d’interaction, polarité
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Philippe Prost</b>


						<br/>

							<i>Analyse relâchée à base de contraintes</i> <br/>

						<a href="actes/taln-2009-court-004.pdf">taln-2009-court-004</a> 
						<a href="bibtex/taln-2009-court-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-004-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-004-key');">mots clés</a> <br/>

							<p id="taln-2009-court-004-abs" class="resume">
							<b>Résumé : </b> La question de la grammaticalité, et celle duale de l’agrammaticalité, sont des sujets délicats à aborder, dès lors que l’on souhaite intégrer différents degrés, tant de grammaticalité que d’agrammaticalité. En termes d’analyse automatique, les problèmes posés sont de l’ordre de la représentation des connaissances, du traitement, et bien évidement de l’évaluation. Dans cet article, nous nous concentrons sur l’aspect traitement, et nous nous penchons sur la question de l’analyse d’énoncés agrammaticaux. Nous explorons la possibilité de fournir une analyse la plus complète possible pour un énoncé agrammatical, sans l’apport d’information complémentaire telle que par le biais de mal-règles ou autre grammaire d’erreurs. Nous proposons une solution algorithmique qui permet l’analyse automatique d’un énoncé agrammatical, sur la seule base d’une grammaire modèle-théorique de bonne formation. Cet analyseur est prouvé générer une solution optimale, selon un critère numérique maximisé.
							</p>

							<p id="taln-2009-court-004-key" class="mots_cles">
							<b>Mots clés : </b> grammaticalité, analyse syntaxique, contraintes, syntaxe modèle-théorique
							</p>

					</div>
					

					<div class="article">

						<b>Marie-Paule Péry-Woodley, Nicholas Asher, Patrice Enjalbert, Farah Benamara, Myriam Bras, Cécile Fabre, Stéphane Ferrari, Lydia-Mai Ho-Dac, Anne Le Draoulec, Yann Mathet, Philippe Muller, Laurent Prévot, Josette Rebeyrolle, Ludovic Tanguy, Marianne Vergez-Couret, Laure Vieu, Antoine Widlöcher</b>


						<br/>

							<i>ANNODIS: une approche outillée de l&#39;annotation de structures discursives</i> <br/>

						<a href="actes/taln-2009-court-005.pdf">taln-2009-court-005</a> 
						<a href="bibtex/taln-2009-court-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-005-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-005-key');">mots clés</a> <br/>

							<p id="taln-2009-court-005-abs" class="resume">
							<b>Résumé : </b> Le projet ANNODIS vise la construction d’un corpus de textes annotés au niveau discursif ainsi que le développement d&#39;outils pour l’annotation et l’exploitation de corpus. Les annotations adoptent deux points de vue complémentaires : une perspective ascendante part d&#39;unités de discours minimales pour construire des structures complexes via un jeu de relations de discours ; une perspective descendante aborde le texte dans son entier et se base sur des indices pré-identifiés pour détecter des structures discursives de haut niveau. La construction du corpus est associée à la création de deux interfaces : la première assiste l&#39;annotation manuelle des relations et structures discursives en permettant une visualisation du marquage issu des prétraitements ; une seconde sera destinée à l&#39;exploitation des annotations. Nous présentons les modèles et protocoles d&#39;annotation élaborés pour mettre en oeuvre, au travers de l&#39;interface dédiée, la campagne d&#39;annotation.
							</p>

							<p id="taln-2009-court-005-key" class="mots_cles">
							<b>Mots clés : </b> annotation de corpus, structures de discours, interface d&#39;annotation
							</p>

					</div>
					

					<div class="article">

						<b>Véronique Moriceau, Xavier Tannier</b>


						<br/>

							<i>Apport de la syntaxe dans un système de question-réponse : étude du système FIDJI.</i> <br/>

						<a href="actes/taln-2009-court-006.pdf">taln-2009-court-006</a> 
						<a href="bibtex/taln-2009-court-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-006-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-006-key');">mots clés</a> <br/>

							<p id="taln-2009-court-006-abs" class="resume">
							<b>Résumé : </b> Cet article présente une série d’évaluations visant à étudier l’apport d’une analyse syntaxique robuste des questions et des documents dans un système de questions-réponses. Ces évaluations ont été effectuées sur le système FIDJI, qui utilise à la fois des informations syntaxiques et des techniques plus “traditionnelles”. La sélection des documents, l’extraction de la réponse ainsi que le comportement selon les différents types de questions ont été étudiés.
							</p>

							<p id="taln-2009-court-006-key" class="mots_cles">
							<b>Mots clés : </b> Systèmes de questions-réponses, analyse syntaxique, évaluation
							</p>

					</div>
					

					<div class="article">

						<b>Dominique Laurent, Sophie Nègre, Patrick Séguéla</b>


						<br/>

							<i>Apport des cooccurrences à la correction et à l&#39;analyse syntaxique</i> <br/>

						<a href="actes/taln-2009-court-007.pdf">taln-2009-court-007</a> 
						<a href="bibtex/taln-2009-court-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-007-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-007-key');">mots clés</a> <br/>

							<p id="taln-2009-court-007-abs" class="resume">
							<b>Résumé : </b> Le correcteur grammatical Cordial utilise depuis de nombreuses années les cooccurrences pour la désambiguïsation sémantique. Un dictionnaire de cooccurrences ayant été constitué pour les utilisateurs du logiciel de correction et d&#39;aides à la rédaction, la grande richesse de ce dictionnaire a incité à l&#39;utiliser intensivement pour la correction, spécialement des homonymes et paronymes. Les résultats obtenus sont spectaculaires sur ces types d&#39;erreurs mais la prise en compte des cooccurrences a également été utilisée avec profit pour la pure correction orthographique et pour le rattachement des groupes en analyse syntaxique.
							</p>

							<p id="taln-2009-court-007-key" class="mots_cles">
							<b>Mots clés : </b> cooccurrences, collocations, correction grammaticale
							</p>

					</div>
					

					<div class="article">

						<b>Daoud Daoud, Mohammad Daoud</b>


						<br/>

							<i>Arabic Disambiguation Using Dependency Grammar</i> <br/>

						<a href="actes/taln-2009-court-008.pdf">taln-2009-court-008</a> 
						<a href="bibtex/taln-2009-court-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-008-abs');">abstract</a>
							<a onclick="toggle('taln-2009-court-008-key');">keywords</a> <br/>

							<p id="taln-2009-court-008-abs" class="abstract">
							<b>Abstract : </b> In this paper, we present a new approach to disambiguation Arabic using a joint rule-based model which is conceptualized using Dependency Grammar. This approach helps in highly accurate analysis of sentences. The analysis produces a semantic net like structure expressed by means of Universal Networking Language (UNL) - a recently proposed interlingua. Extremely varied and complex phenomena of Arabic language have been addressed.
							</p>

							<p id="taln-2009-court-008-key" class="keywords">
							<b>Keywords : </b> Dependency Grammar, Arabic Language, Disambiguation, EnCo, UNL
							</p>

					</div>
					

					<div class="article">

						<b>Claude de Loupy, Michaël Bagur, Helena Blancafort</b>


						<br/>

							<i>Association automatique de lemmes et de paradigmes de flexion à un mot inconnu</i> <br/>

						<a href="actes/taln-2009-court-009.pdf">taln-2009-court-009</a> 
						<a href="bibtex/taln-2009-court-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-009-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-009-key');">mots clés</a> <br/>

							<p id="taln-2009-court-009-abs" class="resume">
							<b>Résumé : </b> La maintenance et l’enrichissement des lexiques morphosyntaxiques sont souvent des tâches fastidieuses. Dans cet article nous présentons la mise en place d’une procédure de guessing de flexion afin d’aider les linguistes dans leur travail de lexicographes. Le guesser développé ne fait pas qu’évaluer l’étiquette morphosyntaxique comme c’est généralement le cas. Il propose pour un mot français inconnu, un ou plusieurs candidats-lemmes, ainsi que les paradigmes de flexion associés (formes fléchies et étiquettes morphosyntaxiques). Dans cet article, nous décrivons le modèle probabiliste utilisé ainsi que les résultats obtenus. La méthode utilisée permet de réduire considérablement le nombre de règles à valider, permettant ainsi un gain de temps important.
							</p>

							<p id="taln-2009-court-009-key" class="mots_cles">
							<b>Mots clés : </b> guesser, lexiques morphosyntaxiques, aide aux linguistes, induction des règles de flexion
							</p>

					</div>
					

					<div class="article">

						<b>Matthieu Vernier, Laura Monceaux, Béatrice Daille, Estelle Dubreil</b>


						<br/>

							<i>Catégorisation sémantico-discursive des évaluations exprimées dans la blogosphère</i> <br/>

						<a href="actes/taln-2009-court-010.pdf">taln-2009-court-010</a> 
						<a href="bibtex/taln-2009-court-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-010-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-010-key');">mots clés</a> <br/>

							<p id="taln-2009-court-010-abs" class="resume">
							<b>Résumé : </b> Les blogs constituent un support d’observations idéal pour des applications liées à la fouille d’opinion. Toutefois, ils imposent de nouvelles problématiques et de nouveaux défis au regard des méthodes traditionnelles du domaine. De ce fait, nous proposons une méthode automatique pour la détection et la catégorisation des évaluations localement exprimées dans un corpus de blogs multi-domaine. Celle-ci rend compte des spécificités du langage évaluatif décrites dans deux théories linguistiques. L’outil développé au sein de la plateforme UIMA vise d’une part à construire automatiquement une grammaire du langage évaluatif, et d’autre part à utiliser cette grammaire pour la détection et la catégorisation des passages évaluatifs d’un texte. La catégorisation traite en particulier l’aspect axiologique de l’évaluation, sa configuration d’énonciation et sa modalité dans le discours.
							</p>

							<p id="taln-2009-court-010-key" class="mots_cles">
							<b>Mots clés : </b> fouille d’opinion, langage évaluatif, catégorisation des évaluations
							</p>

					</div>
					

					<div class="article">

						<b>Stéphanie Weiser, Martin Coste, Florence Amardeilh</b>


						<br/>

							<i>Chaîne de traitement linguistique : du repérage d&#39;expressions temporelles au peuplement d&#39;une ontologie de tourisme</i> <br/>

						<a href="actes/taln-2009-court-011.pdf">taln-2009-court-011</a> 
						<a href="bibtex/taln-2009-court-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-011-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-011-key');">mots clés</a> <br/>

							<p id="taln-2009-court-011-abs" class="resume">
							<b>Résumé : </b> Cet article présente la chaîne de traitement linguistique réalisée pour la mise en place d&#39;une plateforme touristique sur Internet. Les premières étapes de cette chaîne sont le repérage et l&#39;annotation des expressions temporelles présentes dans des pages Web. Ces deux tâches sont effectuées à l&#39;aide de patrons linguistiques. Elles soulèvent de nombreux questionnements auxquels nous tentons de répondre, notamment au sujet de la définition des informations à extraire, du format d&#39;annotation et des contraintes. L&#39;étape suivante consiste en l&#39;exploitation des données annotées pour le peuplement d&#39;une ontologie du tourisme. Nous présentons les règles d&#39;acquisition nécessaires pour alimenter la base de connaissance du projet. Enfin, nous exposons une évaluation du système d&#39;annotation. Cette évaluation permet de juger aussi bien le repérage des expressions temporelles que leur annotation.
							</p>

							<p id="taln-2009-court-011-key" class="mots_cles">
							<b>Mots clés : </b> Annotation, expressions temporelles, ontologies, base de connaissance, tourisme
							</p>

					</div>
					

					<div class="article">

						<b>Yue Ma, Laurent Audibert</b>


						<br/>

							<i>Détection des contradictions dans les annotations sémantiques</i> <br/>

						<a href="actes/taln-2009-court-012.pdf">taln-2009-court-012</a> 
						<a href="bibtex/taln-2009-court-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-012-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-012-key');">mots clés</a> <br/>

							<p id="taln-2009-court-012-abs" class="resume">
							<b>Résumé : </b> L’annotation sémantique a pour objectif d’apporter au texte une représentation explicite de son interprétation sémantique. Dans un précédent article, nous avons proposé d’étendre les ontologies par des règles d’annotation sémantique. Ces règles sont utilisées pour l’annotation sémantique d’un texte au regard d’une ontologie dans le cadre d’une plate-forme d’annotation linguistique automatique. Nous présentons dans cet article une mesure, basée sur la valeur de Shapley, permettant d’identifier les règles qui sont sources de contradiction dans l’annotation sémantique. Par rapport aux classiques mesures de précision et de rappel, l’intérêt de cette mesure est de ne pas nécessiter de corpus manuellement annoté, d’être entièrement automatisable et de permettre l’identification des règles qui posent problème.
							</p>

							<p id="taln-2009-court-012-key" class="mots_cles">
							<b>Mots clés : </b> Annotation sémantique, valeur de Shapley, plate-forme d’annotation
							</p>

					</div>
					

					<div class="article">

						<b>Marc Le Tallec, Jeanne Villaneau, Jean-Yves Antoine, Agata Savary, Arielle Syssau-Vaccarella</b>


						<br/>

							<i>Détection des émotions à partir du contenu linguistique d’énoncés oraux : application à un robot compagnon pour enfants fragilisés</i> <br/>

						<a href="actes/taln-2009-court-013.pdf">taln-2009-court-013</a> 
						<a href="bibtex/taln-2009-court-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-013-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-013-key');">mots clés</a> <br/>

							<p id="taln-2009-court-013-abs" class="resume">
							<b>Résumé : </b> Le projet ANR Emotirob aborde la question de la détection des émotions sous un cadre original : concevoir un robot compagnon émotionnel pour enfants fragilisés. Notre approche consiste à combiner détection linguistique et prosodie. Nos expériences montrent qu&#39;un sujet humain peut estimer de manière fiable la valence émotionnelle d&#39;un énoncé à partir de son contenu propositionnel. Nous avons donc développé un premier modèle de détection linguistique qui repose sur le principe de compositionnalité des émotions : les mots simples ont une valence émotionnelle donnée et les prédicats modifient la valence de leurs arguments. Après une description succincte du système logique de compréhension dont les sorties sont utilisées pour le calcul global de l&#39;émotion, cet article présente la construction d&#39;une norme émotionnelle lexicale de référence, ainsi que d&#39;une ontologie de classes émotionnelles de prédicats, pour des enfants de 5 et 7 ans.
							</p>

							<p id="taln-2009-court-013-key" class="mots_cles">
							<b>Mots clés : </b> Emotion, valence émotionnelle, norme lexicale émotionnelle, robot compagnon, compréhension de parole
							</p>

					</div>
					

					<div class="article">

						<b>Nuria Gala, Véronique Rey, Laurent Tichit</b>


						<br/>

							<i>Dispersion sémantique dans des familles morpho-phonologiques : éléments théoriques et empiriques</i> <br/>

						<a href="actes/taln-2009-court-014.pdf">taln-2009-court-014</a> 
						<a href="bibtex/taln-2009-court-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-014-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-014-key');">mots clés</a> <br/>

							<p id="taln-2009-court-014-abs" class="resume">
							<b>Résumé : </b> Traditionnellement, la morphologie lexicale a été diachronique et a permis de proposer le concept de famille de mots. Ce dernier est repris dans les études en synchronie et repose sur une forte cohérence sémantique entre les mots d’une même famille. Dans cet article, nous proposons une approche en synchronie fondée sur la notion de continuité à la fois phonologique et sémantique. Nous nous intéressons, d’une part, à la morpho-phonologie et, d’autre part, à la dispersion sémantique des mots dans les familles. Une première étude (Gala &amp; Rey, 2008) montrait que les familles de mots obtenues présentaient des espaces sémantiques soit de grande cohésion soit de grande dispersion. Afin de valider ces observations, nous présentons ici une méthode empirique qui permet de pondérer automatiquement les unités de sens d’un mot et d’une famille. Une expérience menée auprès de 30 locuteurs natifs valide notre approche et ouvre la voie pour une étude approfondie du lexique sur ces bases phonologiques et sémantiques.
							</p>

							<p id="taln-2009-court-014-key" class="mots_cles">
							<b>Mots clés : </b> morpho-phonologie lexicale, traitement automatique des familles dérivationnelles, espaces sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>Kévin Séjourné</b>


						<br/>

							<i>Exploitation d’une structure pour les questions enchaînées</i> <br/>

						<a href="actes/taln-2009-court-015.pdf">taln-2009-court-015</a> 
						<a href="bibtex/taln-2009-court-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-015-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-015-key');">mots clés</a> <br/>

							<p id="taln-2009-court-015-abs" class="resume">
							<b>Résumé : </b> Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses (SQR) utilisant des questions enchaînées. La recherche des documents dans un SQR est perturbée par l’absence des éléments utiles à la recherche dans les questions liées, éléments figurant dans les échanges précédents. Les récentes campagnes d’évaluation montrent que ce problème est sous-estimé, et n’a pas fait l’objet de technique dédiée. Afin d’améliorer la recherche des documents dans un SQR nous utilisons une méthode récente d’organisation des informations liées aux interactions entre questions. Celle-ci se base sur l’exploitation d’une structure de données adaptée à la transmission des informations des questions liées jusqu’au moteur d’interrogation. Le moteur d’interrogation doit alors être adapté afin de tirer partie de cette structure de données.
							</p>

							<p id="taln-2009-court-015-key" class="mots_cles">
							<b>Mots clés : </b> Question réponse enchaînée
							</p>

					</div>
					

					<div class="article">

						<b>Alexandre Denis, Matthieu Quignard</b>


						<br/>

							<i>Exploitation du terrain commun pour la production d’expressions référentielles dans les systèmes de dialogue</i> <br/>

						<a href="actes/taln-2009-court-016.pdf">taln-2009-court-016</a> 
						<a href="bibtex/taln-2009-court-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-016-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-016-key');">mots clés</a> <br/>

							<p id="taln-2009-court-016-abs" class="resume">
							<b>Résumé : </b> Cet article présente un moyen de contraindre la production d’expressions référentielles par un système de dialogue en fonction du terrain commun. Cette capacité, fondamentale pour atteindre la compréhension mutuelle, est trop souvent oubliée dans les systèmes de dialogue. Le modèle que nous proposons s’appuie sur une modélisation du processus d’ancrage (grounding process) en proposant un raffinement du statut d’ancrage appliqué à la description des référents. Il décrit quand et comment ce statut doit être révisé en fonction des jugements de compréhension des deux participants ainsi que son influence dans le choix d’une description partagée destinée à la génération d’une expression référentielle.
							</p>

							<p id="taln-2009-court-016-key" class="mots_cles">
							<b>Mots clés : </b> Compréhension mutuelle, processus d’ancrage, référence, génération
							</p>

					</div>
					

					<div class="article">

						<b>Younès Bahou, Amine Bayoudhi, Lamia Hadrich Belguith</b>


						<br/>

							<i>Gestion de dialogue oral Homme-machine en arabe</i> <br/>

						<a href="actes/taln-2009-court-017.pdf">taln-2009-court-017</a> 
						<a href="bibtex/taln-2009-court-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-017-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-017-key');">mots clés</a> <br/>

							<p id="taln-2009-court-017-abs" class="resume">
							<b>Résumé : </b> Dans le présent papier, nous présentons nos travaux sur la gestion du dialogue oral arabe Homme-machine. Ces travaux entrent dans le cadre de la réalisation du serveur vocal interactif SARF (Bahou et al., 2008) offrant des renseignements sur le transport ferroviaire tunisien en langue arabe standard moderne. Le gestionnaire de dialogue que nous proposons est basé sur une approche structurelle et est composé de deux modèles à savoir, le modèle de tâche et le modèle de dialogue. Le premier modèle permet de i) compléter et vérifier l’incohérence des structures sémantiques représentant les sens utiles des énoncés, ii) générer une requête vers l’application et iii) récupérer le résultat et de formuler une réponse à l’utilisateur en langage naturel. Quant au modèle de dialogue, il assure l’avancement du dialogue avec l’utilisateur et l’identification de ses intentions. L’interaction entre ces deux modèles est assurée grâce à un contexte du dialogue permettant le suivi et la mise à jour de l’historique du dialogue.
							</p>

							<p id="taln-2009-court-017-key" class="mots_cles">
							<b>Mots clés : </b> gestion du dialogue Homme-machine, dialogue oral arabe, modèle de tâche, modèle de dialogue
							</p>

					</div>
					

					<div class="article">

						<b>Lionel Clément, Kim Gerdes, Renaud Marlet</b>


						<br/>

							<i>Grammaires d’erreur – correction grammaticale avec analyse profonde et proposition de corrections minimales</i> <br/>

						<a href="actes/taln-2009-court-018.pdf">taln-2009-court-018</a> 
						<a href="bibtex/taln-2009-court-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-018-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-018-key');">mots clés</a> <br/>

							<p id="taln-2009-court-018-abs" class="resume">
							<b>Résumé : </b> Nous présentons un système de correction grammatical ouvert, basé sur des analyses syntaxiques profondes. La spécification grammaticale est une grammaire hors-contexte équipée de structures de traits plates. Après une analyse en forêt partagée où les contraintes d’accord de traits sont relâchées, la détection d’erreur minimise globalement les corrections à effectuer et des phrases alternatives correctes sont automatiquement proposées.
							</p>

							<p id="taln-2009-court-018-key" class="mots_cles">
							<b>Mots clés : </b> Correcteur grammatical, analyse syntaxique, forêt partagée
							</p>

					</div>
					

					<div class="article">

						<b>André Bittar, Laurence Danlos</b>


						<br/>

							<i>Intégration des constructions à verbe support dans TimeML</i> <br/>

						<a href="actes/taln-2009-court-019.pdf">taln-2009-court-019</a> 
						<a href="bibtex/taln-2009-court-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-019-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-019-key');">mots clés</a> <br/>

							<p id="taln-2009-court-019-abs" class="resume">
							<b>Résumé : </b> Le langage TimeML a été conçu pour l’annotation des informations temporelles dans les textes, notamment les événements, les expressions de temps et les relations entre les deux. Des consignes d’annotation générales ont été élaborées afin de guider l’annotateur dans cette tâche, mais certains phénomènes linguistiques restent à traiter en détail. Un problème commun dans les tâches de TAL, que ce soit en traduction, en génération ou en compréhension, est celui de l’encodage des constructions à verbe support. Relativement peu d’attention a été portée, jusqu’à maintenant, sur ce problème dans le cadre du langage TimeML. Dans cet article, nous proposons des consignes d’annotation pour les constructions à verbe support.
							</p>

							<p id="taln-2009-court-019-key" class="mots_cles">
							<b>Mots clés : </b> TimeML, verbes support, discours, sémantique
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Sagot, Elsa Tolone</b>


						<br/>

							<i>Intégrer les tables du Lexique-Grammaire à un analyseur syntaxique robuste à grande échelle</i> <br/>

						<a href="actes/taln-2009-court-020.pdf">taln-2009-court-020</a> 
						<a href="bibtex/taln-2009-court-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-020-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-020-key');">mots clés</a> <br/>

							<p id="taln-2009-court-020-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous montrons comment nous avons converti les tables du Lexique-Grammaire en un format TAL, celui du lexique Lefff, permettant ainsi son intégration dans l’analyseur syntaxique FRMG. Nous présentons les fondements linguistiques de ce processus de conversion et le lexique obtenu. Nous validons le lexique obtenu en évaluant l’analyseur syntaxique FRMG sur le corpus de référence de la campagne EASy selon qu’il utilise les entrées verbales du Lefff ou celles des tables des verbes du Lexique-Grammaire ainsi converties.
							</p>

							<p id="taln-2009-court-020-key" class="mots_cles">
							<b>Mots clés : </b> Lexiques syntaxiques, Lexique-Grammaire, analyse syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>Cédric Messiant, Takuya Nakamura, Stavroula Voyatzi</b>


						<br/>

							<i>La complémentarité des approches manuelle et automatique en acquisition lexicale</i> <br/>

						<a href="actes/taln-2009-court-021.pdf">taln-2009-court-021</a> 
						<a href="bibtex/taln-2009-court-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-021-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-021-key');">mots clés</a> <br/>

							<p id="taln-2009-court-021-abs" class="resume">
							<b>Résumé : </b> Les ressources lexicales sont essentielles pour obtenir des systèmes de traitement des langues performants. Ces ressources peuvent être soit construites à la main, soit acquises automatiquement à partir de gros corpus. Dans cet article, nous montrons la complémentarité de ces deux approches. Pour ce faire, nous utilisons l’exemple de la sous-catégorisation verbale en comparant un lexique acquis par des méthodes automatiques (LexSchem) avec un lexique construit manuellement (Le Lexique-Grammaire). Nous montrons que les informations acquises par ces deux méthodes sont bien distinctes et qu’elles peuvent s’enrichir mutuellement.
							</p>

							<p id="taln-2009-court-021-key" class="mots_cles">
							<b>Mots clés : </b> verbe, syntaxe, lexique, sous-catégorisation
							</p>

					</div>
					

					<div class="article">

						<b>Dominique Laurent, Sophie Nègre, Patrick Séguéla</b>


						<br/>

							<i>L&#39;analyseur syntaxique Cordial dans Passage</i> <br/>

						<a href="actes/taln-2009-court-022.pdf">taln-2009-court-022</a> 
						<a href="bibtex/taln-2009-court-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-022-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-022-key');">mots clés</a> <br/>

							<p id="taln-2009-court-022-abs" class="resume">
							<b>Résumé : </b> Cordial est un analyseur syntaxique et sémantique développé par la société Synapse Développement. Largement utilisé par les laboratoires de TALN depuis plus de dix ans, cet analyseur participe à la campagne Passage (&#34;Produire des Annotations Syntaxiques à Grande Échelle&#34;). Comment fonctionne cet analyseur ? Quels résultats a-t-il obtenu lors de la première phase d&#39;évaluation de cette campagne ? Au-delà de ces questions, cet article montre en quoi les contraintes industrielles façonnent les outils d&#39;analyse automatique du langage naturel.
							</p>

							<p id="taln-2009-court-022-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique, analyse sémantique, évaluation, Passage
							</p>

					</div>
					

					<div class="article">

						<b>Antoine Widlöcher, Yann Mathet</b>


						<br/>

							<i>La plate-forme Glozz : environnement d’annotation et d’exploration de corpus</i> <br/>

						<a href="actes/taln-2009-court-023.pdf">taln-2009-court-023</a> 
						<a href="bibtex/taln-2009-court-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-023-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-023-key');">mots clés</a> <br/>

							<p id="taln-2009-court-023-abs" class="resume">
							<b>Résumé : </b> La nécessité d’une interaction systématique entre modèles, traitements et corpus impose la disponibilité d’annotations de référence auxquelles modèles et traitements pourront être confrontés. Or l’établissement de telles annotations requiert un cadre formel permettant la représentation d’objets linguistiques variés, et des applications permettant à l’annotateur de localiser sur corpus et de caractériser les occurrences des phénomènes observés. Si différents outils d’annotation ont vu le jour, ils demeurent souvent fortement liés à un modèle théorique et à des objets linguistiques particuliers, et ne permettent que marginalement d’explorer certaines structures plus récemment appréhendées expérimentalement, notamment à granularité élevée et en matière d’analyse du discours. La plate-forme Glozz répond à ces différentes contraintes et propose un environnement d’exploration de corpus et d’annotation fortement configurable et non limité a priori au contexte discursif dans lequel elle a initialement vu le jour.
							</p>

							<p id="taln-2009-court-023-key" class="mots_cles">
							<b>Mots clés : </b> Linguistique de corpus, Annotation, Plate-forme logicielle
							</p>

					</div>
					

					<div class="article">

						<b>Pierre-André Buvet, Emmanuel Cartier, Fabrice Issac, Yassine Madiouni, Michel Mathieu-Colas, Salah Mejri</b>


						<br/>

							<i>Morfetik, ressource lexicale pour le TAL</i> <br/>

						<a href="actes/taln-2009-court-024.pdf">taln-2009-court-024</a> 
						<a href="bibtex/taln-2009-court-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-024-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-024-key');">mots clés</a> <br/>

							<p id="taln-2009-court-024-abs" class="resume">
							<b>Résumé : </b> Le traitement automatique des langues exige un recensement lexical aussi rigoureux que possible. Dans ce but, nous avons développé un dictionnaire morphologique du français, conçu comme le point de départ d’un système modulaire (Morfetik) incluant un moteur de flexion, des interfaces de consultation et d’interrogation et des outils d’exploitation. Nous présentons dans cet article, après une brève description du dictionnaire de base (lexique des mots simples), quelques-uns des outils informatiques liés à cette ressource : un moteur de recherche des lemmes et des formes fléchies ; un moteur de flexion XML et MySQL ; des outils NLP permettant d’exploiter le dictionnaire ainsi généré ; nous présentons notamment un analyseur linguistique développé dans notre laboratoire. Nous comparons dans une dernière partie Morfetik avec d’autres ressources analogues du français : Morphalou, Lexique3 et le DELAF.
							</p>

							<p id="taln-2009-court-024-key" class="mots_cles">
							<b>Mots clés : </b> dictionnaire morphologique du français, CMLF, analyse linguistique des textes
							</p>

					</div>
					

					<div class="article">

						<b>Fabien Poulard, Stergos Afantenos, Nicolas Hernandez</b>


						<br/>

							<i>Nouvelles considérations pour la détection de réutilisation de texte</i> <br/>

						<a href="actes/taln-2009-court-025.pdf">taln-2009-court-025</a> 
						<a href="bibtex/taln-2009-court-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-025-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-025-key');">mots clés</a> <br/>

							<p id="taln-2009-court-025-abs" class="resume">
							<b>Résumé : </b> Dans cet article nous nous intéressons au problème de la détection de réutilisation de texte. Plus particulièrement, étant donné un document original et un ensemble de documents candidats — thématiquement similaires au premier — nous cherchons à classer ceux qui sont dérivés du document original et ceux qui ne le sont pas. Nous abordons le problème selon deux approches : dans la première, nous nous intéressons aux similarités discursives entre les documents, dans la seconde au recouvrement de n-grams hapax. Nous présentons le résultat d’expérimentations menées sur un corpus de presse francophone construit dans le cadre du projet ANR PIITHIE.
							</p>

							<p id="taln-2009-court-025-key" class="mots_cles">
							<b>Mots clés : </b> réutilisation de texte, recouvrement de n-grams hapax, similarités discursives, corpus journalistique francophone
							</p>

					</div>
					

					<div class="article">

						<b>Anne Garcia-Fernandez, Sophie Rosset, Anne Vilnat</b>


						<br/>

							<i>Collecte et analyses de réponses naturelles pour les systèmes de questions-réponses</i> <br/>

						<a href="actes/taln-2009-court-026.pdf">taln-2009-court-026</a> 
						<a href="bibtex/taln-2009-court-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-026-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-026-key');">mots clés</a> <br/>

							<p id="taln-2009-court-026-abs" class="resume">
							<b>Résumé : </b> Notre travail se situe dans le cadre des systèmes de réponse a une question et à pour but de fournir une réponse en langue naturelle aux questions posées en langue naturelle. Cet article présente une expérience permettant d’analyser les réponses de locuteurs du français à des questions que nous leur posons. L’expérience se déroule à l’écrit comme à l’oral et propose à des locuteurs français des questions relevant de différents types sémantiques et syntaxiques. Nous mettons en valeur une large variabilité dans les formes de réponses possibles en langue française. D’autre part nous établissons un certain nombre de liens entre formulation de question et formulation de réponse. Nous proposons d’autre part une comparaison des réponses selon la modalité oral / écrit. Ces résultats peuvent être intégrés à des systèmes existants pour produire une réponse en langue naturelle de façon dynamique.
							</p>

							<p id="taln-2009-court-026-key" class="mots_cles">
							<b>Mots clés : </b> systèmes de réponse à une question, expérience, variations linguistiques, réponse en langue naturelle
							</p>

					</div>
					

					<div class="article">

						<b>Vincent Claveau</b>


						<br/>

							<i>La /fOnetizasjc/ comme un problème de translittération</i> <br/>

						<a href="actes/taln-2009-court-027.pdf">taln-2009-court-027</a> 
						<a href="bibtex/taln-2009-court-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-027-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-027-key');">mots clés</a> <br/>

							<p id="taln-2009-court-027-abs" class="resume">
							<b>Résumé : </b> La phonétisation est une étape essentielle pour le traitement de l’oral. Dans cet article, nous décrivons un système automatique de phonétisation de mots isolés qui est simple, portable et performant. Il repose sur une approche par apprentissage ; le système est donc construit à partir d’exemples de mots et de leur représentation phonétique. Nous utilisons pour cela une technique d’inférence de règles de réécriture initialement développée pour la translittération et la traduction. Pour évaluer les performances de notre approche, nous avons utilisé plusieurs jeux de données couvrant différentes langues et divers alphabets phonétiques, tirés du challenge Pascal Pronalsyl. Les très bons résultats obtenus égalent ou dépassent ceux des meilleurs systèmes de l’état de l’art.
							</p>

							<p id="taln-2009-court-027-key" class="mots_cles">
							<b>Mots clés : </b> Phonétisation, phonémisation, inférence de règles de réécriture, challenge Pronalsyl, conversion graphème-phonème, translittération
							</p>

					</div>
					

					<div class="article">

						<b>Josep Maria Crego, Aurélien Max, François Yvon</b>


						<br/>

							<i>Plusieurs langues (bien choisies) valent mieux qu’une : traduction statistique multi-source par renforcement lexical</i> <br/>

						<a href="actes/taln-2009-court-028.pdf">taln-2009-court-028</a> 
						<a href="bibtex/taln-2009-court-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-028-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-028-key');">mots clés</a> <br/>

							<p id="taln-2009-court-028-abs" class="resume">
							<b>Résumé : </b> Les systèmes de traduction statistiques intègrent différents types de modèles dont les prédictions sont combinées, lors du décodage, afin de produire les meilleures traductions possibles. Traduire correctement des mots polysémiques, comme, par exemple, le mot avocat du français vers l’anglais (lawyer ou avocado), requiert l’utilisation de modèles supplémentaires, dont l’estimation et l’intégration s’avèrent complexes. Une alternative consiste à tirer parti de l’observation selon laquelle les ambiguïtés liées à la polysémie ne sont pas les mêmes selon les langues source considérées. Si l’on dispose, par exemple, d’une traduction vers l’espagnol dans laquelle avocat a été traduit par aguacate, alors la traduction de ce mot vers l’anglais n’est plus ambiguë. Ainsi, la connaissance d’une traduction français!espagnol permet de renforcer la sélection de la traduction avocado pour le système français!anglais. Dans cet article, nous proposons d’utiliser des documents en plusieurs langues pour renforcer les choix lexicaux effectués par un système de traduction automatique. En particulier, nous montrons une amélioration des performances sur plusieurs métriques lorsque les traductions auxiliaires utilisées sont obtenues manuellement.
							</p>

							<p id="taln-2009-court-028-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique statistique, désambiguïsation lexicale, réévaluation de listes d’hypothèses
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Leon Bouraoui, Philippe Boissière, Mustapha Mojahid, Nadine Vigouroux, Aurélie Lagarrigue, Frédéric Vella, Jean-Luc Nespoulous</b>


						<br/>

							<i>Problématique d&#39;analyse et de modélisation des erreurs en production écrite. Approche interdisciplinaire</i> <br/>

						<a href="actes/taln-2009-court-029.pdf">taln-2009-court-029</a> 
						<a href="bibtex/taln-2009-court-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-029-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-029-key');">mots clés</a> <br/>

							<p id="taln-2009-court-029-abs" class="resume">
							<b>Résumé : </b> L&#39;objectif du travail présenté ici est la modélisation de la détection et la correction des erreurs orthographiques et dactylographiques, plus particulièrement dans le contexte des handicaps langagiers. Le travail est fondé sur une analyse fine des erreurs d’écriture commises. La première partie de cet article est consacrée à une description précise de la faute. Dans la seconde partie, nous analysons l’erreur (1) en déterminant la nature de la faute (typographique, orthographique, ou grammaticale) et (2) en explicitant sa conséquence sur le niveau de perturbation linguistique (phonologique, orthographique, morphologique ou syntaxique). Il résulte de ce travail un modèle général des erreurs (une grille) que nous présenterons, ainsi que les résultats statistiques correspondants. Enfin, nous montrerons sur des exemples, l’utilité de l’apport de cette grille, en soumettant ces types de fautes à quelques correcteurs. Nous envisageons également les implications informatiques de ce travail.
							</p>

							<p id="taln-2009-court-029-key" class="mots_cles">
							<b>Mots clés : </b> Typologie et analyse d’erreurs textuelles, assistance à la saisie de textes
							</p>

					</div>
					

					<div class="article">

						<b>Rémy Kessler, Nicolas Béchet, Juan-Manuel Torres-Moreno, Mathieu Roche, Marc El-Bèze</b>


						<br/>

							<i>Profilage de candidatures assisté par Relevance Feedback</i> <br/>

						<a href="actes/taln-2009-court-030.pdf">taln-2009-court-030</a> 
						<a href="bibtex/taln-2009-court-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-030-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-030-key');">mots clés</a> <br/>

							<p id="taln-2009-court-030-abs" class="resume">
							<b>Résumé : </b> Le marché d’offres d’emploi et des candidatures sur Internet connaît une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de texte libre) qu’il n’est plus possible de traiter manuellement. Une analyse et catégorisation assistées nous semble pertinente en réponse à cette problématique. Nous proposons E-Gen, système qui a pour but l’analyse et catégorisation assistés d’offres d’emploi et des réponses des candidats. Dans cet article nous présentons plusieurs stratégies, reposant sur les modèles vectoriel et probabiliste, afin de résoudre la problématique du profilage des candidatures en fonction d’une offre précise. Nous avons évalué une palette de mesures de similarité afin d’effectuer un classement pertinent des candidatures au moyen des courbes ROC. L’utilisation d’une forme de relevance feedback a permis de surpasser nos résultats sur ce problème difficile et sujet à une grande subjectivité.
							</p>

							<p id="taln-2009-court-030-key" class="mots_cles">
							<b>Mots clés : </b> Classification, recherche d’information, Ressources humaines, modèle probabiliste, mesures de similarité, Relevance Feedback
							</p>

					</div>
					

					<div class="article">

						<b>Thierry Hamon, Natalia Grabar</b>


						<br/>

							<i>Profilage sémantique endogène des relations de synonymie au sein de Gene Ontology</i> <br/>

						<a href="actes/taln-2009-court-031.pdf">taln-2009-court-031</a> 
						<a href="bibtex/taln-2009-court-031.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-031-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-031-key');">mots clés</a> <br/>

							<p id="taln-2009-court-031-abs" class="resume">
							<b>Résumé : </b> Le calcul de la similarité sémantique entre les termes repose sur l’existence et l’utilisation de ressources sémantiques. Cependant de telles ressources, qui proposent des équivalences entre entités, souvent des relations de synonymie, doivent elles-mêmes être d’abord analysées afin de définir des zones de fiabilité où la similarité sémantique est plus forte. Nous proposons une méthode d’acquisition de synonymes élémentaires grâce à l’exploitation des terminologies structurées au travers l’analyse de la structure syntaxique des termes complexes et de leur compositionnalité. Les synonymes acquis sont ensuite profilés grâce aux indicateurs endogènes inférés automatiquement à partir de ces mêmes terminologies (d’autres types de relations, inclusions lexicales, productivité, forme des composantes connexes). Dans le domaine biomédical, il existe de nombreuses terminologies structurées qui peuvent être exploitées pour la constitution de ressources sémantiques. Le travail présenté ici exploite une de ces terminologies, Gene Ontology.
							</p>

							<p id="taln-2009-court-031-key" class="mots_cles">
							<b>Mots clés : </b> Terminologie, distance sémantique, relations sémantiques, synonymie
							</p>

					</div>
					

					<div class="article">

						<b>Fériel Ben Fraj, Chiraz Ben Othmane Zribi, Mohamed Ben Ahmed</b>


						<br/>

							<i>Quels attributs discriminants pour une analyse syntaxique par classification de textes en langue arabe ?</i> <br/>

						<a href="actes/taln-2009-court-032.pdf">taln-2009-court-032</a> 
						<a href="bibtex/taln-2009-court-032.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-032-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-032-key');">mots clés</a> <br/>

							<p id="taln-2009-court-032-abs" class="resume">
							<b>Résumé : </b> Dans le cadre dune approche déterministe et incrémentale danalyse syntaxique par classification de textes en langue arabe, nous avons prévu de prendre en considération un ensemble varié dattributs discriminants afin de mieux assister la procédure de classification dans ses prises de décisions à travers les différentes étapes danalyse. Ainsi, en plus des attributs morpho-syntaxiques du mot en cours danalyse et des informations contextuelles des mots lavoisinant, nous avons ajouté des informations compositionnelles extraites du fragment de larbre syntaxique déjà construit lors de létape précédente de lanalyse en cours. Ce papier présente notre approche danalyse syntaxique par classification et vise lexposition dune justification expérimentale de lapport de chaque type dattributs discriminants et spécialement ceux compositionnels dans ladite analyse syntaxique.
							</p>

							<p id="taln-2009-court-032-key" class="mots_cles">
							<b>Mots clés : </b> analyse syntaxique incrémentale, langue arabe, apprentissage automatique, classification, attributs discriminants
							</p>

					</div>
					

					<div class="article">

						<b>Richard Beaufort, Anne Dister, Hubert Naets, Kévin Macé, Cédrick Fairon</b>


						<br/>

							<i>Recto /Verso Un système de conversion automatique ancienne / nouvelle orthographe à visée linguistique et didactique</i> <br/>

						<a href="actes/taln-2009-court-033.pdf">taln-2009-court-033</a> 
						<a href="bibtex/taln-2009-court-033.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-033-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-033-key');">mots clés</a> <br/>

							<p id="taln-2009-court-033-abs" class="resume">
							<b>Résumé : </b> Cet article présente Recto /Verso, un système de traitement automatique du langage dédié à l’application des rectifications orthographiques de 1990. Ce système a été développé dans le cadre de la campagne de sensibilisation réalisée en mars dernier par le Service et le Conseil de la langue française et de la politique linguistique de la Communauté française de Belgique. Nous commençons par rappeler les motivations et le contenu de la réforme proposée, et faisons le point sur les principes didactiques retenus dans le cadre de la campagne. La plus grande partie de l’article est ensuite consacrée à l’implémentation du système. Nous terminons enfin par une première analyse de l’impact de la campagne sur les utilisateurs.
							</p>

							<p id="taln-2009-court-033-key" class="mots_cles">
							<b>Mots clés : </b> Rectifications orthographiques de 1990, conversion ancienne / nouvelle orthographe, objectifs didactiques, machines à états finis
							</p>

					</div>
					

					<div class="article">

						<b>Véronique Malaisé, Luit Gazendam, Willemijn Heeren, Roeland Ordelman, Hennie Brugman</b>


						<br/>

							<i>Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs</i> <br/>

						<a href="actes/taln-2009-court-034.pdf">taln-2009-court-034</a> 
						<a href="bibtex/taln-2009-court-034.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-034-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-034-key');">mots clés</a> <br/>

							<p id="taln-2009-court-034-abs" class="resume">
							<b>Résumé : </b> L’accès aux documents multimédia, dans une archive audiovisuelle, dépend en grande partie de la quantité et de la qualité des métadonnées attachées aux documents, notamment la description de leur contenu. Cependant, l’annotation manuelle des collections est astreignante pour le personnel. De nombreuses archives évoluent vers des méthodes d’annotation (semi-)automatiques pour la création et/ou l’amélioration des métadonnées. Le project CATCH-CHOICE, fondé par NWO, s’est penché sur l’extraction de mots clés à partir de resources textuelles liées aux programmes TV destinés à être archivés (péritextes), en collaboration avec les archives audiovisuelles néerlandaises, Sound and Vision. Cet article se penche sur la question de l’adéquation des transcriptions de Reconnaissance Automatique de la Parole développés dans le projet CATCH-CHoral pour la génération automatique de mots-clés : les mots-clés extraits de ces ressources sont évalués par rapport à des annotations manuelles et par rapport à des mots-clés générés à partir de péritextes décrivant les programmes télévisuels.
							</p>

							<p id="taln-2009-court-034-key" class="mots_cles">
							<b>Mots clés : </b> Extraction de mots clés, Reconnaissance Automatique de la Parole, Documents Audiovisuels
							</p>

					</div>
					

					<div class="article">

						<b>Florian Boudin, Juan-Manuel Torres-Moreno</b>


						<br/>

							<i>Résumé automatique multi-document et indépendance de la langue : une première évaluation en français</i> <br/>

						<a href="actes/taln-2009-court-035.pdf">taln-2009-court-035</a> 
						<a href="bibtex/taln-2009-court-035.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-035-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-035-key');">mots clés</a> <br/>

							<p id="taln-2009-court-035-abs" class="resume">
							<b>Résumé : </b> Le résumé automatique de texte est une problématique difficile, fortement dépendante de la langue et qui peut nécessiter un ensemble de données d’apprentissage conséquent. L’approche par extraction peut aider à surmonter ces difficultés. (Mihalcea, 2004) a démontré l’intérêt des approches à base de graphes pour l’extraction de segments de texte importants. Dans cette étude, nous décrivons une approche indépendante de la langue pour la problématique du résumé automatique multi-documents. L’originalité de notre méthode repose sur l’utilisation d’une mesure de similarité permettant le rapprochement de segments morphologiquement proches. De plus, c’est à notre connaissance la première fois que l’évaluation d’une approche de résumé automatique multi-document est conduite sur des textes en français.
							</p>

							<p id="taln-2009-court-035-key" class="mots_cles">
							<b>Mots clés : </b> Résumé automatique de texte, Approches à base de graphes, Extraction d’information
							</p>

					</div>
					

					<div class="article">

						<b>Laurent Bozzi, Philippe Suignard, Claire Waast-Richard</b>


						<br/>

							<i>Segmentation et classification non supervisée de conversations téléphoniques automatiquement retranscrites</i> <br/>

						<a href="actes/taln-2009-court-036.pdf">taln-2009-court-036</a> 
						<a href="bibtex/taln-2009-court-036.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-036-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-036-key');">mots clés</a> <br/>

							<p id="taln-2009-court-036-abs" class="resume">
							<b>Résumé : </b> Cette étude porte sur l’analyse de conversations entre des clients et des téléconseillers d’EDF. Elle propose une chaîne de traitements permettant d’automatiser la détection des sujets abordés dans chaque conversation. L’aspect multi-thématique des conversations nous incite à trouver une unité de documents entre le simple tour de parole et la conversation entière. Cette démarche enchaîne une étape de segmentation de la conversation en thèmes homogènes basée sur la notion de cohésion lexicale, puis une étape de text-mining comportant une analyse linguistique enrichie d’un vocabulaire métier spécifique à EDF, et enfin une classification non supervisée des segments obtenus. Plusieurs algorithmes de segmentation ont été évalués sur un corpus de test, segmenté et annoté manuellement : le plus « proche » de la segmentation de référence est C99. Cette démarche, appliquée à la fois sur un corpus de conversations transcrites à la main, et sur les mêmes conversations décodées par un moteur de reconnaissance vocale, aboutit quasiment à l’obtention des 20 mêmes classes thématiques.
							</p>

							<p id="taln-2009-court-036-key" class="mots_cles">
							<b>Mots clés : </b> audio-mining, text mining, segmentation, classification, catégorisation, reconnaissance vocale, données textuelles, conversations téléphoniques, centre d’appel
							</p>

					</div>
					

					<div class="article">

						<b>Sopheap Seng, Laurent Besacier, Brigitte Bigi, Eric Castelli</b>


						<br/>

							<i>Segmentation multiple d’un flux de données textuelles pour la modélisation statistique du langage</i> <br/>

						<a href="actes/taln-2009-court-037.pdf">taln-2009-court-037</a> 
						<a href="bibtex/taln-2009-court-037.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-037-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-037-key');">mots clés</a> <br/>

							<p id="taln-2009-court-037-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous traitons du problème de la modélisation statistique du langage pour les langues peu dotées et sans segmentation entre les mots. Tandis que le manque de données textuelles a un impact sur la performance des modèles, les erreurs introduites par la segmentation automatique peuvent rendre ces données encore moins exploitables. Pour exploiter au mieux les données textuelles, nous proposons une méthode qui effectue des segmentations multiples sur le corpus d’apprentissage au lieu d’une segmentation unique. Cette méthode basée sur les automates d’état finis permet de retrouver les n-grammes non trouvés par la segmentation unique et de générer des nouveaux n-grammes pour l’apprentissage de modèle du langage. L’application de cette approche pour l’apprentissage des modèles de langage pour les systèmes de reconnaissance automatique de la parole en langue khmère et vietnamienne s’est montrée plus performante que la méthode par segmentation unique, à base de règles.
							</p>

							<p id="taln-2009-court-037-key" class="mots_cles">
							<b>Mots clés : </b> segmentation multiple, langue non segmentée, modélisation statistique du langage
							</p>

					</div>
					

					<div class="article">

						<b>Mathieu Lafourcade, Alain Joubert, Stéphane Riou</b>


						<br/>

							<i>Sens et usages d’un terme dans un réseau lexical évolutif</i> <br/>

						<a href="actes/taln-2009-court-038.pdf">taln-2009-court-038</a> 
						<a href="bibtex/taln-2009-court-038.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-038-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-038-key');">mots clés</a> <br/>

							<p id="taln-2009-court-038-abs" class="resume">
							<b>Résumé : </b> L’obtention d’informations lexicales fiables est un enjeu primordial en TALN, mais cette collecte peut s’avérer difficile. L’approche présentée ici vise à pallier les écueils de cette difficulté en faisant participer un grand nombre de personnes à un projet contributif via des jeux accessibles sur le web. Ainsi, les joueurs vont construire le réseau lexical, en fournissant de plusieurs manières possibles des associations de termes à partir d&#39;un terme cible et d&#39;une consigne correspondant à une relation typée. Le réseau lexical ainsi produit est de grande taille et comporte une trentaine de types de relations. A partir de cette ressource, nous abordons la question de la détermination des différents sens et usages d’un terme. Ceci est réalisé en analysant les relations entre ce terme et ses voisins immédiats dans le réseau et en calculant des cliques ou des quasi-cliques. Ceci nous amène naturellement à introduire la notion de similarité entre cliques, que nous interprétons comme une mesure de similarité entre ces différents sens et usages. Nous pouvons ainsi construire pour un terme son arbre des usages, qui est une structure de données exploitable en désambiguïsation de sens. Nous présentons quelques résultats obtenus en soulignant leur caractère évolutif.
							</p>

							<p id="taln-2009-court-038-key" class="mots_cles">
							<b>Mots clés : </b> Traitement Automatique du Langage Naturel, réseau lexical évolutif, relations typées pondérées, similarité entre sens et usages, arbre des usages
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Leon Bouraoui, Nadine Vigouroux</b>


						<br/>

							<i>Traitement automatique de disfluences dans un corpus linguistiquement contraint</i> <br/>

						<a href="actes/taln-2009-court-039.pdf">taln-2009-court-039</a> 
						<a href="bibtex/taln-2009-court-039.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-039-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-039-key');">mots clés</a> <br/>

							<p id="taln-2009-court-039-abs" class="resume">
							<b>Résumé : </b> Cet article présente un travail de modélisation et de détection des phénomènes de disfluence. Une des spécificité de ce travail est le cadre dans lequel il se situe: le contrôle de la navigation aérienne. Nous montrons ce que ce cadre particulier implique certains choix concernant la modélisation et l&#39;implémentation. Ainsi, nous constatons que la modélisation fondée sur la syntaxe, souvent utilisée dans le traitement des langues naturelles, n&#39;est pas la plus appropriée ici. Nous expliquons la façon dont l&#39;implémentation a été réalisée. Dans une dernière partie, nous présentons la validation de ce dispositif, effectuée sur 400 énoncés.
							</p>

							<p id="taln-2009-court-039-key" class="mots_cles">
							<b>Mots clés : </b> Dialogue oral spontané, Analyse linguistique de corpus, Compréhension robuste, Contrôle Aérien, Phraséologie, Disfluences, Modèles de langage, Traitement Automatique du Langage Naturel
							</p>

					</div>
					

					<div class="article">

						<b>Laura Kallmeyer, Wolfgang Maier, Yannick Parmentier</b>


						<br/>

							<i>Un Algorithme d’Analyse de Type Earley pour Grammaires à Concaténation d’Intervalles</i> <br/>

						<a href="actes/taln-2009-court-040.pdf">taln-2009-court-040</a> 
						<a href="bibtex/taln-2009-court-040.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-040-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-040-key');">mots clés</a> <br/>

							<p id="taln-2009-court-040-abs" class="resume">
							<b>Résumé : </b> Nous présentons ici différents algorithmes d’analyse pour grammaires à concaténation d’intervalles (Range Concatenation Grammar, RCG), dont un nouvel algorithme de type Earley, dans le paradigme de l’analyse déductive. Notre travail est motivé par l’intérêt porté récemment à ce type de grammaire, et comble un manque dans la littérature existante.
							</p>

							<p id="taln-2009-court-040-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique déductive, grammaires à concaténation d’intervalles
							</p>

					</div>
					

					<div class="article">

						<b>Jérôme Lehuen, Thierry Lemeunier</b>


						<br/>

							<i>Un Analyseur Sémantique pour le DHM Modélisation – Réalisation – Évaluation</i> <br/>

						<a href="actes/taln-2009-court-041.pdf">taln-2009-court-041</a> 
						<a href="bibtex/taln-2009-court-041.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-041-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-041-key');">mots clés</a> <br/>

							<p id="taln-2009-court-041-abs" class="resume">
							<b>Résumé : </b> Cet article décrit un modèle de langage dédié au dialogue homme-machine, son implémentation en CLIPS, ainsi qu’une évaluation comparative. Notre problématique n’est ni d’analyser des grands corpus, ni de proposer une grammaire à grande couverture. Notre objectif est de produire des représentations sémantiques utilisables par un module de dialogue à partir d’énoncés oraux courts, le plus souvent agrammaticaux. Une démarche pragmatique nous a conduit à fonder l’analyse sur des principes simples mais efficaces dans le cadre que nous nous sommes fixé. L’algorithme retenu s’inspire de l’analyse tabulaire. L’évaluation que nous présentons repose sur le corpus MEDIA qui a fait l’objet d’une annotation sémantique manuelle de référence pour une campagne d’évaluation d’analyseurs sémantiques pour le dialogue. Les résultats que nous obtenons place notre analyseur dans le trio de tête des systèmes évalués lors de la campagne de juin 2005, et nous confortent dans nos choix d’algorithme et de représentation des connaissances.
							</p>

							<p id="taln-2009-court-041-key" class="mots_cles">
							<b>Mots clés : </b> Analyse sémantique tabulaire, contexte dialogique, évaluation
							</p>

					</div>
					

					<div class="article">

						<b>Silvia Fernández Sabido, Juan-Manuel Torres-Moreno</b>


						<br/>

							<i>Une approche exploratoire de compression automatique de phrases basée sur des critères thermodynamiques</i> <br/>

						<a href="actes/taln-2009-court-042.pdf">taln-2009-court-042</a> 
						<a href="bibtex/taln-2009-court-042.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-042-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-042-key');">mots clés</a> <br/>

							<p id="taln-2009-court-042-abs" class="resume">
							<b>Résumé : </b> Nous présentons une approche exploratoire basée sur des notions thermodynamiques de la Physique statistique pour la compression de phrases. Nous décrivons le modèle magnétique des verres de spins, adapté à notre conception de la problématique. Des simulations Métropolis Monte-Carlo permettent d’introduire des fluctuations thermiques pour piloter la compression. Des comparaisons intéressantes de notre méthode ont été réalisées sur un corpus en français.
							</p>

							<p id="taln-2009-court-042-key" class="mots_cles">
							<b>Mots clés : </b> Compression de phrases, Résumé automatique, Résumé par extraction, Enertex, Mécanique statistique
							</p>

					</div>
					

					<div class="article">

						<b>Sebastián Peña Saldarriaga, Emmanuel Morin, Christian Viard-Gaudin</b>


						<br/>

							<i>Un nouveau schéma de pondération pour la catégorisation de documents manuscrits</i> <br/>

						<a href="actes/taln-2009-court-043.pdf">taln-2009-court-043</a> 
						<a href="bibtex/taln-2009-court-043.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-043-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-043-key');">mots clés</a> <br/>

							<p id="taln-2009-court-043-abs" class="resume">
							<b>Résumé : </b> Les schémas de pondération utilisés habituellement en catégorisation de textes, et plus généralement en recherche d’information (RI), ne sont pas adaptés à l’utilisation de données liées à des textes issus d’un processus de reconnaissance de l’écriture. En particulier, les candidats-mot à la reconnaissance ne pourraient être exploités sans introduire de fausses occurrences de termes dans le document. Dans cet article nous présentons un nouveau schéma de pondération permettant d’exploiter les listes de candidats-mot. Il permet d’estimer le pouvoir discriminant d’un terme en fonction de la probabilité a posteriori d’un candidat-mot dans une liste de candidats. Les résultats montrent que le taux de classification de documents fortement dégradés peut être amélioré en utilisant le schéma proposé.
							</p>

							<p id="taln-2009-court-043-key" class="mots_cles">
							<b>Mots clés : </b> Catégorisation de textes, écriture en-ligne, n-best candidats, pondération
							</p>

					</div>
					

					<div class="article">

						<b>Yves Scherrer</b>


						<br/>

							<i>Un système de traduction automatique paramétré par des atlas dialectologiques</i> <br/>

						<a href="actes/taln-2009-court-044.pdf">taln-2009-court-044</a> 
						<a href="bibtex/taln-2009-court-044.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-044-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-044-key');">mots clés</a> <br/>

							<p id="taln-2009-court-044-abs" class="resume">
							<b>Résumé : </b> Contrairement à la plupart des systèmes de traitement du langage, qui s’appliquent à des langues écrites et standardisées, nous présentons ici un système de traduction automatique qui prend en compte les spécificités des dialectes. En général, les dialectes se caractérisent par une variation continue et un manque de données textuelles en qualité et quantité suffisantes. En même temps, du moins en Europe, les dialectologues ont étudié en détail les caractéristiques linguistiques des dialectes. Nous soutenons que des données provenant d’atlas dialectologiques peuvent être utilisées pour paramétrer un système de traduction automatique. Nous illustrons cette idée avec le prototype d’un système de traduction basé sur des règles, qui traduit de l’allemand standard vers les différents dialectes de Suisse allemande. Quelques exemples linguistiquement motivés serviront à exposer l’architecture de ce système.
							</p>

							<p id="taln-2009-court-044-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique, dialectes, langues proches, langues germaniques
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Cédric Chappelier, Emmanuel Eckard</b>


						<br/>

							<i>Utilisation de PLSI en recherche d’information Représentation des requêtes</i> <br/>

						<a href="actes/taln-2009-court-045.pdf">taln-2009-court-045</a> 
						<a href="bibtex/taln-2009-court-045.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-045-abs');">résumé</a>

							<p id="taln-2009-court-045-abs" class="resume">
							<b>Résumé : </b> Le modèle PLSI (« Probabilistic Latent Semantic Indexing ») offre une approche de l’indexation de documents fondée sur des modèles probabilistes de catégories sémantiques latentes et a conduit à des applications dans différents domaines. Toutefois, ce modèle rend impossible le traitement de documents inconnus au moment de l’apprentissage, problème particulièrement sensible pour la représentation des requêtes dans le cadre de la recherche d’information. Une méthode, dite de « folding-in », permet dans une certaine mesure de contourner ce problème, mais présente des faiblesses. Cet article introduit nouvelle une mesure de similarité document-requête pour PLSI, fondée sur lesmodèles de langue, où le problème du « folding-in » ne se pose pas. Nous comparons cette nouvelle similarité aux noyaux de Fisher, l’état de l’art en la matière. Nous présentons aussi une évaluation de PLSI sur un corpus de recherche d’information de près de 7500 documents et de plus d’un million d’occurrences de termes provenant de la collection TREC–AP, une taille considérable dans le cadre de PLSI.
							</p>


					</div>
					

					<div class="article">

						<b>Olivier Ferret</b>


						<br/>

							<i>Utiliser des sens de mots pour la segmentation thématique ?</i> <br/>

						<a href="actes/taln-2009-court-046.pdf">taln-2009-court-046</a> 
						<a href="bibtex/taln-2009-court-046.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-court-046-abs');">résumé</a>
							<a onclick="toggle('taln-2009-court-046-key');">mots clés</a> <br/>

							<p id="taln-2009-court-046-abs" class="resume">
							<b>Résumé : </b> La segmentation thématique est un domaine de l’analyse discursive ayant donné lieu à de nombreux travaux s’appuyant sur la notion de cohésion lexicale. La plupart d’entre eux n’exploitent que la simple récurrence lexicale mais quelques uns ont néanmoins exploré l’usage de connaissances rendant compte de cette cohésion lexicale. Celles-ci prennent généralement la forme de réseaux lexicaux, soit construits automatiquement à partir de corpus, soit issus de dictionnaires élaborés manuellement. Dans cet article, nous examinons dans quelle mesure une ressource d’une nature un peu différente peut être utilisée pour caractériser la cohésion lexicale des textes. Il s’agit en l’occurrence de sens de mots induits automatiquement à partir de corpus, à l’instar de ceux produits par la tâche «Word Sense Induction and Discrimination » de l’évaluation SemEval 2007. Ce type de ressources apporte une structuration des réseaux lexicaux au niveau sémantique dont nous évaluons l’apport pour la segmentation thématique.
							</p>

							<p id="taln-2009-court-046-key" class="mots_cles">
							<b>Mots clés : </b> Segmentation thématique, désambiguïsation sémantique
							</p>

					</div>
					

				<h1 id="démonstration">Démonstrations</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Francis Brunet-Manquat, Jérôme Goulian</b>


						<br/>

							<i>ACOLAD un environnement pour l’édition de corpus de dépendances</i> <br/>

						<a href="actes/taln-2009-demo-001.pdf">taln-2009-demo-001</a> 
						<a href="bibtex/taln-2009-demo-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-001-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-001-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-001-abs" class="resume">
							<b>Résumé : </b> Dans cette démonstration, nous présentons le prototype d’un environnement open-source pour l’édition de corpus de dépendances. Cet environnement, nommé ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks).
							</p>

							<p id="taln-2009-demo-001-key" class="mots_cles">
							<b>Mots clés : </b> dépendances, chunk, édition
							</p>

					</div>
					

					<div class="article">

						<b>Houda Bouamor, Aurélien Max, Anne Vilnat</b>


						<br/>

							<i>Amener des utilisateurs à créer et évaluer des paraphrases par le jeu</i> <br/>

						<a href="actes/taln-2009-demo-002.pdf">taln-2009-demo-002</a> 
						<a href="bibtex/taln-2009-demo-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-002-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-002-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-002-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une application sur le web pour l’acquisition de paraphrases phrastiques et sous-phrastiques sous forme de jeu. L’application permet l’acquisition à la fois de paraphrases et de jugements humains multiples sur ces paraphrases, ce qui constitue des données particulièrement utiles pour les applications du TAL basées sur les phénomènes paraphrastiques.
							</p>

							<p id="taln-2009-demo-002-key" class="mots_cles">
							<b>Mots clés : </b> Paraphrase, acquisition de données, évaluation de données
							</p>

					</div>
					

					<div class="article">

						<b>Adrien Lardilleux, Yves Lepage</b>


						<br/>

							<i>anymalign : un outil d’alignement sous-phrastique libre pour les êtres humains</i> <br/>

						<a href="actes/taln-2009-demo-003.pdf">taln-2009-demo-003</a> 
						<a href="bibtex/taln-2009-demo-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-003-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-003-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-003-abs" class="resume">
							<b>Résumé : </b> Nous présentons anymalign, un aligneur sous-phrastique grand public. Ses résultats ont une qualité qui rivalise avec le meilleur outil du domaine, GIZA++. Il est rapide et simple d’utilisation, et permet de produire dictionnaires et autres tables de traduction en une seule commande. À notre connaissance, c’est le seul outil au monde permettant d’aligner un nombre quelconque de langues simultanément. Il s’agit donc du premier aligneur sousphrastique réellement multilingue.
							</p>

							<p id="taln-2009-demo-003-key" class="mots_cles">
							<b>Mots clés : </b> alignement sous-phrastique, multilinguisme, table de traduction
							</p>

					</div>
					

					<div class="article">

						<b>Jean Charlet, Sylvie Szulman, Nathalie Aussenac-Gilles, Adeline Nazarenko, Nathalie Hernandez, Nadia Nadah, Éric Sardet, Jean Delahousse, Guy Pierra</b>


						<br/>

							<i>Apport des outils de TAL à la construction d’ontologies : propositions au sein de la plateforme DaFOE</i> <br/>

						<a href="actes/taln-2009-demo-004.pdf">taln-2009-demo-004</a> 
						<a href="bibtex/taln-2009-demo-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-004-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-004-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-004-abs" class="resume">
							<b>Résumé : </b> La construction d’ontologie à partir de textes fait l’objet d’études depuis plusieurs années dans le domaine de l’ingénierie des ontologies. Un cadre méthodologique en quatre étapes (constitution d’un corpus de documents, analyse linguistique du corpus, conceptualisation, opérationnalisation de l’ontologie) est commun à la plupart des méthodes de construction d’ontologies à partir de textes. S’il existe plusieurs plateformes de traitement automatique de la langue (TAL) permettant d’analyser automatiquement les corpus et de les annoter tant du point de vue syntaxique que statistique, il n’existe actuellement aucune procédure généralement acceptée, ni a fortiori aucun ensemble cohérent d’outils supports, permettant de concevoir de façon progressive, explicite et traçable une ontologie de domaine à partir d’un ensemble de ressources informationnelles relevant de ce domaine. Le but de ce court article est de présenter les propositions développées, au sein du projet ANR DaFOE 4app, pour favoriser l’émergence d’un tel ensemble d’outils.
							</p>

							<p id="taln-2009-demo-004-key" class="mots_cles">
							<b>Mots clés : </b> Ontologie, construction d’ontologie, TALN
							</p>

					</div>
					

					<div class="article">

						<b>Davy Weissenbacher, Elisa Pieri, Sophia Ananiadou, Brian Rea, Farida Vis, Yuwei Lin, Rob Procter, Peter Halfpenny</b>


						<br/>

							<i>ASSIST : un moteur de recherche spécialisé pour l’analyse des cadres d’expériences</i> <br/>

						<a href="actes/taln-2009-demo-005.pdf">taln-2009-demo-005</a> 
						<a href="bibtex/taln-2009-demo-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-005-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-005-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-005-abs" class="resume">
							<b>Résumé : </b> L’analyse qualitative des données demande au sociologue un important travail de sélection et d’interprétation des documents. Afin de faciliter ce travail, cette communauté c’est dotée d’outils informatique mais leur fonctionnalités sont encore limitées. Le projet ASSIST est une étude exploratoire pour préciser les modules de traitement automatique des langues (TAL) permettant d’assister le sociologue dans son travail d’analyse. Nous présentons le moteur de recherche réalisé et nous justifions le choix des composants de TAL intégrés au prototype.
							</p>

							<p id="taln-2009-demo-005-key" class="mots_cles">
							<b>Mots clés : </b> Recherche d’information, Extraction d’information, Terminologie
							</p>

					</div>
					

					<div class="article">

						<b>Ivan Šmilauer</b>


						<br/>

							<i>CETLEF.fr - diagnostic automatique des erreurs de déclinaison tchèque dans un outil ELAO</i> <br/>

						<a href="actes/taln-2009-demo-006.pdf">taln-2009-demo-006</a> 
						<a href="bibtex/taln-2009-demo-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-006-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-006-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-006-abs" class="resume">
							<b>Résumé : </b> CETLEF.fr – une application Web dynamique – propose des exercices de déclinaison tchèque avec un diagnostic automatique des erreurs. Le diagnostic a nécessité l&#39;élaboration d&#39;un modèle formel spécifique de la déclinaison contenant un classement des types paradigmatiques et des règles pour la réalisation des alternances morphématiques. Ce modèle est employé pour l&#39;annotation des formes requises, nécessaire pour le diagnostic, mais également pour une présentation didactique sur la plateforme apprenant. Le diagnostic est effectué par comparaison d&#39;une production erronée avec des formes hypothétiques générées à partir du radical de la forme requise et des différentes désinences casuelles. S&#39;il existe une correspondance, l&#39;erreur est interprétée d&#39;après les différences dans les traits morphologiques de la forme requise et de la forme hypothétique. La majorité des erreurs commises peut être interprétée à l&#39;aide de cette technique.
							</p>

							<p id="taln-2009-demo-006-key" class="mots_cles">
							<b>Mots clés : </b> morphologie flexionnelle, déclinaison tchèque, acquisition d&#39;une langue étrangère, diagnostic des erreurs et feedback, ELAO
							</p>

					</div>
					

					<div class="article">

						<b>Georges Fafiotte, Achille Falaise, Jérôme Goulian</b>


						<br/>

							<i>CIFLI-SurviTra, deux facettes : démonstrateur de composants de TA fondée sur UNL, et phrasebook multilingue</i> <br/>

						<a href="actes/taln-2009-demo-007.pdf">taln-2009-demo-007</a> 
						<a href="bibtex/taln-2009-demo-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-007-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-007-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-007-abs" class="resume">
							<b>Résumé : </b> CIFLI-SurviTra (&#34;Survival Translation&#34; assistant) est une plate-forme destinée à favoriser l&#39;ingénierie et la mise au point de composants UNL de TA, à partir d&#39;une mémoire de traduction formée de livres de phrases multilingues avec variables lexicales. SurviTra est aussi un phrasebook digital multilingue, assistant linguistique pour voyageurs monolingues (français, hindi, tamoul, anglais) en situation de &#34;survie linguistique&#34;. Le corpus d’un domaine-pilote (&#34;Restaurant&#34;) a été structuré et construit : sous-domaines de phrases alignées et classes lexicales de locutions quadrilingues, graphes UNL, dictionnaires UW++/français et UW++/hindi par domaines. L’approche, générique, est applicable à d’autres langues. Le prototype d’assistant linguistique (application Web, à interface textuelle) peut évoluer vers une application UNL embarquée sur SmartPhone, avec Traitement de Parole et multimodalité.
							</p>

							<p id="taln-2009-demo-007-key" class="mots_cles">
							<b>Mots clés : </b> TA via UNL, démonstrateur de composants UNL, assistant linguistique sur le Web, phrasebook digital multilingue, mémoire de traduction, collecte collaborative de corpus
							</p>

					</div>
					

					<div class="article">

						<b>Stefanos Petrakis, Manfred Klenner, Étienne Ailloud, Angela Fahrni</b>


						<br/>

							<i>Composition multilingue de sentiments</i> <br/>

						<a href="actes/taln-2009-demo-008.pdf">taln-2009-demo-008</a> 
						<a href="bibtex/taln-2009-demo-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-008-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-008-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-008-abs" class="resume">
							<b>Résumé : </b> Nous présentons ici PolArt, un outil multilingue pour l’analyse de sentiments qui aborde la composition des sentiments en appliquant des transducteurs en cascade. La compositionnalité est assurée au moyen de polarités préalables extraites d’un lexique et des règles de composition appliquées de manière incrémentielle.
							</p>

							<p id="taln-2009-demo-008-key" class="mots_cles">
							<b>Mots clés : </b> Analyse de sentiments
							</p>

					</div>
					

					<div class="article">

						<b>Motasem Alrahabi, Jean-Pierre Desclés</b>


						<br/>

							<i>EXCOM : Plate-forme d&#39;annotation sémantique de textes multilingues</i> <br/>

						<a href="actes/taln-2009-demo-009.pdf">taln-2009-demo-009</a> 
						<a href="bibtex/taln-2009-demo-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-009-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-009-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-009-abs" class="resume">
							<b>Résumé : </b> Nous proposons une plateforme d‟annotation sémantique, appelée « EXCOM ». Basée sur la méthode de l‟ « Exploration Contextuelle », elle permet, à travers une diversité de langues, de procéder à des annotations automatiques de segments textuels par l&#39;analyse des formes de surface dans leur contexte. Les textes sont traités selon des « points de vue » discursifs dont les valeurs sont organisées dans une « carte sémantique ». L‟annotation se base sur un ensemble de règles linguistiques, écrites par un analyste, qui permettent d‟identifier les représentations textuelles sous-jacentes aux différentes catégories de la carte. Le système offre, à travers deux types d‟interfaces (développeur ou utilisateur), une chaîne de traitements automatiques de textes qui comprend la segmentation, l‟annotation et d‟autres fonctionnalités de post-traitement. Les documents annotés peuvent être utilisés, par exemple, pour des systèmes de recherche d‟information, de veille, de classification ou de résumé automatique.
							</p>

							<p id="taln-2009-demo-009-key" class="mots_cles">
							<b>Mots clés : </b> Plate-forme, Annotation automatique, Exploration Contextuelle, analyse sémantique, marqueurs discursifs, carte sémantique, multilinguisme
							</p>

					</div>
					

					<div class="article">

						<b>Antoine Widlöcher, Yann Mathet</b>


						<br/>

							<i>La plate-forme d’annotation Glozz</i> <br/>

						<a href="actes/taln-2009-demo-010.pdf">taln-2009-demo-010</a> 
						<a href="bibtex/taln-2009-demo-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-010-key');">mots clés</a> <br/>


							<p id="taln-2009-demo-010-key" class="mots_cles">
							<b>Mots clés : </b> Linguistique de corpus, Annotation, Plate-forme logicielle
							</p>

					</div>
					

					<div class="article">

						<b>Blin Raoul</b>


						<br/>

							<i>SAGACE-v3.3 ; Analyseur de corpus pour langues non flexionnelles</i> <br/>

						<a href="actes/taln-2009-demo-011.pdf">taln-2009-demo-011</a> 
						<a href="bibtex/taln-2009-demo-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-011-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-011-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-011-abs" class="resume">
							<b>Résumé : </b> Nous présentons la dernière version du logiciel SAGACE, analyseur de corpus pour langues faiblement flexionnelles (par exemple japonais ou chinois). Ce logiciel est distribué avec un lexique où les catégories sont exprimées à l&#39;aide de systèmes de traits.
							</p>

							<p id="taln-2009-demo-011-key" class="mots_cles">
							<b>Mots clés : </b> corpus, lexique, analyseur, japonais, chinois
							</p>

					</div>
					

					<div class="article">

						<b>Nicolas Hernandez, Fabien Poulard, Stergos Afantenos, Matthieu Vernier, Jérôme Rocheteau</b>


						<br/>

							<i>Apache UIMA pour le Traitement Automatique des Langues</i> <br/>

						<a href="actes/taln-2009-demo-012.pdf">taln-2009-demo-012</a> 
						<a href="bibtex/taln-2009-demo-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-012-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-012-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-012-abs" class="resume">
							<b>Résumé : </b> L’objectif de la démonstration est d’une part de faire un retour d’expérience sur la solution logicielle Apache UIMA comme infrastructure de développement d’applications distribuées de TAL, et d’autre part de présenter les développements réalisés par l’équipe TALN du LINA pour permettre à la communauté de s’approprier ce « framework ».
							</p>

							<p id="taln-2009-demo-012-key" class="mots_cles">
							<b>Mots clés : </b> Apache UIMA, Applications du TAL, Infrastructure logicielle
							</p>

					</div>
					

					<div class="article">

						<b>Jérôme Lehuen, Thierry Lemeunier</b>


						<br/>

							<i>Un Analyseur Sémantique pour le DHM</i> <br/>

						<a href="actes/taln-2009-demo-013.pdf">taln-2009-demo-013</a> 
						<a href="bibtex/taln-2009-demo-013.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Jacques Vergne</b>


						<br/>

							<i>Un chunker multilingue endogène</i> <br/>

						<a href="actes/taln-2009-demo-014.pdf">taln-2009-demo-014</a> 
						<a href="bibtex/taln-2009-demo-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2009-demo-014-abs');">résumé</a>
							<a onclick="toggle('taln-2009-demo-014-key');">mots clés</a> <br/>

							<p id="taln-2009-demo-014-abs" class="resume">
							<b>Résumé : </b> Le chunking consiste à segmenter un texte en chunks, segments sous-phrastiques qu&#39;Abney a défini approximativement comme des groupes accentuels. Traditionnellement, le chunking utilise des ressources monolingues, le plus souvent exhaustives, quelquefois partielles : des mots grammaticaux et des ponctuations, qui marquent souvent des débuts et fins de chunk. Mais cette méthode, si l&#39;on veut l&#39;étendre à de nombreuses langues, nécessite de multiplier les ressources monolingues. Nous présentons une nouvelle méthode : le chunking endogène, qui n&#39;utilise aucune ressource hormis le texte analysé lui-même. Cette méthode prolonge les travaux de Zipf : la minimisation de l&#39;effort de communication conduit les locuteurs à raccourcir les mots fréquents. On peut alors caractériser un chunk comme étant la période des fonctions périodiques correllées longueur et effectif des mots sur l&#39;axe syntagmatique. Cette méthode originale présente l&#39;avantage de s&#39;appliquer à un grand nombre de langues d&#39;écriture alphabétique, avec le même algorithme, sans aucune ressource.
							</p>

							<p id="taln-2009-demo-014-key" class="mots_cles">
							<b>Mots clés : </b> chunking, multilingue, endogène, longueur des mots, effectif des mots
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>