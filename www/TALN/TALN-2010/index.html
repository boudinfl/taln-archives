<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>TALN'2010</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>TALN'2010, 17e conférence sur le Traitement Automatique des Langues Naturelles</h1>
				<h2>Montréal (Canada), du 2010-07-19 au 2010-07-23</h2>
				<p>Président(s) : Philippe Langlais, Michel Gagnon</p>
				<p>Taux d'acceptation :
							papiers longs (44.4%)
							papiers courts (52.9%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#invite">Invités</a></li>
				<li><a href="#long">Papiers longs</a></li>
				<li><a href="#court">Papiers courts</a></li>
				<li><a href="#démonstration">Démonstrations</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="invite">Invités</h1>
			

					<div class="article">

						<b>Igor Mel’čuk</b>


						<br/>

							<i>La phraséologie en langue, en dictionnaire et en TALN</i> <br/>

						<a href="actes/taln-2010-invite-001.pdf">taln-2010-invite-001</a> 
						<a href="bibtex/taln-2010-invite-001.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Pierre Isabelle</b>


						<br/>

							<i>La montée en puissance des recherches en traduction automatique statistique</i> <br/>

						<a href="actes/taln-2010-invite-002.pdf">taln-2010-invite-002</a> 
						<a href="bibtex/taln-2010-invite-002.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Gerald Penn</b>


						<br/>

							<i>The Quantitative Study of Writing Systems</i> <br/>

						<a href="actes/taln-2010-invite-003.pdf">taln-2010-invite-003</a> 
						<a href="bibtex/taln-2010-invite-003.bib">bibtex</a> 



					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="long">Papiers longs</h1>
			

					

					

					

					<div class="article">

						<b>Holger Schwenk</b>


						<br/>

							<i>Adaptation d’un Système de Traduction Automatique Statistique avec des Ressources monolingues</i> <br/>

						<a href="actes/taln-2010-long-001.pdf">taln-2010-long-001</a> 
						<a href="bibtex/taln-2010-long-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-001-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-001-key');">mots clés</a> <br/>

							<p id="taln-2010-long-001-abs" class="resume">
							<b>Résumé : </b> Les performances d’un système de traduction statistique dépendent beaucoup de la qualité et de la quantité des données d’apprentissage disponibles. La plupart des textes parallèles librement disponibles proviennent d’organisations internationales. Le jargon observé dans ces textes n’est pas très adapté pour construire un système de traduction pour d’autres domaines. Nous présentons dans cet article une technique pour adapter le modèle de traduction à un domaine différent en utilisant des textes dans la langue source uniquement. Nous obtenons des améliorations significatives du score BLEU dans des systèmes de traduction de l’arabe vers le français et vers l’anglais.
							</p>

							<p id="taln-2010-long-001-key" class="mots_cles">
							<b>Mots clés : </b> Traduction statistique, adaptation du modèle de traduction, corpus monolingue, apprentissage non-supervisé
							</p>

					</div>
					

					<div class="article">

						<b>Julien Bourdaillet, Stéphane Huet, Philippe Langlais</b>


						<br/>

							<i>Alignement de traductions rares à l’aide de paires de phrases non alignées</i> <br/>

						<a href="actes/taln-2010-long-002.pdf">taln-2010-long-002</a> 
						<a href="bibtex/taln-2010-long-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-002-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-002-key');">mots clés</a> <br/>

							<p id="taln-2010-long-002-abs" class="resume">
							<b>Résumé : </b> Bien souvent, le sens d’un mot ou d’une expression peut être rendu dans une autre langue par plusieurs traductions. Parmi celles-ci, certaines se révèlent très fréquentes alors que d’autres le sont beaucoup moins, conformément à une loi zipfienne. La googlisation de notre monde n’échappe pas aux mémoires de traduction, qui mettent souvent à mal ou simplement ignorent ces traductions rares qui sont souvent de bonne qualité. Dans cet article, nous nous intéressons à ces traductions rares sous l’angle du repérage de traductions. Nous argumentons qu’elles sont plus difficiles à identifier que les traductions plus fréquentes. Nous décrivons une approche originale qui permet de mieux les identifier en tirant profit de l’alignement au niveau des mots de paires de phrases qui ne sont pas alignées. Nous montrons que cette approche permet d’améliorer l’identification de ces traductions rares.
							</p>

							<p id="taln-2010-long-002-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique statistique, alignement de mots, traduction rares, contrôle de pertinence
							</p>

					</div>
					

					<div class="article">

						<b>Pascal Denis, Benoît Sagot</b>


						<br/>

							<i>Exploitation d’une ressource lexicale pour la construction d’un étiqueteur morpho-syntaxique état-de-l’art du français</i> <br/>

						<a href="actes/taln-2010-long-003.pdf">taln-2010-long-003</a> 
						<a href="bibtex/taln-2010-long-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-003-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-003-key');">mots clés</a> <br/>

							<p id="taln-2010-long-003-abs" class="resume">
							<b>Résumé : </b> Cet article présente MEltfr, un étiqueteur morpho-syntaxique automatique du français. Il repose sur un modèle probabiliste séquentiel qui bénéficie d’informations issues d’un lexique exogène, à savoir le Lefff. Evalué sur le FTB, MEltfr atteint un taux de précision de 97.75% (91.36% sur les mots inconnus) sur un jeu de 29 étiquettes. Ceci correspond à une diminution du taux d’erreur de 18% (36.1% sur les mots inconnus) par rapport au même modèle sans couplage avec le Lefff. Nous étudions plus en détail la contribution de cette ressource, au travers de deux séries d’expériences. Celles-ci font apparaître en particulier que la contribution des traits issus du Lefff est de permettre une meilleure couverture, ainsi qu’une modélisation plus fine du contexte droit des mots.
							</p>

							<p id="taln-2010-long-003-key" class="mots_cles">
							<b>Mots clés : </b> Etiquetage morpho-syntaxique, modèles à maximisation d’entropie, français, lexique
							</p>

					</div>
					

					<div class="article">

						<b>Olivier Ferret</b>


						<br/>

							<i>Similarité sémantique et extraction de synonymes à partir de corpus</i> <br/>

						<a href="actes/taln-2010-long-004.pdf">taln-2010-long-004</a> 
						<a href="bibtex/taln-2010-long-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-004-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-004-key');">mots clés</a> <br/>

							<p id="taln-2010-long-004-abs" class="resume">
							<b>Résumé : </b> La définition de mesures sémantiques au niveau lexical a fait l’objet de nombreux travaux depuis plusieurs années. Dans cet article, nous nous focalisons plus spécifiquement sur les mesures de nature distributionnelle. Bien que différentes évaluations ont été réalisées les concernant, il reste difficile à établir si une mesure donnant de bons résultats dans un cadre d’évaluation peut être appliquée plus largement avec le même succès. Dans le travail présenté, nous commençons par sélectionner une mesure de similarité sur la base d’un test de type TOEFL étendu. Nous l’appliquons ensuite au problème de l’extraction de synonymes à partir de corpus en comparant nos résultats avec ceux de (Curran &amp; Moens, 2002). Enfin, nous testons l’intérêt pour cette tâche d’extraction de synonymes d’une méthode d’amélioration de la qualité des données distributionnelles proposée dans (Zhitomirsky-Geffet &amp; Dagan, 2009).
							</p>

							<p id="taln-2010-long-004-key" class="mots_cles">
							<b>Mots clés : </b> extraction de synonymes, similarité sémantique, méthodes distributionnelles
							</p>

					</div>
					

					<div class="article">

						<b>Simon Charest, Éric Brunelle, Jean Fontaine</b>


						<br/>

							<i>Au-delà de la paire de mots : extraction de cooccurrences syntaxiques multilexémiques</i> <br/>

						<a href="actes/taln-2010-long-005.pdf">taln-2010-long-005</a> 
						<a href="bibtex/taln-2010-long-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-005-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-005-key');">mots clés</a> <br/>

							<p id="taln-2010-long-005-abs" class="resume">
							<b>Résumé : </b> Cet article décrit l’élaboration de la deuxième édition du dictionnaire de cooccurrences du logiciel d’aide à la rédaction Antidote. Cette nouvelle mouture est le résultat d’une refonte complète du processus d’extraction, ayant principalement pour but l’extraction de cooccurrences de plus de deux unités lexicales. La principale contribution de cet article est la description d’une technique originale pour l’extraction de cooccurrences de plus de deux mots conservant une structure syntaxique complète.
							</p>

							<p id="taln-2010-long-005-key" class="mots_cles">
							<b>Mots clés : </b> Antidote, cooccurrences, collocations, expressions multimots
							</p>

					</div>
					

					<div class="article">

						<b>Adil El Ghali, Yann Vigile Hoareau</b>


						<br/>

							<i>Une approche cognitive de la fouille de grandes collections de documents</i> <br/>

						<a href="actes/taln-2010-long-006.pdf">taln-2010-long-006</a> 
						<a href="bibtex/taln-2010-long-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-006-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-006-key');">mots clés</a> <br/>

							<p id="taln-2010-long-006-abs" class="resume">
							<b>Résumé : </b> La récente éclosion du Web2.0 engendre un accroissement considérable de volumes textuels et intensifie ainsi l’importance d’une réflexion sur l’exploitation des connaissances à partir de grandes collections de documents. Dans cet article, nous présentons une approche de rechercher d’information qui s’inspire des certaines recherches issues de la psychologie cognitive pour la fouille de larges collections de documents. Nous utilisons un document comme requête permettant de récupérer des informations à partir d’une collection représentée dans un espace sémantique. Nous définissons les notions d’identité sémantique et de pollution sémantique dans un espace de documents. Nous illustrons notre approche par la description d’un système appelé BRAT (Blogosphere Random Analysis using Texts) basé sur les notions préalablement introduites d’identité et de pollution sématique appliquées à une tâche d’identification des actualités dans la blogosphère mondiale lors du concours TREC’09. Les premiers résultats produits sont tout à fait encourageant et indiquent les pistes des recherches à mettre en oeuvre afin d’améliorer les performances de BRAT.
							</p>

							<p id="taln-2010-long-006-key" class="mots_cles">
							<b>Mots clés : </b> Fouille de textes, Random-Indexing, Cognition, Marche aléatoire
							</p>

					</div>
					

					<div class="article">

						<b>Jonathan Marchand, Bruno Guillaume, Guy Perrier</b>


						<br/>

							<i>Motifs de graphe pour le calcul de dépendances syntaxiques complètes</i> <br/>

						<a href="actes/taln-2010-long-007.pdf">taln-2010-long-007</a> 
						<a href="bibtex/taln-2010-long-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-007-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-007-key');">mots clés</a> <br/>

							<p id="taln-2010-long-007-abs" class="resume">
							<b>Résumé : </b> Cet article propose une méthode pour calculer les dépendances syntaxiques d’un énoncé à partir du processus d’analyse en constituants. L’objectif est d’obtenir des dépendances complètes c’est-à-dire contenant toutes les informations nécessaires à la construction de la sémantique. Pour l’analyse en constituants, on utilise le formalisme des grammaires d’interaction : celui-ci place au cœur de la composition syntaxique un mécanisme de saturation de polarités qui peut s’interpréter comme la réalisation d’une relation de dépendance. Formellement, on utilise la notion de motifs de graphes au sens de la réécriture de graphes pour décrire les conditions nécessaires à la création d’une dépendance.
							</p>

							<p id="taln-2010-long-007-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique, dépendance, grammaires d’interaction, polarité
							</p>

					</div>
					

					<div class="article">

						<b>Juliette Thuilier, Gwendoline Fox, Benoît Crabbé</b>


						<br/>

							<i>Approche quantitative en syntaxe : l’exemple de l’alternance de position de l’adjectif épithète en français</i> <br/>

						<a href="actes/taln-2010-long-008.pdf">taln-2010-long-008</a> 
						<a href="bibtex/taln-2010-long-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-008-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-008-key');">mots clés</a> <br/>

							<p id="taln-2010-long-008-abs" class="resume">
							<b>Résumé : </b> Cet article présente une analyse statistique sur des données de syntaxe qui a pour but d’aider à mieux cerner le phénomène d’alternance de position de l’adjectif épithète par rapport au nom en français. Nous montrons comment nous avons utilisé les corpus dont nous disposons (French Treebank et le corpus de l’Est-Républicain) ainsi que les ressources issues du traitement automatique des langues, pour mener à bien notre étude. La modélisation à partir de 13 variables relevant principalement des propriétés du syntagme adjectival, de celles de l’item adjectival, ainsi que de contraintes basées sur la fréquence, permet de prédire à plus de 93% la position de l’adjectif. Nous insistons sur l’importance de contraintes relevant de l’usage pour le choix de la position de l’adjectif, notamment à travers la fréquence d’occurrence de l’adjectif, et la fréquence de contextes dans lesquels il apparaît.
							</p>

							<p id="taln-2010-long-008-key" class="mots_cles">
							<b>Mots clés : </b> Syntaxe probabiliste, linguistique de corpus, adjectif épithète, régression logistique
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Blache</b>


						<br/>

							<i>Un modèle de caractérisation de la complexité syntaxique</i> <br/>

						<a href="actes/taln-2010-long-009.pdf">taln-2010-long-009</a> 
						<a href="bibtex/taln-2010-long-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-009-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-009-key');">mots clés</a> <br/>

							<p id="taln-2010-long-009-abs" class="resume">
							<b>Résumé : </b> Cet article présente un modèle de la complexité syntaxique. Il réunit un ensemble d’indices de complexité et les représente à l’aide d’un cadre formel homogène, offrant ainsi la possibilité d’une quantification automatique : le modèle proposé permet d’associer à chaque phrase un indice reflétant sa complexité.
							</p>

							<p id="taln-2010-long-009-key" class="mots_cles">
							<b>Mots clés : </b> Complexité syntaxique, analyse syntaxique automatique, parser humain
							</p>

					</div>
					

					<div class="article">

						<b>Éric Villemonte De La Clergerie</b>


						<br/>

							<i>Convertir des dérivations TAG en dépendances</i> <br/>

						<a href="actes/taln-2010-long-010.pdf">taln-2010-long-010</a> 
						<a href="bibtex/taln-2010-long-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-010-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-010-key');">mots clés</a> <br/>

							<p id="taln-2010-long-010-abs" class="resume">
							<b>Résumé : </b> Les structures de dépendances syntaxiques sont importantes et bien adaptées comme point de départ de diverses applications. Dans le cadre de l’analyseur TAG FRMG, nous présentons les détails d’un processus de conversion de forêts partagées de dérivations en forêts partagées de dépendances. Des éléments d’information sont fournis sur un algorithme de désambiguisation sur ces forêts de dépendances.
							</p>

							<p id="taln-2010-long-010-key" class="mots_cles">
							<b>Mots clés : </b> dépendances, analyse syntaxique, TAG, forêt partagée
							</p>

					</div>
					

					<div class="article">

						<b>Shamima Mithun, Leila Kosseim</b>


						<br/>

							<i>A Hybrid Approach to Utilize Rhetorical Relations for Blog Summarization</i> <br/>

						<a href="actes/taln-2010-long-011.pdf">taln-2010-long-011</a> 
						<a href="bibtex/taln-2010-long-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-011-abs');">abstract</a>
							<a onclick="toggle('taln-2010-long-011-key');">keywords</a> <br/>

							<p id="taln-2010-long-011-abs" class="abstract">
							<b>Abstract : </b> The availability of huge amounts of online opinions has created a new need to develop effective query-based opinion summarizers to analyze this information in order to facilitate decision making at every level. To develop an effective opinion summarization approach, we have targeted to resolve specifically Question Irrelevancy and Discourse Incoherency problems which have been found to be the most frequently occurring problems for opinion summarization. To address these problems, we have introduced a hybrid approach by combining text schema and rhetorical relations to exploit intra-sentential rhetorical relations. To evaluate our approach, we have built a system called BlogSum and have compared BlogSum-generated summaries after applying rhetorical structuring to BlogSum-generated candidate sentences without utilizing rhetorical relations using the Text Analysis Conference (TAC) 2008 data for summary contents. Evaluation results show that our approach improves summary contents by reducing question irrelevant sentences.
							</p>

							<p id="taln-2010-long-011-key" class="keywords">
							<b>Keywords : </b> Blog Summarization, Rhetorical Relations, Text Schema
							</p>

					</div>
					

					<div class="article">

						<b>Richard Beaufort, Sophie Roekhaut, Louise-Amélie Cougnon, Cédrick Fairon</b>


						<br/>

							<i>Une approche hybride traduction/correction pour la normalisation des SMS</i> <br/>

						<a href="actes/taln-2010-long-012.pdf">taln-2010-long-012</a> 
						<a href="bibtex/taln-2010-long-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-012-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-012-key');">mots clés</a> <br/>

							<p id="taln-2010-long-012-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthode hybride de normalisation des SMS, à mi-chemin entre correction orthographique et traduction automatique. La partie du système qui assure la normalisation utilise exclusivement des modèles entraînés sur corpus. Evalué en français par validation croisée, le système obtient un taux d’erreur au mot de 9.3% et un score BLEU de 0.83.
							</p>

							<p id="taln-2010-long-012-key" class="mots_cles">
							<b>Mots clés : </b> SMS, normalisation, machines à états finis, approche hybride, orienté traduction, orienté correction, apprentissage sur corpus
							</p>

					</div>
					

					<div class="article">

						<b>Guillaume Wisniewski, Aurélien Max, François Yvon</b>


						<br/>

							<i>Recueil et analyse d’un corpus écologique de corrections orthographiques extrait des révisions de Wikipédia</i> <br/>

						<a href="actes/taln-2010-long-013.pdf">taln-2010-long-013</a> 
						<a href="bibtex/taln-2010-long-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-013-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-013-key');">mots clés</a> <br/>

							<p id="taln-2010-long-013-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous introduisons une méthode à base de règles permettant d’extraire automatiquement de l’historique des éditions de l’encyclopédie collaborative Wikipédia des corrections orthographiques. Cette méthode nous a permis de construire un corpus d’erreurs composé de 72 483 erreurs lexicales (non-word errors) et 74 100 erreurs grammaticales (real-word errors). Il n’existe pas, à notre connaissance, de plus gros corpus d’erreurs écologiques librement disponible. En outre, les techniques mises en oeuvre peuvent être facilement transposées à de nombreuses autres langues. La collecte de ce corpus ouvre de nouvelles perspectives pour l’étude des erreurs fréquentes ainsi que l’apprentissage et l’évaluation des correcteurs orthographiques automatiques. Plusieurs expériences illustrant son intérêt sont proposées.
							</p>

							<p id="taln-2010-long-013-key" class="mots_cles">
							<b>Mots clés : </b> ressources pour le TAL, correction orthographique, Wikipédia
							</p>

					</div>
					

					<div class="article">

						<b>Eric Charton, Michel Gagnon, Benoit Ozell</b>


						<br/>

							<i>Extension d’un système d’étiquetage d’entités nommées en étiqueteur sémantique</i> <br/>

						<a href="actes/taln-2010-long-014.pdf">taln-2010-long-014</a> 
						<a href="bibtex/taln-2010-long-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-014-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-014-key');">mots clés</a> <br/>

							<p id="taln-2010-long-014-abs" class="resume">
							<b>Résumé : </b> L’étiquetage sémantique consiste à associer un ensemble de propriétés à une séquence de mots contenue dans un texte. Bien que proche de la tâche d’étiquetage par entités nommées, qui revient à attribuer une classe de sens à un mot, la tâche d’étiquetage ou d’annotation sémantique cherche à établir la relation entre l’entité dans son texte et sa représentation ontologique. Nous présentons un étiqueteur sémantique qui s’appuie sur un étiqueteur d’entités nommées pour mettre en relation un mot ou un groupe de mots avec sa représentation ontologique. Son originalité est d’utiliser une ontologie intermédiaire de nature statistique pour établir ce lien.
							</p>

							<p id="taln-2010-long-014-key" class="mots_cles">
							<b>Mots clés : </b> Étiqueteur sémantique, Entités nommées, Analyse sémantique, Ontologie
							</p>

					</div>
					

					<div class="article">

						<b>Jasmina Milićević</b>


						<br/>

							<i>Extraction de paraphrases sémantiques et lexico-syntaxiques de corpus parallèles bilingues</i> <br/>

						<a href="actes/taln-2010-long-015.pdf">taln-2010-long-015</a> 
						<a href="bibtex/taln-2010-long-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-015-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-015-key');">mots clés</a> <br/>

							<p id="taln-2010-long-015-abs" class="resume">
							<b>Résumé : </b> Nous présentons le travail en cours effectué dans le cadre d’un projet d’extraction de paraphrases à partir de textes parallèles bilingues. Nous identifions des paraphrases sémantiques et lexico-syntaxiques, qui mettent en jeu des opérations relativement complexes sur les structures sémantiques et syntaxiques de phrases, et les décrivons au moyen de règles de paraphrasage de type Sens-Texte, utilisables dans diverses applications de TALN.
							</p>

							<p id="taln-2010-long-015-key" class="mots_cles">
							<b>Mots clés : </b> paraphrase lexico-syntaxique, paraphrase sémanique, règles de paraphrasage, corpus bilingues, théorie Sens-Texte
							</p>

					</div>
					

					<div class="article">

						<b>Lydia-Mai Ho-Dac, Marie-Paule Péry-Woodley, Ludovic Tanguy</b>


						<br/>

							<i>Anatomie des structures énumératives</i> <br/>

						<a href="actes/taln-2010-long-016.pdf">taln-2010-long-016</a> 
						<a href="bibtex/taln-2010-long-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-016-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-016-key');">mots clés</a> <br/>

							<p id="taln-2010-long-016-abs" class="resume">
							<b>Résumé : </b> Cet article présente les premiers résultats d’une campagne d’annotation de corpus à grande échelle réalisée dans le cadre du projet ANNODIS. Ces résultats concernent la partie descendante du dispositif d’annotation, et plus spécifiquement les structures énumératives. Nous nous intéressons à la structuration énumérative en tant que stratégie de base de mise en texte, apparaissant à différents niveaux de granularité, associée à différentes fonctions discursives, et signalée par des indices divers. Avant l’annotation manuelle, une étape de pré-traitement a permis d’obtenir le marquage systématique de traits associés à la signalisation de l’organisation du discours. Nous décrivons cette étape de marquage automatique, ainsi que la procédure d’annotation. Nous proposons ensuite une première typologie des structures énumératives basée sur la description quantitative des données annotées manuellement, prenant en compte la couverture textuelle, la composition et les types d’indices.
							</p>

							<p id="taln-2010-long-016-key" class="mots_cles">
							<b>Mots clés : </b> Annotation de corpus, organisation du discours, structure énumérative, signalisation
							</p>

					</div>
					

					<div class="article">

						<b>Fadila Hadouche, Guy Lapalme, Marie-Claude L’Homme</b>


						<br/>

							<i>Identification des actants et circonstants par apprentissage machine</i> <br/>

						<a href="actes/taln-2010-long-017.pdf">taln-2010-long-017</a> 
						<a href="bibtex/taln-2010-long-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-017-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-017-key');">mots clés</a> <br/>

							<p id="taln-2010-long-017-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous traitons de l’identification automatique des participants actants et circonstants de lexies prédicatives verbales tirées d’un corpus spécialisé en langue française. Les actants contribuent à la réalisation du sens de la lexie alors que les circonstants sont optionnels : ils ajoutent une information supplémentaire qui ne fait pas partie intégrante du sémantisme de la lexie. Nous proposons une classification de ces participants par apprentissage machine basée sur un corpus de lexies verbales du domaine de l’informatique, lexies qui ont été annotées manuellement avec des rôles sémantiques. Nous présentons des features qui nous permettent d’identifier les participants et de distinguer les actants des circonstants.
							</p>

							<p id="taln-2010-long-017-key" class="mots_cles">
							<b>Mots clés : </b> Structure actancielle, actants et circonstants, features de classification
							</p>

					</div>
					

					<div class="article">

						<b>Marie-Noëlle Bessagnet, Mauro Gaio, Eric Kergosien, Christian Sallaberry</b>


						<br/>

							<i>Extraction automatique d&#39;un lexique à connotation géographique à des fins ontologiques dans un corpus de récits de voyage</i> <br/>

						<a href="actes/taln-2010-long-018.pdf">taln-2010-long-018</a> 
						<a href="bibtex/taln-2010-long-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-018-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-018-key');">mots clés</a> <br/>

							<p id="taln-2010-long-018-abs" class="resume">
							<b>Résumé : </b> Le but de ces travaux est d’extraire un lexique en analysant les relations entre des syntagmes nominaux et des syntagmes verbaux dans les textes de notre corpus, essentiellement des récits de voyage. L’hypothèse que nous émettons est de pouvoir établir une catégorisation des syntagmes nominaux associés à des Entités Nommées de type lieu à l’aide de l’analyse des relations verbales. En effet, nous disposons d’une chaine de traitement automatique qui extrait, interprète et valide des Entités Nommées de type lieu dans des documents textuels. Ce travail est complété par l’analyse des relations verbales associées à ces EN, candidates à l’enrichissement d’une ontologie.
							</p>

							<p id="taln-2010-long-018-key" class="mots_cles">
							<b>Mots clés : </b> Entité nommée, ontologie, relations verbales, patrons linguistiques
							</p>

					</div>
					

					<div class="article">

						<b>Stanislas Oger, Mickael Rouvier, Georges Linarès</b>


						<br/>

							<i>Classification du genre vidéo reposant sur des transcriptions automatiques</i> <br/>

						<a href="actes/taln-2010-long-019.pdf">taln-2010-long-019</a> 
						<a href="bibtex/taln-2010-long-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-019-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-019-key');">mots clés</a> <br/>

							<p id="taln-2010-long-019-abs" class="resume">
							<b>Résumé : </b> Dans cet article nous proposons une nouvelle méthode pour l’identification du genre vidéo qui repose sur une analyse de leur contenu linguistique. Cette approche consiste en l’analyse des mots apparaissant dans les transcriptions des pistes audio des vidéos, obtenues à l’aide d’un système de reconnaissance automatique de la parole. Les expériences sont réalisées sur un corpus composé de dessins animés, de films, de journaux télévisés, de publicités, de documentaires, d’émissions de sport et de clips de musique. L’approche proposée permet d’obtenir un taux de bonne classification de 74% sur cette tâche. En combinant cette approche avec des méthodes reposant sur des paramètres acoustiques bas-niveau, nous obtenons un taux de bonne classification de 95%.
							</p>

							<p id="taln-2010-long-019-key" class="mots_cles">
							<b>Mots clés : </b> classification de genre vidéo, traitement audio de la vidéo, extraction de paramètres linguistiques
							</p>

					</div>
					

					<div class="article">

						<b>Christian Raymond, Julien Fayolle</b>


						<br/>

							<i>Reconnaissance robuste d’entités nommées sur de la parole transcrite automatiquement</i> <br/>

						<a href="actes/taln-2010-long-020.pdf">taln-2010-long-020</a> 
						<a href="bibtex/taln-2010-long-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-020-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-020-key');">mots clés</a> <br/>

							<p id="taln-2010-long-020-abs" class="resume">
							<b>Résumé : </b> Les transcriptions automatiques de parole constituent une ressource importante, mais souvent bruitée, pour décrire des documents multimédia contenant de la parole (e.g. journaux télévisés). En vue d’améliorer la recherche documentaire, une étape d’extraction d’information à caractère sémantique, précédant l’indexation, permet de faire face au problème des transcriptions imparfaites. Parmis ces contenus informatifs, on compte les entités nommées (e.g. noms de personnes) dont l’extraction est l’objet de ce travail. Les méthodes traditionnelles de reconnaissance basées sur une définition manuelle de grammaires formelles donnent de bons résultats sur du texte ou des transcriptions propres manuellement produites, mais leurs performances se trouvent fortement affectées lorsqu’elles sont appliquées sur des transcriptions automatiques. Nous présentons, ici, trois méthodes pour la reconnaissance d’entités nommées basées sur des algorithmes d’apprentissage automatique : les champs conditionnels aléatoires, les machines à de support, et les transducteurs à états finis. Nous présentons également une méthode pour rendre consistantes les données d’entrainement lorsqu’elles sont annotées suivant des conventions légèrement différentes. Les résultats montrent que les systèmes d’étiquetage obtenus sont parmi les plus robustes sur les données d’évaluation de la campagne ESTER 2 dans les conditions où la transcription automatique est particulièrement bruitée.
							</p>

							<p id="taln-2010-long-020-key" class="mots_cles">
							<b>Mots clés : </b> étiqueteur d’entités nommées, transcription automatique de parole, apprentissage automatique, champs conditionnels aléatoires, machines à vecteurs de support, transducteurs à états finis
							</p>

					</div>
					

					<div class="article">

						<b>Younès Bahou, Abir Masmoudi, Lamia Hadrich Belguith</b>


						<br/>

							<i>Traitement des disfluences dans le cadre de la compréhension automatique de l’oral arabe spontané</i> <br/>

						<a href="actes/taln-2010-long-021.pdf">taln-2010-long-021</a> 
						<a href="bibtex/taln-2010-long-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-021-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-021-key');">mots clés</a> <br/>

							<p id="taln-2010-long-021-abs" class="resume">
							<b>Résumé : </b> Les disfluences inhérents de toute parole spontanée sont un vrai défi pour les systèmes de compréhension de la parole. Ainsi, nous proposons dans cet article, une méthode originale pour le traitement des disfluences (plus précisément, les autocorrections, les répétitions, les hésitations et les amorces) dans le cadre de la compréhension automatique de l’oral arabe spontané. Notre méthode est basée sur une analyse à la fois robuste et partielle, des énoncés oraux arabes. L’idée consiste à combiner une technique de reconnaissance de patrons avec une analyse sémantique superficielle par segments conceptuels. Cette méthode a été testée à travers le module de compréhension du système SARF, un serveur vocal interactif offrant des renseignements sur le transport ferroviaire tunisien (Bahou et al., 2008). Les résultats d’évaluation de ce module montrent que la méthode proposée est très prometteuse. En effet, les mesures de rappel, de précision et de F-Measure sont respectivement de 79.23%, 74.09% et 76.57%.
							</p>

							<p id="taln-2010-long-021-key" class="mots_cles">
							<b>Mots clés : </b> disfluences, segment conceptuel, reconnaissance de patrons, parole arabe spontanée
							</p>

					</div>
					

					<div class="article">

						<b>Camille Guinaudeau, Guillaume Gravier, Pascale Sébillot</b>


						<br/>

							<i>Utilisation de relations sémantiques pour améliorer la segmentation thématique de documents télévisuels</i> <br/>

						<a href="actes/taln-2010-long-022.pdf">taln-2010-long-022</a> 
						<a href="bibtex/taln-2010-long-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-022-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-022-key');">mots clés</a> <br/>

							<p id="taln-2010-long-022-abs" class="resume">
							<b>Résumé : </b> Les méthodes de segmentation thématique exploitant une mesure de la cohésion lexicale peuvent être appliquées telles quelles à des transcriptions automatiques de programmes télévisuels. Cependant, elles sont moins efficaces dans ce contexte, ne prenant en compte ni les particularités des émissions TV, ni celles des transcriptions. Nous étudions ici l’apport de relations sémantiques pour rendre les techniques de segmentation thématique plus robustes. Nous proposons une méthode pour exploiter ces relations dans une mesure de la cohésion lexicale et montrons qu’elles permettent d’augmenter la F1-mesure de +1.97 et +11.83 sur deux corpus composés respectivement de 40h de journaux télévisés et de 40h d’émissions de reportage. Ces améliorations démontrent que les relations sémantiques peuvent rendre les méthodes de segmentation moins sensibles aux erreurs de transcription et au manque de répétitions constaté dans certaines émissions télévisées.
							</p>

							<p id="taln-2010-long-022-key" class="mots_cles">
							<b>Mots clés : </b> Segmentation thématique, documents oraux, cohésion lexicale, relations sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>Clémentine Adam, Philippe Muller, Cécile Fabre</b>


						<br/>

							<i>Une évaluation de l’impact des types de textes sur la tâche de segmentation thématique</i> <br/>

						<a href="actes/taln-2010-long-023.pdf">taln-2010-long-023</a> 
						<a href="bibtex/taln-2010-long-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-023-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-023-key');">mots clés</a> <br/>

							<p id="taln-2010-long-023-abs" class="resume">
							<b>Résumé : </b> Cette étude a pour but de contribuer à la définition des objectifs de la segmentation thématique (ST), en incitant à prendre en considération le paramètre du type de textes dans cette tâche. Notre hypothèse est que, si la ST est certes pertinente pour traiter certains textes dont l’organisation est bien thématique, elle n’est pas adaptée à la prise en compte d’autres modes d’organisation (temporelle, rhétorique), et ne peut pas être appliquée sans précaution à des textes tout-venants. En comparant les performances d’un système de ST sur deux corpus, à organisation thématique &#34;forte&#34; et &#34;faible&#34;, nous montrons que cette tâche est effectivement sensible à la nature des textes.
							</p>

							<p id="taln-2010-long-023-key" class="mots_cles">
							<b>Mots clés : </b> Segmentation thématique, organisation textuelle, cohésion lexicale, voisins distributionnels
							</p>

					</div>
					

					<div class="article">

						<b>Ludovic Jean-Louis, Romaric Besançon, Olivier Ferret</b>


						<br/>

							<i>Utilisation d’indices temporels pour la segmentation événementielle de textes</i> <br/>

						<a href="actes/taln-2010-long-024.pdf">taln-2010-long-024</a> 
						<a href="bibtex/taln-2010-long-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-024-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-024-key');">mots clés</a> <br/>

							<p id="taln-2010-long-024-abs" class="resume">
							<b>Résumé : </b> Dans le domaine de l’Extraction d’Information, une place importante est faite à l’extraction d’événements dans des dépêches d’actualité, particulièrement justifiée dans le contexte d’applications de veille. Or il est fréquent qu’une dépêche d’actualité évoque plusieurs événements de même nature pour les comparer. Nous proposons dans cet article d’étudier des méthodes pour segmenter les textes en séparant les événements, dans le but de faciliter le rattachement des informations pertinentes à l’événement principal. L’idée est d’utiliser des modèles d’apprentissage statistique exploitant les marqueurs temporels présents dans les textes pour faire cette segmentation. Nous présentons plus précisément deux modèles (HMM et CRF) entraînés pour cette tâche et, en faisant une évaluation de ces modèles sur un corpus de dépêches traitant d’événements sismiques, nous montrons que les méthodes proposées permettent d’obtenir des résultats au moins aussi bons que ceux d’une approche ad hoc, avec une approche beaucoup plus générique.
							</p>

							<p id="taln-2010-long-024-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’information, extraction d’événements, segmentation de textes, indices temporels, apprentissage statistique
							</p>

					</div>
					

					<div class="article">

						<b>Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Patricia Velázquez-Morales, Eric Sanjuan</b>


						<br/>

							<i>Évaluation automatique de résumés avec et sans référence</i> <br/>

						<a href="actes/taln-2010-long-025.pdf">taln-2010-long-025</a> 
						<a href="bibtex/taln-2010-long-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-025-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-025-key');">mots clés</a> <br/>

							<p id="taln-2010-long-025-abs" class="resume">
							<b>Résumé : </b> Nous étudions différentes méthodes d’évaluation de résumé de documents basées sur le contenu. Nous nous intéressons en particulier à la corrélation entre les mesures d’évaluation avec et sans référence humaine. Nous avons développé FRESA, un nouveau système d’évaluation fondé sur le contenu qui calcule les divergences entre les distributions de probabilité. Nous appliquons notre système de comparaison aux diverses mesures d’évaluation bien connues en résumé de texte telles que la Couverture, Responsiveness, Pyramids et Rouge en étudiant leurs associations dans les tâches du résumé multi-document générique (francais/anglais), focalisé (anglais) et résumé mono-document générique (français/espagnol).
							</p>

							<p id="taln-2010-long-025-key" class="mots_cles">
							<b>Mots clés : </b> Mesures d’évaluation, Résumé automatique de textes
							</p>

					</div>
					

					<div class="article">

						<b>Pierre-Etienne Genest, Guy Lapalme, Mehdi Yousfi-Monod</b>


						<br/>

							<i>Jusqu’où peut-on aller avec les méthodes par extraction pour la rédaction de résumés?</i> <br/>

						<a href="actes/taln-2010-long-026.pdf">taln-2010-long-026</a> 
						<a href="bibtex/taln-2010-long-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-026-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-026-key');">mots clés</a> <br/>

							<p id="taln-2010-long-026-abs" class="resume">
							<b>Résumé : </b> La majorité des systèmes de résumés automatiques sont basés sur l’extraction de phrases, or on les compare le plus souvent avec des résumés rédigés manuellement par abstraction. Nous avons mené une expérience dans le but d’établir une limite supérieure aux performances auxquelles nous pouvons nous attendre avec une approche par extraction. Cinq résumeurs humains ont composé 88 résumés de moins de 100 mots, en extrayant uniquement des phrases présentes intégralement dans les documents d’entrée. Les résumés ont été notés sur la base de leur contenu, de leur niveau linguistique et de leur qualité globale par les évaluateurs de NIST dans le cadre de la compétition TAC 2009. Ces résumés ont obtenus de meilleurs scores que l’ensemble des 52 systèmes automatiques participant à la compétition, mais de nettement moins bons que ceux obtenus par les résumeurs humains pouvant formuler les phrases de leur choix dans le résumé. Ce grand écart montre l’insuffisance des méthodes par extraction pure.
							</p>

							<p id="taln-2010-long-026-key" class="mots_cles">
							<b>Mots clés : </b> Résumés automatiques, résumés par extraction, résumés manuels
							</p>

					</div>
					

					<div class="article">

						<b>Anne Garcia-Fernandez, Sophie Rosset, Anne Vilnat</b>


						<br/>

							<i>Comment formule-t-on une réponse en langue naturelle ?</i> <br/>

						<a href="actes/taln-2010-long-027.pdf">taln-2010-long-027</a> 
						<a href="bibtex/taln-2010-long-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-027-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-027-key');">mots clés</a> <br/>

							<p id="taln-2010-long-027-abs" class="resume">
							<b>Résumé : </b> Cet article présente l’étude d’un corpus de réponses formulées par des humains à des questions factuelles. Des observations qualitatives et quantitatives sur la reprise d’éléments de la question dans les réponses sont exposées. La notion d’information-réponse est introduite et une étude de la présence de cet élément dans le corpus est proposée. Enfin, les formulations des réponses sont étudiées.
							</p>

							<p id="taln-2010-long-027-key" class="mots_cles">
							<b>Mots clés : </b> systèmes de réponse à une question, variations linguistiques, réponse en langue naturelle, oral et écrit
							</p>

					</div>
					

					<div class="article">

						<b>Do Thi Ngoc Diep, Laurent Besacier, Eric Castelli</b>


						<br/>

							<i>Apprentissage non supervisé pour la traduction automatique : application à un couple de langues peu doté</i> <br/>

						<a href="actes/taln-2010-long-028.pdf">taln-2010-long-028</a> 
						<a href="bibtex/taln-2010-long-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-028-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-028-key');">mots clés</a> <br/>

							<p id="taln-2010-long-028-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthode non-supervisée pour extraire des paires de phrases parallèles à partir d’un corpus comparable. Un système de traduction automatique est utilisé pour exploiter le corpus comparable et détecter les paires de phrases parallèles. Un processus itératif est exécuté non seulement pour augmenter le nombre de paires de phrases parallèles extraites, mais aussi pour améliorer la qualité globale du système de traduction. Une comparaison avec une méthode semi-supervisée est présentée également. Les expériences montrent que la méthode non-supervisée peut être réellement appliquée dans le cas où on manque de données parallèles. Bien que les expériences préliminaires soient menées sur la traduction français-anglais, cette méthode non-supervisée est également appliquée avec succès à un couple de langues peu doté : vietnamien-français.
							</p>

							<p id="taln-2010-long-028-key" class="mots_cles">
							<b>Mots clés : </b> apprentissage non-supervisé, système de traduction automatique, corpus comparable, paires de phrases parallèles
							</p>

					</div>
					

					<div class="article">

						<b>Ahmed El Kholy, Nizar Habash</b>


						<br/>

							<i>Orthographic and Morphological Processing for English-Arabic Statistical Machine Translation</i> <br/>

						<a href="actes/taln-2010-long-029.pdf">taln-2010-long-029</a> 
						<a href="bibtex/taln-2010-long-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-029-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-029-key');">mots clés</a> <br/>

							<p id="taln-2010-long-029-abs" class="resume">
							<b>Résumé : </b> De nombreux travaux en Traduction Automatique Statistique (TAS) pour des langues d’entrée morphologiquement riches montrent que la ségmentation morphologique et la normalisation orthographique améliorent la qualité des traductions en diminuant la sparsité des données. Dans cet article, nous étudions l’impact de ce prétraitement pour la TAS vers une langue de sortie riche morphologiquement, comme l’Arabe. Nous explorons l’espace des schémas de segmentation et des options de normalisation possibles. Nous évaluons seulement la sortie sous une forme désegmentée et enrichie orthographiquement. Nos résultats montrent d’une part que le meilleur schéma pour la ségmentation est celui de la Penn Arabic Treebank. D’autre part, la meilleure procédure de prétraitement consiste à entraîner le système sur des données normalisées orthographiquement, puis à enrichir et désegmenter les traductions en sortie.
							</p>

							<p id="taln-2010-long-029-key" class="mots_cles">
							<b>Mots clés : </b> Langue Arabe, Morphologie, Ségmentation, Désegmentation, La Traduction Automatique Statistique
							</p>

					</div>
					

					<div class="article">

						<b>Marine Carpuat, Yuval Marton, Nizar Habash</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Reordering Matrix Post-verbal Subjects for Arabic-to-English SMT</i> <br/>

						<a href="actes/taln-2010-long-030.pdf">taln-2010-long-030</a> 
						<a href="bibtex/taln-2010-long-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-030-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-030-key');">mots clés</a> <br/>

							<p id="taln-2010-long-030-abs" class="resume">
							<b>Résumé : </b> Distinguer les constructions verbe-sujet (VS) des propositions principales (“matrice”) et subordonnées (“non-matrice”) améliore notre nouveau modèle de réordonnancement pour l’alignement des mots en Traduction Automatique Statistique (TAS) arabe-anglais (Carpuat et al., 2010). D’une part, la majorité des constructions verbe-sujet (VS) dans les propositions principales doivent être réordonnancées en anglais, alors que l’ordre du verbe et du sujet est préservé dans la moitié des cas de constructions VS subordonnées. D’autre part, nous constatons que notre analyseur syntaxique parvient à mieux identifier les constructions VS des propositions principales. Ces observations nous amènent à limiter le réordonnancement des constructions VS à celles des propositions principales lors de l’alignement des mots. Cette technique améliore substantiellement la performance d’un système de TAS conventionnel, et d’un système qui réordonnance toutes les constructions VS. L’amélioration des mesures BLEU et TER obtenue par simple réordonnancement représente presque la moitié de l’amélioration obtenue lorsque le modèle d’alignement des mots est entraîné sur un corpus parallèle d’une taille cinq fois supérieure.
							</p>

							<p id="taln-2010-long-030-key" class="mots_cles">
							<b>Mots clés : </b> Analyse morpho-syntaxique de l’arabe, Traduction automatique statistique, VS, VSO
							</p>

					</div>
					

					<div class="article">

						<b>Michael Zock, Guy Lapalme</b>


						<br/>

							<i>Du TAL au TIL</i> <br/>

						<a href="actes/taln-2010-long-031.pdf">taln-2010-long-031</a> 
						<a href="bibtex/taln-2010-long-031.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-031-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-031-key');">mots clés</a> <br/>

							<p id="taln-2010-long-031-abs" class="resume">
							<b>Résumé : </b> Historiquement deux types de traitement de la langue ont été étudiés: le traitement par le cerveau (approche psycholinguistique) et le traitement par la machine (approche TAL). Nous pensons qu’il y a place pour un troisième type: le traitement interactif de la langue (TIL), l’ordinateur assistant le cerveau. Ceci correspond à un besoin réel dans la mesure où les gens n’ont souvent que des connaissances partielles par rapport au problème à résoudre. Le but du TIL est de construire des ponts entre ces connaissances momentanées d’un utilisateur et la solution recherchée. À l&#39;aide de quelques exemples, nous essayons de montrer que ceci est non seulement faisable et souhaitable, mais également d’un coût très raisonnable.
							</p>

							<p id="taln-2010-long-031-key" class="mots_cles">
							<b>Mots clés : </b> traitement interactif de la langue, prise en compte de l&#39;usager, outils de traitement de la langue, apprentissage des langues, dictionnaires, livres de phrases, concordanciers, traduction
							</p>

					</div>
					

					<div class="article">

						<b>Piet Mertens</b>


						<br/>

							<i>Restrictions de sélection et réalisations syntagmatiques dans DICOVALENCE Conversion vers un format utilisable en TAL</i> <br/>

						<a href="actes/taln-2010-long-032.pdf">taln-2010-long-032</a> 
						<a href="bibtex/taln-2010-long-032.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-032-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-032-key');">mots clés</a> <br/>

							<p id="taln-2010-long-032-abs" class="resume">
							<b>Résumé : </b> Cet article décrit des modifications du dictionnaire de valence des verbes du français DICOVALENCE qui visent à le rendre neutre par rapport aux modèles syntaxiques, à expliciter certaines informations sur le cadre de sous-catégorisation et à le rendre ainsi directement utilisable en TAL. Les informations explicitées sont les suivantes : (a) les fonctions syntaxiques des arguments verbaux, (b) les restrictions de sélection portant sur ces arguments et (c) leurs réalisations syntagmatiques possibles. Les restrictions sont exprimées à l’aide de traits sémantiques. L’article décrit aussi le calcul de ces traits sémantiques à partir des paradigmes des pronoms (et d’éléments similaires) associés aux arguments. On obtient un format indépendant du modèle syntaxique, dont l’interprétation est transparente.
							</p>

							<p id="taln-2010-long-032-key" class="mots_cles">
							<b>Mots clés : </b> lexiques syntaxiques, restrictions de sélection, traits sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>Caroline Barrière</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Recherche contextuelle d’équivalents en banque de terminologie</i> <br/>

						<a href="actes/taln-2010-long-033.pdf">taln-2010-long-033</a> 
						<a href="bibtex/taln-2010-long-033.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-033-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-033-key');">mots clés</a> <br/>

							<p id="taln-2010-long-033-abs" class="resume">
							<b>Résumé : </b> Notre recherche démontre que l’utilisation du contenu d’un texte à traduire permet de mieux cibler dans une banque de terminologie les équivalents terminologiques pertinents à ce texte. Une banque de terminologie a comme particularité qu’elle catégorise ses entrées (fiches) en leur assignant un ou des domaines provenant d’une liste de domaines préétablie. La stratégie ici présentée repose sur l’utilisation de cette information sur les domaines. Un algorithme a été développé pour l’assignation automatique d’un profil de domaines à un texte. Celui-ci est combiné à un algorithme d’appariement entre les domaines d’un terme présent dans la banque de terminologie et le profil de domaines du texte. Pour notre expérimentation, des résumés bilingues (français et anglais) provenant de huit revues scientifiques nous fournissent un ensemble de 1130 paires d’équivalents terminologiques et le Grand Dictionnaire Terminologique (Office Québécois de la Langue Française) nous sert de ressource terminologique. Sur notre ensemble, nous démontrons une réduction de 75% du rang moyen de l’équivalent correct en comparaison avec un choix au hasard.
							</p>

							<p id="taln-2010-long-033-key" class="mots_cles">
							<b>Mots clés : </b> recherche contextuelle, équivalents terminologiques, banque de terminologie, désambiguïsation par domaine
							</p>

					</div>
					

					<div class="article">

						<b>Guillaume Bonfante, Bruno Guillaume, Mathieu Morey, Guy Perrier</b>


						<br/>

							<i>Réécriture de graphes de dépendances pour l’interface syntaxe-sémantique</i> <br/>

						<a href="actes/taln-2010-long-034.pdf">taln-2010-long-034</a> 
						<a href="bibtex/taln-2010-long-034.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-034-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-034-key');">mots clés</a> <br/>

							<p id="taln-2010-long-034-abs" class="resume">
							<b>Résumé : </b> Nous définissons le beta-calcul, un calcul de réécriture de graphes, que nous proposons d’utiliser pour étudier les liens entre différentes représentations linguistiques. Nous montrons comment transformer une analyse syntaxique en une représentation sémantique par la composition de deux jeux de règles de beta-calcul. Le premier souligne l’importance de certaines informations syntaxiques pour le calcul de la sémantique et explicite le lien entre syntaxe et sémantique sous-spécifiée. Le second décompose la recherche de modèles pour les représentations sémantiques sous-spécifiées.
							</p>

							<p id="taln-2010-long-034-key" class="mots_cles">
							<b>Mots clés : </b> Dépendances, réécriture de graphes, interface syntaxe-sémantique, DMRS
							</p>

					</div>
					

					<div class="article">

						<b>Karën Fort, Claire François, Maha Ghribi</b>


						<br/>

							<i>Évaluer des annotations manuelles dispersées : les coefficients sont-ils suffisants pour estimer l’accord inter-annotateurs ?</i> <br/>

						<a href="actes/taln-2010-long-035.pdf">taln-2010-long-035</a> 
						<a href="bibtex/taln-2010-long-035.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-035-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-035-key');">mots clés</a> <br/>

							<p id="taln-2010-long-035-abs" class="resume">
							<b>Résumé : </b> L’objectif des travaux présentés dans cet article est l’évaluation de la qualité d’annotations manuelles de relations de renommage de gènes dans des résumés scientifiques, annotations qui présentent la caractéristique d’être très dispersées. Pour cela, nous avons calculé et comparé les coefficients les plus communément utilisés, entre autres kappa (Cohen, 1960) et pi (Scott, 1955), et avons analysé dans quelle mesure ils sont adaptés à nos données. Nous avons également étudié les différentes pondérations applicables à ces coefficients permettant de calculer le kappa pondéré (Cohen, 1968) et l’alpha (Krippendorff, 1980, 2004). Nous avons ainsi étudié le biais induit par la grande prévalence d’une catégorie et défini un mode de calcul des distances entre catégories reposant sur les annotations réalisées.
							</p>

							<p id="taln-2010-long-035-key" class="mots_cles">
							<b>Mots clés : </b> Annotation manuelle, évaluation, accord inter-annotateurs
							</p>

					</div>
					

					<div class="article">

						<b>Phuong Le-Hong, Azim Roussanaly, Thi Minh Huyen Nguyen, Mathias Rossignol</b>


						<br/>

							<i>An empirical study of maximum entropy approach for part-of-speech tagging of Vietnamese texts</i> <br/>

						<a href="actes/taln-2010-long-036.pdf">taln-2010-long-036</a> 
						<a href="bibtex/taln-2010-long-036.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-036-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-036-key');">mots clés</a> <br/>

							<p id="taln-2010-long-036-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article une étude empirique de l’application de l’approche de l’entropie maximale pour l’étiquetage syntaxique de textes vietnamiens. Le vietnamien est une langue qui possède des caractéristiques spéciales qui la distinguent largement des langues occidentales. Notremeilleur étiqueteur explore et inclut des connaissances utiles qui, en terme de performance pour l’étiquetage de textes vietnamiens, fournit un taux de précision globale de 93.40% et de 80.69% pour les mots inconnus sur un ensemble de test du corpus arboré vietnamien. Notre étiqueteur est nettement supérieur à celui qui est en train d’être utilisé pour développer le corpus arboré vietnamien, et à l’heure actuelle c’est le meilleur résultat obtenu pour l’étiquetage de textes vietnamiens.
							</p>

							<p id="taln-2010-long-036-key" class="mots_cles">
							<b>Mots clés : </b> Etiqueteur syntaxique, entropie maximale, texte, vietnamien
							</p>

					</div>
					

					<div class="article">

						<b>Lyne Da Sylva</b>


						<br/>

							<i>Extraction semi-automatique d’un vocabulaire savant de base pour l’indexation automatique</i> <br/>

						<a href="actes/taln-2010-long-037.pdf">taln-2010-long-037</a> 
						<a href="bibtex/taln-2010-long-037.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-037-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-037-key');">mots clés</a> <br/>

							<p id="taln-2010-long-037-abs" class="resume">
							<b>Résumé : </b> Le projet décrit vise à soutenir les efforts de constitution de ressources lexicales utiles à l’indexation automatique. Un type de vocabulaire utile à l’indexation est défini, le vocabulaire savant de base, qui peut s’articuler avec le vocabulaire spécialisé pour constituer des entrées d’index structurées. On présente les résultats d’ une expérimentation d’ extraction (semi-)automatique des mots du vocabulaire savant de base à partir d’un corpus ciblé, constitué de résumés d’articles scientifiques en français et en anglais. La tâche d’extraction a réussi à doubler une liste originale constituée manuellement pour le français. La comparaison est établie avec une expérimentation similaire effectuée pour l’anglais sur un corpus plus grand et contenant des résumés d’articles non seulement en sciences pures mais aussi en sciences humaines et sociales.
							</p>

							<p id="taln-2010-long-037-key" class="mots_cles">
							<b>Mots clés : </b> classes de vocabulaire, indexation automatique, extraction automatique, corpus, approche basée sur les corpus, vocabulaire savant de base, ressources lexicales, français
							</p>

					</div>
					

					<div class="article">

						<b>Jean-François Lavallée, Philippe Langlais</b>


						<br/>

							<i>Apprentissage non supervisé de la morphologie d’une langue par généralisation de relations analogiques</i> <br/>

						<a href="actes/taln-2010-long-038.pdf">taln-2010-long-038</a> 
						<a href="bibtex/taln-2010-long-038.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-038-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-038-key');">mots clés</a> <br/>

							<p id="taln-2010-long-038-abs" class="resume">
							<b>Résumé : </b> Bien que les approches fondées sur la théorie de l’information sont prédominantes dans le domaine de l’analyse morphologique non supervisée, depuis quelques années, d’autres approches ont gagné en popularité, dont celles basées sur l’analogie formelle. Cette dernière reste tout de même marginale due notamment à son coût de calcul élevé. Dans cet article, nous proposons un algorithme basé sur l’analogie formelle capable de traiter les lexiques volumineux. Nous introduisons pour cela le concept de règle de cofacteur qui permet de généraliser l’information capturée par une analogie tout en contrôlant les temps de traitement. Nous comparons notre système à 2 systèmes : Morfessor (Creutz &amp; Lagus, 2005), un système de référence dans de nombreux travaux sur l’analyse morphologique et le système analogique décrit par Langlais (2009). Nous en montrons la supériorité pour 3 des 5 langues étudiées ici : le finnois, le turc, et l’allemand.
							</p>

							<p id="taln-2010-long-038-key" class="mots_cles">
							<b>Mots clés : </b> Analyse morphologique non supervisée, Analogie formelle, Approche à base de graphe
							</p>

					</div>
					

					<div class="article">

						<b>Vincent Claveau, Ewa Kijak</b>


						<br/>

							<i>Analyse morphologique en terminologie biomédicale par alignement et apprentissage non-supervisé</i> <br/>

						<a href="actes/taln-2010-long-039.pdf">taln-2010-long-039</a> 
						<a href="bibtex/taln-2010-long-039.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-039-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-039-key');">mots clés</a> <br/>

							<p id="taln-2010-long-039-abs" class="resume">
							<b>Résumé : </b> Dans le domaine biomédical, beaucoup de termes sont des composés savants (composés de plusieurs racines gréco-latines). L’étude de leur morphologie est importante pour de nombreuses applications puisqu’elle permet de structurer ces termes, de les rechercher efficacement, de les traduire... Dans cet article, nous proposons de suivre une démarche originale mais fructueuse pour mener cette analyse morphologique sur des termes simples en français, en nous appuyant sur une langue pivot, le japonais, et plus précisément sur les termes écrits en kanjis. Pour cela nous avons développé un algorithme d’alignement de termes spécialement adapté à cette tâche. C’est cet alignement d’un terme français avec sa traduction en kanjis qui fournit en même temps une décomposition en morphe et leur étiquetage par les kanjis correspondants. Évalué sur un jeu de données conséquent, notre approche obtient une précision supérieure à 70% et montrent son bien fondé en comparaison avec les techniques existantes. Nous illustrons également l’intérêt de notre démarche au travers de deux applications directes de ces alignements : la traduction de termes inconnus et la découverte de relations entre morphes pour la tructuration terminologique.
							</p>

							<p id="taln-2010-long-039-key" class="mots_cles">
							<b>Mots clés : </b> Alignement, terminologie, morphologie, analogie, traduction de terme, kanji
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Sagot, Géraldine Walther</b>


						<br/>

							<i>Développement de ressources pour le persan: lexique morphologique et chaîne de traitements de surface</i> <br/>

						<a href="actes/taln-2010-long-040.pdf">taln-2010-long-040</a> 
						<a href="bibtex/taln-2010-long-040.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-long-040-abs');">résumé</a>
							<a onclick="toggle('taln-2010-long-040-key');">mots clés</a> <br/>

							<p id="taln-2010-long-040-abs" class="resume">
							<b>Résumé : </b> Nous présentons PerLex, un lexique morphologique du persan à large couverture et librement disponible, accompagné d’une chaîne de traitements de surface pour cette langue. Nous décrivons quelques caractéristiques de la morphologie du persan, et la façon dont nous l’avons représentée dans le formalisme lexical Alexina, sur lequel repose PerLex. Nous insistons sur la méthodologie que nous avons employée pour construire les entrées lexicales à partir de diverses sources, ainsi que sur les problèmes liés à la normalisation typographique. Le lexique obtenu a une couverture satisfaisante sur un corpus de référence, et devrait donc constituer un bon point de départ pour le développement d’un lexique syntaxique du persan.
							</p>

							<p id="taln-2010-long-040-key" class="mots_cles">
							<b>Mots clés : </b> Lexique morphologique, Persan, Développement de lexiques, Traitements de surface
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="court">Papiers courts</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Louise Deléger, Bruno Cartoni</b>


						<br/>

							<i>Adjectifs relationnels et langue de spécialité : vérification d’une hypothèse linguistique en corpus comparable médical</i> <br/>

						<a href="actes/taln-2010-court-001.pdf">taln-2010-court-001</a> 
						<a href="bibtex/taln-2010-court-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-001-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-001-key');">mots clés</a> <br/>

							<p id="taln-2010-court-001-abs" class="resume">
							<b>Résumé : </b> Cet article présente une étude en corpus comparable médical pour confirmer la préférence d’utilisation des adjectifs relationnels dans les langues de spécialité et examiner plus finement l’alternance entre syntagmes nominaux avec adjectifs relationnels et syntagmes avec complément prépositionnel.
							</p>

							<p id="taln-2010-court-001-key" class="mots_cles">
							<b>Mots clés : </b> corpus comparables monolingues, morphologie constructionnelle, langue de spécialité
							</p>

					</div>
					

					<div class="article">

						<b>Mathieu Lafourcade, Alain Joubert</b>


						<br/>

							<i>Détermination et pondération des raffinements d’un terme à partir de son arbre des usages nommés</i> <br/>

						<a href="actes/taln-2010-court-002.pdf">taln-2010-court-002</a> 
						<a href="bibtex/taln-2010-court-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-002-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-002-key');">mots clés</a> <br/>

							<p id="taln-2010-court-002-abs" class="resume">
							<b>Résumé : </b> Grâce à la participation d’un grand nombre de personnes via des jeux accessibles sur le web, nous avons construit un réseau lexical évolutif de grande taille pour le Français. A partir de cette ressource, nous avons abordé la question de la détermination des sens d’usage d’un terme, puis après avoir introduit la notion de similarité entre ces différents usages, nous avons pu obtenir pour un terme son arbre des usages : la racine regroupe tous les usages du terme et une descente dans l’arbre correspond à un raffinement de ces usages. Le nommage des différents noeuds est effectué lors d’une descente en largeur. En simplifiant l’arbre des usages nommés, nous déterminons les différents sens d’un terme, sens que nous introduisons dans le réseau lexical en tant que noeuds de raffinement du terme considéré. Nous terminons par une évaluation empirique des résultats obtenus.
							</p>

							<p id="taln-2010-court-002-key" class="mots_cles">
							<b>Mots clés : </b> réseau lexical, arbre des usages nommés d’un terme, pondération des sens d’un terme
							</p>

					</div>
					

					<div class="article">

						<b>Jonathan Chevelu, Yves Lepage, Thierry Moudenc, Ghislain Putois</b>


						<br/>

							<i>L’évaluation des paraphrases : pour une prise en compte de la tâche</i> <br/>

						<a href="actes/taln-2010-court-003.pdf">taln-2010-court-003</a> 
						<a href="bibtex/taln-2010-court-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-003-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-003-key');">mots clés</a> <br/>

							<p id="taln-2010-court-003-abs" class="resume">
							<b>Résumé : </b> Les définitions des paraphrases privilégient généralement la conservation du sens. Cet article démontre par l’absurde qu’une évaluation uniquement basée sur la conservation du sens permet à un système inutile de production de paraphrase d’être jugé meilleur qu’un système au niveau de l’état de l’art. La conservation du sens n’est donc pas l’unique critère des paraphrases. Nous exhibons les trois objectifs des paraphrases : la conservation du sens, la naturalité et l’adaptation à la tâche. La production de paraphrase est alors un compromis dépendant de la tâche entre ces trois critères et ceux-ci doivent être pris en compte lors des évaluations.
							</p>

							<p id="taln-2010-court-003-key" class="mots_cles">
							<b>Mots clés : </b> Générateur de paraphrase, évaluation des paraphrases
							</p>

					</div>
					

					<div class="article">

						<b>Olivier Collin, Benoît Gaillard, Jean-Léon Bouraoui</b>


						<br/>

							<i>Constitution d&#39;une ressource sémantique issue du treillis des catégories de Wikipedia</i> <br/>

						<a href="actes/taln-2010-court-004.pdf">taln-2010-court-004</a> 
						<a href="bibtex/taln-2010-court-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-004-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-004-key');">mots clés</a> <br/>

							<p id="taln-2010-court-004-abs" class="resume">
							<b>Résumé : </b> Le travail présenté dans cet article s&#39;inscrit dans le thème de l&#39;acquisition automatique de ressources sémantiques s&#39;appuyant sur les données de Wikipedia. Nous exploitons le graphe des catégories associées aux pages de Wikipedia à partir duquel nous extrayons une hiérarchie de catégories parentes, sémantiquement et thématiquement liées. Cette extraction est le résultat d&#39;une stratégie de plus court chemin appliquée au treillis global des catégories. Chaque page peut ainsi être représentée dans l&#39;espace de ses catégories propres, ainsi que des catégories parentes. Nous montrons la possibilité d&#39;utiliser cette ressource pour deux applications. La première concerne l&#39;indexation et la classification des pages de Wikipedia. La seconde concerne la désambiguïsation dans le cadre d&#39;un traducteur de requêtes français/anglais. Ce dernier travail a été réalisé en exploitant les catégories des pages anglaises.
							</p>

							<p id="taln-2010-court-004-key" class="mots_cles">
							<b>Mots clés : </b> Wikipedia, plus court chemin, désambiguïsation, classification, traduction de requête
							</p>

					</div>
					

					<div class="article">

						<b>Laurence Danlos, Benoît Sagot</b>


						<br/>

							<i>Ponctuations fortes abusives</i> <br/>

						<a href="actes/taln-2010-court-005.pdf">taln-2010-court-005</a> 
						<a href="bibtex/taln-2010-court-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-005-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-005-key');">mots clés</a> <br/>

							<p id="taln-2010-court-005-abs" class="resume">
							<b>Résumé : </b> Certaines ponctuations fortes sont « abusivement » utilisées à la place de ponctuations faibles, débouchant sur des phrases graphiques qui ne sont pas des phrases grammaticales. Cet article présente une étude sur corpus de ce phénomène et une ébauche d’outil pour repérer automatiquement les ponctuations fortes abusives.
							</p>

							<p id="taln-2010-court-005-key" class="mots_cles">
							<b>Mots clés : </b> pseudo-phrase, phrase averbale, analyse syntaxique et sémantique
							</p>

					</div>
					

					<div class="article">

						<b>Véronique Moriceau, Xavier Tannier, Mathieu Falco</b>


						<br/>

							<i>Une étude des questions “complexes” en question-réponse</i> <br/>

						<a href="actes/taln-2010-court-006.pdf">taln-2010-court-006</a> 
						<a href="bibtex/taln-2010-court-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-006-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-006-key');">mots clés</a> <br/>

							<p id="taln-2010-court-006-abs" class="resume">
							<b>Résumé : </b> La plupart des systèmes de question-réponse ont été conçus pour répondre à des questions dites “factuelles” (réponses précises comme des dates, des lieux), et peu se sont intéressés au traitement des questions complexes. Cet article présente une typologie des questions en y incluant les questions complexes, ainsi qu’une typologie des formes de réponses attendues pour chaque type de questions. Nous présentons également des expériences préliminaires utilisant ces typologies pour les questions complexes, avec de bons résultats.
							</p>

							<p id="taln-2010-court-006-key" class="mots_cles">
							<b>Mots clés : </b> Système de question-réponse, questions complexes
							</p>

					</div>
					

					<div class="article">

						<b>Muhammad Ghulam Abbas Malik, Christian Boitet, Pushpak Bhattacharyya, Laurent Besacier</b>


						<br/>

							<i>Weak Translation Problems – a case study of Scriptural Translation</i> <br/>

						<a href="actes/taln-2010-court-007.pdf">taln-2010-court-007</a> 
						<a href="bibtex/taln-2010-court-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-007-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-007-key');">mots clés</a> <br/>

							<p id="taln-2010-court-007-abs" class="resume">
							<b>Résumé : </b> La TA généraliste de haute qualité et totalement automatique est considérée comme impossible. Nous nous intéressons aux problèmes de traduction scripturale, qui sont des sous-problèmes faibles du problème général de la traduction. Nous présentons les caractéristiques des problèmes faibles de traduction et les problèmes de traduction scripturale, décrivons différentes approches computationnelles (à états finis, statistiques, et hybrides) et présentons nos résultats sur différentes combinaisons de langues et systèmes d’écriture Indo-Pak.
							</p>

							<p id="taln-2010-court-007-key" class="mots_cles">
							<b>Mots clés : </b> problèmes faibles de traduction, traduction scripturale, traduction interdialectal, transcriptions, translittérations
							</p>

					</div>
					

					<div class="article">

						<b>Fadoua Ataa Allah, Siham Boulaknadel</b>


						<br/>

							<i>Pseudo-racinisation de la langue amazighe</i> <br/>

						<a href="actes/taln-2010-court-008.pdf">taln-2010-court-008</a> 
						<a href="bibtex/taln-2010-court-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-008-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-008-key');">mots clés</a> <br/>

							<p id="taln-2010-court-008-abs" class="resume">
							<b>Résumé : </b> Dans le cadre de la promotion de la langue amazighe, nous avons voulu lui apporter des ressources et outils linguistiques pour son traitement automatique et son intégration dans le domaine des nouvelles technologies de l&#39;information et de la communication. Partant de ce principe, nous avons opté, au sein de l’Institut Royal de la Culture Amazighe, pour une démarche innovante de réalisations progressives de ressources linguistiques et d’outils de base de traitement automatique, qui permettront de préparer le terrain pour d’éventuelles recherches scientifiques. Dans cette perspective, nous avons entrepris de développer, dans un premier temps, un outil de pseudoracinisation basé sur une approche relevant du cas de la morphologie flexionnelle et reposant sur l’élimination d’une liste de suffixes et de préfixes de la langue amazighe. Cette approche permettra de regrouper les mots sémantiquement proches à partir de ressemblances afin d’être exploités dans des applications tel que la recherche d’information et la classification.
							</p>

							<p id="taln-2010-court-008-key" class="mots_cles">
							<b>Mots clés : </b> Langue amazighe, Pseudo-racinisation, Morphologie flexionnelle
							</p>

					</div>
					

					<div class="article">

						<b>François-Régis Chaumartin, Sylvain Kahane</b>


						<br/>

							<i>Une approche paresseuse de l’analyse sémantique ou comment construire une interface syntaxe-sémantique à partir d’exemples</i> <br/>

						<a href="actes/taln-2010-court-009.pdf">taln-2010-court-009</a> 
						<a href="bibtex/taln-2010-court-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-009-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-009-key');">mots clés</a> <br/>

							<p id="taln-2010-court-009-abs" class="resume">
							<b>Résumé : </b> Cet article montre comment calculer une interface syntaxe-sémantique à partir d’un analyseur en dépendance quelconque et interchangeable, de ressources lexicales variées et d’une base d’exemples associés à leur représentation sémantique. Chaque exemple permet de construire une règle d’interface. Nos représentations sémantiques sont des graphes hiérarchisés de relations prédicat-argument entre des acceptions lexicales et notre interface syntaxe-sémantique est une grammaire de correspondance polarisée. Nous montrons comment obtenir un système très modulaire en calculant certaines règles par « soustraction » de règles moins modulaires.
							</p>

							<p id="taln-2010-court-009-key" class="mots_cles">
							<b>Mots clés : </b> Interface syntaxe-sémantique, graphe sémantique, grammaires de dépendance, GUP (Grammaire d’unification polarisée), GUST (Grammaire d’unification Sens-Texte)
							</p>

					</div>
					

					<div class="article">

						<b>Lorenza Russo</b>


						<br/>

							<i>La traduction automatique des pronoms clitiques. Quelle approche pour quels résultats?</i> <br/>

						<a href="actes/taln-2010-court-010.pdf">taln-2010-court-010</a> 
						<a href="bibtex/taln-2010-court-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-010-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-010-key');">mots clés</a> <br/>

							<p id="taln-2010-court-010-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous abordons la problématique de la traduction automatique des pronoms clitiques, en nous focalisant sur la traduction de l’italien vers le français et en comparant les résultats obtenus par trois systèmes : Its-2, développé au LATL (Laboratoire d’Analyse et de Technologie du Langage) et basé sur un analyseur syntaxique profond ; Babelfish, basé sur des règles linguistiques ; et Google Translate, caractérisé par une approche statistique.
							</p>

							<p id="taln-2010-court-010-key" class="mots_cles">
							<b>Mots clés : </b> Analyseur syntaxique, traduction automatique, pronoms clitiques, proclise, enclise
							</p>

					</div>
					

					<div class="article">

						<b>François Morlane-Hondère, Cécile Fabre</b>


						<br/>

							<i>L’antonymie observée avec des méthodes de TAL : une relation à la fois syntagmatique et paradigmatique ?</i> <br/>

						<a href="actes/taln-2010-court-011.pdf">taln-2010-court-011</a> 
						<a href="bibtex/taln-2010-court-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-011-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-011-key');">mots clés</a> <br/>

							<p id="taln-2010-court-011-abs" class="resume">
							<b>Résumé : </b> Cette étude utilise des outils de TAL pour tester l’hypothèse avancée par plusieurs études linguistiques récentes selon laquelle la relation antonymique, classiquement décrite comme une relation paradigmatique, a la particularité de fonctionner également sur le plan syntagmatique, c’est-à-dire de réunir des mots qui sont non seulement substituables mais qui apparaissent également régulièrement dans des relations contextuelles. Nous utilisons deux méthodes – l’analyse distributionnelle pour le plan paradigmatique, la recherche par patrons antonymiques pour le plan syntagmatique. Les résultats montrent que le diagnostic d’antonymie n’est pas significativement meilleur lorsqu’on croise les deux méthodes, puisqu’une partie des antonymes identifiés ne répondent pas au test de substituabilité, ce qui semble confirmer la prépondérance du plan syntagmatique pour l’étude et l’acquisition de cette relation.
							</p>

							<p id="taln-2010-court-011-key" class="mots_cles">
							<b>Mots clés : </b> sémantique lexicale, antonymie, analyse distributionnelle, patrons lexico-syntaxiques
							</p>

					</div>
					

					<div class="article">

						<b>Ludivine Kuznik, Anne-Laure Guénet, Anne Peradotto, Chloé Clavel</b>


						<br/>

							<i>L’apport des concepts métiers pour la classification des questions ouvertes d’enquête</i> <br/>

						<a href="actes/taln-2010-court-012.pdf">taln-2010-court-012</a> 
						<a href="bibtex/taln-2010-court-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-012-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-012-key');">mots clés</a> <br/>

							<p id="taln-2010-court-012-abs" class="resume">
							<b>Résumé : </b> EDF utilise les techniques de Text Mining pour optimiser sa relation client, en analysant des réponses aux questions ouvertes d&#39;enquête de satisfaction, et des retranscriptions de conversations issues des centres d&#39;appels. Dans cet article, nous présentons les différentes contraintes applicatives liées à l’utilisation d’outils de text mining pour l’analyse de données clients. Après une analyse des différents outils présents sur le marché, nous avons identifié la technologie Skill CartridgeTM fournie par la société TEMIS comme la plus adaptée à nos besoins. Cette technologie nous permet une modélisation sémantique de concepts liés au motif d’insatisfaction. L’apport de cette modélisation est illustrée pour une tâche de classification de réponses d’enquêtes de satisfaction chargée d’évaluer la fidélité des clients EDF. La modélisation sémantique a permis une nette amélioration des scores de classification (F-mesure = 75,5%) notamment pour les catégories correspondant à la satisfaction et au mécontentement.
							</p>

							<p id="taln-2010-court-012-key" class="mots_cles">
							<b>Mots clés : </b> outils de text mining, modélisation de concepts métier, classification supervisée
							</p>

					</div>
					

					<div class="article">

						<b>Charles Dejean, Manoel Fortun, Clotilde Massot, Vincent Pottier, Fabien Poulard, Matthieu Vernier</b>


						<br/>

							<i>Un étiqueteur de rôles grammaticaux libre pour le français intégré à Apache UIMA</i> <br/>

						<a href="actes/taln-2010-court-013.pdf">taln-2010-court-013</a> 
						<a href="bibtex/taln-2010-court-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-013-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-013-key');">mots clés</a> <br/>

							<p id="taln-2010-court-013-abs" class="resume">
							<b>Résumé : </b> L’étiquetage des rôles grammaticaux est une tâche de pré-traitement récurrente. Pour le français, deux outils sont majoritairement utilisés : TreeTagger et Brill. Nous proposons une démarche, ne nécessitant aucune ressource, pour la création d’un modèle de Markov caché (HMM) pour palier les problèmes de ces outils, et de licences notamment. Nous distribuons librement toutes les ressources liées à ce travail.
							</p>

							<p id="taln-2010-court-013-key" class="mots_cles">
							<b>Mots clés : </b> étiquetage grammatical, Modèle de Markov caché, UIMA, Brill, TreeTagger
							</p>

					</div>
					

					<div class="article">

						<b>Michel Généreux, Rita Marquilhas, Iris Hendrickx</b>


						<br/>

							<i>Segmentation Automatique de Lettres Historiques</i> <br/>

						<a href="actes/taln-2010-court-014.pdf">taln-2010-court-014</a> 
						<a href="bibtex/taln-2010-court-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-014-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-014-key');">mots clés</a> <br/>

							<p id="taln-2010-court-014-abs" class="resume">
							<b>Résumé : </b> Cet article présente une approche basée sur la comparaison fréquentielle de modèles lexicaux pour la segmentation automatique de textes historiques Portugais. Cette approche traite d’abord le problème de la segmentation comme un problème de classification, en attribuant à chaque élément lexical présent dans la phase d’apprentissage une valeur de saillance pour chaque type de segment. Ces modèles lexicaux permettent à la fois de produire une segmentation et de faire une analyse qualitative de textes historiques. Notre évaluation montre que l’approche adoptée permet de tirer de l’information sémantique que des approches se concentrant sur la détection des frontières séparant les segments ne peuvent acquérir.
							</p>

							<p id="taln-2010-court-014-key" class="mots_cles">
							<b>Mots clés : </b> Corpus comparables, Saillance, Segmentation, Textes historiques
							</p>

					</div>
					

					<div class="article">

						<b>Helena Blancafort, Gaëlle Recourcé, Javier Couto, Benoît Sagot, Rosa Stern, Denis Teyssou</b>


						<br/>

							<i>Traitement des inconnus : une approche systématique de l’incomplétude lexicale</i> <br/>

						<a href="actes/taln-2010-court-015.pdf">taln-2010-court-015</a> 
						<a href="bibtex/taln-2010-court-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-015-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-015-key');">mots clés</a> <br/>

							<p id="taln-2010-court-015-abs" class="resume">
							<b>Résumé : </b> Cet article aborde le phénomène de l’incomplétude des ressources lexicales, c’est-à-dire la problématique des inconnus, dans un contexte de traitement automatique. Nous proposons tout d’abord une définition opérationnelle de la notion d’inconnu. Nous décrivons ensuite une typologie des différentes classes d’inconnus, motivée par des considérations linguistiques et applicatives ainsi que par l’annotation des inconnus d’un petit corpus selon notre typologie. Cette typologie sera mise en oeuvre et validée par l’annotation d’un corpus important de l’Agence France-Presse dans le cadre du projet EDyLex.
							</p>

							<p id="taln-2010-court-015-key" class="mots_cles">
							<b>Mots clés : </b> mots inconnus, incomplétude lexicale, acquisition dynamique des ressources lexicales
							</p>

					</div>
					

					<div class="article">

						<b>Evelyne Jacquey, Laurence Kister, Mick Grzesitchak, Bertrand Gaiffe, Coralie Reutenauer, Sandrine Ollinger, Mathieu Valette</b>


						<br/>

							<i>Thésaurus et corpus de spécialité sciences du langage : approches lexicométriques appliquées à l’analyse de termes en corpus</i> <br/>

						<a href="actes/taln-2010-court-016.pdf">taln-2010-court-016</a> 
						<a href="bibtex/taln-2010-court-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-016-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-016-key');">mots clés</a> <br/>

							<p id="taln-2010-court-016-abs" class="resume">
							<b>Résumé : </b> Cet article s&#39;inscrit dans les recherches sur l&#39;exploitation de ressources terminologiques pour l&#39;analyse de textes de spécialité, leur annotation et leur indexation. Les ressources en présence sont, d&#39;une part, un thesaurus des Sciences du Langage, le Thesaulangue et, d&#39;autre part, un corpus d’échantillons issus de cinq ouvrages relevant du même domaine. L&#39;article a deux objectifs. Le premier est de déterminer dans quelle mesure les termes de Thesaulangue sont représentés dans les textes. Le second est d&#39;évaluer si les occurrences des unités lexicales correspondant aux termes de Thesaulangue relèvent majoritairement d&#39;emplois terminologiques ou de langue courante. A cette fin, les travaux présentés utilisent une mesure de richesse lexicale telle qu&#39;elle a été définie par Brunet (rapporté dans Muller, 1992) dans le domaine de la lexicométrie, l&#39;indice W. Cette mesure est adaptée afin de mesurer la richesse terminologie (co-occurrents lexicaux et sémantiques qui apparaissent dans Thesaulangue).
							</p>

							<p id="taln-2010-court-016-key" class="mots_cles">
							<b>Mots clés : </b> sémantique lexicale, terminologie, corpus, richesse lexicale, lexicométrie
							</p>

					</div>
					

					<div class="article">

						<b>Marc Le Tallec, Jeanne Villaneau, Jean-Yves Antoine, Agata Savary, Arielle Syssau-Vaccarella</b>


						<br/>

							<i>Détection hors contexte des émotions à partir du contenu linguistique d’énoncés oraux : le système EmoLogus</i> <br/>

						<a href="actes/taln-2010-court-017.pdf">taln-2010-court-017</a> 
						<a href="bibtex/taln-2010-court-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-017-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-017-key');">mots clés</a> <br/>

							<p id="taln-2010-court-017-abs" class="resume">
							<b>Résumé : </b> Le projet EmotiRob, soutenu par l&#39;agence nationale de la recherche, s&#39;est donné pour objectif de détecter des émotions dans un contexte d&#39;application original : la réalisation d&#39;un robot compagnon émotionnel pour des enfants fragilisés. Nous présentons dans cet article le système qui caractérise l&#39;émotion induite par le contenu linguistique des propos de l&#39;enfant. Il se base sur un principe de compositionnalité des émotions, avec une valeur émotionnelle fixe attribuée aux mots lexicaux, tandis que les verbes et les adjectifs agissent comme des fonctions dont le résultat dépend de la valeur émotionnelle de leurs arguments. L&#39;article présente la méthode de calcul utilisée, ainsi que la norme lexicale émotionnelle correspondante. Une analyse quantitative et qualitative des premières expérimentations présente les différences entre les sorties du module de détection et l&#39;annotation d&#39;experts, montrant des résultats satisfaisants, avec la bonne détection de la valence émotionnelle dans plus de 90% des cas.
							</p>

							<p id="taln-2010-court-017-key" class="mots_cles">
							<b>Mots clés : </b> Emotion, valence émotionnelle, norme lexicale émotionnelle, robot compagnon, compréhension de parole
							</p>

					</div>
					

					<div class="article">

						<b>Houda Bouamor, Aurélien Max, Anne Vilnat</b>


						<br/>

							<i>Acquisition de paraphrases sous-phrastiques depuis des paraphrases d’énoncés</i> <br/>

						<a href="actes/taln-2010-court-018.pdf">taln-2010-court-018</a> 
						<a href="bibtex/taln-2010-court-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-018-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-018-key');">mots clés</a> <br/>

							<p id="taln-2010-court-018-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons la tâche d’acquisition de paraphrases sous-phrastiques (impliquant des paires de mots ou de groupes de mots), et décrivons plusieurs techniques opérant à différents niveaux. Nous décrivons une évaluation visant à comparer ces techniques et leurs combinaisons sur deux corpus de paraphrases d’énoncés obtenus par traduction multiple. Les conclusions que nous tirons peuvent servir de guide pour améliorer des techniques existantes.
							</p>

							<p id="taln-2010-court-018-key" class="mots_cles">
							<b>Mots clés : </b> Paraphrase, Patrons de correspondances de segments monolingues
							</p>

					</div>
					

					<div class="article">

						<b>Claire Mouton, Gaël de Chalendar</b>


						<br/>

							<i>JAWS : Just Another WordNet Subset</i> <br/>

						<a href="actes/taln-2010-court-019.pdf">taln-2010-court-019</a> 
						<a href="bibtex/taln-2010-court-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-019-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-019-key');">mots clés</a> <br/>

							<p id="taln-2010-court-019-abs" class="resume">
							<b>Résumé : </b> WordNet, une des ressources lexicales les plus utilisées aujourd’hui a été constituée en anglais et les chercheurs travaillant sur d’autres langues souffrent du manque d’une telle ressource. Malgré les efforts fournis par la communauté française, les différents WordNets produits pour la langue française ne sont toujours pas aussi exhaustifs que le WordNet de Princeton. C’est pourquoi nous proposons une méthode novatrice dans la production de termes nominaux instanciant les différents synsets de WordNet en exploitant les propriétés syntaxiques distributionnelles du vocabulaire français. Nous comparons la ressource que nous obtenons avecWOLF et montrons que notre approche offre une couverture plus large.
							</p>

							<p id="taln-2010-court-019-key" class="mots_cles">
							<b>Mots clés : </b> ressources lexicales françaises, WordNet, relations sémantiques, distributions syntaxiques
							</p>

					</div>
					

					<div class="article">

						<b>Caroline Brun, Maud Ehrmann</b>


						<br/>

							<i>Un système de détection d’entités nommées adapté pour la campagne d’évaluation ESTER 2</i> <br/>

						<a href="actes/taln-2010-court-020.pdf">taln-2010-court-020</a> 
						<a href="bibtex/taln-2010-court-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-020-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-020-key');">mots clés</a> <br/>

							<p id="taln-2010-court-020-abs" class="resume">
							<b>Résumé : </b> Dans cet article nous relatons notre participation à la campagne d’évaluation ESTER 2 (Evaluation des Systèmes de Transcription Enrichie d’Emissions Radiophoniques). Après avoir décrit les objectifs de cette campagne ainsi que ses spécificités et difficultés, nous présentons notre système d’extraction d’entités nommées en nous focalisant sur les adaptations réalisées dans le cadre de cette campagne. Nous décrivons ensuite les résultats obtenus lors de la compétition, ainsi que des résultats originaux obtenus par la suite. Nous concluons sur les leçons tirées de cette expérience.
							</p>

							<p id="taln-2010-court-020-key" class="mots_cles">
							<b>Mots clés : </b> entités nommées, évaluation, extraction d’information
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Muller, Philippe Langlais</b>


						<br/>

							<i>Comparaison de ressources lexicales pour l’extraction de synonymes</i> <br/>

						<a href="actes/taln-2010-court-021.pdf">taln-2010-court-021</a> 
						<a href="bibtex/taln-2010-court-021.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Laurence Longo, Amalia Todiraşcu</b>


						<br/>

							<i>RefGen : un module d’identification des chaînes de référence dépendant du genre textuel</i> <br/>

						<a href="actes/taln-2010-court-022.pdf">taln-2010-court-022</a> 
						<a href="bibtex/taln-2010-court-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-022-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-022-key');">mots clés</a> <br/>

							<p id="taln-2010-court-022-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons RefGen, un module d’identification des chaînes de référence pour le français. RefGen effectue une annotation automatique des expressions référentielles puis identifie les relations de coréférence établies entre ces expressions pour former des chaînes de référence. Le calcul de la référence utilise des propriétés des chaînes de référence dépendantes du genre textuel, l’échelle d’accessibilité d’(Ariel, 1990) et une série de filtres lexicaux, morphosyntaxiques et sémantiques. Nous évaluons les premiers résultats de RefGen sur un corpus issu de rapports publics.
							</p>

							<p id="taln-2010-court-022-key" class="mots_cles">
							<b>Mots clés : </b> Chaînes de référence, relation de coréférence, saillance, genre textuel
							</p>

					</div>
					

					<div class="article">

						<b>Rosa Stern, Benoît Sagot</b>


						<br/>

							<i>Détection et résolution d’entités nommées dans des dépêches d’agence</i> <br/>

						<a href="actes/taln-2010-court-023.pdf">taln-2010-court-023</a> 
						<a href="bibtex/taln-2010-court-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-023-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-023-key');">mots clés</a> <br/>

							<p id="taln-2010-court-023-abs" class="resume">
							<b>Résumé : </b> Nous présentons NP, un système de reconnaissance d’entités nommées. Comprenant un module de résolution, il permet d’associer à chaque occurrence d’entité le référent qu’elle désigne parmi les entrées d’un référentiel dédié. NP apporte ainsi des informations pertinentes pour l’exploitation de l’extraction d’entités nommées en contexte applicatif. Ce système fait l’objet d’une évaluation grâce au développement d’un corpus annoté manuellement et adapté aux tâches de détection et de résolution.
							</p>

							<p id="taln-2010-court-023-key" class="mots_cles">
							<b>Mots clés : </b> résolution d’entités nommées, détection d’entités nommées, extraction d’information
							</p>

					</div>
					

					<div class="article">

						<b>Marie-Jean Meurs, Fabrice Lefèvre</b>


						<br/>

							<i>Processus de décision à base de SVM pour la composition d’arbres de frames sémantiques</i> <br/>

						<a href="actes/taln-2010-court-024.pdf">taln-2010-court-024</a> 
						<a href="bibtex/taln-2010-court-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-024-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-024-key');">mots clés</a> <br/>

							<p id="taln-2010-court-024-abs" class="resume">
							<b>Résumé : </b> Cet article présente un processus de décision basé sur des classifieurs à vaste marge (SVMDP) pour extraire l’information sémantique dans un système de dialogue oral. Dans notre composant de compréhension, l’information est représentée par des arbres de frames sémantiques définies selon le paradigme FrameNet. Le processus d’interprétation est réalisé en deux étapes. D’abord, des réseaux bayésiens dynamiques (DBN) sont utilisés comme modèles de génération pour inférer des fragments d’arbres de la requête utilisateur. Ensuite, notre SVMDP dépendant du contexte compose ces fragments afin d’obtenir la représentation sémantique globale du message. Les expériences sont menées sur le corpus de dialogue MEDIA. Une procédure semi-automatique fournit une annotation de référence en frames sur laquelle les paramètres des DBN et SVMDP sont appris. Les résultats montrent que la méthode permet d’améliorer les performances d’identification de frames pour les exemples de test les plus complexes par rapport à un processus de décision déterministe ad hoc.
							</p>

							<p id="taln-2010-court-024-key" class="mots_cles">
							<b>Mots clés : </b> système de dialogue oral, compréhension de la parole, composition sémantique, frame sémantique, séparateur à vaste marge
							</p>

					</div>
					

					<div class="article">

						<b>Béatrice Arnulphy, Xavier Tannier, Anne Vilnat</b>


						<br/>

							<i>Les entités nommées événement et les verbes de cause-conséquence</i> <br/>

						<a href="actes/taln-2010-court-025.pdf">taln-2010-court-025</a> 
						<a href="bibtex/taln-2010-court-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-025-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-025-key');">mots clés</a> <br/>

							<p id="taln-2010-court-025-abs" class="resume">
							<b>Résumé : </b> L’extraction des événements désignés par des noms est peu étudiée dans des corpus généralistes. Si des lexiques de noms déclencheurs d’événements existent, les problèmes de polysémie sont nombreux et beaucoup d’événements ne sont pas introduits par des déclencheurs. Nous nous intéressons dans cet article à une hypothèse selon laquelle les verbes induisant la cause ou la conséquence sont de bons indices quant à la présence d’événements nominaux dans leur cotexte.
							</p>

							<p id="taln-2010-court-025-key" class="mots_cles">
							<b>Mots clés : </b> Entité nommée, événement, rapports de cause et conséquence
							</p>

					</div>
					

					<div class="article">

						<b>Alexander Pak, Patrick Paroubek</b>


						<br/>

							<i>Construction d’un lexique affectif pour le français à partir de Twitter</i> <br/>

						<a href="actes/taln-2010-court-026.pdf">taln-2010-court-026</a> 
						<a href="bibtex/taln-2010-court-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-026-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-026-key');">mots clés</a> <br/>

							<p id="taln-2010-court-026-abs" class="resume">
							<b>Résumé : </b> Un lexique affectif est un outil utile pour l’étude des émotions ainsi que pour la fouille d’opinion et l’analyse des sentiments. Un tel lexique contient des listes de mots annotés avec leurs évaluations émotionnelles. Il existe un certain nombre de lexiques affectifs pour la langue anglaise, espagnole, allemande, mais très peu pour le français. Un travail de longue haleine est nécessaire pour construire et enrichir un lexique affectif. Nous proposons d’utiliser Twitter, la plateforme la plus populaire de microblogging de nos jours, pour recueillir un corpus de textes émotionnels en français. En utilisant l’ensemble des données recueillies, nous avons estimé les normes affectives de chaque mot. Nous utilisons les données de la Norme Affective desMots Anglais (ANEW, Affective Norms of EnglishWords) que nous avons traduite en français afin de valider nos résultats. Les valeurs du coefficient tau de Kendall et du coefficient de corrélation de rang de Spearman montrent que nos scores estimés sont en accord avec les scores ANEW.
							</p>

							<p id="taln-2010-court-026-key" class="mots_cles">
							<b>Mots clés : </b> Analyse de sentiments, ANEW, Twitter
							</p>

					</div>
					

					<div class="article">

						<b>Lei Zhang, Stéphane Ferrari</b>


						<br/>

							<i>Analyse d’opinion : annotation sémantique de textes chinois</i> <br/>

						<a href="actes/taln-2010-court-027.pdf">taln-2010-court-027</a> 
						<a href="bibtex/taln-2010-court-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-027-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-027-key');">mots clés</a> <br/>

							<p id="taln-2010-court-027-abs" class="resume">
							<b>Résumé : </b> Notre travail concerne l’analyse automatique des énoncés d’opinion en chinois. En nous inspirant de la théorie linguistique de l’Appraisal, nous proposons une méthode fondée sur l’usage de lexiques et de règles locales pour déterminer les caractéristiques telles que la Force (intensité), le Focus (prototypicalité) et la polarité de tels énoncés. Nous présentons le modèle et sa mise en oeuvre sur un corpus journalistique. Si pour la détection d’énoncés d’opinion, la précision est bonne (94 %), le taux de rappel (67 %) pose cependant des questions sur l’enrichissement des ressources actuelles.
							</p>

							<p id="taln-2010-court-027-key" class="mots_cles">
							<b>Mots clés : </b> Analyse d’opinion, théorie de l’Appraisal
							</p>

					</div>
					

					<div class="article">

						<b>Peggy Cellier, Thierry Charnois</b>


						<br/>

							<i>Fouille de données séquentielles d’itemsets pour l’apprentissage de patrons linguistiques</i> <br/>

						<a href="actes/taln-2010-court-028.pdf">taln-2010-court-028</a> 
						<a href="bibtex/taln-2010-court-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-028-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-028-key');">mots clés</a> <br/>

							<p id="taln-2010-court-028-abs" class="resume">
							<b>Résumé : </b> Dans cet article nous présentons une méthode utilisant l’extraction de motifs séquentiels d’itemsets pour l’apprentissage automatique de patrons linguistiques. De plus, nous proposons de nous appuyer sur l’ordre partiel existant entre les motifs pour les énumérer de façon structurée et ainsi faciliter leur validation en tant que patrons linguistiques.
							</p>

							<p id="taln-2010-court-028-key" class="mots_cles">
							<b>Mots clés : </b> Fouille de données, motifs séquentiels, extraction d’information, apprentissage de patrons linguistiques
							</p>

					</div>
					

					<div class="article">

						<b>Anouar Ben Hassena, Laurent Miclet</b>


						<br/>

							<i>Tree analogical learning. Application in NLP</i> <br/>

						<a href="actes/taln-2010-court-029.pdf">taln-2010-court-029</a> 
						<a href="bibtex/taln-2010-court-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-029-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-029-key');">mots clés</a> <br/>

							<p id="taln-2010-court-029-abs" class="resume">
							<b>Résumé : </b> En intelligence artificielle, l’analogie est utilisée comme une technique de raisonnement non exact pour la résolution de problèmes, la compréhension du langage naturel, l’apprentissage des règles de classification, etc. Cet article s’intéresse à la proportion analogique, une forme simple du raisonnement par analogie, et présente son application en apprentissage automatique pour le TALN. La proportion analogique est une relation entre quatre objets qui exprime que la manière de transformer le premier objet en le second est la même que la façon de transformer le troisième en le quatrième. Premièrement, nous définissons formellement la proportion analogique entre quatre objets. Nous nous intéressons particulièrement aux objets structurés que sont les arbres ordonnés et étiquetés, avec une définition originale de l’analogie fondée sur l’alignement optimal. Ensuite, nous présentons deux algorithmes qui calculent la dissemblance analogique entre quatre arbres et qui trouvent des solutions, éventuellement approchées, à une équation analogique entre arbres. Nous montrons leur utilisation dans deux applications : l’apprentissage de l’arbre syntaxique d’une phrase et la génération de la prosodie dans la synthèse de parole.
							</p>

							<p id="taln-2010-court-029-key" class="mots_cles">
							<b>Mots clés : </b> Proportion analogique, arbre syntaxique, analyseur syntaxique analogique
							</p>

					</div>
					

					<div class="article">

						<b>Mehdi Embarek, Olivier Ferret</b>


						<br/>

							<i>Adapter un système de question-réponse en domaine ouvert au domaine médical</i> <br/>

						<a href="actes/taln-2010-court-030.pdf">taln-2010-court-030</a> 
						<a href="bibtex/taln-2010-court-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-030-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-030-key');">mots clés</a> <br/>

							<p id="taln-2010-court-030-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons Esculape, un système de question-réponse en français dédié aux médecins généralistes et élaboré à partir d’OEdipe, un système de question-réponse en domaine ouvert. Esculape ajoute à OEdipe la capacité d’exploiter la structure d’un modèle du domaine, le domaine médical dans le cas présent. Malgré l’existence d’un grand nombre de ressources dans ce domaine (UMLS, MeSH ...), il n’est pas possible de se reposer entièrement sur ces ressources, et plus spécifiquement sur les relations qu’elles abritent, pour répondre aux questions. Nous montrons comment surmonter cette difficulté en apprenant de façon supervisée des patrons linguistiques d’extraction de relations et en les appliquant à l’extraction de réponses.
							</p>

							<p id="taln-2010-court-030-key" class="mots_cles">
							<b>Mots clés : </b> systèmes de question-réponse, extraction de relations, domaine médical
							</p>

					</div>
					

					<div class="article">

						<b>Inès Zribi, Souha Mezghani Hammami, Lamia Hadrich Belguith</b>


						<br/>

							<i>L’apport d’une approche hybride pour la reconnaissance des entités nommées en langue arabe</i> <br/>

						<a href="actes/taln-2010-court-031.pdf">taln-2010-court-031</a> 
						<a href="bibtex/taln-2010-court-031.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-031-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-031-key');">mots clés</a> <br/>

							<p id="taln-2010-court-031-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous proposons une méthode hybride pour la reconnaissance des entités nommées pour la langue arabe. Cette méthode profite, d’une part, des avantages de l’utilisation d’une méthode d’apprentissage pour extraire des règles permettant l’identification et la classification des entités nommées. D’autre part, elle repose sur un ensemble de règles extraites manuellement pour corriger et améliorer le résultat de la méthode d’apprentissage. Les résultats de l’évaluation de la méthode proposée sont encourageants. Nous avons obtenu un taux global de F-mesure égal à 79.24%.
							</p>

							<p id="taln-2010-court-031-key" class="mots_cles">
							<b>Mots clés : </b> Traitement de la langue arabe, reconnaissance des entités nommées, méthode d’apprentissage
							</p>

					</div>
					

					<div class="article">

						<b>Richard Moot</b>


						<br/>

							<i>Semi-automated Extraction of a Wide-Coverage Type-Logical Grammar for French</i> <br/>

						<a href="actes/taln-2010-court-032.pdf">taln-2010-court-032</a> 
						<a href="bibtex/taln-2010-court-032.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-032-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-032-key');">mots clés</a> <br/>

							<p id="taln-2010-court-032-abs" class="resume">
							<b>Résumé : </b> Cet article décrit le développement d’une grammaire catégorielle à large couverture du Français, extraite à partir du corpus arboré de Paris 7 et vérifiée et corrigée manuellement. Le grammaire catégorielle résultant est évaluée en utilisant un supertagger et obtient des résultats comparables aux meilleurs supertaggers pour l’Anglais.
							</p>

							<p id="taln-2010-court-032-key" class="mots_cles">
							<b>Mots clés : </b> Extraction de grammaires, grammaires catégorielles, supertagging
							</p>

					</div>
					

					<div class="article">

						<b>Yayoi Nakamura-Delloye, Éric Villemonte De La Clergerie</b>


						<br/>

							<i>Exploitation de résultats d’analyse syntaxique pour extraction semi-supervisée des chemins de relations</i> <br/>

						<a href="actes/taln-2010-court-033.pdf">taln-2010-court-033</a> 
						<a href="bibtex/taln-2010-court-033.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-033-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-033-key');">mots clés</a> <br/>

							<p id="taln-2010-court-033-abs" class="resume">
							<b>Résumé : </b> Le présent article décrit un travail en cours sur l’acquisition des patrons de relations entre entités nommées à partir de résultats d’analyse syntaxique. Sans aucun patron prédéfini, notre méthode fournit des chemins syntaxiques susceptibles de représenter une relation donnée à partir de quelques exemples de couples d’entités nommées entretenant la relation en question.
							</p>

							<p id="taln-2010-court-033-key" class="mots_cles">
							<b>Mots clés : </b> Extraction des connaissances, extraction des patrons, relation des entités nommées, arbre syntaxique dépendanciel
							</p>

					</div>
					

					<div class="article">

						<b>Damien Nouvel, Arnaud Soulet, Jean-Yves Antoine, Nathalie Friburger, Denis Maurel</b>


						<br/>

							<i>Reconnaissance d’entités nommées : enrichissement d’un système à base de connaissances à partir de techniques de fouille de textes</i> <br/>

						<a href="actes/taln-2010-court-034.pdf">taln-2010-court-034</a> 
						<a href="bibtex/taln-2010-court-034.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-034-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-034-key');">mots clés</a> <br/>

							<p id="taln-2010-court-034-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons et analysons les résultats du système de reconnaissance d’entités nommées CasEN lors de sa participation à la campagne d’évaluation Ester2. Nous identifions quelles ont été les difficultés pour notre système, essentiellement : les mots hors-vocabulaire, la métonymie, les frontières des entités nommées. Puis nous proposons une approche pour améliorer les performances de systèmes à base de connaissances, en utilisant des techniques exhaustives de fouille de données séquentielles afin d’extraire des motifs qui représentent les structures linguistiques en jeu lors de la reconnaissance d’entités nommées. Enfin, nous décrivons l’expérimentation menée à cet effet, donnons les résultats obtenus à ce jour et en faisons une première analyse.
							</p>

							<p id="taln-2010-court-034-key" class="mots_cles">
							<b>Mots clés : </b> Reconnaissance d’Entités Nommées, Séquences Hiérarchiques, Motifs, Ester2
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Gaillard, Olivier Collin, Malek Boualem</b>


						<br/>

							<i>Traduction de requêtes basée sur Wikipédia</i> <br/>

						<a href="actes/taln-2010-court-035.pdf">taln-2010-court-035</a> 
						<a href="bibtex/taln-2010-court-035.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-035-abs');">résumé</a>
							<a onclick="toggle('taln-2010-court-035-key');">mots clés</a> <br/>

							<p id="taln-2010-court-035-abs" class="resume">
							<b>Résumé : </b> Cet article s&#39;inscrit dans le domaine de la recherche d&#39;information multilingue. Il propose une méthode de traduction automatique de requêtes basée sur Wikipédia. Une phase d&#39;analyse permet de segmenter la requête en syntagmes ou unités lexicales à traduire en s&#39;appuyant sur les liens multilingues entre les articles de Wikipédia. Une deuxième phase permet de choisir, parmi les traductions possibles, celle qui est la plus cohérente en s&#39;appuyant sur les informations d&#39;ordre sémantique fournies par les catégories associées à chacun des articles de Wikipédia. Cet article justifie que les données issues de Wikipédia sont particulièrement pertinentes pour la traduction de requêtes, détaille l&#39;approche proposée et son implémentation, et en démontre le potentiel par la comparaison du taux d&#39;erreur du prototype de traduction avec celui d&#39;autres services de traduction automatique.
							</p>

							<p id="taln-2010-court-035-key" class="mots_cles">
							<b>Mots clés : </b> recherche d&#39;information multilingue, traduction de requêtes, Wikipédia
							</p>

					</div>
					

					<div class="article">

						<b>Husam Ali, Yllias Chali, Sadid A. Hasan</b>


						<br/>

							<i>Automatic Question Generation from Sentences</i> <br/>

						<a href="actes/taln-2010-court-036.pdf">taln-2010-court-036</a> 
						<a href="bibtex/taln-2010-court-036.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-court-036-abs');">abstract</a>
							<a onclick="toggle('taln-2010-court-036-key');">mots clés</a> <br/>

							<p id="taln-2010-court-036-abs" class="abstract">
							<b>Abstract : </b> Question Generation (QG) and Question Answering (QA) are some of the many challenges for natural language understanding and interfaces. As humans need to ask good questions, the potential benefits from automated QG systems may assist them in meeting useful inquiry needs. In this paper, we consider an automatic Sentence-to-Question generation task, where given a sentence, the Question Generation (QG) system generates a set of questions for which the sentence contains, implies, or needs answers. To facilitate the question generation task, we build elementary sentences from the input complex sentences using a syntactic parser. A named entity recognizer and a part of speech tagger are applied on each of these sentences to encode necessary information.We classify the sentences based on their subject, verb, object and preposition for determining the possible type of questions to be generated. We use the TREC-2007 (Question Answering Track) dataset for our experiments and evaluation.
							</p>

							<p id="taln-2010-court-036-key" class="mots_cles">
							<b>Mots clés : </b> Génération de questions, Analyseur syntaxique, Phrases élémentaires, POS Tagging
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="démonstration">Démonstrations</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Éric Brunelle, Simon Charest</b>


						<br/>

							<i>Présentation du logiciel Antidote HD</i> <br/>

						<a href="actes/taln-2010-demo-001.pdf">taln-2010-demo-001</a> 
						<a href="bibtex/taln-2010-demo-001.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Caroline Barrière</b>


						<br/>

							<i>TerminoWeb : recherche et analyse d’information thématique</i> <br/>

						<a href="actes/taln-2010-demo-002.pdf">taln-2010-demo-002</a> 
						<a href="bibtex/taln-2010-demo-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-002-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-002-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-002-abs" class="resume">
							<b>Résumé : </b> Notre démonstration porte sur le prototype TerminoWeb, une plateforme Web qui permet (1) la construction automatique d’un corpus thématique à partir d’une recherche de documents sur le Web, (2) l’extraction de termes du corpus, et (3) la recherche d’information définitionnelle sur ces termes en corpus. La plateforme intégrant les trois modules, elle aidera un langagier (terminologue, traducteur, rédacteur) à découvrir un nouveau domaine (thème) en facilitant la recherche et l’analyse de documents informatifs pertinents à ce domaine.
							</p>

							<p id="taln-2010-demo-002-key" class="mots_cles">
							<b>Mots clés : </b> information thématique, construction de corpus, extraction de termes, découverte de contextes définitionnels
							</p>

					</div>
					

					<div class="article">

						<b>Christian Boitet, Cong Phap Huynh, Hong Thai Nguyen, Valérie Bellynck</b>


						<br/>

							<i>The iMAG concept: multilingual access gateway to an elected Web sites with incremental quality increase through collaborative post-edition of MT pretranslations</i> <br/>

						<a href="actes/taln-2010-demo-003.pdf">taln-2010-demo-003</a> 
						<a href="bibtex/taln-2010-demo-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-003-abs');">abstract</a>
							<a onclick="toggle('taln-2010-demo-003-key');">keywords</a> <br/>

							<p id="taln-2010-demo-003-abs" class="abstract">
							<b>Abstract : </b> We will demonstrate iMAGs (interactive Multilingual Access Gateways), in particular on a scientific laboratory web site and on the Greater Grenoble (La Métro) web site.
							</p>

							<p id="taln-2010-demo-003-key" class="keywords">
							<b>Keywords : </b> Interactive translation gateway, iMAG, MT post-editing, collaborative translation
							</p>

					</div>
					

					<div class="article">

						<b>Wajdi Zaghouani</b>


						<br/>

							<i>L&#39;intégration d&#39;un outil de repérage d&#39;entités nommées pour la langue arabe dans un système de veille</i> <br/>

						<a href="actes/taln-2010-demo-004.pdf">taln-2010-demo-004</a> 
						<a href="bibtex/taln-2010-demo-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-004-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-004-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-004-abs" class="resume">
							<b>Résumé : </b> Dans cette démonstration, nous présentons l&#39;implémentation d&#39;un outil de repérage d&#39;entités nommées à base de règle pour la langue arabe dans le système de veille médiatique EMM (Europe Media Monitor).
							</p>

							<p id="taln-2010-demo-004-key" class="mots_cles">
							<b>Mots clés : </b> Étiquetage des entités nommées, langue arabe, système de veille médiatique
							</p>

					</div>
					

					<div class="article">

						<b>Olivier Blanc, Noémi Boubel, Jean-Philippe Goldman, Sophie Roekhaut, Anne Catherine Simon, Cédrick Fairon, Richard Beaufort</b>


						<br/>

							<i>Expressive : Génération automatique de parole expressive à partir de données non linguistiques</i> <br/>

						<a href="actes/taln-2010-demo-005.pdf">taln-2010-demo-005</a> 
						<a href="bibtex/taln-2010-demo-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-005-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-005-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-005-abs" class="resume">
							<b>Résumé : </b> Nous présentons Expressive, un système de génération de parole expressive à partir de données non linguistiques. Ce système est composé de deux outils distincts : Taittingen, un générateur automatique de textes d’une grande variété lexico-syntaxique produits à partir d’une représentation conceptuelle du discours, et StyloPhone, un système de synthèse vocale multi-styles qui s’attache à rendre le discours produit attractif et naturel en proposant différents styles vocaux.
							</p>

							<p id="taln-2010-demo-005-key" class="mots_cles">
							<b>Mots clés : </b> Génération de texte, synthèse vocale, expressivité
							</p>

					</div>
					

					<div class="article">

						<b>Fatiha Sadat, Alexandre Terrasa</b>


						<br/>

							<i>Exploitation de Wikipédia pour l’Enrichissement et la Construction des Ressources Linguistiques</i> <br/>

						<a href="actes/taln-2010-demo-006.pdf">taln-2010-demo-006</a> 
						<a href="bibtex/taln-2010-demo-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-006-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-006-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-006-abs" class="resume">
							<b>Résumé : </b> Cet article présente une approche et des résultats utilisant l&#39;encyclopédie en ligne Wikipédia comme ressource semi-structurée de connaissances linguistiques et en particulier comme un corpus comparable pour l’extraction de terminologie bilingue. Cette approche tend à extraire d’abord des paires de terme et traduction à partir de types des informations, liens et textes de Wikipédia. L’étape suivante consiste à l’utilisation de l’information linguistique afin de ré-ordonner les termes et leurs traductions pertinentes et ainsi éliminer les termes cibles inutiles. Les évaluations préliminaires utilisant les paires de langues français-anglais, japonais-français et japonais-anglais ont montré une bonne qualité des paires de termes extraits. Cette étude est très favorable pour la construction et l’enrichissement des ressources linguistiques tels que les dictionnaires et ontologies multilingues. Aussi, elle est très utile pour un système de recherche d’information translinguistique (RIT).
							</p>

							<p id="taln-2010-demo-006-key" class="mots_cles">
							<b>Mots clés : </b> Terminologie bilingue, corpus comparable, Wikipédia, ontologie multilingue
							</p>

					</div>
					

					<div class="article">

						<b>Annelies Braffort, Michael Filhol, Jérémie Segouat</b>


						<br/>

							<i>Traitement automatique des langues des signes : le projet Dicta-Sign, des corpus aux applications</i> <br/>

						<a href="actes/taln-2010-demo-007.pdf">taln-2010-demo-007</a> 
						<a href="bibtex/taln-2010-demo-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-007-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-007-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-007-abs" class="resume">
							<b>Résumé : </b> Cet article présente Dicta-Sign, un projet de recherche sur le traitement automatique des langues des signes (LS), qui aborde un grand nombre de questions de recherche : linguistique de corpus, modélisation linguistique, reconnaissance et génération automatique. L’objectif de ce projet est de réaliser trois applications prototypes destinées aux usagers sourds : un traducteur de termes de LS à LS, un outil de recherche par l’exemple et un Wiki en LS. Pour cela, quatre corpus comparables de cinq heures de dialogue seront produits et analysés. De plus, des avancées significatives sont attendues dans le domaine des outils d’annotation. Dans ce projet, le LIMSI est en charge de l’élaboration des modèles linguistiques et participe aux aspects corpus et génération automatique. Nous nous proposons d’illustrer l’état d’avancement de Dicta-Sign au travers de vidéos extraites du corpus et de démonstrations des outils de traitement et de génération d’animations de signeur virtuel.
							</p>

							<p id="taln-2010-demo-007-key" class="mots_cles">
							<b>Mots clés : </b> Langue des signes, corpus vidéo comparables, reconnaissance automatique, génération automatique
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Philippe Goldman, Kamel Nebhi, Christopher Laenzlinger</b>


						<br/>

							<i>FipsColor : grammaire en couleur interactive pour l’apprentissage du français</i> <br/>

						<a href="actes/taln-2010-demo-008.pdf">taln-2010-demo-008</a> 
						<a href="bibtex/taln-2010-demo-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-008-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-008-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-008-abs" class="resume">
							<b>Résumé : </b> L&#39;analyseur multilingue FiPS permet de transformer une phrase en une structure syntaxique riche et accompagnée d&#39;informations lexicales, grammaticales et thématiques. On décrit ici une application qui adapte les structures en constituants de l’analyseur FiPS à une nomenclature grammaticale permettant la représentation en couleur. Cette application interactive et disponible en ligne (http://latl.unige.ch/fipscolor) peut être utilisée librement par les enseignants et élèves de primaire.
							</p>

							<p id="taln-2010-demo-008-key" class="mots_cles">
							<b>Mots clés : </b> analyse syntaxique, grammaire générative, services web, tei
							</p>

					</div>
					

					<div class="article">

						<b>Yves Scherrer</b>


						<br/>

							<i>Des cartes dialectologiques numérisées pour le TALN</i> <br/>

						<a href="actes/taln-2010-demo-009.pdf">taln-2010-demo-009</a> 
						<a href="bibtex/taln-2010-demo-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-009-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-009-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-009-abs" class="resume">
							<b>Résumé : </b> Cette démonstration présente une interface web pour des données numérisées de l’atlas linguistique de la Suisse allemande. Nous présentons d’abord l’intégration des données brutes et des données interpolées de l’atlas dans une interface basée sur Google Maps. Ensuite, nous montrons des prototypes de systèmes de traduction automatique et d’identification de dialectes qui s’appuient sur ces données dialectologiques numérisées.
							</p>

							<p id="taln-2010-demo-009-key" class="mots_cles">
							<b>Mots clés : </b> Dialectologie, atlas linguistique, traduction automatique, identification de dialectes
							</p>

					</div>
					

					<div class="article">

						<b>Richard Beaufort, Kévin Macé, Cédrick Fairon</b>


						<br/>

							<i>Text-it /Voice-it Une application mobile de normalisation des SMS</i> <br/>

						<a href="actes/taln-2010-demo-010.pdf">taln-2010-demo-010</a> 
						<a href="bibtex/taln-2010-demo-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-010-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-010-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-010-abs" class="resume">
							<b>Résumé : </b> Cet article présente Text-it / Voice-it, une application de normalisation des SMS pour téléphone mobile. L’application permet d’envoyer et de recevoir des SMS normalisés, et offre le choix entre un résultat textuel (Text-it) et vocal (Voice-it).
							</p>

							<p id="taln-2010-demo-010-key" class="mots_cles">
							<b>Mots clés : </b> SMS, normalisation, application, plugin, serveur
							</p>

					</div>
					

					<div class="article">

						<b>Richard Moot</b>


						<br/>

							<i>Wide-Coverage French Syntax and Semantics using Grail</i> <br/>

						<a href="actes/taln-2010-demo-011.pdf">taln-2010-demo-011</a> 
						<a href="bibtex/taln-2010-demo-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-011-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-011-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-011-abs" class="resume">
							<b>Résumé : </b> Cette démonstration décrit Grail : un analyseur syntaxique pour grammaires catégorielles. Elle met l’accent sur les recherches récentes qui ont permis à Grail de donner des analyses syntaxiques et sémantiques du Français. Ces développements sont possibles grâce à une grammaire extraite semiautomatiquement du corpus de Paris 7 ainsi qu’un lexique sémantique qui traduit des combinaisons de mots, des étiquettes syntaxiques et des formules en Discourse Representation Structures.
							</p>

							<p id="taln-2010-demo-011-key" class="mots_cles">
							<b>Mots clés : </b> Discourse Representation Theory, grammaires catégorielles
							</p>

					</div>
					

					<div class="article">

						<b>Asma Ben Abacha, Pierre Zweigenbaum</b>


						<br/>

							<i>MeTAE : Plate-forme d’annotation automatique et d’exploration sémantiques pour le domaine médical</i> <br/>

						<a href="actes/taln-2010-demo-012.pdf">taln-2010-demo-012</a> 
						<a href="bibtex/taln-2010-demo-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-012-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-012-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-012-abs" class="resume">
							<b>Résumé : </b> Nous présentons une plate-forme d’annotation sémantique et d’exploration de textes médicaux, appelée « MeTAE ». Le processus d’annotation automatique comporte une première étape de reconnaissance des entités médicales présentes dans les textes suivie d’une étape d’identification des relations sémantiques qui les relient. Cette identification se fonde sur des patrons linguistiques construits manuellement pour chaque type de relation. MeTAE génère des annotations RDF à partir des informations extraites et offre une interface d’exploration des textes annotés avec des requêtes sous forme de formulaire. La plate-forme peut être utilisée pour analyser sémantiquement les textes médicaux ou interroger la base d’annotation disponible pour avoir une/des réponses à une requête donnée (e.g. « ?X prévient maladie d’Alzheimer », équivalent à la question « comment prévenir la maladie d’Alzheimer ? »). Cette application peut être la base d’un système de questions-réponses pour le domaine médical.
							</p>

							<p id="taln-2010-demo-012-key" class="mots_cles">
							<b>Mots clés : </b> Annotation sémantique, interrogation sémantique, domaine médical
							</p>

					</div>
					

					<div class="article">

						<b>Bruno Guillaume, Guy Perrier</b>


						<br/>

							<i>LEOPAR, un analyseur syntaxique pour les grammaires d’interaction</i> <br/>

						<a href="actes/taln-2010-demo-013.pdf">taln-2010-demo-013</a> 
						<a href="bibtex/taln-2010-demo-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-013-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-013-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-013-abs" class="resume">
							<b>Résumé : </b> Nous présentons ici l’analyseur syntaxique LEOPAR basé sur les grammaires d’interaction ainsi que d’autres outils utiles pour notre chaîne de traitement syntaxique.
							</p>

							<p id="taln-2010-demo-013-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique, grammaires d’interaction, polarités
							</p>

					</div>
					

					<div class="article">

						<b>Julien Bourdaillet, Fabrizio Gotti, Stéphane Huet, Philippe Langlais, Guy Lapalme</b>


						<br/>

							<i>TransSearch : un moteur de recherche de traductions</i> <br/>

						<a href="actes/taln-2010-demo-014.pdf">taln-2010-demo-014</a> 
						<a href="bibtex/taln-2010-demo-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-014-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-014-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-014-abs" class="resume">
							<b>Résumé : </b> Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Cette démonstration vise à présenter le moteur de recherche de traductions TransSearch. Cetteapplication commerciale, accessible sur leWeb, repose d’une part sur l’exploitation d’un bitexte aligné au niveau des phrases, et d’autre part sur des modèles statistiques d’alignement de mots.
							</p>

							<p id="taln-2010-demo-014-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique statistique, repérage de traductions, alignement de mots, requêtes linguistiques
							</p>

					</div>
					

					<div class="article">

						<b>Graham Russell</b>


						<br/>

							<i>Moz: Translation of Structured Terminology-Rich Text</i> <br/>

						<a href="actes/taln-2010-demo-015.pdf">taln-2010-demo-015</a> 
						<a href="bibtex/taln-2010-demo-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2010-demo-015-abs');">résumé</a>
							<a onclick="toggle('taln-2010-demo-015-key');">mots clés</a> <br/>

							<p id="taln-2010-demo-015-abs" class="resume">
							<b>Résumé : </b> Description de Moz, un système d’aide à la traduction conçu pour le traitement de textes structurés ou semi-structurés avec une forte proportion de contenu terminologique. Le système comporte une mémoire de traduction collaborative, qui atteint un niveau élevé de rappel grâce à l’analyse sousphrastique ; il fournit également des dispositifs de communication et de révision. Le système est en production et traduit 140 000 mots par semaine.
							</p>

							<p id="taln-2010-demo-015-key" class="mots_cles">
							<b>Mots clés : </b> Aides à la traduction, sous-langage, analyse conceptuelle
							</p>

					</div>
					

					<div class="article">

						<b>Alexis Nasr, Frédéric Béchet, Jean-François Rey</b>


						<br/>

							<i>MACAON Une chaîne linguistique pour le traitement de graphes de mots</i> <br/>

						<a href="actes/taln-2010-demo-016.pdf">taln-2010-demo-016</a> 
						<a href="bibtex/taln-2010-demo-016.bib">bibtex</a> 



					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>