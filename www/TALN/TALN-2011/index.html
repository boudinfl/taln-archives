<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>TALN'2011</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>TALN'2011, 18e conférence sur le Traitement Automatique des Langues Naturelles</h1>
				<h2>Montpellier (France), du 2011-06-27 au 2011-07-01</h2>
				<p>Président(s) : Mathieu Lafourcade, Violaine Prince</p>
				<p>Taux d'acceptation :
							papiers longs (40.4%)
							papiers courts (54.7%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#invite">Invités</a></li>
				<li><a href="#long">Papiers longs</a></li>
				<li><a href="#court">Papiers courts</a></li>
				<li><a href="#démonstration">Démonstrations</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="invite">Invités</h1>
			

					<div class="article">

						<b>Vladimir A. Fomichov</b>


						<br/>

							<i>The prospects revealed by the theory of K-representations for bioinformatics and Semantic Web</i> <br/>

						<a href="actes/taln-2011-invite-001.pdf">taln-2011-invite-001</a> 
						<a href="bibtex/taln-2011-invite-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-invite-001-abs');">résumé</a>
							<a onclick="toggle('taln-2011-invite-001-key');">mots clés</a> <br/>

							<p id="taln-2011-invite-001-abs" class="resume">
							<b>Résumé : </b> L’article décrit la structure et les applications possibles de la théorie des K-représentations (représentation des connaissances) dans la bioinformatique afin de développer un Réseau Sémantique d’une génération nouvelle. La théorie des K-répresentations est une théorie originale du développement des analyseurs sémantico–syntactiques avec l’utilisation large des moyens formels pour décrire les données d’entrée, intermédiaires et de sortie. Cette théorie est décrit dans la monographie de V. Fomichov (Springer, 2010). La première partie de la théorie est un modèle formel d’un système qui est composé de dix opérations sur les structures conceptuelles. Ce modèle définit une classe nouvelle des langages formels – la classe des SK-langages. Les possibilités larges de construire des répresentations sémantiques des discours compliqués en rapport à la biologie sont manifestes. Une approche formelle nouvelle de l’élaboration des analysateurs multilinguistiques sémantico-syntactiques est décrite. Cet approche a été implémentée sous la forme d&#39;un programme en langage PYTHON.
							</p>

							<p id="taln-2011-invite-001-key" class="mots_cles">
							<b>Mots clés : </b> dialogue homme-machine en langage naturel, algorithme de l‟analyse sémantico-syntactique, sémantique intégrale formelle, théorie des K-représentations, SK-langues, représentation sémantique, bases de données linguistiques, réseau sémantique d’une génération nouvelle, réseau sémantique multilingue, bioinformatique
							</p>

					</div>
					

					<div class="article">

						<b>Nicholas Asher</b>


						<br/>

							<i>Theorie et Praxis Une optique sur les travaux en TAL sur le discours et le dialogue</i> <br/>

						<a href="actes/taln-2011-invite-002.pdf">taln-2011-invite-002</a> 
						<a href="bibtex/taln-2011-invite-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-invite-002-abs');">abstract</a>

							<p id="taln-2011-invite-002-abs" class="abstract">
							<b>Abstract : </b> Discourse parsing is a relatively new field and it differs from parsing in syntax in its pedegree. Parsing and computational models of syntax have the benefit of 50 years of research in generative syntax and reactions to it. Discourse parsing has on the other hand little conceptual help from linguistics or philosophy. Though impressive gains have been registered in discourse parsing with superficial features, theoretical not really come to grips with the theoretical underpinnings of text interpretation, and its interaction especially with lexical semantics, a rather neglected branch of formal semantics. In my talk I will assess the interaction between theoretical linguistics, formal methods, and experimental work on discourse structure and interpretation. Sounding a note of optimism, I will then turn to assessing the situation for the computational analysis of dialogue. I will argue that the view that we are saddled with from Grice and the philosophy of the seventies is inadequate and is great need of revision from work on communication from economics and theoretical computer science
							</p>


					</div>
					

					<div class="article">

						<b>Claire Gardent</b>


						<br/>

							<i>Sentence Generation: Input, Algorithms and Applications</i> <br/>

						<a href="actes/taln-2011-invite-003.pdf">taln-2011-invite-003</a> 
						<a href="bibtex/taln-2011-invite-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-invite-003-abs');">abstract</a>

							<p id="taln-2011-invite-003-abs" class="abstract">
							<b>Abstract : </b> Sentence Generation maps abstract linguistic representations into sentences. A necessary part of any natural language generation system, sentence generation has also recently received increasing attention in applications such as transfer based machine translation (cf. the LOGON project) and natural language interfaces to knowledge bases (e.g., to verbalise, to author and/or to query ontologies). One outstanding issue in Sentence Generation is what it starts from. What is the abstract linguistic representation it generates from? In my talk, I will explore sentence generation from two main input formats (flat semantic formulae and dependency structures) and discuss their impact on efficiency, algorithms and applications. I will start by describing an algorithm that generates from flat semantic formulae, explain why it is computationally intractable and presenting ways of optimising it to make it usable in practice. I will then show how this algorithm can be used to generate paraphrases; to support error mining and to generate teaching material for language learners from an ontology. In the second part of the talk, I will focus on generation from dependency structures. Based on the input data recently made available by the Generation Challenges Surface Realisation Shared Task, I will show how the algorithm previously used to generate from flat semantic formulae can be adapted to generate from dependency structures. I will moreover discuss various issues raised by the GenChal data such as, missing lexical entries and mismatches between dependency and grammar structures.
							</p>


					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="long">Papiers longs</h1>
			

					

					

					

					<div class="article">

						<b>Michael Zock, Guy Lapalme</b>


						<br/>

							<i>Patrons de phrase, raccourcis pour apprendre rapidement à parler une nouvelle langue</i> <br/>

						<a href="actes/taln-2011-long-001.pdf">taln-2011-long-001</a> 
						<a href="bibtex/taln-2011-long-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-001-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-001-key');">mots clés</a> <br/>

							<p id="taln-2011-long-001-abs" class="resume">
							<b>Résumé : </b> Nous décrivons la création d&#39;un environnement web pour aider des apprenants (adolescents ou adultes) à acquérir les automatismes nécessaires pour produire à un débit “normal” les structures fondamentales d’une langue. Notre point de départ est une base de données de phrases, glanées sur le web ou issues de livres scolaires ou de livres de phrases. Ces phrases ont été généralisées (remplacement de mots par des variables) et indexées en termes de buts pour former une arborescence de patrons. Ces deux astuces permettent de motiver l&#39;usage des patrons et de crééer des phrases structurellement identiques à celles rencontrées, tout en étant sémantiquement différentes. Si les notions de &#39;patrons&#39; ou de &#39;phrases à trou implicitement typées&#39; ne sont pas nouvelles, le fait de les avoir portées sur ordinateur pour apprendre des langues l&#39;est. Le système étant conçu pour être ouvert, il permet aux utilisateurs, concepteurs ou apprenants, des changements sur de nombreux points importants : le nom des variables, leurs valeurs, le laps de temps entre une question et sa réponse, etc. La version initiale a été développée pour l’anglais et le japonais. Pour tester la généricité de notre approche nous y avons ajouté relativement facilement le français et le chinois.
							</p>

							<p id="taln-2011-long-001-key" class="mots_cles">
							<b>Mots clés : </b> apprentissage de langues, production de langage, livres de phrases, patrons, schéma de phrase, structures fondamentales
							</p>

					</div>
					

					<div class="article">

						<b>Eric Charton, Michel Gagnon, Benoit Ozell</b>


						<br/>

							<i>Génération automatique de motifs de détection d’entités nommées en utilisant des contenus encyclopédiques</i> <br/>

						<a href="actes/taln-2011-long-002.pdf">taln-2011-long-002</a> 
						<a href="bibtex/taln-2011-long-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-002-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-002-key');">mots clés</a> <br/>

							<p id="taln-2011-long-002-abs" class="resume">
							<b>Résumé : </b> Les encyclopédies numériques contiennent aujourd’hui de vastes inventaires de formes d’écritures pour des noms de personnes, de lieux, de produits ou d’organisation. Nous présentons un système hybride de détection d’entités nommées qui combine un classifieur à base de Champs Conditionnel Aléatoires avec un ensemble de motifs de détection extraits automatiquement d’un contenu encyclopédique. Nous proposons d’extraire depuis des éditions en plusieurs langues de l’encyclopédie Wikipédia de grandes quantités de formes d’écriture que nous utilisons en tant que motifs de détection des entités nommées. Nous décrivons une méthode qui nous assure de ne conserver dans cette ressources que des formes non ambiguës susceptibles de venir renforcer un système de détection d’entités nommées automatique. Nous procédons à un ensemble d’expériences qui nous permettent de comparer un système d’étiquetage à base de CRF avec un système utilisant exclusivement des motifs de détection. Puis nous fusionnons les résultats des deux systèmes et montrons qu’un gain de performances est obtenu grâce à cette proposition.
							</p>

							<p id="taln-2011-long-002-key" class="mots_cles">
							<b>Mots clés : </b> Étiqueteur, Entités nommées, Lexiques
							</p>

					</div>
					

					<div class="article">

						<b>Cédric Lopez, Mathieu Roche</b>


						<br/>

							<i>Approche de construction automatique de titres courts par des méthodes de Fouille du Web</i> <br/>

						<a href="actes/taln-2011-long-003.pdf">taln-2011-long-003</a> 
						<a href="bibtex/taln-2011-long-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-003-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-003-key');">mots clés</a> <br/>

							<p id="taln-2011-long-003-abs" class="resume">
							<b>Résumé : </b> Le titrage automatique de documents textuels est une tâche essentielle pour plusieurs applications (titrage de mails, génération automatique de sommaires, synthèse de documents, etc.). Cette étude présente une méthode de construction de titres courts appliquée à un corpus d’articles journalistiques via des méthodes de Fouille du Web. Il s’agit d’une première étape cruciale dans le but de proposer une méthode de construction de titres plus complexes. Dans cet article, nous présentons une méthode proposant des titres tenant compte de leur cohérence par rapport au texte, par rapport au Web, ainsi que de leur contexte dynamique. L’évaluation de notre approche indique que nos titres construits automatiquement sont informatifs et/ou accrocheurs.
							</p>

							<p id="taln-2011-long-003-key" class="mots_cles">
							<b>Mots clés : </b> Traitement Automatique du Langage Naturel, Fouille du Web, Titrage automatique
							</p>

					</div>
					

					<div class="article">

						<b>Ludovic Jean-Louis, Romaric Besançon, Olivier Ferret, Adrien Durand</b>


						<br/>

							<i>Une approche faiblement supervisée pour l’extraction de relations à large échelle</i> <br/>

						<a href="actes/taln-2011-long-004.pdf">taln-2011-long-004</a> 
						<a href="bibtex/taln-2011-long-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-004-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-004-key');">mots clés</a> <br/>

							<p id="taln-2011-long-004-abs" class="resume">
							<b>Résumé : </b> Les systèmes d’extraction d’information traditionnels se focalisent sur un domaine spécifique et un nombre limité de relations. Les travaux récents dans ce domaine ont cependant vu émerger la problématique des systèmes d’extraction d’information à large échelle. À l’instar des systèmes de question-réponse en domaine ouvert, ces systèmes se caractérisent à la fois par le traitement d’un grand nombre de relations et par une absence de restriction quant aux domaines abordés. Dans cet article, nous présentons un système d’extraction d’information à large échelle fondé sur un apprentissage faiblement supervisé de patrons d’extraction de relations. Cet apprentissage repose sur la donnée de couples d’entités en relation dont la projection dans un corpus de référence permet de constituer la base d’exemples de relations support de l’induction des patrons d’extraction. Nous présentons également les résultats de l’application de cette approche dans le cadre d’évaluation défini par la tâche KBP de l’évaluation TAC 2010.
							</p>

							<p id="taln-2011-long-004-key" class="mots_cles">
							<b>Mots clés : </b> extraction d’information, extraction de relations
							</p>

					</div>
					

					<div class="article">

						<b>Stéphane Huet, Florian Boudin, Juan-Manuel Torres-Moreno</b>


						<br/>

							<i>Utilisation d’un score de qualité de traduction pour le résumé multi-document cross-lingue</i> <br/>

						<a href="actes/taln-2011-long-005.pdf">taln-2011-long-005</a> 
						<a href="bibtex/taln-2011-long-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-005-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-005-key');">mots clés</a> <br/>

							<p id="taln-2011-long-005-abs" class="resume">
							<b>Résumé : </b> Le résumé automatique cross-lingue consiste à générer un résumé rédigé dans une langue différente de celle utilisée dans les documents sources. Dans cet article, nous proposons une approche de résumé automatique multi-document, basée sur une représentation par graphe, qui prend en compte des scores de qualité de traduction lors du processus de sélection des phrases. Nous évaluons notre méthode sur un sous-ensemble manuellement traduit des données utilisées lors de la campagne d’évaluation internationale DUC 2004. Les résultats expérimentaux indiquent que notre approche permet d’améliorer la lisibilité des résumés générés, sans pour autant dégrader leur informativité.
							</p>

							<p id="taln-2011-long-005-key" class="mots_cles">
							<b>Mots clés : </b> Résumé cross-lingue, qualité de traduction, graphe
							</p>

					</div>
					

					<div class="article">

						<b>Cyril Grouin, Louise Deléger, Bruno Cartoni, Sophie Rosset, Pierre Zweigenbaum</b>


						<br/>

							<i>Accès au contenu sémantique en langue de spécialité : extraction des prescriptions et concepts médicaux</i> <br/>

						<a href="actes/taln-2011-long-006.pdf">taln-2011-long-006</a> 
						<a href="bibtex/taln-2011-long-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-006-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-006-key');">mots clés</a> <br/>

							<p id="taln-2011-long-006-abs" class="resume">
							<b>Résumé : </b> Pourtant essentiel pour appréhender rapidement et globalement l’état de santé des patients, l’accès aux informations médicales liées aux prescriptions médicamenteuses et aux concepts médicaux par les outils informatiques se révèle particulièrement difficile. Ces informations sont en effet généralement rédigées en texte libre dans les comptes rendus hospitaliers et nécessitent le développement de techniques dédiées. Cet article présente les stratégies mises en oeuvre pour extraire les prescriptions médicales et les concepts médicaux dans des comptes rendus hospitaliers rédigés en anglais. Nos systèmes, fondés sur des approches à base de règles et d’apprentissage automatique, obtiennent une F1-mesure globale de 0,773 dans l’extraction des prescriptions médicales et dans le repérage et le typage des concepts médicaux.
							</p>

							<p id="taln-2011-long-006-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’information, Indexation contrôlée, Informatique médicale, Concepts médicaux, Prescriptions
							</p>

					</div>
					

					<div class="article">

						<b>Bassam Jabaian, Laurent Besacier, Fabrice Lefèvre</b>


						<br/>

							<i>Comparaison et combinaison d’approches pour la portabilité vers une nouvelle langue d’un système de compréhension de l’oral</i> <br/>

						<a href="actes/taln-2011-long-007.pdf">taln-2011-long-007</a> 
						<a href="bibtex/taln-2011-long-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-007-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-007-key');">mots clés</a> <br/>

							<p id="taln-2011-long-007-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous proposons plusieurs approches pour la portabilité du module de compréhension de la parole (SLU) d’un système de dialogue d’une langue vers une autre. On montre que l’utilisation des traductions automatiques statistiques (SMT) aide à réduire le temps et le cout de la portabilité d’un tel système d’une langue source vers une langue cible. Pour la tache d’étiquetage sémantique on propose d’utiliser soit les champs aléatoires conditionnels (CRF), soit l’approche à base de séquences (PH-SMT). Les résultats expérimentaux montrent l’efficacité des méthodes proposées pour une portabilité rapide du SLU vers une nouvelle langue. On propose aussi deux méthodes pour accroître la robustesse du SLU aux erreurs de traduction. Enfin on montre que la combinaison de ces approches réduit les erreurs du système. Ces travaux sont motivés par la disponibilité du corpus MEDIA français et de la traduction manuelle vers l’italien d’une sous partie de ce corpus.
							</p>

							<p id="taln-2011-long-007-key" class="mots_cles">
							<b>Mots clés : </b> Système de dialogue, compréhension de la parole, portabilité à travers les langues, traduction automatique statistique
							</p>

					</div>
					

					<div class="article">

						<b>Thierry Bazillon, Benjamin Maza, Mickael Rouvier, Frédéric Béchet, Alexis Nasr</b>


						<br/>

							<i>Qui êtes-vous ? Catégoriser les questions pour déterminer le rôle des locuteurs dans des conversations orales</i> <br/>

						<a href="actes/taln-2011-long-008.pdf">taln-2011-long-008</a> 
						<a href="bibtex/taln-2011-long-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-008-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-008-key');">mots clés</a> <br/>

							<p id="taln-2011-long-008-abs" class="resume">
							<b>Résumé : </b> La fouille de données orales est un domaine de recherche visant à caractériser un flux audio contenant de la parole d’un ou plusieurs locuteurs, à l’aide de descripteurs liés à la forme et au contenu du signal. Outre la transcription automatique en mots des paroles prononcées, des informations sur le type de flux audio traité ainsi que sur le rôle et l’identité des locuteurs sont également cruciales pour permettre des requêtes complexes telles que : « chercher des débats sur le thème X », « trouver toutes les interviews de Y », etc. Dans ce cadre, et en traitant des conversations enregistrées lors d’émissions de radio ou de télévision, nous étudions la manière dont les locuteurs expriment des questions dans les conversations, en partant de l’intuition initiale que la forme des questions posées est une signature du rôle du locuteur dans la conversation (présentateur, invité, auditeur, etc.). En proposant une classification du type des questions et en utilisant ces informations en complément des descripteurs généralement utilisés dans la littérature pour classer les locuteurs par rôle, nous espérons améliorer l’étape de classification, et valider par la même occasion notre intuition initiale.
							</p>

							<p id="taln-2011-long-008-key" class="mots_cles">
							<b>Mots clés : </b> Fouille de données orales, Traitement Automatique de la Parole, Annotation de corpus oraux, Classification en rôles de locuteurs
							</p>

					</div>
					

					<div class="article">

						<b>Charles Teissèdre, Delphine Battistelli, Jean-Luc Minel</b>


						<br/>

							<i>Recherche d’information et temps linguistique : une heuristique pour calculer la pertinence des expressions calendaires</i> <br/>

						<a href="actes/taln-2011-long-009.pdf">taln-2011-long-009</a> 
						<a href="bibtex/taln-2011-long-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-009-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-009-key');">mots clés</a> <br/>

							<p id="taln-2011-long-009-abs" class="resume">
							<b>Résumé : </b> A rebours de bon nombre d’applications actuelles offrant des services de recherche d’information selon des critères temporels - applications qui reposent, à y regarder de près, sur une approche consistant à filtrer les résultats en fonction de leur inclusion dans une fenêtre de temps, nous souhaitons illustrer dans cet article l’intérêt d’un service s’appuyant sur un calcul de similarité entre des expressions adverbiales calendaires. Nous décrivons une heuristique pour mesurer la pertinence d’un fragment de texte en prenant en compte la sémantique des expressions calendaires qui y sont présentes. A travers la mise en oeuvre d’un système de recherche d’information, nous montrons comment il est possible de tirer profit de l’indexation d’expressions calendaires présentes dans les textes en définissant des scores de pertinence par rapport à une requête. L’objectif est de faciliter la recherche d’information en offrant la possibilité de croiser des critères de recherche thématique avec des critères temporels.
							</p>

							<p id="taln-2011-long-009-key" class="mots_cles">
							<b>Mots clés : </b> Indexation d’informations calendaires, Recherche d’information, Annotation et extraction d’expressions calendaires
							</p>

					</div>
					

					<div class="article">

						<b>Ismaïl El Maarouf, Jeanne Villaneau, Sophie Rosset</b>


						<br/>

							<i>Extraction de patrons sémantiques appliquée à la classification d&#39;Entités Nommées</i> <br/>

						<a href="actes/taln-2011-long-010.pdf">taln-2011-long-010</a> 
						<a href="bibtex/taln-2011-long-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-010-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-010-key');">mots clés</a> <br/>

							<p id="taln-2011-long-010-abs" class="resume">
							<b>Résumé : </b> La variabilité des corpus constitue un problème majeur pour les systèmes de reconnaissance d&#39;entités nommées. L&#39;une des pistes possibles pour y remédier est l&#39;utilisation d&#39;approches linguistiques pour les adapter à de nouveaux contextes : la construction de patrons sémantiques peut permettre de désambiguïser les entités nommées en structurant leur environnement syntaxico-sémantique. Cet article présente une première réalisation sur un corpus de presse d&#39;un système de correction. Après une étape de segmentation sur des critères discursifs de surface, le système extrait et pondère les patrons liés à une classe d&#39;entité nommée fournie par un analyseur. Malgré des modèles encore relativement élémentaires, les résultats obtenus sont encourageants et montrent la nécessité d&#39;un traitement plus approfondi de la classe Organisation.
							</p>

							<p id="taln-2011-long-010-key" class="mots_cles">
							<b>Mots clés : </b> entités nommées, patrons sémantiques, segmentation discursive de surface
							</p>

					</div>
					

					<div class="article">

						<b>Didier Schwab, Jérôme Goulian, Nathan Guillaume</b>


						<br/>

							<i>Désambiguïsation lexicale par propagation de mesures sémantiques locales par algorithmes à colonies de fourmis</i> <br/>

						<a href="actes/taln-2011-long-011.pdf">taln-2011-long-011</a> 
						<a href="bibtex/taln-2011-long-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-011-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-011-key');">mots clés</a> <br/>

							<p id="taln-2011-long-011-abs" class="resume">
							<b>Résumé : </b> Effectuer une tâche de désambiguïsation lexicale peut permettre d’améliorer de nombreuses applications du traitement automatique des langues comme l’extraction d’informations multilingues, ou la traduction automatique. Schématiquement, il s’agit de choisir quel est le sens le plus approprié pour chaque mot d’un texte. Une des approches classiques consiste à estimer la proximité sémantique qui existe entre deux sens de mots puis de l’étendre à l’ensemble du texte. La méthode la plus directe donne un score à toutes les paires de sens de mots puis choisit la chaîne de sens qui a le meilleur score. La complexité de cet algorithme est exponentielle et le contexte qu’il est calculatoirement possible d’utiliser s’en trouve réduit. Il ne s’agit donc pas d’une solution viable. Dans cet article, nous nous intéressons à une autre méthode, l’adaptation d’un algorithme à colonies de fourmis. Nous présentons ses caractéristiques et montrons qu’il permet de propager à un niveau global les résultats des algorithmes locaux et de tenir compte d’un contexte plus long et plus approprié en un temps raisonnable.
							</p>

							<p id="taln-2011-long-011-key" class="mots_cles">
							<b>Mots clés : </b> Désambiguïsation lexicale, Algorithmes à colonies de fourmis, Mesures sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Sagot, Karën Fort, Gilles Adda, Joseph Mariani, Bernard Lang</b>


						<br/>

							<i>Un turc mécanique pour les ressources linguistiques : critique de la myriadisation du travail parcellisé</i> <br/>

						<a href="actes/taln-2011-long-012.pdf">taln-2011-long-012</a> 
						<a href="bibtex/taln-2011-long-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-012-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-012-key');">mots clés</a> <br/>

							<p id="taln-2011-long-012-abs" class="resume">
							<b>Résumé : </b> Cet article est une prise de position concernant les plate-formes de type Amazon Mechanical Turk, dont l’utilisation est en plein essor depuis quelques années dans le traitement automatique des langues. Ces plateformes de travail en ligne permettent, selon le discours qui prévaut dans les articles du domaine, de faire développer toutes sortes de ressources linguistiques de qualité, pour un prix imbattable et en un temps très réduit, par des gens pour qui il s’agit d’un passe-temps. Nous allons ici démontrer que la situation est loin d’être aussi idéale, que ce soit sur le plan de la qualité, du prix, du statut des travailleurs ou de l’éthique. Nous rappellerons ensuite les solutions alternatives déjà existantes ou proposées. Notre but est ici double : informer les chercheurs, afin qu’ils fassent leur choix en toute connaissance de cause, et proposer des solutions pratiques et organisationnelles pour améliorer le développement de nouvelles ressources linguistiques en limitant les risques de dérives éthiques et légales, sans que cela se fasse au prix de leur coût ou de leur qualité.
							</p>

							<p id="taln-2011-long-012-key" class="mots_cles">
							<b>Mots clés : </b> Amazon Mechanical Turk, ressources linguistiques
							</p>

					</div>
					

					<div class="article">

						<b>Bo Li, Eric Gaussier, Emmanuel Morin, Amir Hazem</b>


						<br/>

							<i>Degré de comparabilité, extraction lexicale bilingue et recherche d’information interlingue</i> <br/>

						<a href="actes/taln-2011-long-013.pdf">taln-2011-long-013</a> 
						<a href="bibtex/taln-2011-long-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-013-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-013-key');">mots clés</a> <br/>

							<p id="taln-2011-long-013-abs" class="resume">
							<b>Résumé : </b> Nous étudions dans cet article le problème de la comparabilité des documents composant un corpus comparable afin d’améliorer la qualité des lexiques bilingues extraits et les performances des systèmes de recherche d’information interlingue. Nous proposons une nouvelle approche qui permet de garantir un certain degré de comparabilité et d’homogénéité du corpus tout en préservant une grande part du vocabulaire du corpus d’origine. Nos expériences montrent que les lexiques bilingues que nous obtenons sont d’une meilleure qualité que ceux obtenus avec les approches précédentes, et qu’ils peuvent être utilisés pour améliorer significativement les systèmes de recherche d’information interlingue.
							</p>

							<p id="taln-2011-long-013-key" class="mots_cles">
							<b>Mots clés : </b> Corpus comparables, comparabilité, lexiques bilingues, recherche d’information interlingue
							</p>

					</div>
					

					<div class="article">

						<b>Nadja Vincze, Yves Bestgen</b>


						<br/>

							<i>Identification de mots germes pour la construction d&#39;un lexique de valence au moyen d&#39;une procédure supervisée</i> <br/>

						<a href="actes/taln-2011-long-014.pdf">taln-2011-long-014</a> 
						<a href="bibtex/taln-2011-long-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-014-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-014-key');">mots clés</a> <br/>

							<p id="taln-2011-long-014-abs" class="resume">
							<b>Résumé : </b> De nombreuses méthodes automatiques de classification de textes selon les sentiments qui y sont exprimés s&#39;appuient sur un lexique dans lequel à chaque entrée est associée une valence. Le plus souvent, ce lexique est construit à partir d&#39;un petit nombre de mots, choisis arbitrairement, qui servent de germes pour déterminer automatiquement la valence d&#39;autres mots. La question de l&#39;optimalité de ces mots germes a bien peu retenu l&#39;attention. Sur la base de la comparaison de cinq méthodes automatiques de construction de lexiques de valence, dont une qui, à notre connaissance, n&#39;a jamais été adaptée au français et une autre développée spécifiquement pour la présente étude, nous montrons l&#39;importance du choix de ces mots germes et l&#39;intérêt de les identifier au moyen d&#39;une procédure d&#39;apprentissage supervisée.
							</p>

							<p id="taln-2011-long-014-key" class="mots_cles">
							<b>Mots clés : </b> Analyse de sentiments, lexique de valence, apprentissage supervisé, analyse sémantique latente
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Muller, Philippe Langlais</b>


						<br/>

							<i>Comparaison d’une approche miroir et d’une approche distributionnelle pour l’extraction de mots sémantiquement reliés</i> <br/>

						<a href="actes/taln-2011-long-015.pdf">taln-2011-long-015</a> 
						<a href="bibtex/taln-2011-long-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-015-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-015-key');">mots clés</a> <br/>

							<p id="taln-2011-long-015-abs" class="resume">
							<b>Résumé : </b> Dans (Muller &amp; Langlais, 2010), nous avons comparé une approche distributionnelle et une variante de l’approche miroir proposée par Dyvik (2002) sur une tâche d’extraction de synonymes à partir d’un corpus en français. Nous présentons ici une analyse plus fine des relations extraites automatiquement en nous intéressant cette fois-ci à la langue anglaise pour laquelle de plus amples ressources sont disponibles. Différentes façons d’évaluer notre approche corroborent le fait que l’approche miroir se comporte globalement mieux que l’approche distributionnelle décrite dans (Lin, 1998), une approche de référence dans le domaine.
							</p>

							<p id="taln-2011-long-015-key" class="mots_cles">
							<b>Mots clés : </b> Sémantique lexicale, similarité distributionnelle, similarité traductionnelle
							</p>

					</div>
					

					<div class="article">

						<b>Yann Mathet, Antoine Widlöcher</b>


						<br/>

							<i>Une approche holiste et unifiée de l’alignement et de la mesure d’accord inter-annotateurs</i> <br/>

						<a href="actes/taln-2011-long-016.pdf">taln-2011-long-016</a> 
						<a href="bibtex/taln-2011-long-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-016-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-016-key');">mots clés</a> <br/>

							<p id="taln-2011-long-016-abs" class="resume">
							<b>Résumé : </b> L’alignement et la mesure d’accord sur des textes multi-annotés sont des enjeux majeurs pour la constitution de corpus de référence. Nous défendons dans cet article l’idée que ces deux tâches sont par essence interdépendantes, la mesure d’accord nécessitant de s’appuyer sur des annotations alignées, tandis que les choix d’alignements ne peuvent se faire qu’à l’aune de la mesure qu’ils induisent. Nous proposons des principes formels relevant cette gageure, qui s’appuient notamment sur la notion de désordre du système constitué par l’ensemble des jeux d’annotations d’un texte. Nous posons que le meilleur alignement est celui qui minimise ce désordre, et que la valeur de désordre obtenue rend compte simultanément du taux d’accord. Cette approche, qualifiée d’holiste car prenant en compte l’intégralité du système pour opérer, est algorithmiquement lourde, mais nous sommes parvenus à produire une implémentation d’une version légèrement dégradée de cette dernière, et l’avons intégrée à la plate-forme d’annotation Glozz.
							</p>

							<p id="taln-2011-long-016-key" class="mots_cles">
							<b>Mots clés : </b> Alignement d’annotations, mesure d’accord inter-annotateurs, linguistique de corpus
							</p>

					</div>
					

					<div class="article">

						<b>André Bittar, Pascal Amsili, Pascal Denis</b>


						<br/>

							<i>French TimeBank : un corpus de référence sur la temporalité en français</i> <br/>

						<a href="actes/taln-2011-long-017.pdf">taln-2011-long-017</a> 
						<a href="bibtex/taln-2011-long-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-017-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-017-key');">mots clés</a> <br/>

							<p id="taln-2011-long-017-abs" class="resume">
							<b>Résumé : </b> Cet article a un double objectif : d’une part, il s’agit de présenter à la communauté un corpus récemment rendu public, le French Time Bank (FTiB), qui consiste en une collection de textes journalistiques annotés pour les temps et les événements selon la norme ISO-TimeML ; d’autre part, nous souhaitons livrer les résultats et réflexions méthodologiques que nous avons pu tirer de la réalisation de ce corpus de référence, avec l’idée que notre expérience pourra s’avérer profitable au-delà de la communauté intéressée par le traitement de la temporalité.
							</p>

							<p id="taln-2011-long-017-key" class="mots_cles">
							<b>Mots clés : </b> Annotation temporelle, corpus, ISO-TimeML
							</p>

					</div>
					

					<div class="article">

						<b>Edmond Lassalle</b>


						<br/>

							<i>Acquisition automatique de terminologie à partir de corpus de texte</i> <br/>

						<a href="actes/taln-2011-long-018.pdf">taln-2011-long-018</a> 
						<a href="bibtex/taln-2011-long-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-018-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-018-key');">mots clés</a> <br/>

							<p id="taln-2011-long-018-abs" class="resume">
							<b>Résumé : </b> Les applications de recherche d&#39;informations chez Orange sont confrontées à des flux importants de données textuelles, recouvrant des domaines larges et évoluant très rapidement. Un des problèmes à résoudre est de pouvoir analyser très rapidement ces flux, à un niveau élevé de qualité. Le recours à un modèle d&#39;analyse sémantique, comme solution, n&#39;est viable qu&#39;en s&#39;appuyant sur l&#39;apprentissage automatique pour construire des grandes bases de connaissances dédiées à chaque application. L&#39;extraction terminologique décrite dans cet article est un composant amont de ce dispositif d&#39;apprentissage. Des nouvelles méthodes d&#39;acquisition, basée sur un modèle hybride (analyse par grammaires de chunking et analyse statistique à deux niveaux), ont été développées pour répondre aux contraintes de performance et de qualité.
							</p>

							<p id="taln-2011-long-018-key" class="mots_cles">
							<b>Mots clés : </b> Apprentissage automatique, acquisition terminologique, entropie, grammaires de chunking
							</p>

					</div>
					

					<div class="article">

						<b>Amir Hazem, Emmanuel Morin, Sebastián Peña Saldarriaga</b>


						<br/>

							<i>Métarecherche pour l’extraction lexicale bilingue à partir de corpus comparables</i> <br/>

						<a href="actes/taln-2011-long-019.pdf">taln-2011-long-019</a> 
						<a href="bibtex/taln-2011-long-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-019-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-019-key');">mots clés</a> <br/>

							<p id="taln-2011-long-019-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article une nouvelle manière d’aborder le problème de l’acquisition automatique de paires de mots en relation de traduction à partir de corpus comparables. Nous décrivons tout d’abord les approches standard et par similarité interlangue traditionnellement dédiées à cette tâche. Nous réinterprétons ensuite la méthode par similarité interlangue et motivons un nouveau modèle pour reformuler cette approche inspirée par les métamoteurs de recherche d’information. Les résultats empiriques que nous obtenons montrent que les performances de notre modèle sont toujours supérieures à celles obtenues avec l’approche par similarité interlangue, mais aussi comme étant compétitives par rapport à l’approche standard.
							</p>

							<p id="taln-2011-long-019-key" class="mots_cles">
							<b>Mots clés : </b> Corpus comparables, lexiques bilingues, métarecherche
							</p>

					</div>
					

					<div class="article">

						<b>Alain Joubert, Mathieu Lafourcade, Didier Schwab, Michael Zock</b>


						<br/>

							<i>Évaluation et consolidation d’un réseau lexical via un outil pour retrouver le mot sur le bout de la langue</i> <br/>

						<a href="actes/taln-2011-long-020.pdf">taln-2011-long-020</a> 
						<a href="bibtex/taln-2011-long-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-020-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-020-key');">mots clés</a> <br/>

							<p id="taln-2011-long-020-abs" class="resume">
							<b>Résumé : </b> Depuis septembre 2007, un réseau lexical de grande taille pour le Français est en cours de construction à l&#39;aide de méthodes fondées sur des formes de consensus populaire obtenu via des jeux (projet JeuxDeMots). L’intervention d’experts humains est marginale en ce qu&#39;elle représente moins de 0,5% des relations du réseau et se limite à des corrections, à des ajustements ainsi qu’à la validation des sens de termes. Pour évaluer la qualité de cette ressource construite par des participants de jeu (utilisateurs non experts) nous adoptons une démarche similaire à celle de sa construction, à savoir, la ressource doit être validée sur un vocabulaire de classe ouverte, par des non-experts, de façon stable (persistante dans le temps). Pour ce faire, nous proposons de vérifier si notre ressource est capable de servir de support à la résolution du problème nommé &#39;Mot sur le Bout de la Langue&#39; (MBL). A l&#39;instar de JeuxdeMots, l&#39;outil développé peut être vu comme un jeu en ligne. Tout comme ce dernier, il permet d’acquérir de nouvelles relations, constituant ainsi un enrichissement de notre réseau lexical.
							</p>

							<p id="taln-2011-long-020-key" class="mots_cles">
							<b>Mots clés : </b> Réseau lexical, JeuxDeMots, évaluation, outil de MBL, mot sur le bout de la langue
							</p>

					</div>
					

					<div class="article">

						<b>Matthieu Vernier, Laura Monceaux, Béatrice Daille</b>


						<br/>

							<i>Identifier la cible d’un passage d’opinion dans un corpus multithématique</i> <br/>

						<a href="actes/taln-2011-long-021.pdf">taln-2011-long-021</a> 
						<a href="bibtex/taln-2011-long-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-021-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-021-key');">mots clés</a> <br/>

							<p id="taln-2011-long-021-abs" class="resume">
							<b>Résumé : </b> L’identification de la cible d’une d’opinion fait l’objet d’une attention récente en fouille d’opinion. Les méthodes existantes ont été testées sur des corpus monothématiques en anglais. Elles permettent principalement de traiter les cas où la cible se situe dans la même phrase que l’opinion. Dans cet article, nous abordons cette problématique pour le français dans un corpus multithématique et nous présentons une nouvelle méthode pour identifier la cible d’une opinion apparaissant hors du contexte phrastique. L’évaluation de la méthode montre une amélioration des résultats par rapport à l’existant.
							</p>

							<p id="taln-2011-long-021-key" class="mots_cles">
							<b>Mots clés : </b> Fouille d’opinions, Identification des cibles, Méthode RankSVM
							</p>

					</div>
					

					<div class="article">

						<b>Matthieu Constant, Isabelle Tellier, Denys Duchier, Yoann Dupont, Anthony Sigogne, Sylvie Billot</b>


						<br/>

							<i>Intégrer des connaissances linguistiques dans un CRF : application à l’apprentissage d’un segmenteur-étiqueteur du français</i> <br/>

						<a href="actes/taln-2011-long-022.pdf">taln-2011-long-022</a> 
						<a href="bibtex/taln-2011-long-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-022-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-022-key');">mots clés</a> <br/>

							<p id="taln-2011-long-022-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous synthétisons les résultats de plusieurs séries d’expériences réalisées à l’aide de CRF (Conditional Random Fields ou “champs markoviens conditionnels”) linéaires pour apprendre à annoter des textes français à partir d’exemples, en exploitant diverses ressources linguistiques externes. Ces expériences ont porté sur l’étiquetage morphosyntaxique intégrant l’identification des unités polylexicales. Nous montrons que le modèle des CRF est capable d’intégrer des ressources lexicales riches en unités multi-mots de différentes manières et permet d’atteindre ainsi le meilleur taux de correction d’étiquetage actuel pour le français.
							</p>

							<p id="taln-2011-long-022-key" class="mots_cles">
							<b>Mots clés : </b> Etiquetagemorphosyntaxique,Modèle CRF, Ressources lexicales, Segmentation, Unités polylexicales
							</p>

					</div>
					

					<div class="article">

						<b>Pierre Magistry, Benoît Sagot</b>


						<br/>

							<i>Segmentation et induction de lexique non-supervisées du mandarin</i> <br/>

						<a href="actes/taln-2011-long-023.pdf">taln-2011-long-023</a> 
						<a href="bibtex/taln-2011-long-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-023-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-023-key');">mots clés</a> <br/>

							<p id="taln-2011-long-023-abs" class="resume">
							<b>Résumé : </b> Pour la plupart des langues utilisant l&#39;alphabet latin, le découpage d&#39;un texte selon les espaces et les symboles de ponctuation est une bonne approximation d&#39;un découpage en unités lexicales. Bien que cette approximation cache de nombreuses difficultés, elles sont sans comparaison avec celles que l&#39;on rencontre lorsque l&#39;on veut traiter des langues qui, comme le chinois mandarin, n&#39;utilisent pas l&#39;espace. Un grand nombre de systèmes de segmentation ont été proposés parmi lesquels certains adoptent une approche non-supervisée motivée linguistiquement. Cependant les méthodes d&#39;évaluation communément utilisées ne rendent pas compte de toutes les propriétés de tels systèmes. Dans cet article, nous montrons qu&#39;un modèle simple qui repose sur une reformulation en termes d&#39;entropie d&#39;une hypothèse indépendante de la langue énoncée par Harris (1955), permet de segmenter un corpus et d&#39;en extraire un lexique. Testé sur le corpus de l&#39;Academia Sinica, notre système permet l&#39;induction d&#39;une segmentation et d&#39;un lexique qui ont de bonnes propriétés intrinsèques et dont les caractéristiques sont similaires à celles du lexique sous-jacent au corpus segmenté manuellement. De plus, on constate une certaine corrélation entre les résultats du modèle de segmentation et les structures syntaxiques fournies par une sous-partie arborée corpus.
							</p>

							<p id="taln-2011-long-023-key" class="mots_cles">
							<b>Mots clés : </b> Segmentation non-supervisée, entropie, induction de lexique, unité lexicale, chinois mandarin
							</p>

					</div>
					

					<div class="article">

						<b>Delphine Bernhard, Bruno Cartoni, Delphine Tribout</b>


						<br/>

							<i>Évaluer la pertinence de la morphologie constructionnelle dans les systèmes de Question-Réponse</i> <br/>

						<a href="actes/taln-2011-long-024.pdf">taln-2011-long-024</a> 
						<a href="bibtex/taln-2011-long-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-024-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-024-key');">mots clés</a> <br/>

							<p id="taln-2011-long-024-abs" class="resume">
							<b>Résumé : </b> Les connaissances morphologiques sont fréquemment utilisées en Question-Réponse afin de faciliter l’appariement entre mots de la question et mots du passage contenant la réponse. Il n’existe toutefois pas d’étude qualitative et quantitative sur les phénomènes morphologiques les plus pertinents pour ce cadre applicatif. Dans cet article, nous présentons une analyse détaillée des phénomènes de morphologie constructionnelle permettant de faire le lien entre question et réponse. Pour ce faire, nous avons constitué et annoté un corpus de paires de questions-réponses, qui nous a permis de construire une ressource de référence, utile pour l’évaluation de la couverture de ressources et d’outils d’analyse morphologique. Nous détaillons en particulier les phénomènes de dérivation et de composition et montrons qu’il reste un nombre important de relations morphologiques dérivationnelles pour lesquelles il n’existe pas encore de ressource exploitable pour le français.
							</p>

							<p id="taln-2011-long-024-key" class="mots_cles">
							<b>Mots clés : </b> Évaluation, Morphologie, Ressources, Système de Question-Réponse
							</p>

					</div>
					

					<div class="article">

						<b>Julien Gosme, Yves Lepage</b>


						<br/>

							<i>Structure des trigrammes inconnus et lissage par analogie</i> <br/>

						<a href="actes/taln-2011-long-025.pdf">taln-2011-long-025</a> 
						<a href="bibtex/taln-2011-long-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-025-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-025-key');">mots clés</a> <br/>

							<p id="taln-2011-long-025-abs" class="resume">
							<b>Résumé : </b> Nous montrons dans une série d’expériences sur quatre langues, sur des échantillons du corpus Europarl, que, dans leur grande majorité, les trigrammes inconnus d’un jeu de test peuvent être reconstruits par analogie avec des trigrammes hapax du corpus d’entraînement. De ce résultat, nous dérivons une méthode de lissage simple pour les modèles de langue par trigrammes et obtenons de meilleurs résultats que les lissages de Witten-Bell, Good-Turing et Kneser-Ney dans des expériences menées en onze langues sur la partie commune d’Europarl, sauf pour le finnois et, dans une moindre mesure, le français.
							</p>

							<p id="taln-2011-long-025-key" class="mots_cles">
							<b>Mots clés : </b> analogie, trigrammes inconnus, trigrammes hapax, modèle de langue trigrammes, Europarl
							</p>

					</div>
					

					<div class="article">

						<b>Joseph Le Roux, Benoît Favre, Seyed Abolghasem Mirroshandel, Alexis Nasr</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>Modèles génératif et discriminant en analyse syntaxique : expériences sur le corpus arboré de Paris 7</i> <br/>

						<a href="actes/taln-2011-long-026.pdf">taln-2011-long-026</a> 
						<a href="bibtex/taln-2011-long-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-026-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-026-key');">mots clés</a> <br/>

							<p id="taln-2011-long-026-abs" class="resume">
							<b>Résumé : </b> Nous présentons une architecture pour l’analyse syntaxique en deux étapes. Dans un premier temps un analyseur syntagmatique construit, pour chaque phrase, une liste d’analyses qui sont converties en arbres de dépendances. Ces arbres sont ensuite réévalués par un réordonnanceur discriminant. Cette méthode permet de prendre en compte des informations auxquelles l’analyseur n’a pas accès, en particulier des annotations fonctionnelles. Nous validons notre approche par une évaluation sur le corpus arboré de Paris 7. La seconde étape permet d’améliorer significativement la qualité des analyses retournées, quelle que soit la métrique utilisée.
							</p>

							<p id="taln-2011-long-026-key" class="mots_cles">
							<b>Mots clés : </b> analyse syntaxique, corpus arboré, apprentissage automatique, réordonnancement discriminant
							</p>

					</div>
					

					<div class="article">

						<b>Anne-Lyse Minard, Anne-Laure Ligozat, Brigitte Grau</b>


						<br/>

							<i>Apport de la syntaxe pour l’extraction de relations en domaine médical</i> <br/>

						<a href="actes/taln-2011-long-027.pdf">taln-2011-long-027</a> 
						<a href="bibtex/taln-2011-long-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-027-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-027-key');">mots clés</a> <br/>

							<p id="taln-2011-long-027-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous nous intéressons à l’identification de relations entre entités en domaine de spécialité, et étudions l’apport d’informations syntaxiques. Nous nous plaçons dans le domaine médical, et analysons des relations entre concepts dans des comptes-rendus médicaux, tâche évaluée dans la campagne i2b2 en 2010. Les relations étant exprimées par des formulations très variées en langue, nous avons procédé à l’analyse des phrases en extrayant des traits qui concourent à la reconnaissance de la présence d’une relation et nous avons considéré l’identification des relations comme une tâche de classification multi-classes, chaque catégorie de relation étant considérée comme une classe. Notre système de référence est celui qui a participé à la campagne i2b2, dont la F-mesure est d’environ 0,70. Nous avons évalué l’apport de la syntaxe pour cette tâche, tout d’abord en ajoutant des attributs syntaxiques à notre classifieur, puis en utilisant un apprentissage fondé sur la structure syntaxique des phrases (apprentissage à base de tree kernels) ; cette dernière méthode améliore les résultats de la classification de 3%.
							</p>

							<p id="taln-2011-long-027-key" class="mots_cles">
							<b>Mots clés : </b> extraction de relation, domaine médical, apprentissage multi-classes, tree kernel
							</p>

					</div>
					

					<div class="article">

						<b>Guillaume Bonfante, Bruno Guillaume, Mathieu Morey, Guy Perrier</b>


						<br/>

							<i>Enrichissement de structures en dépendances par réécriture de graphes</i> <br/>

						<a href="actes/taln-2011-long-028.pdf">taln-2011-long-028</a> 
						<a href="bibtex/taln-2011-long-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-028-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-028-key');">mots clés</a> <br/>

							<p id="taln-2011-long-028-abs" class="resume">
							<b>Résumé : </b> Nous montrons comment enrichir une annotation en dépendances syntaxiques au format du French Treebank de Paris 7 en utilisant la réécriture de graphes, en vue du calcul de sa représentation sémantique. Le système de réécriture est composé de règles grammaticales et lexicales structurées en modules. Les règles lexicales utilisent une information de contrôle extraite du lexique des verbes français Dicovalence.
							</p>

							<p id="taln-2011-long-028-key" class="mots_cles">
							<b>Mots clés : </b> dépendance, French Treebank, réécriture de graphes, Dicovalence
							</p>

					</div>
					

					<div class="article">

						<b>Alexander Pak, Patrick Paroubek</b>


						<br/>

							<i>Classification en polarité de sentiments avec une représentation textuelle à base de sous-graphes d’arbres de dépendances</i> <br/>

						<a href="actes/taln-2011-long-029.pdf">taln-2011-long-029</a> 
						<a href="bibtex/taln-2011-long-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-029-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-029-key');">mots clés</a> <br/>

							<p id="taln-2011-long-029-abs" class="resume">
							<b>Résumé : </b> Les approches classiques à base de n-grammes en analyse supervisée de sentiments ne peuvent pas correctement identifier les expressions complexes de sentiments à cause de la perte d’information induite par l’approche « sac de mots » utilisée pour représenter les textes. Dans notre approche, nous avons recours à des sous-graphes extraits des graphes de dépendances syntaxiques comme traits pour la classification de sentiments. Nous représentons un texte par un vecteur composé de ces sous-graphes syntaxiques et nous employons un classifieurs SVM état-de-l’art pour identifier la polarité d’un texte. Nos évaluations expérimentales sur des critiques de jeux vidéo montrent que notre approche à base de sous-graphes est meilleure que les approches standard à modèles « sac de mots » et n-grammes. Dans cet article nous avons travaillé sur le français, mais notre approche peut facilement être adaptée à d’autres langues.
							</p>

							<p id="taln-2011-long-029-key" class="mots_cles">
							<b>Mots clés : </b> analyse de sentiments, analyse syntaxique, arbre de dépendances, SVM
							</p>

					</div>
					

					<div class="article">

						<b>Sylvain Kahane</b>


						<br/>

							<i>Une modélisation des dites alternances de portée des quantifieurs par des opérations de combinaison des groupes nominaux</i> <br/>

						<a href="actes/taln-2011-long-030.pdf">taln-2011-long-030</a> 
						<a href="bibtex/taln-2011-long-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-030-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-030-key');">mots clés</a> <br/>

							<p id="taln-2011-long-030-abs" class="resume">
							<b>Résumé : </b> Nous montrons que les différentes interprétations d’une combinaison de plusieurs GN peuvent être modélisées par deux opérations de combinaison sur les référents de ces GN, appelées combinaison cumulative et combinaison distributive. Nous étudions aussi bien les GN définis et indéfinis que les GN quantifiés ou pluriels et nous montrons comment la combinaison d’un GN avec d’autres éléments peut induire des interprétations collective ou individualisante. Selon la façon dont un GN se combine avec d’autres GN, le calcul de son référent peut être fonction de ces derniers ; ceci définit une relation d’ancrage de chaque GN, qui induit un ordre partiel sur les GN. Considérer cette relation plutôt que la relation converse de portée simplifie le calcul de l’interprétation des GN et des énoncés. Des représentations sémantiques graphiques et algébriques sans considération de la portée sont proposées pour les dites alternances de portée.
							</p>

							<p id="taln-2011-long-030-key" class="mots_cles">
							<b>Mots clés : </b> portée des quantifieurs, cumulatif, collectif, distributif, référent de discours, ancrage
							</p>

					</div>
					

					<div class="article">

						<b>Delphine Bernhard, Anne-Laure Ligozat</b>


						<br/>

							<i>Analyse automatique de la modalité et du niveau de certitude : application au domaine médical</i> <br/>

						<a href="actes/taln-2011-long-031.pdf">taln-2011-long-031</a> 
						<a href="bibtex/taln-2011-long-031.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-031-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-031-key');">mots clés</a> <br/>

							<p id="taln-2011-long-031-abs" class="resume">
							<b>Résumé : </b> De nombreux phénomènes linguistiques visent à exprimer le doute ou l’incertitude de l’énonciateur, ainsi que la subjectivité potentielle du point de vue. La prise en compte de ces informations sur le niveau de certitude est primordiale pour de nombreuses applications du traitement automatique des langues, en particulier l’extraction d’information dans le domaine médical. Dans cet article, nous présentons deux systèmes qui analysent automatiquement les niveaux de certitude associés à des problèmes médicaux mentionnés dans des compte-rendus cliniques en anglais. Le premier système procède par apprentissage supervisé et obtient une f-mesure de 0,93. Le second système utilise des règles décrivant des déclencheurs linguistiques spécifiques et obtient une f-mesure de 0,90.
							</p>

							<p id="taln-2011-long-031-key" class="mots_cles">
							<b>Mots clés : </b> Modalité épistémique, Niveau de certitude, Domaine médical
							</p>

					</div>
					

					<div class="article">

						<b>Laurence Danlos</b>


						<br/>

							<i>Analyse discursive et informations de factivité</i> <br/>

						<a href="actes/taln-2011-long-032.pdf">taln-2011-long-032</a> 
						<a href="bibtex/taln-2011-long-032.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-032-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-032-key');">mots clés</a> <br/>

							<p id="taln-2011-long-032-abs" class="resume">
							<b>Résumé : </b> Les annotations discursives proposées dans le cadre de théories discursives comme RST (Rhetorical Structure Theory) ou SDRT (Segmented Dicourse Representation Theory) ont comme point fort de construire une structure discursive globale liant toutes les informations données dans un texte. Les annotations discursives proposées dans le PDTB (Penn Discourse Tree Bank) ont comme point fort d’identifier la “source” de chaque information du texte—répondant ainsi à la question qui a dit ou pense quoi ? Nous proposons une approche unifiée pour les annotations discursives alliant les points forts de ces deux courants de recherche. Cette approche unifiée repose crucialement sur des information de factivité, telles que celles qui sont annotées dans le corpus (anglais) FactBank.
							</p>

							<p id="taln-2011-long-032-key" class="mots_cles">
							<b>Mots clés : </b> Discours, Analyse discursive, Factivité (véracité), Interface syntaxe-sémantique, RST, SDRT, PDTB, FactBank
							</p>

					</div>
					

					<div class="article">

						<b>Camille Dutrey, Houda Bouamor, Delphine Bernhard, Aurélien Max</b>


						<br/>

							<i>Paraphrases et modifications locales dans l’historique des révisions deWikipédia</i> <br/>

						<a href="actes/taln-2011-long-033.pdf">taln-2011-long-033</a> 
						<a href="bibtex/taln-2011-long-033.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-033-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-033-key');">mots clés</a> <br/>

							<p id="taln-2011-long-033-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous analysons les modifications locales disponibles dans l’historique des révisions de la version française de Wikipédia. Nous définissons tout d’abord une typologie des modifications fondée sur une étude détaillée d’un large corpus de modifications. Puis, nous détaillons l’annotation manuelle d’une partie de ce corpus afin d’évaluer le degré de complexité de la tâche d’identification automatique de paraphrases dans ce genre de corpus. Enfin, nous évaluons un outil d’identification de paraphrases à base de règles sur un sous-ensemble de notre corpus.
							</p>

							<p id="taln-2011-long-033-key" class="mots_cles">
							<b>Mots clés : </b> Wikipédia, révisions, identification de paraphrases
							</p>

					</div>
					

					<div class="article">

						<b>Patrick Saint-Dizier</b>


						<br/>

							<i>&lt;TextCoop&gt;: un analyseur de discours basé sur les grammaires logiques</i> <br/>

						<a href="actes/taln-2011-long-034.pdf">taln-2011-long-034</a> 
						<a href="bibtex/taln-2011-long-034.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-034-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-034-key');">mots clés</a> <br/>

							<p id="taln-2011-long-034-abs" class="resume">
							<b>Résumé : </b> Dans ce document, nous présentons les principales caractéristiques de &lt;TextCoop&gt;, un environnement basé sur les grammaires logiques dédié à l’analyse de structures discursives. Nous étudions en particulier le langage DisLog qui fixe la structure des règles et des spécifications qui les accompagnent. Nous présentons la structure du moteur de &lt;TextCoop&gt; en indiquant au fur et à mesure du texte l’état du travail, les performances et les orientations en particulier en matière d’environnement, d’aide à l’écriture de règles et de développement applicatif.
							</p>

							<p id="taln-2011-long-034-key" class="mots_cles">
							<b>Mots clés : </b> grammaire du discours, programmation en logique, grammaires logiques
							</p>

					</div>
					

					<div class="article">

						<b>Katya Alahverdzhieva, Alex Lascarides</b>


						<br/>

							<i>Integration of Speech and Deictic Gesture in a Multimodal Grammar</i> <br/>

						<a href="actes/taln-2011-long-035.pdf">taln-2011-long-035</a> 
						<a href="bibtex/taln-2011-long-035.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-035-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-035-key');">mots clés</a> <br/>

							<p id="taln-2011-long-035-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une analyse à base de contraintes de la relation forme-sens des gestes déictiques et de leur signal de parole synchrone. En nous basant sur une étude empirique de corpus multimodaux, nous définissons quels énoncés multimodaux sont bien formés, et lesquels ne pourraient jamais produire le sens voulu dans la situation communicative. Plus précisément, nous formulons une grammaire multimodale dont les règles de construction utilisent la prosodie, la syntaxe et la sémantique de la parole, la forme et le sens du signal déictique, ainsi que la performance temporelle de la parole et la deixis afin de contraindre la production d’un arbre de syntaxe combinant parole et gesture déictique ainsi que la représentation unifiée du sens pour l’action multimodale correspondant à cet arbre. La contribution de notre projet est double : nous ajoutons aux ressources existantes pour le TAL un corpus annoté de parole et de gestes, et nous créons un cadre théorique pour la grammaire au sein duquel la composition sémantique d’un énoncé découle de la synchronie entre geste et parole.
							</p>

							<p id="taln-2011-long-035-key" class="mots_cles">
							<b>Mots clés : </b> Deixis, parole et geste, grammaires multimodales
							</p>

					</div>
					

					<div class="article">

						<b>Adrien Lardilleux, François Yvon, Yves Lepage</b>


						<br/>

							<i>Généralisation de l’alignement sous-phrastique par échantillonnage</i> <br/>

						<a href="actes/taln-2011-long-036.pdf">taln-2011-long-036</a> 
						<a href="bibtex/taln-2011-long-036.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-036-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-036-key');">mots clés</a> <br/>

							<p id="taln-2011-long-036-abs" class="resume">
							<b>Résumé : </b> L’alignement sous-phrastique consiste à extraire des traductions d’unités textuelles de grain inférieur à la phrase à partir de textes multilingues parallèles alignés au niveau de la phrase. Un tel alignement est nécessaire, par exemple, pour entraîner des systèmes de traduction statistique. L’approche standard pour réaliser cette tâche implique l’estimation successive de plusieurs modèles probabilistes de complexité croissante et l’utilisation d’heuristiques qui permettent d’aligner des mots isolés, puis, par extension, des groupes de mots. Dans cet article, nous considérons une approche alternative, initialement proposée dans (Lardilleux &amp; Lepage, 2008), qui repose sur un principe beaucoup plus simple, à savoir la comparaison des profils d’occurrences dans des souscorpus obtenus par échantillonnage. Après avoir analysé les forces et faiblesses de cette approche, nous montrons comment améliorer la détection d’unités de traduction longues, et évaluons ces améliorations sur des tâches de traduction automatique.
							</p>

							<p id="taln-2011-long-036-key" class="mots_cles">
							<b>Mots clés : </b> alignement sous-phrastique, traduction automatique par fragments
							</p>

					</div>
					

					<div class="article">

						<b>Nadi Tomeh, Alexandre Allauzen, François Yvon</b>


						<br/>

							<i>Estimation d’un modèle de traduction à partir d’alignements mot-à-mot non-déterministes</i> <br/>

						<a href="actes/taln-2011-long-037.pdf">taln-2011-long-037</a> 
						<a href="bibtex/taln-2011-long-037.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-037-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-037-key');">mots clés</a> <br/>

							<p id="taln-2011-long-037-abs" class="resume">
							<b>Résumé : </b> Dans les systèmes de traduction statistique à base de segments, le modèle de traduction est estimé à partir d’alignements mot-à-mot grâce à des heuristiques d’extraction et de valuation. Bien que ces alignements mot-à-mot soient construits par des modèles probabilistes, les processus d’extraction et de valuation utilisent ces modèles en faisant l’hypothèse que ces alignements sont déterministes. Dans cet article, nous proposons de lever cette hypothèse en considérant l’ensemble de la matrice d’alignement, d’une paire de phrases, chaque association étant valuée par sa probabilité. En comparaison avec les travaux antérieurs, nous montrons qu’en utilisant un modèle exponentiel pour estimer de manière discriminante ces probabilités, il est possible d’obtenir des améliorations significatives des performances de traduction. Ces améliorations sont mesurées à l’aide de la métrique BLEU sur la tâche de traduction de l’arabe vers l’anglais de l’évaluation NIST MT’09, en considérant deux types de conditions selon la taille du corpus de données parallèles utilisées.
							</p>

							<p id="taln-2011-long-037-key" class="mots_cles">
							<b>Mots clés : </b> traduction statistique, modèles de traduction à base de segments, modèles d’alignement mot-à-mot
							</p>

					</div>
					

					<div class="article">

						<b>Houda Bouamor, Aurélien Max, Anne Vilnat</b>


						<br/>

							<i>Combinaison d’informations pour l’alignement monolingue</i> <br/>

						<a href="actes/taln-2011-long-038.pdf">taln-2011-long-038</a> 
						<a href="bibtex/taln-2011-long-038.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-long-038-abs');">résumé</a>
							<a onclick="toggle('taln-2011-long-038-key');">mots clés</a> <br/>

							<p id="taln-2011-long-038-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous décrivons une nouvelle méthode d’alignement automatique de paraphrases d’énoncés. Nous utilisons des méthodes développées précédemment afin de produire différentes approches hybrides (hybridations). Ces différentes méthodes permettent d’acquérir des équivalences textuelles à partir d’un corpus monolingue parallèle. L’hybridation combine des informations obtenues par diverses techniques : alignements statistiques, approche symbolique, fusion d’arbres syntaxiques et alignement basé sur des distances d’édition. Nous avons évalué l’ensemble de ces résultats et nous constatons une amélioration sur l’acquisition de paraphrases sous-phrastiques.
							</p>

							<p id="taln-2011-long-038-key" class="mots_cles">
							<b>Mots clés : </b> Paraphrase sous-phrastique, corpus parallèle monolingue, hybridation
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="court">Papiers courts</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Jean-Yves Antoine, Marc Le Tallec, Jeanne Villaneau</b>


						<br/>

							<i>Evaluation de la détection des émotions, des opinions ou des sentiments : dictature de la majorité ou respect de la diversité d’opinions ?</i> <br/>

						<a href="actes/taln-2011-court-001.pdf">taln-2011-court-001</a> 
						<a href="bibtex/taln-2011-court-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-001-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-001-key');">mots clés</a> <br/>

							<p id="taln-2011-court-001-abs" class="resume">
							<b>Résumé : </b> Détection d’émotion, fouille d’opinion et analyse des sentiments sont généralement évalués par comparaison des réponses du système concerné par rapport à celles contenues dans un corpus de référence. Les questions posées dans cet article concernent à la fois la définition de la référence et la fiabilité des métriques les plus fréquemment utilisées pour cette comparaison. Les expérimentations menées pour évaluer le système de détection d’émotions EmoLogus servent de base de réflexion pour ces deux problèmes. L’analyse des résultats d’EmoLogus et la comparaison entre les différentes métriques remettent en cause le choix du vote majoritaire comme référence. Par ailleurs elles montrent également la nécessité de recourir à des outils statistiques plus évolués que ceux généralement utilisés pour obtenir des évaluations fiables de systèmes qui travaillent sur des données intrinsèquement subjectives et incertaines.
							</p>

							<p id="taln-2011-court-001-key" class="mots_cles">
							<b>Mots clés : </b> Détection d’émotion, analyse de sentiments, fouille d’opinion ; Evaluation : métrique d’évaluation, constitution de référence, analyse statistique des résultats
							</p>

					</div>
					

					<div class="article">

						<b>Violeta Seretan</b>


						<br/>

							<i>A Collocation-Driven Approach to Text Summarization</i> <br/>

						<a href="actes/taln-2011-court-002.pdf">taln-2011-court-002</a> 
						<a href="bibtex/taln-2011-court-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-002-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-002-key');">mots clés</a> <br/>

							<p id="taln-2011-court-002-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous décrivons une nouvelle approche pour la création de résumés extractifs – tâche qui consiste à créer automatiquement un résumé pour un document en sélectionnant un sous-ensemble de ses phrases – qui exploite des informations collocationnelles spécifiques à un domaine, acquises préalablement à partir d’un corpus de développement. Un extracteur de collocations fondé sur l’analyse syntaxique est utilisé afin d’inférer un modèle de contenu qui est ensuite appliqué au document à résumer. Cette approche a été utilisée pour la création des versions simples pour les articles de Wikipedia en anglais, dans le cadre d’un projet visant la création automatique d’articles simplifiées, similaires aux articles recensées dans Simple English Wikipedia. Une évaluation du système développé reste encore à faire. Toutefois, les résultats préalables obtenus pour les articles sur des villes montrent le potentiel de cette approche guidée par collocations pour la sélection des phrases pertinentes.
							</p>

							<p id="taln-2011-court-002-key" class="mots_cles">
							<b>Mots clés : </b> résumé de texte automatique, résumé extractif, statistiques de co-occurrence, collocations, analyse syntaxique, Wikipedia
							</p>

					</div>
					

					<div class="article">

						<b>Thomas François, Patrick Watrin</b>


						<br/>

							<i>Quel apport des unités polylexicales dans une formule de lisibilité pour le français langue étrangère</i> <br/>

						<a href="actes/taln-2011-court-003.pdf">taln-2011-court-003</a> 
						<a href="bibtex/taln-2011-court-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-003-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-003-key');">mots clés</a> <br/>

							<p id="taln-2011-court-003-abs" class="resume">
							<b>Résumé : </b> Cette étude envisage l’emploi des unités polylexicales (UPs) comme prédicteurs dans une formule de lisibilité pour le français langue étrangère. À l’aide d’un extracteur d’UPs combinant une approche statistique à un filtre linguistique, nous définissons six variables qui prennent en compte la densité et la probabilité des UPs nominales, mais aussi leur structure interne. Nos expérimentations concluent à un faible pouvoir prédictif de ces six variables et révèlent qu’une simple approche basée sur la probabilité moyenne des n-grammes des textes est plus efficace.
							</p>

							<p id="taln-2011-court-003-key" class="mots_cles">
							<b>Mots clés : </b> Lisibilité du FLE, unités polylexicales nominales, modèles N-grammes
							</p>

					</div>
					

					<div class="article">

						<b>Frédéric Béchet, Benoît Sagot, Rosa Stern</b>


						<br/>

							<i>Coopération de méthodes statistiques et symboliques pour l’adaptation non-supervisée d’un système d’étiquetage en entités nommées</i> <br/>

						<a href="actes/taln-2011-court-004.pdf">taln-2011-court-004</a> 
						<a href="bibtex/taln-2011-court-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-004-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-004-key');">mots clés</a> <br/>

							<p id="taln-2011-court-004-abs" class="resume">
							<b>Résumé : </b> La détection et le typage des entités nommées sont des tâches pour lesquelles ont été développés à la fois des systèmes symboliques et probabilistes. Nous présentons les résultats d’une expérience visant à faire interagir le système à base de règles NP, développé sur des corpus provenant de l’AFP, intégrant la base d’entités Aleda et qui a une bonne précision, et le système LIANE, entraîné sur des transcriptions de l’oral provenant du corpus ESTER et qui a un bon rappel. Nous montrons qu’on peut adapter à un nouveau type de corpus, de manière non supervisée, un système probabiliste tel que LIANE grâce à des corpus volumineux annotés automatiquement par NP. Cette adaptation ne nécessite aucune annotation manuelle supplémentaire et illustre la complémentarité des méthodes numériques et symboliques pour la résolution de tâches linguistiques.
							</p>

							<p id="taln-2011-court-004-key" class="mots_cles">
							<b>Mots clés : </b> Détection d’entités nommées, adaptation à un nouveau domaine, coopération entre approches probabilistes et symboliques
							</p>

					</div>
					

					<div class="article">

						<b>Nuria Gala, Nabil Hathout, Alexis Nasr, Véronique Rey, Selja Seppälä</b>


						<br/>

							<i>Création de clusters sémantiques dans des familles morphologiques à partir du TLFi</i> <br/>

						<a href="actes/taln-2011-court-005.pdf">taln-2011-court-005</a> 
						<a href="bibtex/taln-2011-court-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-005-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-005-key');">mots clés</a> <br/>

							<p id="taln-2011-court-005-abs" class="resume">
							<b>Résumé : </b> La constitution de ressources linguistiques est une tâche longue et coûteuse. C’est notamment le cas pour les ressources morphologiques. Ces ressources décrivent de façon approfondie et explicite l’organisation morphologique du lexique complétée d’informations sémantiques exploitables dans le domaine du TAL. Le travail que nous présentons dans cet article s’inscrit dans cette perspective et, plus particulièrement, dans l’optique d’affiner une ressource existante en s’appuyant sur des informations sémantiques obtenues automatiquement. Notre objectif est de caractériser sémantiquement des familles morpho-phonologiques (des mots partageant une même racine et une continuité de sens). Pour ce faire, nous avons utilisé des informations extraites du TLFi annoté morpho-syntaxiquement. Les premiers résultats de ce travail seront analysés et discutés.
							</p>

							<p id="taln-2011-court-005-key" class="mots_cles">
							<b>Mots clés : </b> Ressources lexicales, familles morphologiques, clusters sémantiques, mesure de Lesk
							</p>

					</div>
					

					<div class="article">

						<b>Louis de Viron, Delphine Bernhard, Véronique Moriceau, Xavier Tannier</b>


						<br/>

							<i>Génération automatique de questions à partir de textes en français</i> <br/>

						<a href="actes/taln-2011-court-006.pdf">taln-2011-court-006</a> 
						<a href="bibtex/taln-2011-court-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-006-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-006-key');">mots clés</a> <br/>

							<p id="taln-2011-court-006-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article un générateur automatique de questions pour le français. Le système de génération procède par transformation de phrases déclaratives en interrogatives et se base sur une analyse syntaxique préalable de la phrase de base. Nous détaillons les différents types de questions générées. Nous présentons également une évaluation de l’outil, qui démontre que 41 % des questions générées par le système sont parfaitement bien formées.
							</p>

							<p id="taln-2011-court-006-key" class="mots_cles">
							<b>Mots clés : </b> génération de questions, analyse syntaxique, transformation syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco, Anne-Laure Ligozat, Isabelle Robba, Anne Vilnat</b>


						<br/>

							<i>Sélection de réponses à des questions dans un corpus Web par validation</i> <br/>

						<a href="actes/taln-2011-court-007.pdf">taln-2011-court-007</a> 
						<a href="bibtex/taln-2011-court-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-007-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-007-key');">mots clés</a> <br/>

							<p id="taln-2011-court-007-abs" class="resume">
							<b>Résumé : </b> Les systèmes de questions réponses recherchent la réponse à une question posée en langue naturelle dans un ensemble de documents. Les collectionsWeb diffèrent des articles de journaux de par leurs structures et leur style. Pour tenir compte de ces spécificités nous avons développé un système fondé sur une approche robuste de validation où des réponses candidates sont extraites à partir de courts passages textuels puis ordonnées par apprentissage. Les résultats montrent une amélioration du MRR (Mean Reciprocal Rank) de 48% par rapport à la baseline.
							</p>

							<p id="taln-2011-court-007-key" class="mots_cles">
							<b>Mots clés : </b> systèmes de questions réponses, validation de réponses, analyse de documents Web
							</p>

					</div>
					

					<div class="article">

						<b>Wei Wang, Romaric Besançon, Olivier Ferret, Brigitte Grau</b>


						<br/>

							<i>Filtrage de relations pour l’extraction d’information non supervisée</i> <br/>

						<a href="actes/taln-2011-court-008.pdf">taln-2011-court-008</a> 
						<a href="bibtex/taln-2011-court-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-008-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-008-key');">mots clés</a> <br/>

							<p id="taln-2011-court-008-abs" class="resume">
							<b>Résumé : </b> Le domaine de l’extraction d’information s’est récemment développé en limitant les contraintes sur la définition des informations à extraire, ouvrant la voie à des applications de veille plus ouvertes. Dans ce contexte de l’extraction d’information non supervisée, nous nous intéressons à l’identification et la caractérisation de nouvelles relations entre des types d’entités fixés. Un des défis de cette tâche est de faire face à la masse importante de candidats pour ces relations lorsque l’on considère des corpus de grande taille. Nous présentons dans cet article une approche pour le filtrage des relations combinant méthode heuristique et méthode par apprentissage. Nous évaluons ce filtrage de manière intrinsèque et par son impact sur un regroupement sémantique des relations.
							</p>

							<p id="taln-2011-court-008-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’information non supervisée, filtrage, apprentissage automatique, clustering
							</p>

					</div>
					

					<div class="article">

						<b>Béatrice Arnulphy, Xavier Tannier, Anne Vilnat</b>


						<br/>

							<i>Un lexique pondéré des noms d’événements en français</i> <br/>

						<a href="actes/taln-2011-court-009.pdf">taln-2011-court-009</a> 
						<a href="bibtex/taln-2011-court-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-009-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-009-key');">mots clés</a> <br/>

							<p id="taln-2011-court-009-abs" class="resume">
							<b>Résumé : </b> Cet article décrit une étude sur l’annotation automatique des noms d’événements dans les textes en français. Plusieurs lexiques existants sont utilisés, ainsi que des règles syntaxiques d’extraction, et un lexique composé de façon automatique, permettant de fournir une valeur sur le niveau d’ambiguïté du mot en tant qu’événement. Cette nouvelle information permettrait d’aider à la désambiguïsation des noms d’événements en contexte.
							</p>

							<p id="taln-2011-court-009-key" class="mots_cles">
							<b>Mots clés : </b> extraction d’information, événements nominaux, lexiques
							</p>

					</div>
					

					<div class="article">

						<b>Stéphane Huet, Fabrice Lefèvre</b>


						<br/>

							<i>Alignement automatique pour la compréhension littérale de l’oral par approche segmentale</i> <br/>

						<a href="actes/taln-2011-court-010.pdf">taln-2011-court-010</a> 
						<a href="bibtex/taln-2011-court-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-010-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-010-key');">mots clés</a> <br/>

							<p id="taln-2011-court-010-abs" class="resume">
							<b>Résumé : </b> Les approches statistiques les plus performantes actuellement pour la compréhension automatique du langage naturel nécessitent une annotation segmentale des données d’entraînement. Nous étudions dans cet article une alternative permettant d’obtenir de façon non-supervisée un alignement segmental d’unités conceptuelles sur les mots. L’impact de l’alignement automatique sur les performances du système de compréhension est évalué sur une tâche de dialogue oral.
							</p>

							<p id="taln-2011-court-010-key" class="mots_cles">
							<b>Mots clés : </b> Alignement non-supervisé, compréhension de la parole
							</p>

					</div>
					

					<div class="article">

						<b>Romain Deveaud, Eric Sanjuan, Patrice Bellot</b>


						<br/>

							<i>Ajout d’informations contextuelles pour la recherche de passages au sein de Wikipédia</i> <br/>

						<a href="actes/taln-2011-court-011.pdf">taln-2011-court-011</a> 
						<a href="bibtex/taln-2011-court-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-011-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-011-key');">mots clés</a> <br/>

							<p id="taln-2011-court-011-abs" class="resume">
							<b>Résumé : </b> La recherche de passages consiste à extraire uniquement des passages pertinents par rapport à une requête utilisateur plutôt qu’un ensemble de documents entiers. Cette récupération de passages est souvent handicapée par le manque d’informations complémentaires concernant le contexte de la recherche initiée par l’utilisateur. Des études montrent que l’ajout d’informations contextuelles par l’utilisateur peut améliorer les performances des systèmes de recherche de passages. Nous confirmons ces observations dans cet article, et nous introduisons également une méthode d’enrichissement de la requête à partir d’informations contextuelles issues de documents encyclopédiques. Nous menons des expérimentations en utilisant la collection et les méthodes d’évaluation proposées par la campagne INEX. Les résultats obtenus montrent que l’ajout d’informations contextuelles permet d’améliorer significativement les performances de notre système de recherche de passages. Nous observons également que notre approche automatique obtient les meilleurs résultats parmi les différentes approches que nous évaluons.
							</p>

							<p id="taln-2011-court-011-key" class="mots_cles">
							<b>Mots clés : </b> Recherche de passages, enrichissement de requêtes, contexte, Wikipedia, INEX, entropie
							</p>

					</div>
					

					<div class="article">

						<b>Jana Strnadová, Benoît Sagot</b>


						<br/>

							<i>Construction d’un lexique des adjectifs dénominaux</i> <br/>

						<a href="actes/taln-2011-court-012.pdf">taln-2011-court-012</a> 
						<a href="bibtex/taln-2011-court-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-012-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-012-key');">mots clés</a> <br/>

							<p id="taln-2011-court-012-abs" class="resume">
							<b>Résumé : </b> Après une brève analyse linguistique des adjectifs dénominaux en français, nous décrivons le processus automatique que nous avons mis en place à partir de lexiques et de corpus volumineux pour construire un lexique d’adjectifs dénominaux dérivés de manière régulière. Nous estimons à la fois la précision et la couverture du lexique dérivationnel obtenu. À terme, ce lexique librement disponible aura été validé manuellement et contiendra également les adjectifs dénominaux à base supplétive.
							</p>

							<p id="taln-2011-court-012-key" class="mots_cles">
							<b>Mots clés : </b> Adjectifs dénominaux, dérivation morphologique, lexique dérivationnel
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Sagot, Géraldine Walther, Pegah Faghiri, Pollet Samvelian</b>


						<br/>

							<i>Développement de ressources pour le persan : PerLex 2, nouveau lexique morphologique et MEltfa, étiqueteur morphosyntaxique</i> <br/>

						<a href="actes/taln-2011-court-013.pdf">taln-2011-court-013</a> 
						<a href="bibtex/taln-2011-court-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-013-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-013-key');">mots clés</a> <br/>

							<p id="taln-2011-court-013-abs" class="resume">
							<b>Résumé : </b> Nous présentons une nouvelle version de PerLex, lexique morphologique du persan, une version corrigée et partiellement réannotée du corpus étiqueté BijanKhan (BijanKhan, 2004) et MEltfa, un nouvel étiqueteur morphosyntaxique librement disponible pour le persan. Après avoir développé une première version de PerLex (Sagot &amp; Walther, 2010), nous en proposons donc ici une version améliorée. Outre une validation manuelle partielle, PerLex 2 repose désormais sur un inventaire de catégories linguistiquement motivé. Nous avons également développé une nouvelle version du corpus BijanKhan : elle contient des corrections significatives de la tokenisation ainsi qu&#39;un réétiquetage à l&#39;aide des nouvelles catégories. Cette nouvelle version du corpus a enfin été utilisée pour l&#39;entraînement de MEltfa, notre étiqueteur morphosyntaxique pour le persan librement disponible, s&#39;appuyant à la fois sur ce nouvel inventaire de catégories, sur PerLex 2 et sur le système d&#39;étiquetage MElt (Denis &amp; Sagot, 2009).
							</p>

							<p id="taln-2011-court-013-key" class="mots_cles">
							<b>Mots clés : </b> Ressource lexicale, validation, étiqueteur morphosyntaxique, persan, catégories, PerLex, MElt
							</p>

					</div>
					

					<div class="article">

						<b>Mirabela Navlea, Amalia Todiraşcu</b>


						<br/>

							<i>Identification de cognats à partir de corpus parallèles français-roumain</i> <br/>

						<a href="actes/taln-2011-court-014.pdf">taln-2011-court-014</a> 
						<a href="bibtex/taln-2011-court-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-014-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-014-key');">mots clés</a> <br/>

							<p id="taln-2011-court-014-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthode hybride d’identification de cognats français - roumain. Cette méthode exploite des corpus parallèles alignés au niveau propositionnel, lemmatisés et étiquetés (avec des propriétés morphosyntaxiques). Notre méthode combine des techniques statistiques et des informations linguistiques pour améliorer les résultats obtenus. Nous évaluons le module d’identification de cognats et nous faisons une comparaison avec des méthodes statistiques pures, afin d’étudier l’impact des informations linguistiques utilisées sur la qualité des résultats obtenus. Nous montrons que l’utilisation des informations linguistiques augmente significativement la performance de la méthode.
							</p>

							<p id="taln-2011-court-014-key" class="mots_cles">
							<b>Mots clés : </b> cognat, identification de cognats, corpus parallèles alignés au niveau propositionnel
							</p>

					</div>
					

					<div class="article">

						<b>Richard Beaufort, Sophie Roekhaut</b>


						<br/>

							<i>Le TAL au service de l’ALAO/ELAO L’exemple des exercices de dictée automatisés</i> <br/>

						<a href="actes/taln-2011-court-015.pdf">taln-2011-court-015</a> 
						<a href="bibtex/taln-2011-court-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-015-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-015-key');">mots clés</a> <br/>

							<p id="taln-2011-court-015-abs" class="resume">
							<b>Résumé : </b> Ce papier s’inscrit dans le cadre général de l’Apprentissage et de l’Enseignement des Langues Assistés par Ordinateur, et concerne plus particulièrement l’automatisation des exercices de dictée. Il présente une méthode de correction des copies d’apprenants qui se veut originale en deux points. Premièrement, la méthode exploite la composition d’automates à états finis pour détecter et pour analyser les erreurs. Deuxièmement, elle repose sur une analyse morphosyntaxique automatique de l’original de la dictée, ce qui facilite la production de diagnostics.
							</p>

							<p id="taln-2011-court-015-key" class="mots_cles">
							<b>Mots clés : </b> ALAO/ELAO, exercices de dictée, alignement, diagnostic, machines à états finis
							</p>

					</div>
					

					<div class="article">

						<b>Maxime Amblard, Michel Musiol, Manuel Rebuschi</b>


						<br/>

							<i>Une analyse basée sur la S-DRT pour la modélisation de dialogues pathologiques</i> <br/>

						<a href="actes/taln-2011-court-016.pdf">taln-2011-court-016</a> 
						<a href="bibtex/taln-2011-court-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-016-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-016-key');">mots clés</a> <br/>

							<p id="taln-2011-court-016-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons la définition et l’étude d’un corpus de dialogues entre un schizophrène et un interlocuteur ayant pour objectif la conduite et le maintien de l’échange. Nous avons identifié des discontinuités significatives chez les schizophrènes paranoïdes. Une représentation issue de la S-DRT (sa partie pragmatique) permet de rendre compte des ces usages non standards.
							</p>

							<p id="taln-2011-court-016-key" class="mots_cles">
							<b>Mots clés : </b> S-DRT, interaction verbale, schizophrénie, dialogue pathologique, incohérence pragmatique
							</p>

					</div>
					

					<div class="article">

						<b>Anne Göhring, Martin Volk</b>


						<br/>

							<i>The Text+Berg Corpus An Alpine French-German Parallel Resource</i> <br/>

						<a href="actes/taln-2011-court-017.pdf">taln-2011-court-017</a> 
						<a href="bibtex/taln-2011-court-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-017-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-017-key');">mots clés</a> <br/>

							<p id="taln-2011-court-017-abs" class="resume">
							<b>Résumé : </b> Cet article présente un corpus parallèle français-allemand de plus de 4 millions de mots issu de la numérisation d’un corpus alpin multilingue. Ce corpus est une précieuse ressource pour de nombreuses études de linguistique comparée et du patrimoine culturel ainsi que pour le développement d’un système statistique de traduction automatique dans un domaine spécifique. Nous avons annoté un échantillon de ce corpus parallèle et aligné les structures arborées au niveau des mots, des constituants et des phrases. Cet “alpine treebank” est le premier corpus arboré parallèle français-allemand de haute qualité (manuellement contrôlé), de libre accès et dans un domaine et un genre nouveau : le récit d’alpinisme.
							</p>

							<p id="taln-2011-court-017-key" class="mots_cles">
							<b>Mots clés : </b> corpus alpin français-allemand, structures arborées parallèles, annotation morphosyntaxique du français
							</p>

					</div>
					

					<div class="article">

						<b>Aurélien Bossard, Émilie Guimier De Neef</b>


						<br/>

							<i>Ordonner un résumé automatique multi-documents fondé sur une classification des phrases en classes lexicales</i> <br/>

						<a href="actes/taln-2011-court-018.pdf">taln-2011-court-018</a> 
						<a href="bibtex/taln-2011-court-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-018-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-018-key');">mots clés</a> <br/>

							<p id="taln-2011-court-018-abs" class="resume">
							<b>Résumé : </b> Nous présentons différentes méthodes de réordonnancement de phrases pour le résumé automatique fondé sur une classification des phrases à résumer en classes thématiques. Nous comparons ces méthodes à deux baselines : ordonnancement des phrases selon leur pertinence et ordonnancement selon la date et la position dans le document d’origine. Nous avons fait évaluer les résumés obtenus sur le corpus RPM2 par 4 annotateurs et présentons les résultats.
							</p>

							<p id="taln-2011-court-018-key" class="mots_cles">
							<b>Mots clés : </b> Résumé automatique, ordonnancement de phrases
							</p>

					</div>
					

					<div class="article">

						<b>Fériel Ben Fraj</b>


						<br/>

							<i>Construction d’une grammaire d’arbres adjoints pour la langue arabe</i> <br/>

						<a href="actes/taln-2011-court-019.pdf">taln-2011-court-019</a> 
						<a href="bibtex/taln-2011-court-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-019-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-019-key');">mots clés</a> <br/>

							<p id="taln-2011-court-019-abs" class="resume">
							<b>Résumé : </b> La langue arabe présente des spécificités qui la rendent plus ambigüe que d’autres langues naturelles. Sa morphologie, sa syntaxe ainsi que sa sémantique sont en corrélation et se complètent l’une l’autre. Dans le but de construire une grammaire qui soit adaptée à ces spécificités, nous avons conçu et développé une application d’aide à la création des règles syntaxiques licites suivant le formalisme d’arbres adjoints. Cette application est modulaire et enrichie par des astuces de contrôle de la création et aussi d’une interface conviviale pour assister l’utilisateur final dans la gestion des créations prévues.
							</p>

							<p id="taln-2011-court-019-key" class="mots_cles">
							<b>Mots clés : </b> Outil semi-automatique, grammaire d’arbres adjoints, langue arabe, traits d’unification
							</p>

					</div>
					

					<div class="article">

						<b>Enrique Henestroza Anguiano, Pascal Denis</b>


						<br/>

							<i>FreDist : Automatic construction of distributional thesauri for French</i> <br/>

						<a href="actes/taln-2011-court-020.pdf">taln-2011-court-020</a> 
						<a href="bibtex/taln-2011-court-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-020-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-020-key');">mots clés</a> <br/>

							<p id="taln-2011-court-020-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons FreDist, un logiciel libre pour la construction automatique de thésaurus distributionnels à partir de corpus de texte, ainsi qu’une évaluation des différents ressources ainsi produites. Suivant les travaux de (Lin, 1998) et (Curran, 2004), nous utilisons un corpus journalistique de grande taille et implémentons différentes options pour : le type de relation contexte lexical, la fonction de poids, et la fonction de mesure de similarité. Prenant l’EuroWordNet français et le WOLF comme références, notre évaluation révèle, de manière originale, que c’est l’approche qui combine contextes linéaires (ici, de type bigrammes) et contextes syntaxiques qui semble fournir le meilleur thésaurus. Enfin, nous espérons que notre logiciel, distribué avec nos meilleurs thésaurus pour le français, seront utiles à la communauté TAL.
							</p>

							<p id="taln-2011-court-020-key" class="mots_cles">
							<b>Mots clés : </b> thésaurus distributionnel, similarité sémantique, méthodes non supervisées, lexique
							</p>

					</div>
					

					<div class="article">

						<b>Ali Reza Ebadat, Vincent Claveau, Pascale Sébillot</b>


						<br/>

							<i>Using shallow linguistic features for relation extraction in bio-medical texts</i> <br/>

						<a href="actes/taln-2011-court-021.pdf">taln-2011-court-021</a> 
						<a href="bibtex/taln-2011-court-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-021-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-021-key');">mots clés</a> <br/>

							<p id="taln-2011-court-021-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous proposons de modéliser la tâche d’extraction de relations à partir de corpus textuels comme un problème de classification. Nous montrons que, dans ce cadre, des représentations fondées sur des informations linguistiques de surface sont suffisantes pour que des algorithmes d’apprentissage artificiel standards les exploitant rivalisent avec les meilleurs systèmes d’extraction de relations reposant sur des connaissances issues d’analyses profondes (analyses syntaxiques ou sémantiques). Nous montrons également qu’en prenant davantage en compte les spécificités de la tâche d’extraction à réaliser et des données disponibles, il est possible d’obtenir des méthodes encore plus efficaces tout en exploitant ces informations simples. La technique originale à base d’apprentissage « paresseux » et de modèles de langue que nous évaluons en extraction d’interactions géniques sur les données du challenge LLL2005 dépasse les résultats de l’état de l’art.
							</p>

							<p id="taln-2011-court-021-key" class="mots_cles">
							<b>Mots clés : </b> Extraction de relations, classification, apprentissage paresseux, modèle de langue, analyse linguistique de surface
							</p>

					</div>
					

					<div class="article">

						<b>Julien Lebranchu, Yann Mathet</b>


						<br/>

							<i>Vers une prise en charge approfondie des phénomènes itératifs par TimeML</i> <br/>

						<a href="actes/taln-2011-court-022.pdf">taln-2011-court-022</a> 
						<a href="bibtex/taln-2011-court-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-022-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-022-key');">mots clés</a> <br/>

							<p id="taln-2011-court-022-abs" class="resume">
							<b>Résumé : </b> Les travaux menés ces dernières années autour de l’itération en langue, tant par la communauté linguistique que par celle du TAL, ont mis au jour des phénomènes particuliers, non réductibles aux représentations temporelles classiques. En particulier, une itération ne saurait structurellement être réduite à une simple énumération de procès, et du point de vue de l’aspect, met en jeu simultanément deux visées aspectuelles indépendantes. Le formalisme TimeML, qui a vocation à annoter les informations temporelles portées par un texte, intègre déjà des éléments relatifs aux itérations, mais ne prend pas en compte ces dernières avancées. C’est ce que nous entreprenons de faire dans cet article, en proposant une extension à ce formalisme.
							</p>

							<p id="taln-2011-court-022-key" class="mots_cles">
							<b>Mots clés : </b> TimeML, discours, sémantique, phénomènes itératifs
							</p>

					</div>
					

					<div class="article">

						<b>Noémi Boubel, Yves Bestgen</b>


						<br/>

							<i>Une procédure pour identifier les modifieurs de la valence affective d&#39;un mot dans des textes</i> <br/>

						<a href="actes/taln-2011-court-023.pdf">taln-2011-court-023</a> 
						<a href="bibtex/taln-2011-court-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-023-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-023-key');">mots clés</a> <br/>

							<p id="taln-2011-court-023-abs" class="resume">
							<b>Résumé : </b> Cette recherche s&#39;inscrit dans le champ de la fouille d&#39;opinion et, plus particulièrement, dans celui de l&#39;analyse de la polarité d’une phrase ou d&#39;un syntagme. Dans ce cadre, la prise en compte du contexte linguistique dans lequel apparaissent les mots porteurs de valence est particulièrement importante. Nous proposons une méthodologie pour extraire automatiquement de corpus de textes de telles expressions linguistiques. Cette approche s&#39;appuie sur un corpus de textes, ou d&#39;extraits de textes, dont la valence est connue, sur un lexique de valence construit à partir de ce corpus au moyen d&#39;une procédure automatique et sur un analyseur syntaxique. Une étude exploratoire, limitée à la seule relation syntaxique associant un adverbe à un adjectif, laisse entrevoir les potentialités de l&#39;approche.
							</p>

							<p id="taln-2011-court-023-key" class="mots_cles">
							<b>Mots clés : </b> modifieurs de valence, fouille d’opinion, lexique de valence
							</p>

					</div>
					

					<div class="article">

						<b>Yann Mathet, Antoine Widlöcher</b>


						<br/>

							<i>Stratégie d’exploration de corpus multi-annotés avec GlozzQL</i> <br/>

						<a href="actes/taln-2011-court-024.pdf">taln-2011-court-024</a> 
						<a href="bibtex/taln-2011-court-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-024-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-024-key');">mots clés</a> <br/>

							<p id="taln-2011-court-024-abs" class="resume">
							<b>Résumé : </b> La multiplication des travaux sur corpus, en linguistique computationnelle et en TAL, conduit à la multiplication des campagnes d’annotation et des corpus multi-annotés, porteurs d’informations relatives à des phénomènes variés, envisagés par des annotateurs multiples, parfois automatiques. Pour mieux comprendre les phénomènes que ces campagnes prennent pour objets, ou pour contrôler les données en vue de l’établissement d’un corpus de référence, il est nécessaire de disposer d’outils permettant d’explorer les annotations. Nous présentons une stratégie possible et son opérationalisation dans la plate-forme Glozz par le langage GlozzQL.
							</p>

							<p id="taln-2011-court-024-key" class="mots_cles">
							<b>Mots clés : </b> Corpus, Annotation, Exploration, GlozzQL
							</p>

					</div>
					

					<div class="article">

						<b>Fadila Hadouche, Guy Lapalme, Marie-Claude L’Homme</b>


						<br/>

							<i>Attribution de rôles sémantiques aux actants des lexies verbales</i> <br/>

						<a href="actes/taln-2011-court-025.pdf">taln-2011-court-025</a> 
						<a href="bibtex/taln-2011-court-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-025-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-025-key');">mots clés</a> <br/>

							<p id="taln-2011-court-025-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous traitons de l’attribution des rôles sémantiques aux actants de lexies verbales en corpus spécialisé en français. Nous proposons une classification de rôles sémantiques par apprentissage machine basée sur un corpus de lexies verbales annotées manuellement du domaine de l’informatique et d’Internet. Nous proposons également une méthode de partitionnement semi-supervisé pour prendre en compte l’annotation de nouvelles lexies ou de nouveaux rôles sémantiques et de les intégrés dans le système. Cette méthode de partitionnement permet de regrouper les instances d’actants selon les valeurs communes correspondantes aux traits de description des actants dans des groupes d’instances d’actants similaires. La classification de rôles sémantique a obtenu une F-mesure de 93% pour Patient, de 90% pour Agent, de 85% pour Destination et de 76% pour les autres rôles pris ensemble. Quand au partitionnement en regroupant les instances selon leur similarité donne une F-mesure de 88% pour Patient, de 81% pour Agent, de 58% pour Destination et de 46% pour les autres rôles.
							</p>

							<p id="taln-2011-court-025-key" class="mots_cles">
							<b>Mots clés : </b> Rôles sémantiques, traits syntaxiques, classification, partitionnement semi-supervisé
							</p>

					</div>
					

					<div class="article">

						<b>Olivier Ferret</b>


						<br/>

							<i>Utiliser l’amorçage pour améliorer une mesure de similarité sémantique</i> <br/>

						<a href="actes/taln-2011-court-026.pdf">taln-2011-court-026</a> 
						<a href="bibtex/taln-2011-court-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-026-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-026-key');">mots clés</a> <br/>

							<p id="taln-2011-court-026-abs" class="resume">
							<b>Résumé : </b> Les travaux sur les mesures de similarité sémantique de nature distributionnelle ont abouti à un certain consensus quant à leurs performances et ont montré notamment que leurs résultats sont surtout intéressants pour des mots de forte fréquence et une similarité sémantique étendue, non restreinte aux seuls synonymes. Dans cet article, nous proposons une méthode d’amélioration d’une mesure de similarité classique permettant de rééquilibrer ses résultats pour les mots de plus faible fréquence. Cette méthode est fondée sur un mécanisme d’amorçage : un ensemble d’exemples et de contre-exemples de mots sémantiquement liés sont sélectionnés de façon non supervisée à partir des résultats de la mesure initiale et servent à l’entraînement d’un classifieur supervisé. Celui-ci est ensuite utilisé pour réordonner les voisins sémantiques initiaux. Nous évaluons l’intérêt de ce réordonnancement pour un large ensemble de noms anglais couvrant différents domaines fréquentiels.
							</p>

							<p id="taln-2011-court-026-key" class="mots_cles">
							<b>Mots clés : </b> Extraction de voisins sémantiques, similarité sémantique, méthodes distributionnelles
							</p>

					</div>
					

					<div class="article">

						<b>Richard Moot, Laurent Prévot, Christian Retoré</b>


						<br/>

							<i>Un calcul de termes typés pour la pragmatique lexicale: chemins et voyageurs fictifs dans un corpus de récits de voyage</i> <br/>

						<a href="actes/taln-2011-court-027.pdf">taln-2011-court-027</a> 
						<a href="bibtex/taln-2011-court-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-027-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-027-key');">mots clés</a> <br/>

							<p id="taln-2011-court-027-abs" class="resume">
							<b>Résumé : </b> Ce travail s’inscrit dans l’analyse automatique d’un corpus de récits de voyage. À cette fin, nous raffinons la sémantique de Montague pour rendre compte des phénomènes d’adaptation du sens des mots au contexte dans lequel ils apparaissent. Ici, nous modélisons les constructions de type ’le chemin descend pendant une demi-heure’ où ledit chemin introduit un voyageur fictif qui le parcourt, en étendant des idées que le dernier auteur a développé avec Bassac et Mery. Cette introduction du voyageur utilise la montée de type afin que le quantificateur introduisant le voyageur porte sur toute la phrase et que les propriétés du chemin ne deviennent pas des propriétés du voyageur, fût-il fictif. Cette analyse sémantique (ou plutôt sa traduction en lambda-DRT) est d’ores et déjà implantée pour une partie du lexique de Grail.
							</p>

							<p id="taln-2011-court-027-key" class="mots_cles">
							<b>Mots clés : </b> Sémantique lexicale, pragmatique, sémantique compositionnelle
							</p>

					</div>
					

					<div class="article">

						<b>Brigitte Bigi, Cristel Portes, Agnès Steuckardt, Marion Tellier</b>


						<br/>

							<i>Catégoriser les réponses aux interruptions dans les débats politiques</i> <br/>

						<a href="actes/taln-2011-court-028.pdf">taln-2011-court-028</a> 
						<a href="bibtex/taln-2011-court-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-028-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-028-key');">mots clés</a> <br/>

							<p id="taln-2011-court-028-abs" class="resume">
							<b>Résumé : </b> Cet article traite de l’analyse de débats politiques selon une orientation multimodale. Nous étudions plus particulièrement les réponses aux interruptions lors d’un débat à l’Assemblée nationale. Nous proposons de procéder à l’analyse via des annotations systématiques de différentes modalités. L’analyse argumentative nous a amenée à proposer une typologie de ces réponses. Celle-ci a été mise à l’épreuve d’une classification automatique. La difficulté dans la construction d’un tel système réside dans la nature même des données : multimodales, parfois manquantes et incertaines.
							</p>

							<p id="taln-2011-court-028-key" class="mots_cles">
							<b>Mots clés : </b> corpus, annotations, multimodalité, classification supervisée
							</p>

					</div>
					

					<div class="article">

						<b>Ludovic Bonnefoy, Patrice Bellot, Michel Benoit</b>


						<br/>

							<i>Mesure non-supervisée du degré d’appartenance d’une entité à un type</i> <br/>

						<a href="actes/taln-2011-court-029.pdf">taln-2011-court-029</a> 
						<a href="bibtex/taln-2011-court-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-029-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-029-key');">mots clés</a> <br/>

							<p id="taln-2011-court-029-abs" class="resume">
							<b>Résumé : </b> La recherche d’entités nommées a été le sujet de nombreux travaux. Cependant, la construction des ressources nécessaires à de tels systèmes reste un problème majeur. Dans ce papier, nous proposons une méthode complémentaire aux outils capables de reconnaître des entités de types larges, dont l’objectif est de déterminer si une entité est d’un type donné, et ce de manière non-supervisée et quel que soit le type. Nous proposons pour cela une approche basée sur la comparaison de modèles de langage estimés à partir du Web. L’intérêt de notre approche est validé par une évaluation sur 100 entités et 273 types différents.
							</p>

							<p id="taln-2011-court-029-key" class="mots_cles">
							<b>Mots clés : </b> typage d’entités nommées, comparaison de distribution de mots, divergence de Kullback-Leibler
							</p>

					</div>
					

					<div class="article">

						<b>Laurence Danlos, Charlotte Roze</b>


						<br/>

							<i>Traduction (automatique) des connecteurs de discours</i> <br/>

						<a href="actes/taln-2011-court-030.pdf">taln-2011-court-030</a> 
						<a href="bibtex/taln-2011-court-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-030-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-030-key');">mots clés</a> <br/>

							<p id="taln-2011-court-030-abs" class="resume">
							<b>Résumé : </b> En nous appuyant sur des données fournies par le concordancier bilingue TransSearch qui intègre un alignement statistique au niveau des mots, nous avons effectué une annotation semi-manuelle de la traduction anglaise de deux connecteurs du français. Les résultats de cette annotation montrent que les traductions de ces connecteurs ne correspondent pas aux « transpots » identifiés par TransSearch et encore moins à ce qui est proposé dans les dictionnaires bilingues.
							</p>

							<p id="taln-2011-court-030-key" class="mots_cles">
							<b>Mots clés : </b> Traduction (automatique), TransSearch, Discours
							</p>

					</div>
					

					<div class="article">

						<b>Bruno Cartoni, Louise Deléger</b>


						<br/>

							<i>Découverte de patrons paraphrastiques en corpus comparable: une approche basée sur les n-grammes</i> <br/>

						<a href="actes/taln-2011-court-031.pdf">taln-2011-court-031</a> 
						<a href="bibtex/taln-2011-court-031.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-031-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-031-key');">mots clés</a> <br/>

							<p id="taln-2011-court-031-abs" class="resume">
							<b>Résumé : </b> Cet article présente l’utilisation d’un corpus comparable pour l’extraction de patrons de paraphrases. Nous présentons une méthode empirique basée sur l’appariement de n-grammes, permettant d’extraire des patrons de paraphrases dans des corpus comparables d’une même langue (le français), du même domaine (la médecine) mais de registres de langues différents (spécialisé ou grand public). Cette méthode confirme les résultats précédents basés sur des méthodes à base de patrons, et permet d’identifier de nouveaux patrons, apportant également un regard nouveau sur les différences entre les discours de langue générale et spécialisée.
							</p>

							<p id="taln-2011-court-031-key" class="mots_cles">
							<b>Mots clés : </b> Identification de paraphrases, extraction de patrons, type de discours, domaine médical, corpus comparable monolingue
							</p>

					</div>
					

					<div class="article">

						<b>Alexis Kauffmann</b>


						<br/>

							<i>Prise en compte de la sous-catégorisation verbale dans un lexique bilingue anglais-japonais</i> <br/>

						<a href="actes/taln-2011-court-032.pdf">taln-2011-court-032</a> 
						<a href="bibtex/taln-2011-court-032.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-032-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-032-key');">mots clés</a> <br/>

							<p id="taln-2011-court-032-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une méthode de détection des correspondances bilingues de sous-catégorisation verbale à partir de données lexicales monolingues. Nous évoquons également la structure de ces lexiques et leur utilisation en traduction automatique (TA) à base linguistique anglais-japonais. Les lexiques sont utilisés par un programme de TA fonctionnant selon une architecture classique dite &#34;à transfert&#34;, et leur structure permet une classification précise des sous-catégorisations verbales. Nos travaux ont permis une amélioration des données de sous-catégorisation des lexiques pour les verbes japonais et leurs équivalents anglais, en utilisant des données linguistiques compilées à partir d&#39;un corpus de textes extrait du web. De plus, le fonctionnement du programme de TA a pu ^etre amélioré en utilisant ces données.
							</p>

							<p id="taln-2011-court-032-key" class="mots_cles">
							<b>Mots clés : </b> bases de données lexicales, sous-catégorisation verbale, traduction automatique à base linguistique, japonais
							</p>

					</div>
					

					<div class="article">

						<b>Yayoi Nakamura-Delloye</b>


						<br/>

							<i>Extraction non-supervisée de relations basée sur la dualité de la représentation</i> <br/>

						<a href="actes/taln-2011-court-033.pdf">taln-2011-court-033</a> 
						<a href="bibtex/taln-2011-court-033.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-033-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-033-key');">mots clés</a> <br/>

							<p id="taln-2011-court-033-abs" class="resume">
							<b>Résumé : </b> Nous proposons dans cet article une méthode non-supervisée d’extraction des relations entre entités nommées. La méthode proposée se caractérise par l’utilisation de résultats d’analyses syntaxiques, notamment les chemins syntaxiques reliant deux entités nommées dans des arbres de dépendance. Nous avons également exploité la dualité de la représentation des relations sémantiques et le résultat de notre expérience comparative a montré que cette approche améliorait les rappels.
							</p>

							<p id="taln-2011-court-033-key" class="mots_cles">
							<b>Mots clés : </b> Extraction des connaissances, relations entre entités nommées, dualité relationnelle
							</p>

					</div>
					

					<div class="article">

						<b>Corinna Anderson, Christophe Cerisara, Claire Gardent</b>


						<br/>

							<i>Vers la détection des dislocations à gauche dans les transcriptions automatiques du Français parlé</i> <br/>

						<a href="actes/taln-2011-court-034.pdf">taln-2011-court-034</a> 
						<a href="bibtex/taln-2011-court-034.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-034-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-034-key');">mots clés</a> <br/>

							<p id="taln-2011-court-034-abs" class="resume">
							<b>Résumé : </b> Ce travail prend place dans le cadre plus général du développement d’une plate-forme d’analyse syntaxique du français parlé. Nous décrivons la conception d’un modèle automatique pour résoudre le lien anaphorique présent dans les dislocations à gauche dans un corpus de français parlé radiophonique. La détection de ces structures devrait permettre à terme d’améliorer notre analyseur syntaxique en enrichissant les informations prises en compte dans nos modèles automatiques. La résolution du lien anaphorique est réalisée en deux étapes : un premier niveau à base de règles filtre les configurations candidates, et un second niveau s’appuie sur un modèle appris selon le critère du maximum d’entropie. Une évaluation expérimentale réalisée par validation croisée sur un corpus annoté manuellement donne une F-mesure de l’ordre de 40%.
							</p>

							<p id="taln-2011-court-034-key" class="mots_cles">
							<b>Mots clés : </b> Détection des dislocations à gauche, Maximum Entropy, français parlé
							</p>

					</div>
					

					<div class="article">

						<b>Nabil Hathout, Fiammetta Namer</b>


						<br/>

							<i>Règles et paradigmes en morphologie informatique lexématique</i> <br/>

						<a href="actes/taln-2011-court-035.pdf">taln-2011-court-035</a> 
						<a href="bibtex/taln-2011-court-035.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-035-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-035-key');">mots clés</a> <br/>

							<p id="taln-2011-court-035-abs" class="resume">
							<b>Résumé : </b> Les familles de mots produites par deux analyseurs morphologiques, DériF (basé sur des règles) et Morphonette (basé sur l&#39;analogie), appliqués à un même corpus lexical, sont comparées. Cette comparaison conduit à l&#39;examen de trois sous-ensembles : 
			- un sous-ensemble commun aux deux systèmes dont la taille montre que, malgré leurs différences, les approches expérimentées par chaque système sont valides et décrivent en partie la même réalité morphologique.
			- un sous-ensemble propre à DériF et un autre à Morphonette. Ces ensembles (a) nous renseignent sur les caractéristiques propres à chaque système, et notamment sur ce que l&#39;autre ne peut pas produire, (b) ils mettent en évidence les erreurs d’un système, en ce qu’elles n’apparaissent pas dans l’autre, (c) ils font apparaître certaines limites de la description, notamment celles qui sont liées aux objets et aux notions théoriques comme les familles morphologiques, les bases, l&#39;existence de RCL « transversales » entre les lexèmes qui n&#39;ont pas de relation d&#39;ascendance ou de descendance.
							</p>

							<p id="taln-2011-court-035-key" class="mots_cles">
							<b>Mots clés : </b> morphologie constructionnelle, analyse automatique, règles, analogie, familles morphologiques, comparaison, synergie
							</p>

					</div>
					

					<div class="article">

						<b>Andrea Gesmundo</b>


						<br/>

							<i>Bidirectional Sequence Classification for Tagging Tasks with Guided Learning</i> <br/>

						<a href="actes/taln-2011-court-036.pdf">taln-2011-court-036</a> 
						<a href="bibtex/taln-2011-court-036.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-036-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-036-key');">mots clés</a> <br/>

							<p id="taln-2011-court-036-abs" class="resume">
							<b>Résumé : </b> Dans cet article nous présentons une série d’adaptations de l’algorithme du &#34;cadre d’apprenstissage guidé&#34; pour résoudre différentes tâches d’étiquetage. La spécificité du système proposé réside dans sa capacité à apprendre l’ordre de l’inférence avec les paramètres du classifieur local au lieu de la forcer dans un ordre pré-défini (de gauche à droite). L’algorithme d’entraînement est basé sur l’algorithme du &#34;perceptron&#34;. Nous appliquons le système à différents types de tâches d’étiquetage pour atteindre des résultats au niveau de l’état de l’art en un court temps d’exécution.
							</p>

							<p id="taln-2011-court-036-key" class="mots_cles">
							<b>Mots clés : </b> Bidirectionnel, Classification de Séquence, Apprentissage Guidé
							</p>

					</div>
					

					<div class="article">

						<b>Dominique Legallois, Peggy Cellier, Thierry Charnois</b>


						<br/>

							<i>Calcul de réseaux phrastiques pour l’analyse et la navigation textuelle</i> <br/>

						<a href="actes/taln-2011-court-037.pdf">taln-2011-court-037</a> 
						<a href="bibtex/taln-2011-court-037.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-037-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-037-key');">mots clés</a> <br/>

							<p id="taln-2011-court-037-abs" class="resume">
							<b>Résumé : </b> Le travail présente une méthode de navigation dans les textes, fondée sur la répétition lexicale. La méthode choisie est celle développée par le linguiste Hoey. Son application manuelle à des textes de grandeur conséquente est problématique. Nous proposons dans cet article un processus automatique qui permet d’analyser selon cette méthode des textes de grande taille ; des expériences ont été menées appliquant le processus à différents types de textes (narratif, expositif) et montrant l’intérêt de l’approche.
							</p>

							<p id="taln-2011-court-037-key" class="mots_cles">
							<b>Mots clés : </b> Réseau phrastique, Appariement de phrases, Analyse textuelle, Navigation textuelle
							</p>

					</div>
					

					<div class="article">

						<b>Achille Falaise, Agnès Tutin, Olivier Kraif</b>


						<br/>

							<i>Exploitation d&#39;un corpus arboré pour non spécialistes par des requêtes guidées et des requêtes sémantiques</i> <br/>

						<a href="actes/taln-2011-court-038.pdf">taln-2011-court-038</a> 
						<a href="bibtex/taln-2011-court-038.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-038-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-038-key');">mots clés</a> <br/>

							<p id="taln-2011-court-038-abs" class="resume">
							<b>Résumé : </b> L&#39;exploitation de corpus analysés syntaxiquement (ou corpus arborés) pour le public non spécialiste n&#39;est pas un problème trivial. Si la communauté du TAL souhaite mettre à la disposition des chercheurs non-informaticiens des corpus comportant des annotations linguistiques complexes, elle doit impérativement développer des interfaces simples à manipuler mais permettant des recherches fines. Dans cette communication, nous présentons les modes de recherche « grand public » développé(e)s dans le cadre du projet Scientext, qui met à disposition un corpus d&#39;écrits scientifiques interrogeable par partie textuelle, par partie du discours et par fonction syntaxique. Les modes simples sont décrits : un mode libre et guidé, où l&#39;utilisateur sélectionne lui-même les éléments de la requête, et un mode sémantique, qui comporte des grammaires locales préétablies à l&#39;aide des fonctions syntaxiques.
							</p>

							<p id="taln-2011-court-038-key" class="mots_cles">
							<b>Mots clés : </b> environnement d&#39;étude de corpus, corpus étiquetés et arborés, création de grammaires assistée, visualisation d&#39;information linguistique
							</p>

					</div>
					

					<div class="article">

						<b>Mohammad Daoud, Christian Boitet</b>


						<br/>

							<i>Communautés Internet comme sources de préterminologie</i> <br/>

						<a href="actes/taln-2011-court-039.pdf">taln-2011-court-039</a> 
						<a href="bibtex/taln-2011-court-039.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-039-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-039-key');">mots clés</a> <br/>

							<p id="taln-2011-court-039-abs" class="resume">
							<b>Résumé : </b> Cet article décrit deux expériences sur la construction de ressources terminologiques multilingues (preterminologies) préliminaires, mais grandes, grâce à des communautés Internet, et s&#39;appuie sur ces expériences pour cibler des données terminologiques plus raffinées venant de communautés Internet et d&#39;applications Web 2.0. La première expérience est une passerelle de contribution pour le site Web de la Route de la Soie numérique (DSR). Les visiteurs contribuent en effet à un référentiel lexical multilingue dédié, pendant qu&#39;ils visitent et lisent les livres archivés, parce qu&#39;ils sont intéressés par le domaine et ont tendance à être polygottes. Nous avons recueilli 1400 contributions lexicales en 4 mois. La seconde expérience est basée sur le JeuxDeMots arabe, où les joueurs en ligne contribuent à un réseau lexical arabe. L&#39;expérience a entraîné une croissance régulière du nombre de joueurs et de contributions, ces dernières contenant des termes absents et des mots de dialectes oraux.
							</p>

							<p id="taln-2011-court-039-key" class="mots_cles">
							<b>Mots clés : </b> terminologie, préterminologie, approches collaboratives, réseaux lexicaux, DSR, jeux sérieux
							</p>

					</div>
					

					<div class="article">

						<b>Wigdan Mekki, Julien Gosme, Fathi Debili, Yves Lepage, Nadine Lucas</b>


						<br/>

							<i>Évaluation de G-LexAr pour la traduction automatique statistique</i> <br/>

						<a href="actes/taln-2011-court-040.pdf">taln-2011-court-040</a> 
						<a href="bibtex/taln-2011-court-040.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-040-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-040-key');">mots clés</a> <br/>

							<p id="taln-2011-court-040-abs" class="resume">
							<b>Résumé : </b> G-LexAr est un analyseur morphologique de l’arabe qui a récemment reçu des améliorations substantielles. Cet article propose une évaluation de cet analyseur en tant qu’outil de pré-traitement pour la traduction automatique statistique, ce dont il n’a encore jamais fait l’objet. Nous étudions l’impact des différentes formes proposées par son analyse (voyellation, lemmatisation et segmentation) sur un système de traduction arabe-anglais, ainsi que l’impact de la combinaison de ces formes. Nos expériences montrent que l’utilisation séparée de chacune de ces formes n’a que peu d’influence sur la qualité des traductions obtenues, tandis que leur combinaison y contribue de façon très bénéfique.
							</p>

							<p id="taln-2011-court-040-key" class="mots_cles">
							<b>Mots clés : </b> traduction automatique statistique, analyse morphologique, pré-traitement de l’arabe
							</p>

					</div>
					

					<div class="article">

						<b>Marion laignelet, Mouna Kamel, Nathalie Aussenac-Gilles</b>


						<br/>

							<i>Enrichir la notion de patron par la prise en compte de la structure textuelle - Application à la construction d’ontologie</i> <br/>

						<a href="actes/taln-2011-court-041.pdf">taln-2011-court-041</a> 
						<a href="bibtex/taln-2011-court-041.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-041-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-041-key');">mots clés</a> <br/>

							<p id="taln-2011-court-041-abs" class="resume">
							<b>Résumé : </b> La projection de patrons lexico-syntaxiques sur corpus est une des manières privilégiées pour identifier des relations sémantiques précises entre éléments lexicaux. Dans cet article, nous proposons d’étendre la notion de patron en prenant en compte la sémantique que véhiculent les éléments de structure d’un document (définitions, titres, énumérations) dans l’identification de relations. Nous avons testé cette hypothèse dans le cadre de la construction d’ontologies à partir de textes fortement structurés du domaine de la cartographie.
							</p>

							<p id="taln-2011-court-041-key" class="mots_cles">
							<b>Mots clés : </b> Construction d’ontologie, patron lexico-syntaxique, structure textuelle
							</p>

					</div>
					

					<div class="article">

						<b>Lorenza Russo, Éric Wehrli</b>


						<br/>

							<i>La traduction automatique des séquences clitiques dans un traducteur à base de règles</i> <br/>

						<a href="actes/taln-2011-court-042.pdf">taln-2011-court-042</a> 
						<a href="bibtex/taln-2011-court-042.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-042-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-042-key');">mots clés</a> <br/>

							<p id="taln-2011-court-042-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous discutons la méthodologie utilisée par Its-2, un système de traduction à base de règles, pour la traduction des pronoms clitiques. En particulier, nous nous focalisons sur les séquences clitiques, pour la traduction automatique entre le français et l’anglais. Une évaluation basée sur un corpus de phrases construites montre le potentiel de notre approche pour des traductions de bonne qualité.
							</p>

							<p id="taln-2011-court-042-key" class="mots_cles">
							<b>Mots clés : </b> Analyseur syntaxique, traduction automatique, pronom clitique, séquences clitiques
							</p>

					</div>
					

					<div class="article">

						<b>Lorenza Russo, Yves Scherrer, Jean-Philippe Goldman, Sharid Loáiciga, Luka Nerima, Éric Wehrli</b>


						<br/>

							<i>Étude inter-langues de la distribution et des ambiguïtés syntaxiques des pronoms</i> <br/>

						<a href="actes/taln-2011-court-043.pdf">taln-2011-court-043</a> 
						<a href="bibtex/taln-2011-court-043.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-043-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-043-key');">mots clés</a> <br/>

							<p id="taln-2011-court-043-abs" class="resume">
							<b>Résumé : </b> Ce travail décrit la distribution des pronoms selon le style de texte (littéraire ou journalistique) et selon la langue (français, anglais, allemand et italien). Sur la base d’un étiquetage morpho-syntaxique effectué automatiquement puis vérifié manuellement, nous pouvons constater que la proportion des différents types de pronoms varie selon le type de texte et selon la langue. Nous discutons les catégories les plus ambiguës de manière détaillée. Comme nous avons utilisé l’analyseur syntaxique Fips pour l’étiquetage des pronoms, nous l’avons également évalué et obtenu une précision moyenne de plus de 95%.
							</p>

							<p id="taln-2011-court-043-key" class="mots_cles">
							<b>Mots clés : </b> Pronoms, ambiguïté pronominale, étiquetage morpho-syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>Yves Scherrer, Lorenza Russo, Jean-Philippe Goldman, Sharid Loáiciga, Luka Nerima, Éric Wehrli</b>


						<br/>

							<i>La traduction automatique des pronoms. Problèmes et perspectives</i> <br/>

						<a href="actes/taln-2011-court-044.pdf">taln-2011-court-044</a> 
						<a href="bibtex/taln-2011-court-044.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-044-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-044-key');">mots clés</a> <br/>

							<p id="taln-2011-court-044-abs" class="resume">
							<b>Résumé : </b> Dans cette étude, notre système de traduction automatique, Its-2, a fait l’objet d’une évaluation manuelle de la traduction des pronoms pour cinq paires de langues et sur deux corpus : un corpus littéraire et un corpus de communiqués de presse. Les résultats montrent que les pourcentages d’erreurs peuvent atteindre 60% selon la paire de langues et le corpus. Nous discutons ainsi deux pistes de recherche pour l’amélioration des performances de Its-2 : la résolution des ambiguïtés d’analyse et la résolution des anaphores pronominales.
							</p>

							<p id="taln-2011-court-044-key" class="mots_cles">
							<b>Mots clés : </b> Pronoms, traduction automatique, analyse syntaxique, anaphores pronominales
							</p>

					</div>
					

					<div class="article">

						<b>Daniel Kayser</b>


						<br/>

							<i>Ressources lexicales pour une sémantique inférentielle : un exemple, le mot « quitter »</i> <br/>

						<a href="actes/taln-2011-court-045.pdf">taln-2011-court-045</a> 
						<a href="bibtex/taln-2011-court-045.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-045-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-045-key');">mots clés</a> <br/>

							<p id="taln-2011-court-045-abs" class="resume">
							<b>Résumé : </b> On étudie environ 500 occurrences du verbe « quitter » en les classant selon les inférences qu’elles suggèrent au lecteur. On obtient ainsi 43 « schémas inférentiels ». Ils ne s’excluent pas l’un l’autre : si plusieurs d’entre eux s’appliquent, les inférences produites se cumulent ; cependant, comme l’auteur sait que le lecteur dispose de tels schémas, s’il veut l’orienter vers une seule interprétation, il fournit des indices permettant d’éliminer les autres. On conjecture que ces schémas présentent des régularités observables sur des familles de mots, que ces régularités proviennent du fonctionnement d’opérations génériques, et qu’il est donc sans gravité de ne pas être exhaustif, dans la mesure où ces opérations permettent d’engendrer les schémas manquants en cas de besoin.
							</p>

							<p id="taln-2011-court-045-key" class="mots_cles">
							<b>Mots clés : </b> Sémantique lexicale, Inférence, Glissements de sens
							</p>

					</div>
					

					<div class="article">

						<b>Caroline Brun</b>


						<br/>

							<i>Un système de détection d’opinions fondé sur l’analyse syntaxique profonde</i> <br/>

						<a href="actes/taln-2011-court-046.pdf">taln-2011-court-046</a> 
						<a href="bibtex/taln-2011-court-046.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-046-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-046-key');">mots clés</a> <br/>

							<p id="taln-2011-court-046-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons un système de détection d’opinions construit à partir des sorties d’un analyseur syntaxique robuste produisant des analyses profondes. L’objectif de ce système est l’extraction d’opinions associées à des produits (les concepts principaux) ainsi qu’aux concepts qui leurs sont associés (en anglais «features-based opinion extraction»). Suite à une étude d’un corpus cible, notre analyseur syntaxique est enrichi par l’ajout de polarité aux éléments pertinents du lexique et par le développement de règles génériques et spécialisées permettant l’extraction de relations sémantiques d’opinions, qui visent à alimenter un modèle de représentation des opinions. Une première évaluation montre des résultats très encourageants, mais de nombreuses perspectives restent à explorer.
							</p>

							<p id="taln-2011-court-046-key" class="mots_cles">
							<b>Mots clés : </b> détection d’opinions, analyse de sentiments, analyse syntaxique robuste, extraction d’information
							</p>

					</div>
					

					<div class="article">

						<b>Caroline Hagège, Denys Proux, Quentin Gicquel, Stefan Darmoni, Suzanne Pereira, Frédérique Segond, Marie-Helène Metzger</b>


						<br/>

							<i>Développement d’un système de détection des infections associées aux soins à partir de l’analyse de comptes-rendus d’hospitalisation</i> <br/>

						<a href="actes/taln-2011-court-047.pdf">taln-2011-court-047</a> 
						<a href="bibtex/taln-2011-court-047.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-court-047-abs');">résumé</a>
							<a onclick="toggle('taln-2011-court-047-key');">mots clés</a> <br/>

							<p id="taln-2011-court-047-abs" class="resume">
							<b>Résumé : </b> Cet article décrit la première version et les résultats de l’évaluation d’un système de détection des épisodes d’infections associées aux soins. Cette détection est basée sur l’analyse automatique de comptes-rendus d’hospitalisation provenant de différents hôpitaux et différents services. Ces comptes-rendus sont sous forme de texte libre. Le système de détection a été développé à partir d’un analyseur linguistique que nous avons adapté au domaine médical et extrait à partir des documents des indices pouvant conduire à une suspicion d’infection. Un traitement de la négation et un traitement temporel des textes sont effectués permettant de restreindre et de raffiner l’extraction d’indices. Nous décrivons dans cet article le système que nous avons développé et donnons les résultats d’une évaluation préliminaire.
							</p>

							<p id="taln-2011-court-047-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’information médicale, compte-rendus d’hospitalisation, infection nosocomiale, analyse syntaxique
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="démonstration">Démonstrations</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Richard Beaufort, Sophie Roekhaut</b>


						<br/>

							<i>PLATON, Plateforme d’apprentissage et d’enseignement de l’orthographe sur le Net</i> <br/>

						<a href="actes/taln-2011-demo-001.pdf">taln-2011-demo-001</a> 
						<a href="bibtex/taln-2011-demo-001.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Annelies Braffort, Laurence Bolot</b>


						<br/>

							<i>SpatiAnn, un outil pour annoter l’utilisation de l’espace dans les corpus vidéo</i> <br/>

						<a href="actes/taln-2011-demo-002.pdf">taln-2011-demo-002</a> 
						<a href="bibtex/taln-2011-demo-002.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>François Brown de Colstoun, Estelle Delpech, Etienne Monneret</b>


						<br/>

							<i>Libellex : une plateforme multiservices pour la gestion des contenus multilingues</i> <br/>

						<a href="actes/taln-2011-demo-003.pdf">taln-2011-demo-003</a> 
						<a href="bibtex/taln-2011-demo-003.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Jacques Chauché</b>


						<br/>

							<i>Une application de la grammaire structurelle: L’analyseur syntaxique du français SYGFRAN</i> <br/>

						<a href="actes/taln-2011-demo-004.pdf">taln-2011-demo-004</a> 
						<a href="bibtex/taln-2011-demo-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-demo-004-abs');">résumé</a>
							<a onclick="toggle('taln-2011-demo-004-key');">mots clés</a> <br/>

							<p id="taln-2011-demo-004-abs" class="resume">
							<b>Résumé : </b> La démonstration présentée produit une analyse syntaxique du français. Elle est écrite en SYGMART, fournie avec les actes, exécutable à l’adresse : http ://www.lirmm.fr/ chauche/ExempleAnl.html et téléchargeable à l’adresse : http ://www.sygtext.fr.
							</p>

							<p id="taln-2011-demo-004-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>François-Régis Chaumartin</b>


						<br/>

							<i>Proxem Ubiq : une solution d’e-réputation par analyse de feedbacks clients</i> <br/>

						<a href="actes/taln-2011-demo-005.pdf">taln-2011-demo-005</a> 
						<a href="bibtex/taln-2011-demo-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-demo-005-key');">mots clés</a> <br/>


							<p id="taln-2011-demo-005-key" class="mots_cles">
							<b>Mots clés : </b> e-réputation, reconnaissance d’entités nommées, classification, clustering, analyse syntaxique, apprentissage
							</p>

					</div>
					

					<div class="article">

						<b>Béatrice Daille, Christine Jacquin, Laura Monceaux, Emmanuel Morin, Jérome Rocheteau</b>


						<br/>

							<i>TTC TermSuite : une chaîne de traitement pour la fouille terminologique multilingue</i> <br/>

						<a href="actes/taln-2011-demo-006.pdf">taln-2011-demo-006</a> 
						<a href="bibtex/taln-2011-demo-006.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Rodolfo Delmonte, Vincenzo Pallotta, Violeta Seretan, Lammert Vrieling, David Walker</b>


						<br/>

							<i>An Interaction Mining Suite Based On Natural Language Understanding</i> <br/>

						<a href="actes/taln-2011-demo-007.pdf">taln-2011-demo-007</a> 
						<a href="bibtex/taln-2011-demo-007.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>François-Xavier Desmarais, Éric Charton</b>


						<br/>

							<i>Démonstration de l&#39;API de NLGbAse</i> <br/>

						<a href="actes/taln-2011-demo-008.pdf">taln-2011-demo-008</a> 
						<a href="bibtex/taln-2011-demo-008.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Michel Généreux</b>


						<br/>

							<i>Système d’analyse de la polarité de dépêches financières</i> <br/>

						<a href="actes/taln-2011-demo-009.pdf">taln-2011-demo-009</a> 
						<a href="bibtex/taln-2011-demo-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2011-demo-009-abs');">résumé</a>
							<a onclick="toggle('taln-2011-demo-009-key');">mots clés</a> <br/>

							<p id="taln-2011-demo-009-abs" class="resume">
							<b>Résumé : </b> Nous présentons un système pour la classification en continu de dépêches financières selon une polarité positive ou négative. La démonstration permettra ainsi d’observer quelles sont les dépêches les plus à même de faire varier la valeur d’actions cotées en bourse, au moment même de la démonstration. Le système traitera de dépêches écrites en anglais et en français.
							</p>

							<p id="taln-2011-demo-009-key" class="mots_cles">
							<b>Mots clés : </b> Analyse de Sentiments, Linguistique de Corpus, Dépêches Financières
							</p>

					</div>
					

					<div class="article">

						<b>Laurence Longo, Amalia Todirascu</b>


						<br/>

							<i>RefGen, outil d’identification automatique des chaînes de référence en français</i> <br/>

						<a href="actes/taln-2011-demo-010.pdf">taln-2011-demo-010</a> 
						<a href="bibtex/taln-2011-demo-010.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Clément de Groc, Javier Couto, Helena Blancafort, Claude de Loupy</b>


						<br/>

							<i>Babouk – exploration orientée du web pour la constitution de corpus et de terminologies</i> <br/>

						<a href="actes/taln-2011-demo-011.pdf">taln-2011-demo-011</a> 
						<a href="bibtex/taln-2011-demo-011.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Cyril Grouin, Louise Deléger, Anne-Lyse Minard, Anne-Laure Ligozat, Asma Ben Abacha, Delphine Bernhard, Bruno Cartoni, Brigitte Grau, Sophie Rosset, Pierre Zweigenbaum</b>


						<br/>

							<i>Extraction d’informations médicales au LIMSI</i> <br/>

						<a href="actes/taln-2011-demo-012.pdf">taln-2011-demo-012</a> 
						<a href="bibtex/taln-2011-demo-012.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Juyeon Kang, Jean-Pierre Desclés</b>


						<br/>

							<i>Système d’analyse catégorielle ACCG : adéquation au traitement de problèmes syntaxiques complexes</i> <br/>

						<a href="actes/taln-2011-demo-013.pdf">taln-2011-demo-013</a> 
						<a href="bibtex/taln-2011-demo-013.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Jimmy Ma, Mickaël Mounier, Helena Blancafort, Javier Couto, Claude de Loupy</b>


						<br/>

							<i>LOL : Langage objet dédié à la programmation linguistique</i> <br/>

						<a href="actes/taln-2011-demo-014.pdf">taln-2011-demo-014</a> 
						<a href="bibtex/taln-2011-demo-014.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Yann Mathet, Antoine Widlöcher</b>


						<br/>

							<i>Aligner : un outil d’alignement et de mesure d’accord inter-annotateurs</i> <br/>

						<a href="actes/taln-2011-demo-015.pdf">taln-2011-demo-015</a> 
						<a href="bibtex/taln-2011-demo-015.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Yann Mathet, Antoine Widlöcher</b>


						<br/>

							<i>GlozzQL : un langage de requêtes incrémental pour les textes annotés</i> <br/>

						<a href="actes/taln-2011-demo-016.pdf">taln-2011-demo-016</a> 
						<a href="bibtex/taln-2011-demo-016.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Frédéric Meunier, Laurence Danlos, Vanessa Combet</b>


						<br/>

							<i>EASYTEXT : un système opérationnel de génération de textes</i> <br/>

						<a href="actes/taln-2011-demo-017.pdf">taln-2011-demo-017</a> 
						<a href="bibtex/taln-2011-demo-017.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Yoann Moreau, Eric SanJuan, Patrice Bellot</b>


						<br/>

							<i>Restad : un logiciel d’indexation et de stockage relationnel de contenus XML</i> <br/>

						<a href="actes/taln-2011-demo-018.pdf">taln-2011-demo-018</a> 
						<a href="bibtex/taln-2011-demo-018.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Gaëlle Recourcé</b>


						<br/>

							<i>Une chaîne d’analyse des e-mails pour l’aide à la gestion de sa messagerie</i> <br/>

						<a href="actes/taln-2011-demo-019.pdf">taln-2011-demo-019</a> 
						<a href="bibtex/taln-2011-demo-019.bib">bibtex</a> 



					</div>
					

					<div class="article">

						<b>Jean Rohmer</b>


						<br/>

							<i>Démonstration d’un outil de « Calcul Littéraire »</i> <br/>

						<a href="actes/taln-2011-demo-020.pdf">taln-2011-demo-020</a> 
						<a href="bibtex/taln-2011-demo-020.bib">bibtex</a> 



					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>