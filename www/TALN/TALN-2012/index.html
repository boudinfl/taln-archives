<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>TALN'2012</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>TALN'2012, 19e conférence sur le Traitement Automatique des Langues Naturelles</h1>
				<h2>Grenoble (France), du 2012-06-04 au 2012-06-08</h2>
				<p>Président(s) : Georges Antoniadis, Hervé Blanchon</p>
				<p>Taux d'acceptation :
							papiers longs (38.7%)
							papiers courts (47.5%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#long">Papiers longs</a></li>
				<li><a href="#court">Papiers courts</a></li>
				<li><a href="#démonstration">Démonstrations</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="long">Papiers longs</h1>
			

					<div class="article">

						<b>Anne-Lyse Minard, Anne-Laure Ligozat, Brigitte Grau</b>


						<br/>

							<i>Simplification de phrases pour l&#39;extraction de relations</i> <br/>

						<a href="actes/taln-2012-long-001.pdf">taln-2012-long-001</a> 
						<a href="bibtex/taln-2012-long-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-001-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-001-key');">mots clés</a> <br/>

							<p id="taln-2012-long-001-abs" class="resume">
							<b>Résumé : </b> L’extraction de relations par apprentissage nécessite un corpus annoté de très grande taille pour couvrir toutes les variations d’expressions des relations. Pour contrer ce problème, nous proposons une méthode de simplification de phrases qui permet de réduire la variabilité syntaxique des relations. Elle nécessite l’annotation d’un petit corpus qui sera par la suite augmenté automatiquement. La première étape est l’annotation des simplifications grâce à un classifieur à base de CRF, puis l’extraction des relations, et ensuite une complétion automatique du corpus d’entraînement des simplifications grâce aux résultats de l’extraction des relations. Les premiers résultats que nous avons obtenus pour la tâche d’extraction de relations d’i2b2 2010 sont très encourageants.
							</p>

							<p id="taln-2012-long-001-key" class="mots_cles">
							<b>Mots clés : </b> Extraction de relations, simplification de phrases, apprentissage automatique
							</p>

					</div>
					

					<div class="article">

						<b>Asma Ben Abacha, Pierre Zweigenbaum, Aurélien Max</b>


						<br/>

							<i>Extraction d&#39;information automatique en domaine médical par projection inter-langue : vers un passage à l&#39;échelle</i> <br/>

						<a href="actes/taln-2012-long-002.pdf">taln-2012-long-002</a> 
						<a href="bibtex/taln-2012-long-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-002-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-002-key');">mots clés</a> <br/>

							<p id="taln-2012-long-002-abs" class="resume">
							<b>Résumé : </b> Cette recherche est issue de notre volonté de tester de nouvelles méthodes automatiques d’annotation ou d’extraction d’information à partir d’une langue L1 en exploitant des ressources et des outils disponibles pour une autre langue L2. Cette approche repose sur le passage par un corpus parallèle (L1-L2) aligné au niveau des phrases et des mots. Pour faire face au manque de corpus médicaux français annotés, nous nous intéressons au couple de langues (françaisanglais) dans le but d’annoter automatiquement des textes médicaux en français. En particulier, nous nous intéressons dans cet article à la reconnaissance des entités médicales. Nous évaluons dans un premier temps notre méthode de reconnaissance d’entités médicales sur le corpus anglais. Dans un second temps, nous évaluons la reconnaissance des entités médicales du corpus français par projection des annotations du corpus anglais. Nous abordons également le problème de l’hétérogénéité des données en exploitant un corpus extrait du Web et nous proposons une méthode statistique pour y pallier.
							</p>

							<p id="taln-2012-long-002-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’information, projection d’annotation, reconnaissance des entités médicales, apprentissage
							</p>

					</div>
					

					<div class="article">

						<b>Ludovic Jean-Louis, Romaric Besançon, Olivier Ferret</b>


						<br/>

							<i>Une méthode d&#39;extraction d&#39;information fondée sur les graphes pour le remplissage de formulaires</i> <br/>

						<a href="actes/taln-2012-long-003.pdf">taln-2012-long-003</a> 
						<a href="bibtex/taln-2012-long-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-003-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-003-key');">mots clés</a> <br/>

							<p id="taln-2012-long-003-abs" class="resume">
							<b>Résumé : </b> Dans les systèmes d’extraction d’information sur des événements, une tâche importante est le remplissage automatique de formulaires regroupant les informations sur un événement donné à partir d’un texte non structuré. Ce remplissage de formulaire peut s’avérer difficile lorsque l’information est dispersée dans tout le texte et mélangée à des éléments d’information liés à un autre événement similaire. Nous proposons dans cet article une approche en deux étapes pour ce problème : d’abord une segmentation du texte en événements pour sélectionner les phrases relatives au même événement ; puis une méthode de sélection dans les phrases sélectionnées des entités liées à l’événement. Une évaluation de cette approche sur un corpus annoté de dépêches dans le domaine des événements sismiques montre un F-score de 72% pour la tâche de remplissage de formulaires.
							</p>

							<p id="taln-2012-long-003-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’information, segmentation de texte, remplissage de formulaires
							</p>

					</div>
					

					<div class="article">

						<b>Anaïs Lefeuvre, Richard Moot, Christian Retoré, Noémie-Fleur Sandillon-Rezer</b>


						<br/>

							<i>Traitement automatique sur corpus de récits de voyages pyrénéens : Une analyse syntaxique, sémantique et temporelle</i> <br/>

						<a href="actes/taln-2012-long-004.pdf">taln-2012-long-004</a> 
						<a href="bibtex/taln-2012-long-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-004-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-004-key');">mots clés</a> <br/>

							<p id="taln-2012-long-004-abs" class="resume">
							<b>Résumé : </b> Cet article présente notre utilisation de la théorie des types dans laquelle nous nous situons pour l’analyse syntaxique, sémantique et pour la construction du lexique. Notre outil, Grail permet de traiter le discours automatiquement à partir du texte brut et nous le testons sur un corpus de récit de voyages pyrénéens, Ititpy. Nous expliquons donc notre usage des grammaires catégorielles et plus particulièrement du calcul de Lambek et la correspondance entre ces catégories et le lambda-calcul simplement typé dans le cadre de la DRT. Une flexibilité du typage doit être autorisée dans certains cas et bloquée dans d’autres. Quelques phénomènes linguistiques participant à une forme de glissement de sens provocant des conflits de types sont présentés. Nous expliquons ensuite nos motivations d’ordre pragmatique à utiliser un système à sortes et types variables en sémantique lexicale puis notre traitement compositionnel du temps des évènements inspiré du Binary Tense de (Verkuyl, 2008).
							</p>

							<p id="taln-2012-long-004-key" class="mots_cles">
							<b>Mots clés : </b> compositionalité, interface syntaxe-sémantique, interface sémantique-pragmatique, grammaire catégorielle, théorie des types, récit de voyage
							</p>

					</div>
					

					<div class="article">

						<b>Matthieu Constant, Anthony Sigogne, Patrick Watrin</b>

						- <span class="important">Prix du Meilleur Papier</span>

						<br/>

							<i>La reconnaissance des mots composés à l&#39;épreuve de l&#39;analyse syntaxique et vice-versa : évaluation de deux stratégies discriminantes</i> <br/>

						<a href="actes/taln-2012-long-005.pdf">taln-2012-long-005</a> 
						<a href="bibtex/taln-2012-long-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-005-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-005-key');">mots clés</a> <br/>

							<p id="taln-2012-long-005-abs" class="resume">
							<b>Résumé : </b> Nous proposons deux stratégies discriminantes d’intégration des mots composés dans un processus réel d’analyse syntaxique : (i) pré-segmentation lexicale avant analyse, (ii) post-segmentation lexicale après analyse au moyen d’un réordonnanceur. Le segmenteur de l’approche (i) se fonde sur un modèle CRF et permet d’obtenir un reconnaisseur de mots composés état-de-l’art. Le réordonnanceur de l’approche (ii) repose sur un modèle MaxEnt intégrant des traits dédiés aux mots composés. Nous montrons que les deux approches permettent de combler jusqu’à 18% de l’écart entre un analyseur baseline et un analyseur avec segmentation parfaite et jusqu’à 25% pour la reconnaissance des mots composés.
							</p>

							<p id="taln-2012-long-005-key" class="mots_cles">
							<b>Mots clés : </b> Mots composés, analyse syntaxique, champs markoviens aléatoires, réordonnanceur
							</p>

					</div>
					

					<div class="article">

						<b>Ramadan Alfared, Denis Bechet, Alexander Dikovsky</b>


						<br/>

							<i>Calcul des cadres de sous catégorisation des noms déverbaux français (le cas du génitif)</i> <br/>

						<a href="actes/taln-2012-long-006.pdf">taln-2012-long-006</a> 
						<a href="bibtex/taln-2012-long-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-006-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-006-key');">mots clés</a> <br/>

							<p id="taln-2012-long-006-abs" class="resume">
							<b>Résumé : </b> L’analyse syntaxique fine en dépendances nécessite la connaissance des cadres de souscatégorisation des unités lexicales. Le cas des verbes étant bien étudié, nous nous intéressons dans cet article au cas des noms communs dérivés de verbes. Notre intérêt principal est de calculer le cadre de sous-catégorisation des noms déverbaux à partir de celui du verbe d’origine pour le français. Or, pour ce faire il faut disposer d’une liste représentative de noms déverbaux français. Pour calculer cette liste nous utilisons un algorithme simplifié de repérage des noms déverbaux, l’appliquons à un corpus et comparons la liste obtenue avec la liste Verbaction des déverbaux exprimant l’action ou l’activité du verbe. Pour les noms déverbaux ainsi obtenus et attestés ensuite par une expertise linguistique, nous analysons la provenance des groupes prépositionnels subordonnés des déverbaux dans des contextes différents en tenant compte du verbe d’origine. L’analyse est effectuée sur le corpus Paris 7 et est limitée au cas le plus fréquent du génitif, c’est-à-dire des groupes prépositionnels introduits par de, des, etc.
							</p>

							<p id="taln-2012-long-006-key" class="mots_cles">
							<b>Mots clés : </b> nom déverbal, cadre de sous-catégorisation, groupe prépositionnel, analyse en dépendances
							</p>

					</div>
					

					<div class="article">

						<b>Vincent Claveau</b>


						<br/>

							<i>Vectorisation, Okapi et calcul de similarité pour le TAL : pour oublier enfin le TF-IDF</i> <br/>

						<a href="actes/taln-2012-long-007.pdf">taln-2012-long-007</a> 
						<a href="bibtex/taln-2012-long-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-007-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-007-key');">mots clés</a> <br/>

							<p id="taln-2012-long-007-abs" class="resume">
							<b>Résumé : </b> Dans cette prise de position, nous nous intéressons au calcul de similarité (ou distances) entre textes, problématique présente dans de nombreuses tâches de TAL. Nous nous efforçons de montrer que ce qui n’est souvent qu’un composant dans des systèmes plus complexes est parfois négligé et des solutions sous-optimales sont employées. Ainsi, le calcul de similarité par TF-IDF/cosinus est souvent présenté comme « état-de-l’art », alors que des alternatives souvent plus performantes sont employées couramment dans le domaine de la Recherche d’Information (RI). Au travers de quelques expériences concernant plusieurs tâches, nous montrons combien ce simple calcul de similarité peut influencer les performances d’un système. Nous considérons plus particulièrement deux alternatives. La première est le schéma de pondération Okapi-BM25, bien connu en RI et directement interchangeable avec le TF-IDF. L’autre, la vectorisation, est une technique de calcul de similarité que nous avons développée et qui offrent d’intéressantes propriétés.
							</p>

							<p id="taln-2012-long-007-key" class="mots_cles">
							<b>Mots clés : </b> Calcul de similarité, modèle vectoriel, TF-IDF, Okapi BM-25, vectorisation
							</p>

					</div>
					

					<div class="article">

						<b>Christophe Benzitoun, Karën Fort, Benoît Sagot</b>


						<br/>

							<i>TCOF-POS : un corpus libre de français parlé annoté en morphosyntaxe</i> <br/>

						<a href="actes/taln-2012-long-008.pdf">taln-2012-long-008</a> 
						<a href="bibtex/taln-2012-long-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-008-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-008-key');">mots clés</a> <br/>

							<p id="taln-2012-long-008-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article un travail portant sur la création d’un corpus de français parlé spontané annoté en morphosyntaxe. Nous détaillons la méthodologie suivie afin d’assurer le contrôle de la qualité de la ressource finale. Ce corpus est d’ores et déjà librement diffusé pour la recherche et peut servir aussi bien de corpus d’apprentissage pour des logiciels que de base pour des descriptions linguistiques. Nous présentons également les résultats obtenus par deux étiqueteurs morphosyntaxiques entrainés sur ce corpus.
							</p>

							<p id="taln-2012-long-008-key" class="mots_cles">
							<b>Mots clés : </b> Etiquetage morpho-syntaxique, français parlé, ressources langagières
							</p>

					</div>
					

					<div class="article">

						<b>Adrien Lardilleux, François Yvon, Yves Lepage</b>


						<br/>

							<i>Alignement sous-phrastique hiérarchique avec Anymalign</i> <br/>

						<a href="actes/taln-2012-long-009.pdf">taln-2012-long-009</a> 
						<a href="bibtex/taln-2012-long-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-009-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-009-key');">mots clés</a> <br/>

							<p id="taln-2012-long-009-abs" class="resume">
							<b>Résumé : </b> Nous présentons un algorithme d’alignement sous-phrastique permettant d’aligner très facilement un couple de phrases à partir d’une matrice d’alignement pré-remplie. Cet algorithme s’inspire de travaux antérieurs sur l’alignement par segmentation binaire récursive ainsi que de travaux sur le clustering de documents. Nous évaluons les alignements produits sur des tâches de traduction automatique et montrons qu’il est possible d’atteindre des résultats du niveau de l’état de l’art, affichant des gains très conséquents allant jusqu’à plus de 4 points BLEU par rapport à nos travaux antérieurs, à l’aide une méthode très simple, indépendante de la taille du corpus à traiter, et produisant directement des alignements symétriques. En utilisant cette méthode en tant qu’extension à l’outil d’extraction de traductions Anymalign, nos expériences nous permettent de cerner certaines limitations de ce dernier et de définir des pistes pour son amélioration.
							</p>

							<p id="taln-2012-long-009-key" class="mots_cles">
							<b>Mots clés : </b> corpus parallèle, alignement sous-phrastique, traduction automatique statistique
							</p>

					</div>
					

					<div class="article">

						<b>Saadane Houda, Semmar Nasredine</b>


						<br/>

							<i>Utilisation de la translittération arabe pour l’amélioration de l’alignement de mots à partir de corpus parallèles français-arabe</i> <br/>

						<a href="actes/taln-2012-long-010.pdf">taln-2012-long-010</a> 
						<a href="bibtex/taln-2012-long-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-010-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-010-key');">mots clés</a> <br/>

							<p id="taln-2012-long-010-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous nous intéressons à l’utilisation de la translittération arabe pour l’amélioration des résultats d’une approche linguistique d’alignement de mots simples et composés à partir de corpus de textes parallèles français-arabe. Cette approche utilise, d’une part, un lexique bilingue et les caractéristiques linguistiques des entités nommées et des cognats pour l’alignement de mots simples, et d’autre part, les relations de dépendance syntaxique pour aligner les mots composés. Nous avons évalué l’aligneur de mots simples et composés intégrant la translittération arabe en utilisant deux procédés : une évaluation de la qualité d’alignement à l’aide d’un alignement de référence construit manuellement et une évaluation de l’impact de cet alignement sur la qualité de la traduction en faisant appel au système de traduction automatique statistique Moses. Les résultats obtenus montrent que la translittération améliore aussi bien la qualité de l’alignement que celle de la traduction.
							</p>

							<p id="taln-2012-long-010-key" class="mots_cles">
							<b>Mots clés : </b> Translittération, alignement de mots, construction de dictionnaires multilingues, traduction automatique, recherche d’information interlingue
							</p>

					</div>
					

					<div class="article">

						<b>Emmanuel Morin, Béatrice Daille</b>


						<br/>

							<i>Compositionnalité et contextes issus de corpus comparables pour la traduction terminologique</i> <br/>

						<a href="actes/taln-2012-long-011.pdf">taln-2012-long-011</a> 
						<a href="bibtex/taln-2012-long-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-011-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-011-key');">mots clés</a> <br/>

							<p id="taln-2012-long-011-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous cherchons à mettre en correspondance de traduction des termes extraits de chaque partie monolingue d’un corpus comparable. Notre objectif concerne l’identification et la traduction de termes spécialisés. Pour ce faire, nous mettons en oeuvre une approche compositionnelle dopée avec des informations contextuelles issues du corpus comparable. Notre évaluation montre que cette approche améliore significativement l’approche compositionnelle de base pour la traduction de termes complexes extraits de corpus comparables.
							</p>

							<p id="taln-2012-long-011-key" class="mots_cles">
							<b>Mots clés : </b> Corpus comparable, compositionnalité, information contextuelle, lexique bilingue
							</p>

					</div>
					

					<div class="article">

						<b>Paul Bédaride</b>


						<br/>

							<i>Raffinement du Lexique des Verbes Français</i> <br/>

						<a href="actes/taln-2012-long-012.pdf">taln-2012-long-012</a> 
						<a href="bibtex/taln-2012-long-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-012-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-012-key');">mots clés</a> <br/>

							<p id="taln-2012-long-012-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article les améliorations apportées à la ressource « Les Verbes Français » afin de la rendre plus formelle et utilisable pour le traitement automatique des langues naturelles. Les informations syntaxiques et sémantiques ont été corrigées, restructurées, unifiées puis intégrées à la version XML de cette ressource, afin de pouvoir être utilisée par un système d’étiquetage de rôles sémantiques.
							</p>

							<p id="taln-2012-long-012-key" class="mots_cles">
							<b>Mots clés : </b> ressource, lexique, verbes, raffinement, étiquetage de rôles sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>François Morlane-Hondère, Cécile Fabre</b>


						<br/>

							<i>Étude des manifestations de la relation de méronymie dans une ressource distributionnelle</i> <br/>

						<a href="actes/taln-2012-long-013.pdf">taln-2012-long-013</a> 
						<a href="bibtex/taln-2012-long-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-013-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-013-key');">mots clés</a> <br/>

							<p id="taln-2012-long-013-abs" class="resume">
							<b>Résumé : </b> Cette étude vise à étudier les manifestations de la relation de méronymie dans une ressource lexicale générée automatiquement à partir d’un corpus de langue générale. La démarche que nous adoptons consiste à recueillir un jeu de couples de méronymes issus d’une ressource externe que nous croisons avec une base distributionnelle calculée à partir d’un corpus de textes encyclopédiques. Une annotation sémantique des mots qui entrent dans ces couples de méronymes montre que la prise en compte de la nature sémantique des mots composant les couples de méronymes permet de mettre au jour des inégalités au niveau du repérage de la relation par la méthode d’analyse distributionnelle.
							</p>

							<p id="taln-2012-long-013-key" class="mots_cles">
							<b>Mots clés : </b> analyse distributionnelle, sémantique lexicale, méronymie, évaluation
							</p>

					</div>
					

					<div class="article">

						<b>Clément de Groc, Xavier Tannier, Claude de Loupy</b>


						<br/>

							<i>Un critère de cohésion thématique fondé sur un graphe de cooccurrences</i> <br/>

						<a href="actes/taln-2012-long-014.pdf">taln-2012-long-014</a> 
						<a href="bibtex/taln-2012-long-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-014-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-014-key');">mots clés</a> <br/>

							<p id="taln-2012-long-014-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous définissons un nouveau critère de cohésion thématique permettant de pondérer les termes d’un lexique thématique en fonction de leur pertinence. Le critère s’inspire des approches Web as corpus pour accumuler des connaissances exogènes sur un lexique. Ces connaissances sont ensuite modélisées sous forme de graphe et un algorithme de marche aléatoire est appliqué pour attribuer un score à chaque terme. Après avoir étudié les performances et la stabilité du critère proposé, nous l’évaluons sur une tâche d’aide à la création de lexiques bilingues.
							</p>

							<p id="taln-2012-long-014-key" class="mots_cles">
							<b>Mots clés : </b> Cohésion thématique, graphe de cooccurrences, marche aléatoire
							</p>

					</div>
					

					<div class="article">

						<b>Houda Bouamor, Aurélien Max, Gabriel Illouz, Anne Vilnat</b>


						<br/>

							<i>Validation sur le Web de reformulations locales: application à la Wikipédia</i> <br/>

						<a href="actes/taln-2012-long-015.pdf">taln-2012-long-015</a> 
						<a href="bibtex/taln-2012-long-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-015-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-015-key');">mots clés</a> <br/>

							<p id="taln-2012-long-015-abs" class="resume">
							<b>Résumé : </b> Ce travail présente des expériences initiales en validation de paraphrases en contexte. Les révisions de Wikipédia nous servent de domaine d’évaluation : pour un énoncé ayant connu une courte révision dans l’encyclopédie, nous disposons d’un ensemble de réécritures possibles, parmi lesquelles nous cherchons à identifier celles qui correspondent à des paraphrases valides. Nous abordons ce problème comme une tâche de classification fondée sur des informations issues du Web, et parvenons à améliorer la performance de plusieurs techniques simples de référence.
							</p>

							<p id="taln-2012-long-015-key" class="mots_cles">
							<b>Mots clés : </b> paraphrase, Wikipédia, aide à la rédaction
							</p>

					</div>
					

					<div class="article">

						<b>Laetitia Brouwers, Delphine Bernhard, Anne-Laure Ligozat, Thomas François</b>


						<br/>

							<i>Simplification syntaxique de phrases pour le français</i> <br/>

						<a href="actes/taln-2012-long-016.pdf">taln-2012-long-016</a> 
						<a href="bibtex/taln-2012-long-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-016-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-016-key');">mots clés</a> <br/>

							<p id="taln-2012-long-016-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthode de simplification syntaxique de textes français. La simplification syntaxique a pour but de rendre des textes plus abordables en simplifiant les éléments qui posent problème à la lecture. La méthode mise en place à cette fin s’appuie tout d’abord sur une étude de corpus visant à étudier les phénomènes linguistiques impliqués dans la simplification de textes en français. Nous avons ainsi constitué un corpus parallèle à partir d’articles de Wikipédia et Vikidia, ce qui a permis d’établir une typologie de simplifications. Dans un second temps, nous avons implémenté un système qui opère des simplifications syntaxiques à partir de ces observations. Des règles de simplification ont été décrites afin de générer des phrases simplifiées. Un module sélectionne ensuite le meilleur ensemble de phrases. Enfin, nous avons mené une évaluation de notre système montrant qu’environ 80% des phrases générées sont correctes.
							</p>

							<p id="taln-2012-long-016-key" class="mots_cles">
							<b>Mots clés : </b> simplification automatique, lisibilité, analyse syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>Iskandar Keskes, Mohamed Mahdi Boudabous, Mohamed Hédi Maâloul, Lamia Hadrich Belguith</b>


						<br/>

							<i>Étude comparative entre trois approches de résumé automatique de documents arabes</i> <br/>

						<a href="actes/taln-2012-long-017.pdf">taln-2012-long-017</a> 
						<a href="bibtex/taln-2012-long-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-017-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-017-key');">mots clés</a> <br/>

							<p id="taln-2012-long-017-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous proposons une étude comparative entre trois approches pour le résumé automatique de documents arabes. Ainsi, nous avons proposé trois méthodes pour l’extraction des phrases les plus représentatives d&#39;un document. La première méthode se base sur une approche symbolique, la deuxième repose sur une approche numérique et la troisième se base sur une approche hybride. Ces méthodes sont implémentées respectivement par le système ARSTResume, le système R.I.A et le système HybridResume. Nous présentons, par la suite, les résultats obtenus par les trois systèmes et nous procédons à une étude comparative entre les résultats obtenus afin de souligner les avantages et les limites de chaque méthode. Les résultats de l’évaluation ont montré que l‘approche numérique est plus performante que l’approche symbolique au niveau des textes longs. Mais, l’intégration de ces deux approches en une approche hybride aboutit aux résultats les plus performants dans notre corpus de textes.
							</p>

							<p id="taln-2012-long-017-key" class="mots_cles">
							<b>Mots clés : </b> Résumé automatique, approche symbolique, approche numérique, approche hybride, document arabe
							</p>

					</div>
					

					<div class="article">

						<b>Ann Bertels, Dirk De Hertog, Kris Heylen</b>


						<br/>

							<i>Etude sémantique des mots-clés et des marqueurs lexicaux stables dans un corpus technique</i> <br/>

						<a href="actes/taln-2012-long-018.pdf">taln-2012-long-018</a> 
						<a href="bibtex/taln-2012-long-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-018-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-018-key');">mots clés</a> <br/>

							<p id="taln-2012-long-018-abs" class="resume">
							<b>Résumé : </b> Cet article présente les résultats d’une analyse sémantique quantitative des unités lexicales spécifiques dans un corpus technique, relevant du domaine des machines-outils pour l’usinage des métaux. L’étude vise à vérifier si et dans quelle mesure les mots-clés du corpus technique sont monosémiques. A cet effet, nous procédons à une analyse statistique de régression simple, qui permet d’étudier la corrélation entre le rang de spécificité des mots-clés et leur rang de monosémie, mais qui soulève des problèmes statistiques et méthodologiques, notamment un biais de fréquence. Pour y remédier, nous adoptons une approche alternative pour le repérage des unités lexicales spécifiques, à savoir l’analyse des marqueurs lexicaux stables ou Stable Lexical Marker Analysis (SLMA). Nous discutons les résultats quantitatifs et statistiques de cette approche dans la perspective de la corrélation entre le rang de spécificité et le rang de monosémie.
							</p>

							<p id="taln-2012-long-018-key" class="mots_cles">
							<b>Mots clés : </b> unités lexicales spécifiques, analyse des mots-clés, analyse des marqueurs lexicaux stables, sémantique quantitative, analyse de régression
							</p>

					</div>
					

					<div class="article">

						<b>Solen Quiniou, Peggy Cellier, Thierry Charnois, Dominique Legallois</b>


						<br/>

							<i>Fouille de graphes sous contraintes linguistiques pour l&#39;exploration de grands textes</i> <br/>

						<a href="actes/taln-2012-long-019.pdf">taln-2012-long-019</a> 
						<a href="bibtex/taln-2012-long-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-019-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-019-key');">mots clés</a> <br/>

							<p id="taln-2012-long-019-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous proposons une approche pour explorer des textes de taille importante en mettant en évidence des sous-parties cohérentes. Cette méthode d’exploration s’appuie sur une représentation en graphe du texte, en utilisant le modèle linguistique de Hoey pour sélectionner et apparier les phrases dans le graphe. Notre contribution porte sur l’utilisation de techniques de fouille de graphes sous contraintes pour extraire des sous-parties pertinentes du texte (c’est-à-dire des collections de sous-réseaux phrastiques homogènes). Nous avons réalisé des expérimentations sur deux textes anglais de taille conséquente pour montrer l’intérêt de l’approche que nous proposons.
							</p>

							<p id="taln-2012-long-019-key" class="mots_cles">
							<b>Mots clés : </b> Fouille de graphes, réseaux phrastiques, analyse textuelle, navigation textuelle
							</p>

					</div>
					

					<div class="article">

						<b>Houda Bouamor, Aurélien Max, Anne Vilnat</b>


						<br/>

							<i>Une étude en 3D de la paraphrase: types de corpus, langues et techniques</i> <br/>

						<a href="actes/taln-2012-long-020.pdf">taln-2012-long-020</a> 
						<a href="bibtex/taln-2012-long-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-020-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-020-key');">mots clés</a> <br/>

							<p id="taln-2012-long-020-abs" class="resume">
							<b>Résumé : </b> Cet article présente une étude détaillée de l’impact du type du corpus sur la tâche d’acquisition de paraphrases sous-phrastiques. Nos expériences sont menées sur deux langues et quatre types de corpus, et incluent une combinaison efficace de quatre systèmes d’acquisition de paraphrases. Nous obtenons une amélioration relative de plus de 27% en F-mesure par rapport au meilleur système, en anglais et en français, ainsi qu’une amélioration relative à notre combinaison de systèmes de 22% pour l’anglais et de 5% pour le français quand tous les types de corpus sont utilisés pour l’acquisition depuis le type de corpus le plus couramment disponible.
							</p>

							<p id="taln-2012-long-020-key" class="mots_cles">
							<b>Mots clés : </b> acquisition de paraphrases, constitution de corpus
							</p>

					</div>
					

					<div class="article">

						<b>Florian Boudin, Nicolas Hernandez</b>


						<br/>

							<i>Détection et correction automatique d&#39;erreurs d&#39;annotation morpho-syntaxique du French TreeBank</i> <br/>

						<a href="actes/taln-2012-long-021.pdf">taln-2012-long-021</a> 
						<a href="bibtex/taln-2012-long-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-021-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-021-key');">mots clés</a> <br/>

							<p id="taln-2012-long-021-abs" class="resume">
							<b>Résumé : </b> La qualité de l’annotation morpho-syntaxique d’un corpus est déterminante pour l’entraînement et l’évaluation de méthodes d’étiquetage. Cet article présente une série d’expériences que nous avons menée sur la détection et la correction automatique des erreurs du French Treebank. Deux méthodes sont utilisées. La première consiste à identifier les mots sans étiquette et leur attribuer celle d’une forme correspondante observée dans le corpus. La seconde méthode utilise les variations de n-gramme pour détecter et corriger les anomalies d’annotation. L’évaluation des corrections apportées au corpus est réalisée de manière extrinsèque en comparant les scores de performance de différentes méthodes d’étiquetage morpho-syntaxique en fonction du niveau de correction. Les résultats montrent une amélioration significative de la précision et indiquent que la qualité du corpus peut être sensiblement améliorée par l’application de méthodes de correction automatique des erreurs d’annotation.
							</p>

							<p id="taln-2012-long-021-key" class="mots_cles">
							<b>Mots clés : </b> Étiquetage morpho-syntaxique, correction automatique, qualité d’annotation
							</p>

					</div>
					

					<div class="article">

						<b>Bruno Guillaume, Guy Perrier</b>


						<br/>

							<i>Annotation sémantique du French Treebank à l’aide de la réécriture modulaire de graphes</i> <br/>

						<a href="actes/taln-2012-long-022.pdf">taln-2012-long-022</a> 
						<a href="bibtex/taln-2012-long-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-022-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-022-key');">mots clés</a> <br/>

							<p id="taln-2012-long-022-abs" class="resume">
							<b>Résumé : </b> Nous proposons d’annoter le French Treebank à l’aide de dépendances sémantiques dans le cadre de la DMRS en partant d’une annotation en dépendances syntaxiques de surface et en utilisant la réécriture modulaire de graphes. L’article présente un certain nombre d’avancées concernant le calcul de réécriture utilisé : l’utilisation de règles pour faire le lien avec des lexiques, en particulier le lexique des verbes de Dicovalence, et l’introduction de filtres pour écarter à certaines étapes les annotations incohérentes. Il présente aussi des avancées dans le système de réécriture lui-même, qui a une plus large couverture (constructions causatives, verbes à montée, . . .) et dont l’ordre des modules a été étudié de façon plus systématique. Ce système a été expérimenté sur l’ensemble du French Treebank à l’aide du prototype GREW, qui implémente le calcul de réécriture utilisé.
							</p>

							<p id="taln-2012-long-022-key" class="mots_cles">
							<b>Mots clés : </b> réécriture de graphes, interface syntaxe-sémantique, dépendances, DMRS
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Blache, Stéphane Rauzy</b>


						<br/>

							<i>Enrichissement du FTB : un treebank hybride constituants/propriétés</i> <br/>

						<a href="actes/taln-2012-long-023.pdf">taln-2012-long-023</a> 
						<a href="bibtex/taln-2012-long-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-023-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-023-key');">mots clés</a> <br/>

							<p id="taln-2012-long-023-abs" class="resume">
							<b>Résumé : </b> Cet article présente les mécanismes de création d’un treebank hybride enrichissant le FTB à l’aide d’annotations dans le formalisme des Grammaires de Propriétés. Ce processus consiste à acquérir une grammaire GP à partir du treebank source et générer automatiquement les structures syntaxiques dans le formalisme cible en s’appuyant sur la spécification d’un schéma d’encodage adapté. Le résultat produit, en partant d’une version du FTB corrigée et modifiée en fonction de nos besoins, constitue une ressource ouvrant de nouvelles perspectives pour le traitement et la description du français.
							</p>

							<p id="taln-2012-long-023-key" class="mots_cles">
							<b>Mots clés : </b> Treebank hybride, French Treebank, Grammaires de Propriétés
							</p>

					</div>
					

					<div class="article">

						<b>Marie Candito, Djamé Seddah</b>


						<br/>

							<i>Le corpus Sequoia : annotation syntaxique et exploitation pour l&#39;adaptation d&#39;analyseur par pont lexical</i> <br/>

						<a href="actes/taln-2012-long-024.pdf">taln-2012-long-024</a> 
						<a href="bibtex/taln-2012-long-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-long-024-abs');">résumé</a>
							<a onclick="toggle('taln-2012-long-024-key');">mots clés</a> <br/>

							<p id="taln-2012-long-024-abs" class="resume">
							<b>Résumé : </b> Nous présentons dans cet article la méthodologie de constitution et les caractéristiques du corpus Sequoia, un corpus en français, syntaxiquement annoté d’après un schéma d’annotation très proche de celui du French Treebank (Abeillé et Barrier, 2004), et librement disponible, en constituants et en dépendances. Le corpus comporte des phrases de quatre origines : Europarl français, le journal l’Est Républicain, Wikipédia Fr et des documents de l’Agence Européenne du Médicament, pour un total de 3204 phrases et 69246 tokens. En outre, nous présentons une application de ce corpus : l’évaluation d’une technique d’adaptation d’analyseurs syntaxiques probabilistes à des domaines et/ou genres autres que ceux du corpus sur lequel ces analyseurs sont entraînés. Cette technique utilise des clusters de mots obtenus d’abord par regroupement morphologique à l’aide d’un lexique, puis par regroupement non supervisé, et permet une nette amélioration de l’analyse des domaines cibles (le corpus Sequoia), tout en préservant le même niveau de performance sur le domaine source (le FTB), ce qui fournit un analyseur multi-domaines, à la différence d’autres techniques d’adaptation comme le self-training.
							</p>

							<p id="taln-2012-long-024-key" class="mots_cles">
							<b>Mots clés : </b> Corpus arboré, analyse syntaxique statistique, adaptation de domaine
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="court">Papiers courts</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Francis Brunet-Manquat, Jérôme Goulian</b>


						<br/>

							<i>ACOLAD Plateforme pour l’édition collaborative dépendancielle</i> <br/>

						<a href="actes/taln-2012-court-001.pdf">taln-2012-court-001</a> 
						<a href="bibtex/taln-2012-court-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-001-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-001-key');">mots clés</a> <br/>

							<p id="taln-2012-court-001-abs" class="resume">
							<b>Résumé : </b> Cet article présente une plateforme open-source pour l’édition collaborative de corpus de dépendances. Cette plateforme, nommée ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de Dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks). Dans cet article, nous présentons la plateforme ACOLAD, puis nous détaillons la représentation pivot utilisée pour gérer les annotations concurrentes, enfin décrivons le mécanisme d’importation de ressources linguistiques externes.
							</p>

							<p id="taln-2012-court-001-key" class="mots_cles">
							<b>Mots clés : </b> annotation collaborative de corpus, annotations concurrentes, dépendances
							</p>

					</div>
					

					<div class="article">

						<b>Anaïs Cadilhac, Farah Benamara, Vladimir Popescu, Nicholas Asher, Mohamadou Seck</b>


						<br/>

							<i>Extraction de préférences à partir de dialogues de négociation</i> <br/>

						<a href="actes/taln-2012-court-002.pdf">taln-2012-court-002</a> 
						<a href="bibtex/taln-2012-court-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-002-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-002-key');">mots clés</a> <br/>

							<p id="taln-2012-court-002-abs" class="resume">
							<b>Résumé : </b> Cet article présente une approche linguistique pour l’extraction d’expressions de préférence à partir de dialogues de négociation. Nous proposons un nouveau schéma d’annotation pour encoder les préférences et les dépendances exprimées linguistiquement dans deux genres de corpus différents. Ensuite, nous proposons une méthode d’apprentissage qui extrait les expressions de préférence en utilisant une combinaison de traits locaux et discursifs. Finalement, nous évaluons la fiabilité de notre approche sur chaque genre de corpus.
							</p>

							<p id="taln-2012-court-002-key" class="mots_cles">
							<b>Mots clés : </b> Préférence, dialogue, apprentissage automatique
							</p>

					</div>
					

					<div class="article">

						<b>Alexandre Denis, Matthieu Quignard, Dominique Freard, Francoise Detienne, Michael Baker, Flore Barcellini</b>


						<br/>

							<i>Détection de conflits dans les communautés épistémiques en ligne</i> <br/>

						<a href="actes/taln-2012-court-003.pdf">taln-2012-court-003</a> 
						<a href="bibtex/taln-2012-court-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-003-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-003-key');">mots clés</a> <br/>

							<p id="taln-2012-court-003-abs" class="resume">
							<b>Résumé : </b> La présence de conflits dans les communautés épistémiques en ligne peut s’avérer bloquante pour l’activité de conception. Nous présentons une étude sur la détection automatique de conflit dans les discussions entre contributeurs Wikipedia qui s’appuie sur des traits de surface tels que la subjectivité ou la connotation des énoncés et évaluons deux règles de décision : l’une découle d’un modèle dialectique en exploitant localement la structure linéaire de la discussion, la subjectivité et la connotation ; l’autre, plus globale, ne s’appuie que sur la taille des fils et les marques de subjectivité au détriment des marques de connotation. Nous montrons que ces deux règles produisent des résultats similaires mais que la simplicité de la règle globale en fait une approche préférée dans la détection des conflits.
							</p>

							<p id="taln-2012-court-003-key" class="mots_cles">
							<b>Mots clés : </b> wikipedia, conflit, syntaxe, sémantique, interaction
							</p>

					</div>
					

					<div class="article">

						<b>Camille Dutrey, Chloé Clavel, Sophie Rosset, Ioana Vasilescu, Martine Adda-Decker</b>


						<br/>

							<i>Quel est l&#39;apport de la détection d&#39;entités nommées pour l&#39;extraction d&#39;information en domaine restreint ?</i> <br/>

						<a href="actes/taln-2012-court-004.pdf">taln-2012-court-004</a> 
						<a href="bibtex/taln-2012-court-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-004-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-004-key');">mots clés</a> <br/>

							<p id="taln-2012-court-004-abs" class="resume">
							<b>Résumé : </b> Les travaux liés à la définition et à la reconnaissance des entités nommées sont généralement envisagés en domaine ouvert, à travers la conception de catégories génériques (noms de personnes, de lieux, etc.) et leur application à des données textuelles issues de la presse (orale comme écrite). Par ailleurs, la fouille des données issues de centres d’appel est stratégique pour une entreprise comme EDF, compte tenu du rôle crucial joué par l’opinion pour les applications marketing, ce qui passe par la définition d’entités d’intérêt propres au domaine. Nous comparons les deux types de modèles d’entités - génériques et spécifiques à un domaine précis - afin d’observer leurs points de recouvrement, via l’annotation manuelle d’un corpus de conversations en centres d’appel. Nous souhaitons ainsi étudier l’apport d’une détection en entités nommées génériques pour l’extraction d’information métier en domaine restreint.
							</p>

							<p id="taln-2012-court-004-key" class="mots_cles">
							<b>Mots clés : </b> entités nommées, concepts métier, extraction d’information, données conversationnelles, annotation
							</p>

					</div>
					

					<div class="article">

						<b>Egle Eensoo, Mathieu Valette</b>


						<br/>

							<i>Sur l&#39;application de méthodes textométriques à la construction de critères de classification en analyse des sentiments</i> <br/>

						<a href="actes/taln-2012-court-005.pdf">taln-2012-court-005</a> 
						<a href="bibtex/taln-2012-court-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-005-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-005-key');">mots clés</a> <br/>

							<p id="taln-2012-court-005-abs" class="resume">
							<b>Résumé : </b> Depuis une dizaine d&#39;années, le TAL s&#39;intéresse à la subjectivité, notamment dans la perspective d&#39;applications telles que la fouille d&#39;opinion et l&#39;analyse des sentiments. Or, la linguistique de corpus outillée par des méthodes textométriques a souvent abordé la question de la subjectivité dans les textes. Notre objectif est de montrer d&#39;une part, ce que pourrait apporter à l&#39;analyse des sentiments l&#39;analyse textométrique et d&#39;autre part, comment mutualiser les avantages d&#39;une association entre celle-ci et une méthode de classification automatique basée sur l&#39;apprentissage supervisé. En nous appuyant sur un corpus de témoignages issus de forums de discussion, nous montrerons que la prise en compte de critères sélectionnés suivant une analyse textométrique permet d&#39;obtenir des résultats de classification satisfaisants par rapport à une vision purement lexicale.
							</p>

							<p id="taln-2012-court-005-key" class="mots_cles">
							<b>Mots clés : </b> linguistique de corpus, textométrie, analyse de sentiments, classification automatique supervisée
							</p>

					</div>
					

					<div class="article">

						<b>Michael Filhol, Annelies Braffort</b>


						<br/>

							<i>Méthodologie d&#39;exploration de corpus et de formalisation de règles grammaticales pour les langues des signes</i> <br/>

						<a href="actes/taln-2012-court-006.pdf">taln-2012-court-006</a> 
						<a href="bibtex/taln-2012-court-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-006-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-006-key');">mots clés</a> <br/>

							<p id="taln-2012-court-006-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthodologie visant, à partir d&#39;une observation de corpus vidéo de langue des signes, à repérer puis formaliser les régularités de structure dans les constructions linguistiques. Cette méthodologie est applicable à tous les niveaux du langage, du sub-lexical à l&#39;énoncé complet. En s&#39;appuyant sur deux exemples, il présente une application de cette méthodologie ainsi que le modèle AZee qui, intégrant la souplesse nécessaire en termes de synchronisation des articulateurs, permet une formalisation des règles repérées.
							</p>

							<p id="taln-2012-court-006-key" class="mots_cles">
							<b>Mots clés : </b> Langue des signes, analyse de corpus, modèle grammatical, synchronisation
							</p>

					</div>
					

					<div class="article">

						<b>Karën Fort, Vincent Claveau</b>


						<br/>

							<i>Annotation manuelle de matchs de foot : Oh la la la ! l&#39;accord inter-annotateurs ! et c&#39;est le but !</i> <br/>

						<a href="actes/taln-2012-court-007.pdf">taln-2012-court-007</a> 
						<a href="bibtex/taln-2012-court-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-007-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-007-key');">mots clés</a> <br/>

							<p id="taln-2012-court-007-abs" class="resume">
							<b>Résumé : </b> Cet article présente une campagne d’annotation de commentaires de matchs de football en français. L’annotation a été réalisée à partir d’un corpus très hétérogène, contenant à la fois des comptes-rendus minute par minute et des transcriptions des commentaires vidéo. Nous montrons ici comment les accords intra- et inter-annotateurs peuvent être utilisés efficacement, en en proposant une définition adaptée à notre type de tâche et en mettant en exergue l’importance de certaines bonnes pratiques concernant leur utilisation. Nous montrons également comment certains indices collectés à l’aide d’outils statistiques simples peuvent être utilisés pour indiquer des pistes de corrections des annotations. Ces différentes propositions nous permettent par ailleurs d’évaluer l’impact des modalités sources de nos textes (oral ou écrit) sur le coût et la qualité des annotations.
							</p>

							<p id="taln-2012-court-007-key" class="mots_cles">
							<b>Mots clés : </b> annotation manuelle, accords inter-annotateurs
							</p>

					</div>
					

					<div class="article">

						<b>Anne Garcia-Fernandez, Olivier Ferret</b>


						<br/>

							<i>Etude de différentes stratégies d&#39;adaptation à un nouveau domaine en fouille d&#39;opinion</i> <br/>

						<a href="actes/taln-2012-court-008.pdf">taln-2012-court-008</a> 
						<a href="bibtex/taln-2012-court-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-008-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-008-key');">mots clés</a> <br/>

							<p id="taln-2012-court-008-abs" class="resume">
							<b>Résumé : </b> Le travail présenté dans cet article se situe dans le contexte de la fouille d’opinion et se focalise sur la détermination de la polarité d’un texte en adoptant une approche par apprentissage. Dans ce cadre, son objet est d’étudier différentes stratégies d’adaptation à un nouveau domaine dans le cas de figure fréquent où des données d’entraînement n’existent que pour un ou plusieurs domaines différents du domaine cible. Cette étude montre en particulier que l’utilisation d’une forme d’auto-apprentissage par laquelle un classifieur annote un corpus du domaine cible et modifie son corpus d’entraînement en y incorporant les textes classés avec la plus grande confiance se révèle comme la stratégie la plus performante et la plus stable pour les différents domaines testés. Cette stratégie s’avère même supérieure dans un nombre significatif de cas à la méthode proposée par (Blitzer et al., 2007) sur les mêmes jeux de test tout en étant plus simple.
							</p>

							<p id="taln-2012-court-008-key" class="mots_cles">
							<b>Mots clés : </b> fouille d’opinion, adaptation à un nouveau domaine, auto-apprentissage
							</p>

					</div>
					

					<div class="article">

						<b>Olivier Kraif, Sascha Diwersy</b>


						<br/>

							<i>Le Lexicoscope : un outil pour l&#39;étude de profils combinatoires et l&#39;extraction de constructions lexico-syntaxiques</i> <br/>

						<a href="actes/taln-2012-court-009.pdf">taln-2012-court-009</a> 
						<a href="bibtex/taln-2012-court-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-009-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-009-key');">mots clés</a> <br/>

							<p id="taln-2012-court-009-abs" class="resume">
							<b>Résumé : </b> Dans le cadre du projet franco-allemand Emolex, dédié à l&#39;étude contrastive de la combinatoire du lexique des émotions en 5 langues, nous avons développé des outils et des méthodes permettant l&#39;extraction, la visualisation et la comparaison de profls combinatoires pour des expressions simples et complexes. Nous présentons ici l&#39;architecture d&#39;ensemble de la plate-forme, conçue pour efectuer des extractions sur des corpus de grandes dimensions (de l&#39;ordre de la centaine de millions de mots) avec des temps de réponse réduits (le corpus étant interrogeable en ligne1). Nous décrivons comment nous avons introduit la notion de pivots complexes, afn de permettre aux utilisateurs de rafner progressivement leurs requêtes pour caractériser des constructions lexico-syntaxiques élaborées. Enfn, nous donnons les premiers résultats d&#39;un module d&#39;extraction automatique d&#39;expressions polylexicales récurrentes.
							</p>

							<p id="taln-2012-court-009-key" class="mots_cles">
							<b>Mots clés : </b> collocations, cooccurrences, profl combinatoire, expressions polylexicales, lexique des émotions
							</p>

					</div>
					

					<div class="article">

						<b>Audrey Laroche</b>


						<br/>

							<i>Analyse des contextes et des candidats dans l&#39;identification des équivalents terminologiques en corpus comparables</i> <br/>

						<a href="actes/taln-2012-court-010.pdf">taln-2012-court-010</a> 
						<a href="bibtex/taln-2012-court-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-010-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-010-key');">mots clés</a> <br/>

							<p id="taln-2012-court-010-abs" class="resume">
							<b>Résumé : </b> L’approche standard d’identification d’équivalents terminologiques à partir de corpus comparables repose sur la comparaison de mots contextuels en langues source et cible et sur l’utilisation d’un lexique bilingue. Nous analysons manuellement, selon des critères linguistiques (parties du discours, spécificité et relations sémantiques), les propriétés des mots contextuels et des erreurs commises par l’approche standard appliquée à la terminologie médicale pour suggérer des améliorations basées sur la sélection de mots contextuels.
							</p>

							<p id="taln-2012-court-010-key" class="mots_cles">
							<b>Mots clés : </b> équivalents terminologiques, vecteurs contextuels, corpus comparables, terminologie médicale, étude qualitative
							</p>

					</div>
					

					<div class="article">

						<b>Emmanuel Planas</b>


						<br/>

							<i>BiTermEx Un prototype d&#39;extraction de mots composés à partir de documents comparables via la méthode compositionnelle</i> <br/>

						<a href="actes/taln-2012-court-011.pdf">taln-2012-court-011</a> 
						<a href="bibtex/taln-2012-court-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-011-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-011-key');">mots clés</a> <br/>

							<p id="taln-2012-court-011-abs" class="resume">
							<b>Résumé : </b> Nous décrivons BiTermEx, un prototype d&#39;expérimentation de l&#39;extraction de terminologie bilingue de mots composés, à partir de documents comparables, via la méthode compositionnelle. Nous expliquons la variation morphologique et la combinaison des constituants lexicaux des termes composés. Cette permet une précision TOP1 de 92% et 97,5% en français anglais, et de 94% en français japonais pour l&#39;alignement de termes composés (textes scientifiques et de vulgarisation scientifique).
							</p>

							<p id="taln-2012-court-011-key" class="mots_cles">
							<b>Mots clés : </b> extraction terminologique, prototype, terminologie bilingue, documents comparables, méthode compositionnelle, mots composés, corpus
							</p>

					</div>
					

					<div class="article">

						<b>Laurie Serrano, Thierry Charnois, Stephan Brunessau, Bruno Grilheres, Maroua Bouzid</b>


						<br/>

							<i>Combinaison d&#39;approches pour l&#39;extraction automatique d&#39;événements</i> <br/>

						<a href="actes/taln-2012-court-012.pdf">taln-2012-court-012</a> 
						<a href="bibtex/taln-2012-court-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-012-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-012-key');">mots clés</a> <br/>

							<p id="taln-2012-court-012-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons un système d’extraction automatique d’événements fondé sur deux approches actuelles en extraction d’information : la première s’appuie sur des règles linguistiques construites manuellement et la seconde se fonde sur un apprentissage automatique de patrons linguistiques. Les expérimentations réalisées montrent que combiner ces deux méthodes d’extraction permet d’améliorer significativement la qualité des événements extraits (amélioration de près de 10 points de F-mesure).
							</p>

							<p id="taln-2012-court-012-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’information, événements, approche symbolique, apprentissage de patrons linguistiques
							</p>

					</div>
					

					<div class="article">

						<b>Isabelle Tellier, Denys Duchier, Iris Eshkol, Arnaud Courmet, Mathieu Martinet</b>


						<br/>

							<i>Apprentissage automatique d&#39;un chunker pour le français</i> <br/>

						<a href="actes/taln-2012-court-013.pdf">taln-2012-court-013</a> 
						<a href="bibtex/taln-2012-court-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-013-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-013-key');">mots clés</a> <br/>

							<p id="taln-2012-court-013-abs" class="resume">
							<b>Résumé : </b> Nous décrivons dans cet article comment nous avons procédé pour apprendre automatiquement un chunker à partir du French Tree Bank, en utilisant les CRF (Conditional Random Fields). Nous avons réalisé diverses expériences, pour reconnaître soit l’ensemble de tous les chunks possibles, soit les seuls groupes nominaux simples. Nous évaluons le chunker obtenu aussi bien de manière interne (sur le French Tree Bank lui-même) qu’externe (sur un corpus distinct transcrit de l’oral), afin de mesurer sa robustesse.
							</p>

							<p id="taln-2012-court-013-key" class="mots_cles">
							<b>Mots clés : </b> chunking, apprentissage automatique, French Tree Bank, CRF
							</p>

					</div>
					

					<div class="article">

						<b>Nikola Tulechki, Ludovic Tanguy</b>


						<br/>

							<i>Effacement de dimensions de similarité textuelle pour l’exploration de collections de rapports d’incidents aéronautiques</i> <br/>

						<a href="actes/taln-2012-court-014.pdf">taln-2012-court-014</a> 
						<a href="bibtex/taln-2012-court-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-014-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-014-key');">mots clés</a> <br/>

							<p id="taln-2012-court-014-abs" class="resume">
							<b>Résumé : </b> Cet article étudie le lien entre la similarité textuelle et une classification extrinsèque dans des collections de rapports d’incidents aéronautiques. Nous cherchons à compléter les stratégies d’analyse de ces collections en établissant automatiquement des liens de similarité entre les documents de façon à ce qu’ils ne reflètent pas l’organisation des schémas de codification utilisés pour leur classement. Afin de mettre en évidence les dimensions de variation transversales à la classification, nous calculons un score de dépendance entre les termes et les classes et excluons du calcul de similarité les termes les plus corrélés à une classe donnée. Nous montrons par une application sur 500 documents que cette méthode permet effectivement de dégager des thématiques qui seraient passées inaperçues au vu de la trop grande saillance des similarités de haut niveau.
							</p>

							<p id="taln-2012-court-014-key" class="mots_cles">
							<b>Mots clés : </b> similarité textuelle, classification de documents, corpus spécialisé
							</p>

					</div>
					

					<div class="article">

						<b>Haithem Afli, Loïc Barrault, Holger Schwenk</b>


						<br/>

							<i>Traduction automatique à partir de corpus comparables: extraction de phrases parallèles à partir de données comparables multimodales</i> <br/>

						<a href="actes/taln-2012-court-015.pdf">taln-2012-court-015</a> 
						<a href="bibtex/taln-2012-court-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-015-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-015-key');">mots clés</a> <br/>

							<p id="taln-2012-court-015-abs" class="resume">
							<b>Résumé : </b> Les performances des systèmes de traduction automatique statistique dépendent de la disponibilité de textes parallèles bilingues, appelés aussi bitextes. Cependant, les corpus parallèles sont des ressources limitées et parfois indisponibles pour certains couples de langues ou domaines. Nous présentons une technique pour l’extraction de phrases parallèles à partir d’un corpus comparable multimodal (audio et texte). Ces enregistrements sont transcrits avec un système de reconnaissance automatique de la parole et traduits avec un système de traduction automatique. Ces traductions sont ensuite utilisées comme requêtes d’un système de recherche d’information pour sélectionner des phrases parallèles sans erreur et générer un bitexte. Plusieurs expériences ont été menées sur les données de la campagne IWSLT’11 (TED) qui montrent la faisabilité de notre approche.
							</p>

							<p id="taln-2012-court-015-key" class="mots_cles">
							<b>Mots clés : </b> Reconnaissance de la parole, traduction automatique statistique, corpus comparables multimodaux, extraction de phrases parallèles
							</p>

					</div>
					

					<div class="article">

						<b>Yacine Ben Yahia, Souha Mezghani Hammami, Lamia Hadrich Belguith</b>


						<br/>

							<i>La reconnaissance automatique de la fonction des pronoms démonstratifs en langue arabe</i> <br/>

						<a href="actes/taln-2012-court-016.pdf">taln-2012-court-016</a> 
						<a href="bibtex/taln-2012-court-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-016-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-016-key');">mots clés</a> <br/>

							<p id="taln-2012-court-016-abs" class="resume">
							<b>Résumé : </b> La résolution d&#39;anaphores est l&#39;une des tâches les plus difficiles du Traitement Automatique du Langage Naturel (TALN). La capacité de classifier les pronoms avant de tenter une tâche de résolution d&#39;anaphores serait importante, puisque pour traiter un pronom cataphorique le système doit chercher l’antécédent dans le segment qui suit le pronom. Alors que, pour le pronom anaphorique, le système doit chercher l’antécédent dans le segment qui précède le pronom. En outre, le nombre des pronoms a été jugée non-trivial dans la langue arabe. C’est dans ce cadre que se situe notre travail qui consiste à proposer une méthode pour la classification automatique des pronoms démonstratifs arabes, basée sur l’apprentissage. Nous avons évalué notre approche sur un corpus composé de 365585 mots contenant 14318 pronoms démonstratifs et nous avons obtenu des résultats encourageants : 99.3% comme F-Mesure.
							</p>

							<p id="taln-2012-court-016-key" class="mots_cles">
							<b>Mots clés : </b> Pronoms démonstratifs, résolution des anaphores, traitement de la langue arabe
							</p>

					</div>
					

					<div class="article">

						<b>Andre Bittar, Caroline Hagège</b>


						<br/>

							<i>Un annotateur automatique d&#39;expressions temporelles du français et son évaluation sur le TimeBank du français</i> <br/>

						<a href="actes/taln-2012-court-017.pdf">taln-2012-court-017</a> 
						<a href="bibtex/taln-2012-court-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-017-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-017-key');">mots clés</a> <br/>

							<p id="taln-2012-court-017-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons un outil d’extraction et de normalisation d’un sous-ensemble d’expressions temporelles développé pour le français. Cet outil est mis au point et utilisé dans le cadre du projet ANR Chronolines1 et il est appliqué sur un corpus fourni par l’AFP. Notre but final dans le cadre du projet est de construire semi-automatiquement des chronologies événementielles à partir de la base de depêches de l’AFP. L’une des étapes du traitement est l’analyse de l’information temporelle véhiculée dans les textes. Nous avons donc développé un annotateur d’expressions temporelles pour le français que nous décrivons dans cet article. Nous présenterons également les résultats de son évaluation.
							</p>

							<p id="taln-2012-court-017-key" class="mots_cles">
							<b>Mots clés : </b> Analyse temporelle, évaluation
							</p>

					</div>
					

					<div class="article">

						<b>Laurence Danlos, Diégo Antolinos-Basso, Chloé Braud, Charlotte Roze</b>


						<br/>

							<i>Vers le FDTB : French Discourse Tree Bank</i> <br/>

						<a href="actes/taln-2012-court-018.pdf">taln-2012-court-018</a> 
						<a href="bibtex/taln-2012-court-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-018-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-018-key');">mots clés</a> <br/>

							<p id="taln-2012-court-018-abs" class="resume">
							<b>Résumé : </b> Nous présentons les premiers pas vers la création d’un corpus annoté en discours pour le français : le French Discourse TreeBank enrichissant le FTB. La méthodologie adoptée s’inspire du Penn Discourse TreeBank (PDTB) mais elle s’en distingue sur au moins deux points à caractère théorique. D’abord, notre objectif est de fournir une couverture totale d’un texte du corpus, tandis que le PDTB ne fournit qu’une couverture partielle, qui ne peut donc pas être qualifiée d’analyse discursive comme celle faite en RST ou SDRT, deux théories majeures sur le discours. Ensuite, nous avons été amenés à définir une nouvelle hiérarchie des relations de discours qui s’inspire de RST, de SDRT et du PDTB.
							</p>

							<p id="taln-2012-court-018-key" class="mots_cles">
							<b>Mots clés : </b> Discours, corpus annoté manuellement, analyse discursive, PDTB, RST, SDRT
							</p>

					</div>
					

					<div class="article">

						<b>Romain Deveaud, Patrice Bellot</b>


						<br/>

							<i>Combinaison de ressources générales pour une contextualisation implicite de requêtes</i> <br/>

						<a href="actes/taln-2012-court-019.pdf">taln-2012-court-019</a> 
						<a href="bibtex/taln-2012-court-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-019-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-019-key');">mots clés</a> <br/>

							<p id="taln-2012-court-019-abs" class="resume">
							<b>Résumé : </b> L’utilisation de sources externes d’informations pour la recherche documentaire a été considérablement étudiée dans le passé. Des améliorations de performances ont été mises en lumière avec des corpus larges ou structurés. Néanmoins, dans ces études les ressources sont souvent utilisées séparément mais rarement combinées. Nous présentons une évaluation de la combinaison de quatre différentes ressources générales, standards et accessibles. Nous utilisons une mesure de distance informative pour extraire les caractéristiques contextuelles des différentes ressources et améliorer la représentation de la requête. Cette évaluation est menée sur une tâche de recherche d’information sur le Web en utilisant le corpus ClueWeb09 et les topics de la piste Web de TREC. Les meilleurs résultats sont obtenus en combinant les quatre ressources, et sont statistiquement significativement supérieurs aux autres approches.
							</p>

							<p id="taln-2012-court-019-key" class="mots_cles">
							<b>Mots clés : </b> Combinaison de ressources, RI contextuelle, recherche web
							</p>

					</div>
					

					<div class="article">

						<b>Souhir Gahbiche-Braham, Hélène Bonneau-Maynard, Thomas Lavergne, François Yvon</b>


						<br/>

							<i>Repérage des entités nommées pour l&#39;arabe : adaptation non-supervisée et combinaison de systèmes</i> <br/>

						<a href="actes/taln-2012-court-020.pdf">taln-2012-court-020</a> 
						<a href="bibtex/taln-2012-court-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-020-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-020-key');">mots clés</a> <br/>

							<p id="taln-2012-court-020-abs" class="resume">
							<b>Résumé : </b> La détection des Entités Nommées (EN) en langue arabe est un prétraitement potentiellement utile pour de nombreuses applications du traitement des langues, en particulier pour la traduction automatique. Cette tâche représente toutefois un sérieux défi, compte tenu des spécificités de l’arabe. Dans cet article, nous présentons un compte-rendu de nos efforts pour développer un système de repérage des EN s’appuyant sur des méthodes statistiques, en détaillant les aspects liés à la sélection des caractéristiques les plus utiles pour la tâche ; puis diverses tentatives pour adapter ce système d’une manière entièrement non supervisée.
							</p>

							<p id="taln-2012-court-020-key" class="mots_cles">
							<b>Mots clés : </b> Adaptation non supervisée, Repérage des entités nommées
							</p>

					</div>
					

					<div class="article">

						<b>Nuria Gala, Caroline Brun</b>


						<br/>

							<i>Propagation de polarités dans des familles de mots : impact de la morphologie dans la construction d&#39;un lexique pour l&#39;analyse de sentiments</i> <br/>

						<a href="actes/taln-2012-court-021.pdf">taln-2012-court-021</a> 
						<a href="bibtex/taln-2012-court-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-021-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-021-key');">mots clés</a> <br/>

							<p id="taln-2012-court-021-abs" class="resume">
							<b>Résumé : </b> Les ressources lexicales sont cruciales pour de nombreuses applications de traitement automatique de la langue (par exemple, l&#39;extraction d&#39;opinions à partir de corpus). Cependant, leur construction pose des problèmes à différents niveaux (coût, couverture, etc.). Dans cet article, nous avons voulu vérifier si les informations morphologiques liées à la dérivation pouvaient être exploitées pour l&#39;annotation automatique d&#39;informations sémantiques. En partant d&#39;une ressource regroupant les mots en familles morphologiques en français, nous avons construit un lexique de polarités pour 4 065 mots, à partir d&#39;une liste initiale d&#39;adjectifs annotés manuellement. Les résultats obtenus montrent que la propagation des polarités est correcte pour 78,89% des familles avec un seul adjectif. Le lexique ainsi obtenu améliore aussi les résultats du système d&#39;extraction d&#39;opinions.
							</p>

							<p id="taln-2012-court-021-key" class="mots_cles">
							<b>Mots clés : </b> ressources lexicales, morphologie dérivationnelle, analyse de sentiments
							</p>

					</div>
					

					<div class="article">

						<b>Alexandre Labadié, Patrice Enjalbert, Stephane Ferrari</b>


						<br/>

							<i>Transitions thématiques : Annotation d&#39;un corpus journalistique et premières analyses</i> <br/>

						<a href="actes/taln-2012-court-022.pdf">taln-2012-court-022</a> 
						<a href="bibtex/taln-2012-court-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-022-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-022-key');">mots clés</a> <br/>

							<p id="taln-2012-court-022-abs" class="resume">
							<b>Résumé : </b> Le travail présenté dans cet article est centré sur la constitution d’un corpus de textes journalistiques annotés au niveau discursif d’un point de vue thématique. Le modèle d’annotation est une segmentation classique, à laquelle nous ajoutons un repérage de zones de transition entre unités thématiques. Nous faisons l’hypothèse que dans un texte bien construit, le scripteur fournit des indications aidant le lecteur à passer d’un sujet à un autre, l’identification de ces indices étant susceptible d’améliorer les procédures de segmentation automatique. Les annotations produites ont fait l’objet d’analyses quantitatives mettant en évidence un ensemble de propriétés des transitions entre thèmes.
							</p>

							<p id="taln-2012-court-022-key" class="mots_cles">
							<b>Mots clés : </b> Structure du discours, segments thématiques, transitions thématiques, annotation
							</p>

					</div>
					

					<div class="article">

						<b>Blandine Plaisantin Alecu, Izabella Thomas, Julie Renahy</b>


						<br/>

							<i>La &#34;multi-extraction&#34; comme stratégie d&#39;acquisition optimisée de ressources terminologiques et non terminologiques</i> <br/>

						<a href="actes/taln-2012-court-023.pdf">taln-2012-court-023</a> 
						<a href="bibtex/taln-2012-court-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-023-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-023-key');">mots clés</a> <br/>

							<p id="taln-2012-court-023-abs" class="resume">
							<b>Résumé : </b> A partir de l&#39;évaluation d&#39;extracteurs de termes menée initialement pour détecter le meilleur outil d&#39;acquisition du lexique d&#39;une langue contrôlée, nous proposons dans cet article une stratégie d&#39;optimisation du processus d&#39;extraction terminologique. Nos travaux, menés dans le cadre du projet ANR Sensunique, prouvent que la « multiextraction », c&#39;est-à-dire la coopération de plusieurs extracteurs de termes, donne des résultats significativement meilleurs que l’extraction via un seul outil. Elle permet à la fois de réduire le silence et de filtrer automatiquement le bruit grâce à la variation d&#39;un indice relatif au potentiel terminologique.
							</p>

							<p id="taln-2012-court-023-key" class="mots_cles">
							<b>Mots clés : </b> terminologie, extraction, langue contrôlée, potentiel terminologique, filtrage de termes
							</p>

					</div>
					

					<div class="article">

						<b>Arnaud Renard, Sylvie Calabretto, Béatrice Rumpler</b>


						<br/>

							<i>Une Approche de Recherche d’Information Structurée fondée sur la Correction d’Erreurs à l’Indexation des Documents</i> <br/>

						<a href="actes/taln-2012-court-024.pdf">taln-2012-court-024</a> 
						<a href="bibtex/taln-2012-court-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-024-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-024-key');">mots clés</a> <br/>

							<p id="taln-2012-court-024-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous nous sommes intéressés à la prise en compte des erreurs dans les contenus textuels des documents XML. Nous proposons une approche visant à diminuer l’impact de ces erreurs sur les systèmes de Recherche d’Information (RI). En effet, ces systèmes produisent des index associant chaque document aux termes qu’il contient. Les erreurs affectent donc la qualité des index ce qui conduit par exemple à considérer à tort des documents mal indexés comme non pertinents (resp. pertinents) vis-à-vis de certaines requêtes. Afin de faire face à ce problème, nous proposons d’inclure un mécanisme de correction d’erreurs lors de la phase d’indexation des documents. Nous avons implémenté cette approche au sein d’un prototype que nous avons évalué dans le cadre de la campagne d’évaluation INEX.
							</p>

							<p id="taln-2012-court-024-key" class="mots_cles">
							<b>Mots clés : </b> Recherche d’information, dysorthographie, correction d’erreurs, xml
							</p>

					</div>
					

					<div class="article">

						<b>Raphaël Rubino, Stéphane Huet, Fabrice Lefèvre, Georges Linarès</b>


						<br/>

							<i>Post-édition statistique pour l&#39;adaptation aux domaines de spécialité en traduction automatique</i> <br/>

						<a href="actes/taln-2012-court-025.pdf">taln-2012-court-025</a> 
						<a href="bibtex/taln-2012-court-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-025-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-025-key');">mots clés</a> <br/>

							<p id="taln-2012-court-025-abs" class="resume">
							<b>Résumé : </b> Cet article présente une approche de post-édition statistique pour adapter aux domaines de spécialité des systèmes de traduction automatique génériques. En utilisant les traductions produites par ces systèmes, alignées avec leur traduction de référence, un modèle de post-édition basé sur un alignement sous-phrastique est construit. Les expériences menées entre le français et l’anglais pour le domaine médical montrent qu’une telle adaptation a posteriori est possible. Deux systèmes de traduction statistiques sont étudiés : une implémentation locale état-de-l’art et un outil libre en ligne. Nous proposons aussi une méthode de sélection de phrases à post-éditer permettant d’emblée d’accroître la qualité des traductions et pour laquelle les scores oracles indiquent des gains encore possibles.
							</p>

							<p id="taln-2012-court-025-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique statistique, post-édition, adaptation aux domaines de spécialité
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Sagot, Marion Richard, Rosa Stern</b>


						<br/>

							<i>Annotation référentielle du Corpus Arboré de Paris 7 en entités nommées</i> <br/>

						<a href="actes/taln-2012-court-026.pdf">taln-2012-court-026</a> 
						<a href="bibtex/taln-2012-court-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-026-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-026-key');">mots clés</a> <br/>

							<p id="taln-2012-court-026-abs" class="resume">
							<b>Résumé : </b> Le Corpus Arboré de Paris 7 (ou French TreeBank) est le corpus de référence pour le français aux niveaux morphosyntaxique et syntaxique. Toutefois, il ne contient pas d’annotations explicites en entités nommées. Ces dernières sont pourtant parmi les informations les plus utiles pour de nombreuses tâches en traitement automatique des langues et de nombreuses applications. De plus, aucun corpus du français annoté en entités nommées et de taille importante ne contient d’annotation référentielle, qui complète les informations de typage et d’empan sur chaque mention par l’indication de l’entité à laquelle elle réfère. Nous avons annoté manuellement avec ce type d’informations, après pré-annotation automatique, le Corpus Arboré de Paris 7. Nous décrivons les grandes lignes du guide d’annotation sous-jacent et nous donnons quelques informations quantitatives sur les annotations obtenues.
							</p>

							<p id="taln-2012-court-026-key" class="mots_cles">
							<b>Mots clés : </b> Résolution d’entités nommées, Corpus annoté, Corpus arboré de Paris 7
							</p>

					</div>
					

					<div class="article">

						<b>Christophe Servan, Simon Petitrenaud</b>


						<br/>

							<i>Utilisation des fonctions de croyance pour l&#39;estimation de paramètres en traduction automatique</i> <br/>

						<a href="actes/taln-2012-court-027.pdf">taln-2012-court-027</a> 
						<a href="bibtex/taln-2012-court-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-027-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-027-key');">mots clés</a> <br/>

							<p id="taln-2012-court-027-abs" class="resume">
							<b>Résumé : </b> Cet article concerne des travaux effectués dans le cadre du 7ème atelier de traduction automatique statistique et du projet ANR COSMAT. Ces travaux se focalisent sur l’estimation de paramètres contenus dans une table de traduction. L’approche classique consiste à estimer ces paramètres à partir de fréquences relatives d’éléments de traduction. Dans notre approche, nous proposons d’utiliser le concept de masses de croyance afin d’estimer ces paramètres. La théorie des fonctions de croyances est une théorie très adaptée à la gestion des incertitudes dans de nombreux domaines. Les expériences basées sur notre approche s’appliquent sur la traduction de la paire de langue français-anglais dans les deux sens de traduction.
							</p>

							<p id="taln-2012-court-027-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique statistique, fonctions de croyance, apprentissage automatique, estimation de paramètres
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Suignard, Frederik Cailliau, Ariane Cavet</b>


						<br/>

							<i>La longueur des tours de parole comme critère de sélection de conversations dans un centre d’appels</i> <br/>

						<a href="actes/taln-2012-court-028.pdf">taln-2012-court-028</a> 
						<a href="bibtex/taln-2012-court-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-028-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-028-key');">mots clés</a> <br/>

							<p id="taln-2012-court-028-abs" class="resume">
							<b>Résumé : </b> Cet article s’intéresse aux conversations téléphoniques d’un Centre d’Appels EDF, automatiquement découpées en « tours de parole » et automatiquement transcrites. Il fait apparaître une relation entre la longueur des tours de parole et leur contenu, en ce qui concerne le vocabulaire qui les compose et les sentiments qui y sont véhiculés. Après avoir montré qu’il y a un intérêt à étudier ces longs tours, l’article analyse leur contenu et liste quelques exemples autour des notions d’argumentation et de réclamation. Il montre ainsi que la longueur des tours de parole peut être un critère utile de sélection de conversations.
							</p>

							<p id="taln-2012-court-028-key" class="mots_cles">
							<b>Mots clés : </b> Centre d’appels, Conversation, Tour de parole, Reconnaissance de Parole
							</p>

					</div>
					

					<div class="article">

						<b>Tristan Vanrullen, Leïla Boutora, Jean Dagron</b>


						<br/>

							<i>Enjeux méthodologiques, linguistiques et informatiques pour le traitement du français écrit des sourds</i> <br/>

						<a href="actes/taln-2012-court-029.pdf">taln-2012-court-029</a> 
						<a href="bibtex/taln-2012-court-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-court-029-abs');">résumé</a>
							<a onclick="toggle('taln-2012-court-029-key');">mots clés</a> <br/>

							<p id="taln-2012-court-029-abs" class="resume">
							<b>Résumé : </b> L’ouverture du Centre National de Réception des Appels d’Urgence (CNRAU) accessible aux sourds et malentendants fait émerger des questions linguistiques qui portent sur le français écrit des sourds, et des questions informatiques dans le domaine du traitement automatique du langage naturel. Le français écrit des sourds, pratiqué par une population hétérogène, comporte des spécificités morpho-syntaxiques et morpholexicales qui peuvent rendre problématique la communication écrite entre les personnes sourdes appelantes et les agents du CNRAU. Un premier corpus de français écrit sourd élicité avec mise en situation d’urgence (FAX-ESSU) a été recueilli dans la perspective de proposer des solutions TAL et linguistiques aux agents du CNRAU dans le cadre de ces échanges écrits. Nous présentons une première étude lexicale, morphosyntaxique et syntaxique de ce corpus reposant en partie sur une chaîne de traitement automatique, afin de valider les phénomènes linguistiques décrits dans la littérature et d&#39;enrichir la connaissance du français écrit des sourds.
							</p>

							<p id="taln-2012-court-029-key" class="mots_cles">
							<b>Mots clés : </b> Français écrit des sourds, TAL, Français Langue Etrangère, linguistique de corpus, lexique, syntaxe, méthodologie
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

				<h1 id="démonstration">Démonstrations</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Bruno Guillaume, Guillame Bonfante, Paul Masson, Mathieu Morey, Guy Perrier</b>


						<br/>

							<i>Grew : un outil de réécriture de graphes pour le TAL</i> <br/>

						<a href="actes/taln-2012-demo-001.pdf">taln-2012-demo-001</a> 
						<a href="bibtex/taln-2012-demo-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-001-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-001-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-001-abs" class="resume">
							<b>Résumé : </b> Nous présentons un outil de réécriture de graphes qui a été conçu spécifiquement pour des applications au TAL. Il permet de décrire des graphes dont les noeuds contiennent des structures de traits et dont les arcs décrivent des relations entre ces noeuds. Nous présentons ici la réécriture de graphes que l’on considère, l’implantation existante et quelques expérimentations.
							</p>

							<p id="taln-2012-demo-001-key" class="mots_cles">
							<b>Mots clés : </b> réécriture de graphes, interface syntaxe-sémantique
							</p>

					</div>
					

					<div class="article">

						<b>Géraldine Damnati</b>


						<br/>

							<i>Interfaces de navigation dans des contenus audio et vidéo</i> <br/>

						<a href="actes/taln-2012-demo-002.pdf">taln-2012-demo-002</a> 
						<a href="bibtex/taln-2012-demo-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-002-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-002-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-002-abs" class="resume">
							<b>Résumé : </b> Deux types de démonstrateurs sont présentés. Une première interface à visée didactique permet d&#39;observer des traitements automatiques sur des documents vidéo. Plusieurs niveaux de représentation peuvent être montrés simultanément, ce qui facilite l&#39;analyse d&#39;approches multi-vues. La seconde interface est une interface opérationnelle de &#34;consommation&#34; de documents audio. Elle offre une expérience de navigation enrichie dans des documents audio grâce à une visualisation de métadonnées extraites automatiquement.
							</p>

							<p id="taln-2012-demo-002-key" class="mots_cles">
							<b>Mots clés : </b> Traitements multi-vues, navigation enrichie
							</p>

					</div>
					

					<div class="article">

						<b>Lionel Clément</b>


						<br/>

							<i>Synthèse de texte avec le logiciel Syntox</i> <br/>

						<a href="actes/taln-2012-demo-003.pdf">taln-2012-demo-003</a> 
						<a href="bibtex/taln-2012-demo-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-003-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-003-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-003-abs" class="resume">
							<b>Résumé : </b> Le logiciel Syntox, dont une interface utilisateur en ligne ce trouve à cette URL : http://www.syntox.net, est une mise en application d’un modèle basé sur les grammaires attribuées, dans le cadre de la synthèse de texte. L’outil est une plateforme d’expérimentation dont l’ergonomie est simple. Syntox est capable de traiter des lexiques et des grammaires volumineux sur des textes ambigus à partir de la description explicite de phénomènes linguistiques.
							</p>

							<p id="taln-2012-demo-003-key" class="mots_cles">
							<b>Mots clés : </b> Synthèse de texte, Grammaire attribuée, Syntaxe
							</p>

					</div>
					

					<div class="article">

						<b>Isabelle Tellier, Yoann Dupont, Arnaud Courmet</b>


						<br/>

							<i>Un segmenteur-étiqueteur et un chunker pour le français</i> <br/>

						<a href="actes/taln-2012-demo-004.pdf">taln-2012-demo-004</a> 
						<a href="bibtex/taln-2012-demo-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-004-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-004-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-004-abs" class="resume">
							<b>Résumé : </b> Nous proposons une démonstration de deux programmes : un segmenteur-étiqueteur POS pour le français et un programme de parenthésage en “chunks” de textes préalablement traités par le programme précédent. Tous deux ont été appris à partir du French Tree Bank.
							</p>

							<p id="taln-2012-demo-004-key" class="mots_cles">
							<b>Mots clés : </b> étiquetage POS, chunking, apprentissage automatique, French Tree Bank, CRF
							</p>

					</div>
					

					<div class="article">

						<b>Brigitte Bigi</b>


						<br/>

							<i>SPPAS : segmentation, phonétisation, alignement, syllabation</i> <br/>

						<a href="actes/taln-2012-demo-005.pdf">taln-2012-demo-005</a> 
						<a href="bibtex/taln-2012-demo-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-005-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-005-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-005-abs" class="resume">
							<b>Résumé : </b> SPPAS est le nouvel outil du LPL pour l’alignement texte/son. La segmentation s’opère en 4 étapes successives dans un processus entièrement automatique ou semi-automatique, à partir d’un fichier audio et d’une transcription. Le résultat comprend la segmentation en unités inter-pausales, en mots, en syllabes et en phonèmes. La version actuelle propose un ensemble de ressources qui permettent le traitement du français, de l’anglais, de l’italien et du chinois. L’ajout de nouvelles langues est facilitée par la simplicité de l’architecture de l’outil et le respect des formats de fichiers les plus usuels. L’outil bénéficie en outre d’une documentation en ligne et d’une interface graphique afin d’en faciliter l’accessibilité aux non-informaticiens. Enfin, SPPAS n’utilise et ne contient que des ressources et programmes sous licence libre GPL.
							</p>

							<p id="taln-2012-demo-005-key" class="mots_cles">
							<b>Mots clés : </b> segmentation, phonétisation, alignement, syllabation
							</p>

					</div>
					

					<div class="article">

						<b>François-Régis Chaumartin</b>


						<br/>

							<i>Solution Proxem d’analyse sémantique verticale : adaptation au domaine des Ressources Humaines</i> <br/>

						<a href="actes/taln-2012-demo-006.pdf">taln-2012-demo-006</a> 
						<a href="bibtex/taln-2012-demo-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-006-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-006-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-006-abs" class="resume">
							<b>Résumé : </b> Proxem développe depuis 2007 une plate-forme de traitement du langage, Antelope, qui permet de construire rapidement des applications sémantiques verticales (par exemple, pour l’e-réputation, la veille économique ou l’analyse d’avis de consommateurs). Antelope a servi à créer une solution pour les Ressources Humaines, utilisée notamment par l’APEC, permettant (1) d’extraire de l’information à partir d’offres et de CVs et (2) de trouver les offres d’emploi correspondant le mieux à un CV (ou réciproquement). Nous présentons ici l’adaptation d’Antelope à un domaine particulier, en l’occurrence les RH.
							</p>

							<p id="taln-2012-demo-006-key" class="mots_cles">
							<b>Mots clés : </b> entités nommées, extraction de relations, création d’ontologies, similarité
							</p>

					</div>
					

					<div class="article">

						<b>Estelle Delpech, Laurent Candillier</b>


						<br/>

							<i>Nomao : un moteur de recherche géolocalisé spécialisé dans la recommandation de lieux et l’e-réputation</i> <br/>

						<a href="actes/taln-2012-demo-007.pdf">taln-2012-demo-007</a> 
						<a href="bibtex/taln-2012-demo-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-007-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-007-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-007-abs" class="resume">
							<b>Résumé : </b> Cette démonstration présente NOMAO, un moteur de recherche géolocalisé qui permet à ses utilisateurs de trouver des lieux (bars, magasins...) qui correspondent à leurs goûts, à ceux de leurs amis et aux recommandations des internautes.
							</p>

							<p id="taln-2012-demo-007-key" class="mots_cles">
							<b>Mots clés : </b> recherche d’information, analyse d’opinion, génération de texte, fouille du web
							</p>

					</div>
					

					<div class="article">

						<b>Samira MOUKRIM</b>


						<br/>

							<i>Le DictAm : Dictionnaire électronique des verbes amazighs-français</i> <br/>

						<a href="actes/taln-2012-demo-008.pdf">taln-2012-demo-008</a> 
						<a href="bibtex/taln-2012-demo-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-008-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-008-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-008-abs" class="resume">
							<b>Résumé : </b> Le DictAm est un dictionnaire électronique des verbes amazighs-français. Il vise à rendre compte de l’ensemble des verbes dans le domaine berbère : conjugaison, diathèse et sens. Le DictAm comporte actuellement près de 3000 verbes dans une soixantaine de parlers berbères. C’est un travail qui est en cours de réalisation et qui a pour ambition de répertorier tous les verbes berbères ainsi que leurs équivalents en français.
							</p>

							<p id="taln-2012-demo-008-key" class="mots_cles">
							<b>Mots clés : </b> Dictionnaire électronique, dimension bilingue, diversité linguistique, verbes
							</p>

					</div>
					

					<div class="article">

						<b>Thomas Hueber, Atef Ben-Youssef, Pierre Badin, Gérard Bailly, Frédéric Eliséi</b>


						<br/>

							<i>Vizart3D : Retour Articulatoire Visuel pour l’Aide à la Prononciation</i> <br/>

						<a href="actes/taln-2012-demo-009.pdf">taln-2012-demo-009</a> 
						<a href="bibtex/taln-2012-demo-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-009-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-009-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-009-abs" class="resume">
							<b>Résumé : </b> L’objectif du système Vizart3D est de fournir à un locuteur, en temps réel, et de façon automatique, un retour visuel sur ses propres mouvements articulatoires. Les applications principales de ce système sont l’aide à l’apprentissage des langues étrangères et la rééducation orthophonique (correction phonétique). Le système Vizart3D est basé sur la tête parlante 3D développée au GIPSA-lab, qui laisse apparaître, en plus des lèvres, les articulateurs de la parole normalement cachés (comme la langue). Cette tête parlante est animée automatiquement à partir du signal audio de parole, à l’aide de techniques de conversion de voix et de régression acoustico-articulatoire par GMM.
							</p>

							<p id="taln-2012-demo-009-key" class="mots_cles">
							<b>Mots clés : </b> retour visuel, aide à la prononciation, GMM, temps réel, tête parlante
							</p>

					</div>
					

					<div class="article">

						<b>Emmanuel Ferragne, Sébastien Flavier, Christian Fressard</b>


						<br/>

							<i>ROCme! : logiciel pour l’enregistrement et la gestion de corpus oraux</i> <br/>

						<a href="actes/taln-2012-demo-010.pdf">taln-2012-demo-010</a> 
						<a href="bibtex/taln-2012-demo-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2012-demo-010-abs');">résumé</a>
							<a onclick="toggle('taln-2012-demo-010-key');">mots clés</a> <br/>

							<p id="taln-2012-demo-010-abs" class="resume">
							<b>Résumé : </b> ROCme! permet une gestion rationalisée, autonome et dématérialisée de l’enregistrement de corpus oraux. Il dispose notamment d’une interface pour le recueil de métadonnées sur les locuteurs totalement paramétrable via des balises XML. Les locuteurs peuvent gérer les réponses au questionnaire, l’enregistrement audio, la lecture, la sauvegarde et le défilement des phrases (ou autres types de corpus) en toute autonomie. ROCme! affiche du texte, avec ou sans mise en forme HTML, des images, du son et des vidéos.
							</p>

							<p id="taln-2012-demo-010-key" class="mots_cles">
							<b>Mots clés : </b> corpus, oral, linguistique, logiciel
							</p>

					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>