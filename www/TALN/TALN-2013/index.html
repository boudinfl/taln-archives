<!DOCTYPE html>
<html lang="fr">
	<head>
		<meta charset="utf-8">
		<title>TALN'2013</title>
		<link rel="stylesheet" href="../../css/style.css">
		<script type="text/javascript">
			function toggle(id) {
				var e = document.getElementById(id);
				if(e.style.display == 'block')
					e.style.display = 'none';
				else
					e.style.display = 'block';
			}
		</script>
	</head>
	<body>
		<div id="container">
			<header>
				<h1><a href="../../index.html">TALN Archives</a></h1>
				<h2>Une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue.</h2>
			</header>

			<section id="info">
				<h1>TALN'2013, 20e conférence sur le Traitement Automatique des Langues Naturelles</h1>
				<h2>Les Sables d'Olonne (France), du 2013-06-17 au 2012-06-21</h2>
				<p>Président(s) : Emmanuel Morin, Yannick Estève</p>
				<p>Taux d'acceptation :
							articles longs (51.4%)
							articles courts (61.4%)
							démonstrations (100.0%)
				</p>
			</section>

			<nav>
				<h1>Table des matières</h1>
				<ul>
				<li><a href="#invite">Conférenciers invités</a></li>
				<li><a href="#charte">Charte Ethique et Big Data</a></li>
				<li><a href="#long">Articles longs</a></li>
				<li><a href="#court">Articles courts</a></li>
				<li><a href="#démonstration">Démonstrations</a></li>
				</ul>
			</nav>

			<section id="content">

				<h1 id="invite">Conférenciers invités</h1>
			

					<div class="article">

						<b>Alexander Fraser</b>


						<br/>

							<i>Améliorer la traduction des langages morphologiquement riches</i> <br/>

						<a href="actes/taln-2013-invite-001.pdf">taln-2013-invite-001</a> 
						<a href="bibtex/taln-2013-invite-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-invite-001-abs');">résumé</a>
							<a onclick="toggle('taln-2013-invite-001-key');">mots clés</a> <br/>

							<p id="taln-2013-invite-001-abs" class="resume">
							<b>Résumé : </b> Si les techniques statistiques pour la traduction automatique ont fait des progrès significatifs au cours des 20 dernières années, les résultats pour la traduction de langues morphologiquement riches sont toujours mitigés par rapport aux précédentes générations de systèmes à base de règles. Les recherches actuelles en traduction statistique de langues morphologiquement riches varient grandement en fonction de la quantité de connaissances linguistiques utilisées et de la nature de ces connaissances. Cette variation est plus importante en langue cible (par exemple, les ressources utilisées en traduction automatique statistique respectueuse de linguistique en arabe, en français et en allemand sont très différentes). La conférence portera sur les techniques état de l’art dédiées à la tâche de traduction statistique pour une langue cible qui est morphologiquement plus riche que la langue source.
							</p>

							<p id="taln-2013-invite-001-key" class="mots_cles">
							<b>Mots clés : </b> traduction statistique, langages morphologiquement riches, connaissances linguistiques
							</p>

					</div>
					

					<div class="article">

						<b>Josiane Mothe</b>


						<br/>

							<i>Recherche d’Information et Traitement Automatique des Langues Naturelles</i> <br/>

						<a href="actes/taln-2013-invite-002.pdf">taln-2013-invite-002</a> 
						<a href="bibtex/taln-2013-invite-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-invite-002-abs');">résumé</a>
							<a onclick="toggle('taln-2013-invite-002-key');">mots clés</a> <br/>

							<p id="taln-2013-invite-002-abs" class="resume">
							<b>Résumé : </b> La recherche d’information s’intéresse à l’accès aux documents et une majorité de travaux dans le domaine s’appuie sur les éléments textuels de ces documents écrits en langage naturel. Les requêtes soumisses par les utilisateurs de moteurs de recherche sont également textuelles, même si elles sont très pauvres d’un point de vue linguistique. Il parait donc naturel que les travaux en recherche d’information cherchent à s’alimenter par les avancées et les résultats en traitement automatique des langues naturelles. Malgré les espoirs déçus des années 80, l’engouement pour l’utilisation du traitement du langage naturel en recherche d’information reste intact, poussé par les nouvelles perspectives offertes. Dans cette conférence, nous balayerons les aspects de la recherche d’information qui se sont le plus appuyés sur des éléments du traitement automatique des langues naturelles. Nous présenterons en particulier quelques résultats relatifs à la reformulation automatique de requêtes, à la prédiction de la difficulté des requêtes, au résumé automatique et à la contextualisation de textes courts ainsi que les perspectives actuelles offertes en particulier par les travaux en linguistique computationnelle.
							</p>

							<p id="taln-2013-invite-002-key" class="mots_cles">
							<b>Mots clés : </b> Recherche d’information, traitement automatique des langues, reformulation de requêtes, difficulté des requêtes, résumé automatique
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="charte">Charte Ethique et Big Data</h1>
			

					

					

					<div class="article">

						<b>Karën Fort, Alain Couillault</b>


						<br/>

							<i>La Charte Éthique et Big Data : pour des ressources pour le TAL (enfin !) traçables et pérennes</i> <br/>

						<a href="actes/taln-2013-charte-001.pdf">taln-2013-charte-001</a> 
						<a href="bibtex/taln-2013-charte-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-charte-001-abs');">résumé</a>
							<a onclick="toggle('taln-2013-charte-001-key');">mots clés</a> <br/>

							<p id="taln-2013-charte-001-abs" class="resume">
							<b>Résumé : </b> La charte Ethique &amp; Big Data a été conçue à l’initiative de l’ATALA, de l’AFCP, de l’APROGED et de CAP DIGITAL, au sein d’un groupe de travail mixte réunissant d’autres partenaires académiques et industriels (tels que le CERSA-CNRS, Digital Ethics, Eptica-Lingway, le cabinet Itéanu ou ELRA/ELDA). Elle se donne comme objectif de fournir des garanties concernant la traçabilité des données (notamment des ressources langagières), leur qualité et leur impact sur l’emploi. Cette charte a été adoptée par Cap Digital (co-rédacteur). Nous avons également proposé à la DGLFLF et à l’ANR de l’utiliser. Elle est aujourd’hui disponible sous forme de wiki, de fichier pdf et il en existe une version en anglais. La charte est décrite en détails dans (Couillault et Fort, 2013).
							</p>

							<p id="taln-2013-charte-001-key" class="mots_cles">
							<b>Mots clés : </b> éthique, big data, ressources langagières
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="long">Articles longs</h1>
			

					

					

					

					<div class="article">

						<b>Fatima Zahra Nejme, Siham Boulaknadel, Driss Aboutajdine</b>


						<br/>

							<i>Analyse Automatique de la Morphologie Nominale Amazighe</i> <br/>

						<a href="actes/taln-2013-long-001.pdf">taln-2013-long-001</a> 
						<a href="bibtex/taln-2013-long-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-001-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-001-key');">mots clés</a> <br/>

							<p id="taln-2013-long-001-abs" class="resume">
							<b>Résumé : </b> Dans le but de préserver le patrimoine amazighe et éviter qu’il soit menacé de disparition, il semble opportun de doter cette langue de moyens nécessaires pour faire face aux enjeux de l&#39;accès au domaine de l&#39;Information et de la Communication (TIC). Dans ce contexte, et dans la perspective de construire des outils et des ressources linguistiques pour le traitement automatique de cette langue, nous avons entrepris de construire un système d’analyse morphologique pour l’amazighe standard du Maroc. Ce système profite des apports des modèles { états finis au sein de l’environnement linguistique de développement NooJ en faisant appel à des règles grammaticales à large couverture.
							</p>

							<p id="taln-2013-long-001-key" class="mots_cles">
							<b>Mots clés : </b> La langue amazighe, TALN, NooJ, analyse morphologique, morphologie flexionnelle, morphologie dérivationnelle
							</p>

					</div>
					

					<div class="article">

						<b>Isabelle Tellier, Yoann Dupont</b>


						<br/>

							<i>Apprentissage symbolique et statistique pour le chunking:comparaison et combinaisons</i> <br/>

						<a href="actes/taln-2013-long-002.pdf">taln-2013-long-002</a> 
						<a href="bibtex/taln-2013-long-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-002-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-002-key');">mots clés</a> <br/>

							<p id="taln-2013-long-002-abs" class="resume">
							<b>Résumé : </b> Nous décrivons dans cet article l’utilisation d’algorithmes d’inférence grammaticale pour la tâche de chunking, pour ensuite les comparer et les combiner avec des CRF (Conditional Random Fields), à l’efficacité éprouvée pour cette tâche. Notre corpus est extrait du French TreeBank. Nous proposons et évaluons deux manières différentes de combiner modèle symbolique et modèle statistique appris par un CRF et montrons qu’ils bénéficient dans les deux cas l’un de l’autre.
							</p>

							<p id="taln-2013-long-002-key" class="mots_cles">
							<b>Mots clés : </b> apprentissage automatique, chunking, CRF, inférence grammaticale, k-RI, FrenchTreeBank
							</p>

					</div>
					

					<div class="article">

						<b>Yllias Chali, Sadid A.Hasan, Mustapha Mojahid</b>


						<br/>

							<i>L’utilisation des POMDP pour les résumés multi-documents orientés par une thématique</i> <br/>

						<a href="actes/taln-2013-long-003.pdf">taln-2013-long-003</a> 
						<a href="bibtex/taln-2013-long-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-003-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-003-key');">mots clés</a> <br/>

							<p id="taln-2013-long-003-abs" class="resume">
							<b>Résumé : </b> L’objectif principal du résumé multi-documents orienté par une thématique est de générer un résumé à partir de documents sources en réponse à une requête formulée par l’utilisateur. Cette tâche est difficile car il n’existe pas de méthode efficace pour mesurer la satisfaction de l’utilisateur. Cela introduit ainsi une incertitude dans le processus de génération de résumé. Dans cet article, nous proposons une modélisation de l’incertitude en formulant notre système de résumé comme un processus de décision markovien partiellement observables (POMDP) car dans de nombreux domaines on a montré que les POMDP permettent de gérer efficacement les incertitudes. Des expériences approfondies sur les jeux de données du banc d’essai DUC ont démontré l’efficacité de notre approche.
							</p>

							<p id="taln-2013-long-003-key" class="mots_cles">
							<b>Mots clés : </b> Résumé multi-document, résumé orienté requête, POMDP
							</p>

					</div>
					

					<div class="article">

						<b>Olivier Ferret</b>


						<br/>

							<i>Sélection non supervisée de relations sémantiques pour améliorer un thésaurus distributionnel</i> <br/>

						<a href="actes/taln-2013-long-004.pdf">taln-2013-long-004</a> 
						<a href="bibtex/taln-2013-long-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-004-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-004-key');">mots clés</a> <br/>

							<p id="taln-2013-long-004-abs" class="resume">
							<b>Résumé : </b> Les travaux se focalisant sur la construction de thésaurus distributionnels ont montré que les relations sémantiques qu’ils recèlent sont principalement fiables pour les mots de forte fréquence. Dans cet article, nous proposons une méthode pour rééquilibrer de tels thésaurus en faveur des mots de fréquence faible sur la base d’un mécanisme d’amorçage : un ensemble d’exemples et de contre-exemples de mots sémantiquement similaires sont sélectionnés de façon non supervisée et utilisés pour entraîner un classifieur supervisé. Celui-ci est ensuite appliqué pour réordonner les voisins sémantiques du thésaurus utilisé pour sélectionner les exemples et contre-exemples. Nous montrons comment les relations entre les constituants de noms composés similaires peuvent être utilisées pour réaliser une telle sélection et comment conjuguer ce critère à un critère déjà expérimenté sur la symétrie des relations sémantiques. Nous évaluons l’intérêt de cette procédure sur un large ensemble de noms en anglais couvrant un vaste spectre de fréquence.
							</p>

							<p id="taln-2013-long-004-key" class="mots_cles">
							<b>Mots clés : </b> Sémantique lexicale, similarité sémantique, thésaurus
							</p>

					</div>
					

					<div class="article">

						<b>Marie Dupuch, Thierry Hamon, Natalia Grabar</b>


						<br/>

							<i>Groupement de termes basé sur des régularités linguistiques et sémantiques dans un contexte cross-langue</i> <br/>

						<a href="actes/taln-2013-long-005.pdf">taln-2013-long-005</a> 
						<a href="bibtex/taln-2013-long-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-005-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-005-key');">mots clés</a> <br/>

							<p id="taln-2013-long-005-abs" class="resume">
							<b>Résumé : </b> Nous proposons d’exploiter des méthodes du Traitement Automatique de Langues dédiées à la structuration de terminologie indépendamment dans deux langues (anglais et français) et de fusionner ensuite les résultats obtenus dans chaque langue. Les termes sont groupés en clusters grâce aux relations générées. L’évaluation de ces relations est effectuée au travers de la comparaison des clusters avec des données de référence et la baseline, tandis que la complémentarité des relations est analysée au travers de leur implication dans la création de clusters de termes. Les résultats obtenus indiquent que : chaque langue contribue de manière équilibrée aux résultats, le nombre de relations hiérarchiques communes est plus grand que le nombre de relations synonymiques communes. Globalement, les résultats montrent que, dans un contexte cross-langue, chaque langue permet de détecter des régularités linguistiques et sémantiques complémentaires. L’union des résultats obtenus dans les deux langues améliore la qualité globale des clusters.
							</p>

							<p id="taln-2013-long-005-key" class="mots_cles">
							<b>Mots clés : </b> Relations sémantiques, termes, domaine de spécialité, médecine, contexte crosslangue
							</p>

					</div>
					

					<div class="article">

						<b>Quentin Pradet, Jeanne Baguenier-Desormeaux, Gaël de Chalendar, Laurence Danlos</b>


						<br/>

							<i>WoNeF : amélioration, extension et évaluation d’une traduction française automatique de WordNet</i> <br/>

						<a href="actes/taln-2013-long-006.pdf">taln-2013-long-006</a> 
						<a href="bibtex/taln-2013-long-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-006-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-006-key');">mots clés</a> <br/>

							<p id="taln-2013-long-006-abs" class="resume">
							<b>Résumé : </b> Identifier les sens possibles des mots du vocabulaire est un problème difficile demandant un travail manuel très conséquent. Ce travail a été entrepris pour l’anglais : le résultat est la base de données lexicale WordNet, pour laquelle il n’existe encore que peu d’équivalents dans d’autres langues. Néanmoins, des traductions automatiques de WordNet vers de nombreuses langues cibles existent, notamment pour le français. JAWS est une telle traduction automatique utilisant des dictionnaires et un modèle de langage syntaxique. Nous améliorons cette traduction, la complétons avec les verbes et adjectifs de WordNet, et démontrons la validité de notre approche via une nouvelle évaluation manuelle. En plus de la version principale nommée WoNeF, nous produisons deux versions supplémentaires : une version à haute précision (93% de précision, jusqu’à 97% pour les noms), et une version à haute couverture contenant 109 447 paires (littéral, synset).
							</p>

							<p id="taln-2013-long-006-key" class="mots_cles">
							<b>Mots clés : </b> WordNet, désambiguïsation lexicale, traduction, ressource
							</p>

					</div>
					

					<div class="article">

						<b>Bassam Jabaian, Fabrice Lefèvre, Laurent Besacier</b>


						<br/>

							<i>Approches statistiques discriminantes pour l’interprétation sémantique multilingue de la parole</i> <br/>

						<a href="actes/taln-2013-long-007.pdf">taln-2013-long-007</a> 
						<a href="bibtex/taln-2013-long-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-007-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-007-key');">mots clés</a> <br/>

							<p id="taln-2013-long-007-abs" class="resume">
							<b>Résumé : </b> Les approches statistiques sont maintenant très répandues dans les différentes applications du traitement automatique de la langue et le choix d’une approche particulière dépend généralement de la tâche visée. Dans le cadre de l’interprétation sémantique multilingue, cet article présente une comparaison entre les méthodes utilisées pour la traduction automatique et celles utilisées pour la compréhension de la parole. Cette comparaison permet de proposer une approche unifiée afin de réaliser un décodage conjoint qui à la fois traduit une phrase et lui attribue ses étiquettes sémantiques. Ce décodage est obtenu par une approche à base de transducteurs à états finis qui permet de composer un graphe de traduction avec un graphe de compréhension. Cette représentation peut être généralisée pour permettre des transmissions d’informations riches entre les composants d’un système d’interaction vocale homme-machine.
							</p>

							<p id="taln-2013-long-007-key" class="mots_cles">
							<b>Mots clés : </b> compréhension multilingue, système de dialogue, CRF, graphes d’hypothèses
							</p>

					</div>
					

					<div class="article">

						<b>Chloé Braud, Pascal Denis</b>


						<br/>

							<i>Identification automatique des relations discursives « implicites » à partir de données annotées et de corpus bruts</i> <br/>

						<a href="actes/taln-2013-long-008.pdf">taln-2013-long-008</a> 
						<a href="bibtex/taln-2013-long-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-008-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-008-key');">mots clés</a> <br/>

							<p id="taln-2013-long-008-abs" class="resume">
							<b>Résumé : </b> Cet article présente un système d’identification des relations discursives dites « implicites » (à savoir, non explicitement marquées par un connecteur) pour le français. Etant donné le faible volume de données annotées disponibles, notre système s’appuie sur des données étiquetées automatiquement en supprimant les connecteurs non ambigus pris comme annotation d’une relation, une méthode introduite par (Marcu et Echihabi, 2002). Comme l’ont montré (Sporleder et Lascarides, 2008) pour l’anglais, cette approche ne généralise pas très bien aux exemples de relations implicites tels qu’annotés par des humains. Nous arrivons au même constat pour le français et, partant du principe que le problème vient d’une différence de distribution entre les deux types de données, nous proposons une série de méthodes assez simples, inspirées par l’adaptation de domaine, qui visent à combiner efficacement données annotées et données artificielles. Nous évaluons empiriquement les différentes approches sur le corpus ANNODIS : nos meilleurs résultats sont de l’ordre de 45.6% d’exactitude, avec un gain significatif de 5.9% par rapport à un système n’utilisant que les données annotées manuellement.
							</p>

							<p id="taln-2013-long-008-key" class="mots_cles">
							<b>Mots clés : </b> analyse du discours, relations implicites, apprentissage automatique
							</p>

					</div>
					

					<div class="article">

						<b>Emmanuel Lassalle, Pascal Denis</b>


						<br/>

							<i>Apprentissage d’une hiérarchie de modèles à paires spécialisés pour la résolution de la coréférence</i> <br/>

						<a href="actes/taln-2013-long-009.pdf">taln-2013-long-009</a> 
						<a href="bibtex/taln-2013-long-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-009-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-009-key');">mots clés</a> <br/>

							<p id="taln-2013-long-009-abs" class="resume">
							<b>Résumé : </b> Nous proposons une nouvelle méthode pour améliorer significativement la performance des modèles à paires de mentions pour la résolution de la coréférence. Étant donné un ensemble d’indicateurs, notre méthode apprend à séparer au mieux des types de paires de mentions en classes d’équivalence, chacune de celles-ci donnant lieu à un modèle de classification spécifique. La procédure algorithmique proposée trouve le meilleur espace de traits (créé à partir de combinaisons de traits élémentaires et d’indicateurs) pour discriminer les paires de mentions coréférentielles. Bien que notre approche explore un très vaste ensemble d’espaces de trait, elle reste efficace en exploitant la structure des hiérarchies construites à partir des indicateurs. Nos expériences sur les données anglaises de la CoNLL-2012 Shared Task indiquent que notre méthode donne des gains de performance par rapport au modèle initial utilisant seulement les traits élémentaires, et ce, quelque soit la méthode de formation des chaînes ou la métrique d’évaluation choisie. Notre meilleur système obtient une moyenne de 67.2 en F1-mesure MUC, B3 et CEAF ce qui, malgré sa simplicité, le situe parmi les meilleurs systèmes testés sur ces données.
							</p>

							<p id="taln-2013-long-009-key" class="mots_cles">
							<b>Mots clés : </b> résolution de la coréférence, apprentissage automatique
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Philippe Fauconnier, Mouna Kamel, Bernard Rothenburger, Nathalie Aussenac-Gilles</b>


						<br/>

							<i>Apprentissage supervisé pour l’identification de relations sémantiques au sein de structures énumératives parallèles</i> <br/>

						<a href="actes/taln-2013-long-010.pdf">taln-2013-long-010</a> 
						<a href="bibtex/taln-2013-long-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-010-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-010-key');">mots clés</a> <br/>

							<p id="taln-2013-long-010-abs" class="resume">
							<b>Résumé : </b> Ce travail s’inscrit dans le cadre de la construction et l’enrichissement d’ontologies à partir de textes de type encyclopédique ou scientifique. L’originalité de notre travail réside dans l’extraction de relations sémantiques exprimées au-delà de la linéarité du texte. Pour cela, nous nous appuyons sur la sémantique véhiculée par les caractères typo-dispositionels qui ont pour fonction de suppléer des formulations strictement linguistiques qui seraient plus difficilement exploitables. L’étude que nous proposons concerne les relations sémantiques portées par les structures énumératives parallèles qui, bien qu’affichant des discontinuités entre ses différents composants, présentent un tout sur le plan sémantique. Ce sont des structures textuelles qui sont propices aux relations hiérarchiques. Après avoir défini une typologie des relations portées par ce type de structure, nous proposons une approche par apprentissage visant à leur identification. Sur la base de traits incorporant informations lexico-syntaxiques et typo-dispositionnelles, les premiers résultats aboutissent à une exactitude de 61,1%.
							</p>

							<p id="taln-2013-long-010-key" class="mots_cles">
							<b>Mots clés : </b> extraction de relations, structures énumératives parallèles, mise en forme matérielle, apprentissage supervisé, construction d’ontologies
							</p>

					</div>
					

					<div class="article">

						<b>Marie-Paule Jacques, Laura Hartwell, Achille Falaise</b>


						<br/>

							<i>Techniques de TAL et corpus pour faciliter les formulations en anglais scientifique écrit</i> <br/>

						<a href="actes/taln-2013-long-011.pdf">taln-2013-long-011</a> 
						<a href="bibtex/taln-2013-long-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-011-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-011-key');">mots clés</a> <br/>

							<p id="taln-2013-long-011-abs" class="resume">
							<b>Résumé : </b> Nous présentons l&#39;adaptation de la base d&#39;écrits scientifiques en ligne Scientext pour un « nouveau » public : chercheurs et autres auteurs français d&#39;écrits scientifiques, ayant besoin de rédiger en anglais. Cette adaptation a consisté à ajouter dans la base des requêtes précodées qui permettent d&#39;afficher les contextes dans lesquels les auteurs d&#39;articles scientifiques en anglais expriment leur objectif de recherche et à enrichir l&#39;interface ScienQuest de nouvelles fonctionnalités pour mémoriser et réafficher les contextes pertinents, pour faciliter la consultation par un public plus large. Les nombreuses descriptions linguistiques de la rhétorique des articles scientifiques insistent sur l&#39;importance de la création et de l&#39;occupation d&#39;une « niche » de recherche. Chercheurs et doctorants ont ici un moyen d&#39;en visualiser des exemples sans connaître sa formulation a priori, via nos requêtes. Notre évaluation sur le corpus de test en donne une précision globale de 86,5 %.
							</p>

							<p id="taln-2013-long-011-key" class="mots_cles">
							<b>Mots clés : </b> anglais, patrons lexico-syntaxiques, ScienQuest, Scientext
							</p>

					</div>
					

					<div class="article">

						<b>Nicolas Hernandez, Florian Boudin</b>


						<br/>

							<i>Construction d’un large corpus écrit libre annoté morpho-syntaxiquement en français</i> <br/>

						<a href="actes/taln-2013-long-012.pdf">taln-2013-long-012</a> 
						<a href="bibtex/taln-2013-long-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-012-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-012-key');">mots clés</a> <br/>

							<p id="taln-2013-long-012-abs" class="resume">
							<b>Résumé : </b> Cet article étudie la possibilité de créer un nouveau corpus écrit en français annoté morphosyntaxiquement à partir d’un corpus annoté existant. Nos objectifs sont de se libérer de la licence d’exploitation contraignante du corpus d’origine et d’obtenir une modernisation perpétuelle des textes. Nous montrons qu’un corpus pré-annoté automatiquement peut permettre d’entraîner un étiqueteur produisant des performances état-de-l’art, si ce corpus est suffisamment grand.
							</p>

							<p id="taln-2013-long-012-key" class="mots_cles">
							<b>Mots clés : </b> corpus arboré, construction de corpus, étiquetage morpho-syntaxique
							</p>

					</div>
					

					<div class="article">

						<b>Anne Abeillé, Benoit Crabbé</b>


						<br/>

							<i>Vers un treebank du français parlé</i> <br/>

						<a href="actes/taln-2013-long-013.pdf">taln-2013-long-013</a> 
						<a href="bibtex/taln-2013-long-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-013-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-013-key');">mots clés</a> <br/>

							<p id="taln-2013-long-013-abs" class="resume">
							<b>Résumé : </b> Nous présentons les premiers résultats d’un corpus arboré pour le français parlé. Il a été réalisé dans le cadre du projet ANR Etape (resp. G. Gravier) en 2011 et 2012. Contrairement à d’autres langues comme l’anglais (voir le Switchboard treebank de (Meteer, 1995)), il n’existe pas de grand corpus oral du francais annoté et validé pour les constituants et les fonctions syntaxiques. Nous souhaitons construire une ressource comparable, qui serait une extension naturelle du Corpus arboré de Paris 7 (FTB : (Abeillé et al., 2003))) basé sur des textes du journal Le Monde. Nous serons ainsi en mesure de comparer, avec des annotations comparables, l’écrit et l’oral. Les premiers résultats, qui consistent à réutiliser l’analyseur de (Petrov et al., 2006) entraîné sur l’écrit, avec une phase de correction manuelle, sont encourageants.
							</p>

							<p id="taln-2013-long-013-key" class="mots_cles">
							<b>Mots clés : </b> Corpus arboré, français parlé, corpus oral, analyse syntaxique automatique
							</p>

					</div>
					

					<div class="article">

						<b>Assaf Urieli, Ludovic Tanguy</b>


						<br/>

							<i>L’apport du faisceau dans l’analyse syntaxique en dépendances par transitions : études de cas avec l’analyseur Talismane</i> <br/>

						<a href="actes/taln-2013-long-014.pdf">taln-2013-long-014</a> 
						<a href="bibtex/taln-2013-long-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-014-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-014-key');">mots clés</a> <br/>

							<p id="taln-2013-long-014-abs" class="resume">
							<b>Résumé : </b> L’analyse syntaxique (ou parsing) en dépendances par transitions se fait souvent de façon déterministe, où chaque étape du parsing propose une seule solution comme entrée de l’étape suivante. Il en va de même pour la chaîne complète d’analyse qui transforme un texte brut en graphe de dépendances, généralement décomposé en quatre modules (segmentation en phrases, en mots, étiquetage et parsing) : chaque module ne fournit qu’une seule solution au module suivant. On sait cependant que certaines ambiguïtés ne peuvent pas être levées sans prendre en considération le niveau supérieur. Dans cet article, nous présentons l’analyseur Talismane, outil libre et complet d’analyse syntaxique probabiliste du français, et nous étudions plus précisément l’apport d’une recherche par faisceau (beam search) à l’analyse syntaxique. Les résultats nous permettent à la fois de dégager la taille de faisceau la plus adaptée (qui permet d’atteindre un score de 88,5 % d’exactitude, légèrement supérieur aux outils comparables), ainsi que les meilleures stratégies concernant sa propagation.
							</p>

							<p id="taln-2013-long-014-key" class="mots_cles">
							<b>Mots clés : </b> Analyse syntaxique en dépendances, ambiguïtés, évaluation, beam search
							</p>

					</div>
					

					<div class="article">

						<b>Anca Simon, Guillaume Gravier, Pascale Sébillot</b>


						<br/>

							<i>Un modèle segmental probabiliste combinant cohésion lexicale et rupture lexicale pour la segmentation thématique</i> <br/>

						<a href="actes/taln-2013-long-015.pdf">taln-2013-long-015</a> 
						<a href="bibtex/taln-2013-long-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-015-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-015-key');">mots clés</a> <br/>

							<p id="taln-2013-long-015-abs" class="resume">
							<b>Résumé : </b> L’identification d’une structure thématique dans des données textuelles quelconques est une tâche difficile. La plupart des techniques existantes reposent soit sur la maximisation d’une mesure de cohésion lexicale au sein d’un segment, soit sur la détection de ruptures lexicales. Nous proposons une nouvelle technique combinant ces deux critères de manière à obtenir le meilleur compromis entre cohésion et rupture. Nous définissons un nouveau modèle probabiliste, fondé sur l’approche proposée par Utiyama et Isahara (2001), en préservant les propriétés d’indépendance au domaine et de faible a priori de cette dernière. Des évaluations sont menées sur des textes écrits et sur des transcriptions automatiques de la parole à la télévision, transcriptions qui ne respectent pas les normes des textes écrits, ce qui accroît la difficulté. Les résultats expérimentaux obtenus démontrent la pertinence de la combinaison des critères de cohésion et de rupture.
							</p>

							<p id="taln-2013-long-015-key" class="mots_cles">
							<b>Mots clés : </b> segmentation thématique, cohésion lexicale, rupture de cohésion, journaux télévisés
							</p>

					</div>
					

					<div class="article">

						<b>Pierre Bourreau</b>


						<br/>

							<i>Traitements d’ellipses : deux approches par les grammaires catégorielles abstraites</i> <br/>

						<a href="actes/taln-2013-long-016.pdf">taln-2013-long-016</a> 
						<a href="bibtex/taln-2013-long-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-016-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-016-key');">mots clés</a> <br/>

							<p id="taln-2013-long-016-abs" class="resume">
							<b>Résumé : </b> L’étude de phénomènes d’ellipses dans les modèles de l’interface syntaxe-sémantique pose certains problèmes du fait que le matériel linguistique effacé au niveau phonologique est néanmoins présent au niveau sémantique. Tel est le cas d’une ellipse verbale ou d’une élision du sujet, par exemple, phénomènes qui interviennent lorsque deux phrases reliées par une conjonction partagent le même verbe, ou le même sujet. Nous proposons un traitement de ces phénomènes dans le formalisme des grammaires catégorielles abstraites selon un patron que nous intitulons extraction/instanciation et que nous implémentons de deux manières différentes dans les ACGs.
							</p>

							<p id="taln-2013-long-016-key" class="mots_cles">
							<b>Mots clés : </b> ellipse, coordination, interface syntaxe-sémantique, grammaires catégorielles abstraites, grammaires d’arbres adjoints, grammaires IO d’arbres
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Blache</b>


						<br/>

							<i>Chunks et activation : un modèle de facilitation du traitement linguistique</i> <br/>

						<a href="actes/taln-2013-long-017.pdf">taln-2013-long-017</a> 
						<a href="bibtex/taln-2013-long-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-017-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-017-key');">mots clés</a> <br/>

							<p id="taln-2013-long-017-abs" class="resume">
							<b>Résumé : </b> Nous proposons dans cet article d’intégrer la notion de chunk au sein d’une architecture globale de traitement de la phrase. Les chunks jouent un rôle important dans les théories cognitives comme ACT-R (Anderson et al., 2004) : il s’agit d’unités de traitement globales auxquelles il est possible d’accéder directement via des buffers en mémoire à court ou long terme. Ces chunks sont construits par une fonction d’activation (processus cognitif pouvant être quantifié) s’appuyant sur l’évaluation de leur relation au contexte. Nous proposons une interprétation de cette théorie appliquée à l’analyse syntaxique. Un mécanisme de construction des chunks est proposé. Nous développons pour cela une fonction d’activation tirant parti de la représentation de l’information linguistique sous forme de contraintes. Cette fonction permet de montrer en quoi les chunks sont faciles à construire et comment leur existence facilite le traitement de la phrase. Plusieurs exemples sont proposés, illustrant cette hypothèse de facilitation.
							</p>

							<p id="taln-2013-long-017-key" class="mots_cles">
							<b>Mots clés : </b> Chunks, ACT-R, activation, mémoire, parsing, traitement de la phrase, expérimentation
							</p>

					</div>
					

					<div class="article">

						<b>Amir Hazem, Emmanuel Morin</b>


						<br/>

							<i>Extraction de lexiques bilingues à partir de corpus comparables par combinaison de représentations contextuelles</i> <br/>

						<a href="actes/taln-2013-long-018.pdf">taln-2013-long-018</a> 
						<a href="bibtex/taln-2013-long-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-018-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-018-key');">mots clés</a> <br/>

							<p id="taln-2013-long-018-abs" class="resume">
							<b>Résumé : </b> La caractérisation du contexte des mots constitue le coeur de la plupart des méthodes d’extraction de lexiques bilingues à partir de corpus comparables. Dans cet article, nous revisitons dans un premier temps les deux principales stratégies de représentation contextuelle, à savoir celle par fenêtre ou sac de mots et celle par relations de dépendances syntaxiques. Dans un second temps, nous proposons deux nouvelles approches qui exploitent ces deux représentations de manière conjointe. Nos expériences montrent une amélioration significative des résultats sur deux corpus de langue de spécialité.
							</p>

							<p id="taln-2013-long-018-key" class="mots_cles">
							<b>Mots clés : </b> Multilingualisme, corpus comparables, lexique bilingue, vecteurs de contexte, dépendances syntaxiques
							</p>

					</div>
					

					<div class="article">

						<b>Vincent Claveau, Abir Ncibi</b>


						<br/>

							<i>Découverte de connaissances dans les séquences par CRF non-supervisés</i> <br/>

						<a href="actes/taln-2013-long-019.pdf">taln-2013-long-019</a> 
						<a href="bibtex/taln-2013-long-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-019-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-019-key');">mots clés</a> <br/>

							<p id="taln-2013-long-019-abs" class="resume">
							<b>Résumé : </b> Les tâches de découverte de connaissances ont pour but de faire émerger des groupes d’entités cohérents. Ils reposent le plus souvent sur du clustering, tout l’enjeu étant de définir une notion de similarité pertinentes entre ces entités. Dans cet article, nous proposons de détourner les champs aléatoires conditionnels (CRF), qui ont montré leur intérêt pour des tâches d’étiquetage supervisées, pour calculer indirectement ces similarités sur des séquences de textes. Pour cela, nous générons des problèmes d’étiquetage factices sur les données à traiter pour faire apparaître des régularités dans les étiquetages des entités. Nous décrivons comment ce cadre peut être mis en oeuvre et l’expérimentons sur deux tâches d’extraction d’informations. Les résultats obtenus démontrent l’intérêt de cette approche non-supervisée, qui ouvre de nombreuses pistes pour le calcul de similarités dans des espaces de représentations complexes de séquences.
							</p>

							<p id="taln-2013-long-019-key" class="mots_cles">
							<b>Mots clés : </b> Découverte de connaissances, CRF, clustering, apprentissage non-supervisé, extraction d’informations
							</p>

					</div>
					

					<div class="article">

						<b>Thomas Gaillat</b>


						<br/>

							<i>Annotation automatique d&#39;un corpus d&#39;apprenants d&#39;anglais avec un jeu d&#39;étiquettes modifié du Penn Treebank</i> <br/>

						<a href="actes/taln-2013-long-020.pdf">taln-2013-long-020</a> 
						<a href="bibtex/taln-2013-long-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-020-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-020-key');">mots clés</a> <br/>

							<p id="taln-2013-long-020-abs" class="resume">
							<b>Résumé : </b> Cet article aborde la problématique de l&#39;annotation automatique d&#39;un corpus d&#39;apprenants d&#39;anglais. L&#39;objectif est de montrer qu&#39;il est possible d&#39;utiliser un étiqueteur PoS pour annoter un corpus d&#39;apprenants afin d&#39;analyser les erreurs faites par les apprenants. Cependant, pour permettre une analyse suffisamment fine, des étiquettes fonctionnelles spécifiques aux phénomènes linguistiques à étudier sont insérées parmi celles de l&#39;étiqueteur. Celuici est entraîné avec ce jeu d&#39;étiquettes étendu sur un corpus de natifs avant d&#39;être appliqué sur le corpus d&#39;apprenants. Dans cette expérience, on s&#39;intéresse aux usages erronés de this et that par les apprenants. On montre comment l&#39;ajout d&#39;une couche fonctionnelle sous forme de nouvelles étiquettes pour ces deux formes, permet de discriminer des usages variables chez les natifs et nonnatifs et, partant, d’identifier des schémas incorrects d&#39;utilisation. Les étiquettes fonctionnelles éclairent sur le fonctionnement discursif.
							</p>

							<p id="taln-2013-long-020-key" class="mots_cles">
							<b>Mots clés : </b> Apprentissage L2, corpus d&#39;apprenants, analyse linguistique d&#39;erreurs, étiquetage automatique, this, that
							</p>

					</div>
					

					<div class="article">

						<b>Franck Sajous, Nabil Hathout, Basilio Calderone</b>


						<br/>

							<i>GLÀFF, un Gros Lexique À tout Faire du Français</i> <br/>

						<a href="actes/taln-2013-long-021.pdf">taln-2013-long-021</a> 
						<a href="bibtex/taln-2013-long-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-021-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-021-key');">mots clés</a> <br/>

							<p id="taln-2013-long-021-abs" class="resume">
							<b>Résumé : </b> Cet article présente GLÀFF, un lexique du français à large couverture extrait du Wiktionnaire, le dictionnaire collaboratif en ligne. GLÀFF contient pour chaque entrée une description morphosyntaxique et une transcription phonémique. Il se distingue des autres lexiques existants principalement par sa taille, sa licence libre et la possibilité de le faire évoluer de façon constante. Nous décrivons ici comment nous l’avons construit, puis caractérisé en le comparant à différentes ressources connues. Cette comparaison montre que sa taille et sa qualité font de GLÀFF un candidat sérieux comme nouvelle ressource standard pour le TAL, la linguistique et la psycholinguistique.
							</p>

							<p id="taln-2013-long-021-key" class="mots_cles">
							<b>Mots clés : </b> Lexique morpho-phonologique, ressources lexicales libres, Wiktionnaire
							</p>

					</div>
					

					<div class="article">

						<b>Authoul Abdul Hay, Olivier Kraif</b>


						<br/>

							<i>Constitution d’une ressource sémantique arabe à partir de corpus multilingue aligné</i> <br/>

						<a href="actes/taln-2013-long-022.pdf">taln-2013-long-022</a> 
						<a href="bibtex/taln-2013-long-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-022-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-022-key');">mots clés</a> <br/>

							<p id="taln-2013-long-022-abs" class="resume">
							<b>Résumé : </b> Cet article porte sur la mise en oeuvre etsur l&#39;étudede techniques d&#39;extraction de relations sémantiques à partir d&#39;un corpus multilingue aligné, en vue de construire une ressource lexicale pour l’arabe. Ces relations sontextraites par transitivité de l&#39;équivalence traductionnelle, deux lexèmes qui possèdent les mêmes équivalents dans une langue cible étant susceptibles de partager un même sens. A partir d’équivalences extraites d’un corpus multilingue aligné, nous tâchons d&#39;extraire des &#34;cliques&#34;, ou sous-graphes maximaux complets connexes, dont toutes les unités sont en interrelation, du fait d&#39;une probable intersection sémantique. Ces cliques présentent l&#39;intérêt de renseigner à la fois sur la synonymie et la polysémie des unités, et d&#39;apporter une forme de désambiguïsation sémantique. Ensuite nous tâchons de relier ces cliques avec un lexique sémantique (de type Wordnet) afin d&#39;évaluer la possibilité de récupérer pour les unités arabes des relations sémantiques définies pour des unités en d’autres langues (français, anglais ou espagnol). Les résultats sont encourageants, et montrent qu’avec des corpus adaptés ces relations pourraient permettrede construire automatiquement un réseau utile pour certaines applications de traitement de la langue arabe.
							</p>

							<p id="taln-2013-long-022-key" class="mots_cles">
							<b>Mots clés : </b> Corpus multilingues alignés, désambigüisation sémantique, cliques, lexiques multilingues, réseaux sémantiques, traitement de l’arabe
							</p>

					</div>
					

					<div class="article">

						<b>Rima Harastani, Béatrice Daille, Emmanuel Morin</b>


						<br/>

							<i>Identification, alignement, et traductions des adjectifs relationnels en corpus comparables</i> <br/>

						<a href="actes/taln-2013-long-023.pdf">taln-2013-long-023</a> 
						<a href="bibtex/taln-2013-long-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-023-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-023-key');">mots clés</a> <br/>

							<p id="taln-2013-long-023-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous extrayons des adjectifs relationnels français et nous les alignons automatiquement avec les noms dont ils sont dérivés en utilisant un corpus monolingue. Les alignements adjectif-nom seront ensuite utilisés dans la traduction compositionelle des termes complexes de la forme [N AdjR] à partir d’un corpus comparable français-anglais. Un nouveau terme [N N0] (ex. cancer du poumon) sera obtenu en remplaçant l’adjectif relationnel Ad jR (ex. pulmonaire) dans [N AdjR] (ex. cancer pulmonaire) par le nom N0 (ex. poumon) avec lequel il est aligné. Si aucune traduction n’est proposée pour [N AdjR], nous considérons que ses traduction(s) sont équivalentes à celle(s) de sa paraphrase [N N0]. Nous expérimentons avec un corpus comparable dans le domaine de cancer du sein, et nous obtenons des alignements adjectif-nom qui aident à traduire des termes complexes de la forme [N AdjR] vers l’anglais avec une précision de 86 %.
							</p>

							<p id="taln-2013-long-023-key" class="mots_cles">
							<b>Mots clés : </b> Adjectifs relationnels, Corpus comparables, Méthode compositionnelle, Termes complexes
							</p>

					</div>
					

					<div class="article">

						<b>Dhouha Bouamor, Nasredine Semmar, Pierre Zweigenbaum</b>


						<br/>

							<i>Utilisation de la similarité sémantique pour l’extraction de lexiques bilingues à partir de corpus comparables</i> <br/>

						<a href="actes/taln-2013-long-024.pdf">taln-2013-long-024</a> 
						<a href="bibtex/taln-2013-long-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-024-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-024-key');">mots clés</a> <br/>

							<p id="taln-2013-long-024-abs" class="resume">
							<b>Résumé : </b> Cet article présente une nouvelle méthode visant à améliorer les résultats de l’approche standard utilisée pour l’extraction de lexiques bilingues à partir de corpus comparables spécialisés. Nous tentons de résoudre le problème de la polysémie des mots dans les vecteurs de contexte par l’introduction d’un processus de désambiguïsation sémantique basé sur WordNet. Pour traduire les vecteurs de contexte, au lieu de considérer toutes les traductions proposées par le dictionnaire bilingue, nous n’utilisons que les mots caractérisant au mieux les contextes en langue cible. Les expériences menées sur deux corpus comparables spécialisés français-anglais (financier et médical) montrent que notre méthode améliore les résultats de l’approche standard plus particulièrement lorsque plusieurs mots du contexte sont ambigus.
							</p>

							<p id="taln-2013-long-024-key" class="mots_cles">
							<b>Mots clés : </b> lexique bilingue, corpus comparable spécialisé, désambiguïsation sémantique, WordNet
							</p>

					</div>
					

					<div class="article">

						<b>Manel Zarrouk, Mathieu Lafourcade, Alain Joubert</b>


						<br/>

							<i>Inférences déductives et réconciliation dans un réseau lexico-sémantique</i> <br/>

						<a href="actes/taln-2013-long-025.pdf">taln-2013-long-025</a> 
						<a href="bibtex/taln-2013-long-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-025-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-025-key');">mots clés</a> <br/>

							<p id="taln-2013-long-025-abs" class="resume">
							<b>Résumé : </b> La construction et la validation des réseaux lexico-sémantiques est un enjeu majeur en TAL. Indépendamment des stratégies de construction utilisées, inférer automatiquement de nouvelles relations à partir de celles déjà existantes est une approche possible pour améliorer la couverture et la qualité globale de la ressource. Dans ce contexte, le moteur d’inférences a pour but de formuler de nouvelles conclusions (c’est-à-dire des relations entre les termes) à partir de prémisses (des relations préexistantes). L’approche que nous proposons est basée sur une méthode de triangulation impliquant la transitivité sémantique avec un mécanisme de blocage pour éviter de proposer des relations douteuses. Les relations inférées sont proposées aux contributeurs pour être validées. Dans le cas d’invalidation, une stratégie de réconciliation est engagée pour identifier la cause de l’inférence erronée : une exception, une erreur dans les prémisses, ou une confusion d’usage causée par la polysémie.
							</p>

							<p id="taln-2013-long-025-key" class="mots_cles">
							<b>Mots clés : </b> inférence de relations, réconciliation, enrichissement, réseau lexical, peuplonomie
							</p>

					</div>
					

					<div class="article">

						<b>Wei Wang, Romaric Besançon, Olivier Ferret, Brigitte Grau</b>


						<br/>

							<i>Regroupement sémantique de relations pour l’extraction d’information non supervisée</i> <br/>

						<a href="actes/taln-2013-long-026.pdf">taln-2013-long-026</a> 
						<a href="bibtex/taln-2013-long-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-026-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-026-key');">mots clés</a> <br/>

							<p id="taln-2013-long-026-abs" class="resume">
							<b>Résumé : </b> Beaucoup des recherches menées en extraction d’information non supervisée se concentrent sur l’extraction des relations et peu de travaux proposent des méthodes pour organiser les relations extraites. Nous présentons dans cet article une méthode de clustering en deux étapes pour regrouper des relations sémantiquement équivalentes : la première étape regroupe des relations proches par leur expression tandis que la seconde fusionne les premiers clusters obtenus sur la base d’une mesure de similarité sémantique. Nos expériences montrent en particulier que les mesures distributionnelles permettent d’obtenir pour cette tâche de meilleurs résultats que les mesures utilisant WordNet. Nous montrons également qu’un clustering à deux niveaux permet non seulement de limiter le nombre de similarités sémantiques à calculer mais aussi d’améliorer la qualité des résultats du clustering.
							</p>

							<p id="taln-2013-long-026-key" class="mots_cles">
							<b>Mots clés : </b> Extraction d’Information Non Supervisée, Similarité Sémantique, Clustering
							</p>

					</div>
					

					<div class="article">

						<b>Christian Retoré</b>


						<br/>

							<i>Sémantique des déterminants dans un cadre richement typé</i> <br/>

						<a href="actes/taln-2013-long-027.pdf">taln-2013-long-027</a> 
						<a href="bibtex/taln-2013-long-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-027-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-027-key');">mots clés</a> <br/>

							<p id="taln-2013-long-027-abs" class="resume">
							<b>Résumé : </b> La variation du sens des mots en contexte nous a conduit à enrichir le système de types utilisés dans notre analyse syntaxico-sémantique du français basé sur les grammaires catégorielles et la sémantique de Montague (ou la lambda-DRT). L’avantage majeur d’une telle sémantique profonde est de représenter le sens par des formules logiques aisément exploitables, par exemple par un moteur d’inférence. Déterminants et quantificateurs jouent un rôle fondamental dans la construction de ces formules, et il nous a fallu leur trouver des termes sémantiques adaptés à ce nouveau cadre. Nous proposons une solution inspirée des opérateurs epsilon et tau de Hilbert, éléments génériques qui s’apparentent à des fonctions de choix. Cette modélisation unifie le traitement des différents types de déterminants et de quantificateurs et autorise le liage dynamique des pronoms. Surtout, cette description calculable des déterminants s’intègre parfaitement à l’analyseur à large échelle du français Grail, tant en théorie qu’en pratique.
							</p>

							<p id="taln-2013-long-027-key" class="mots_cles">
							<b>Mots clés : </b> Analyse sémantique automatique, Sémantique formelle, Compositionnalité
							</p>

					</div>
					

					<div class="article">

						<b>Charlotte Lecluze, Romain Brixtel, Loïs Rigouste, Emmanuel Giguet, Régis Clouard, Gaël Lejeune, Patrick Constant</b>


						<br/>

							<i>Détection de zones parallèles à l’intérieur de multi-documents pour l’alignement multilingue</i> <br/>

						<a href="actes/taln-2013-long-028.pdf">taln-2013-long-028</a> 
						<a href="bibtex/taln-2013-long-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-028-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-028-key');">mots clés</a> <br/>

							<p id="taln-2013-long-028-abs" class="resume">
							<b>Résumé : </b> Cet article aborde une question centrale de l’alignement automatique, celle du diagnostic de parallélisme des documents à aligner. Les recherches en la matière se sont jusqu’alors concentrées sur l’analyse de documents parallèles par nature : corpus de textes réglementaires, documents techniques ou phrases isolées. Les phénomènes d’inversions et de suppressions/ajouts pouvant exister entre les différentes versions d’un document sont ainsi souvent ignorées. Nous proposons donc une méthode pour diagnostiquer en contexte des zones parallèles à l’intérieur des documents. Cette méthode permet la détection d’inversions ou de suppressions entre les documents à aligner. Elle repose sur l’affranchissement de la notion de mot et de phrase, ainsi que sur la prise en compte de la Mise en Forme Matérielle du texte (MFM). Sa mise en oeuvre est basée sur des similitudes de répartition de chaînes de caractères répétées dans les différents documents. Ces répartitions sont représentées sous forme de matrices et l’identification des zones parallèles est effectuée à l’aide de méthodes de traitement d’image.
							</p>

							<p id="taln-2013-long-028-key" class="mots_cles">
							<b>Mots clés : </b> détection et alignement de zones, appariement de N-grammes de caractères, corpus de multidocuments
							</p>

					</div>
					

					<div class="article">

						<b>Ahmed Hamdi, Rahma Boujelbane, Nizar Habash, Alexis Nasr</b>


						<br/>

							<i>Un système de traduction de verbes entre arabe standard et arabe dialectal par analyse morphologique profonde</i> <br/>

						<a href="actes/taln-2013-long-029.pdf">taln-2013-long-029</a> 
						<a href="bibtex/taln-2013-long-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-029-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-029-key');">mots clés</a> <br/>

							<p id="taln-2013-long-029-abs" class="resume">
							<b>Résumé : </b> Le développement d’outils de TAL pour les dialectes de l’arabe se heurte à l’absence de ressources pour ces derniers. Comme conséquence d’une situation de diglossie, il existe une variante de l’arabe, l’arabe moderne standard, pour laquelle de nombreuses ressources ont été développées et ont permis de construire des outils de traitement automatique de la langue. Etant donné la proximité des dialectes de l’arabe, le tunisien dans notre cas, avec l’arabe moderne standard, une voie consiste à réaliser une traduction surfacique du dialecte vers l’arabe moderne standard afin de pouvoir utiliser les outils existants pour l’arabe standard. Nous décrivons dans cet article une architecture pour une telle traduction et nous l’évaluons sur les verbes.
							</p>

							<p id="taln-2013-long-029-key" class="mots_cles">
							<b>Mots clés : </b> dialectes, langues peu dotées, analyse morphologique, traitement automatique de l’arabe
							</p>

					</div>
					

					<div class="article">

						<b>Benoît Sagot, Damien Nouvel, Virginie Mouilleron, Marion Baranes</b>


						<br/>

							<i>Extension dynamique de lexiques morphologiques pour le français à partir d’un flux textuel</i> <br/>

						<a href="actes/taln-2013-long-030.pdf">taln-2013-long-030</a> 
						<a href="bibtex/taln-2013-long-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-030-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-030-key');">mots clés</a> <br/>

							<p id="taln-2013-long-030-abs" class="resume">
							<b>Résumé : </b> L’incomplétude lexicale est un problème récurrent lorsque l’on cherche à traiter le langage naturel dans sa variabilité. Effectivement, il semble aujourd’hui nécessaire de vérifier et compléter régulièrement les lexiques utilisés par les applications qui analysent d’importants volumes de textes. Ceci est plus particulièrement vrai pour les flux textuels en temps réel. Dans ce contexte, notre article présente des solutions dédiées au traitement des mots inconnus d’un lexique. Nous faisons une étude des néologismes (linguistique et sur corpus) et détaillons la mise en oeuvre de modules d’analyse dédiés à leur détection et à l’inférence d’informations (forme de citation, catégorie et classe flexionnelle) à leur sujet. Nous y montrons que nous sommes en mesure, grâce notamment à des modules d’analyse des dérivés et des composés, de proposer en temps réel des entrées pour ajout aux lexiques avec une bonne précision.
							</p>

							<p id="taln-2013-long-030-key" class="mots_cles">
							<b>Mots clés : </b> Néologismes, analyse morphologique, lexiques dynamiques
							</p>

					</div>
					

					<div class="article">

						<b>Damien Nouvel, Jean-Yves Antoine, Nathalie Friburger, Arnaud Soulet</b>


						<br/>

							<i>Fouille de règles d’annotation partielles pour la reconnaissance des entités nommées</i> <br/>

						<a href="actes/taln-2013-long-031.pdf">taln-2013-long-031</a> 
						<a href="bibtex/taln-2013-long-031.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-031-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-031-key');">mots clés</a> <br/>

							<p id="taln-2013-long-031-abs" class="resume">
							<b>Résumé : </b> Ces dernières décennies, l’accroissement des volumes de données a rendu disponible une diversité toujours plus importante de types de contenus échangés (texte, image, audio, vidéo, SMS, tweet, données statistiques, spatiales, etc.). En conséquence, de nouvelles problématiques ont vu le jour, dont la recherche d’information au sein de données potentiellement bruitées. Dans cet article, nous nous penchons sur la reconnaissance d’entités nommées au sein de transcriptions (manuelles ou automatiques) d’émissions radiodiffusées et télévisuelles. À cet effet, nous mettons en oeuvre une approche originale par fouille de données afin d’extraire des motifs, que nous nommons règles d’annotation. Au sein d’un modèle, ces règles réalisent l’annotation automatique de transcriptions. Dans le cadre de la campagne d’évaluation Etape, nous mettons à l’épreuve le système implémenté, mXS, étudions les règles extraites et rapportons les performances du système. Il obtient de bonnes performances, en particulier lorsque les transcriptions sont bruitées.
							</p>

							<p id="taln-2013-long-031-key" class="mots_cles">
							<b>Mots clés : </b> Entités nommées, Fouille de données, Règles d’annotation
							</p>

					</div>
					

					<div class="article">

						<b>Iskandar Keskes, Farah Beanamara, Lamia Hadrich Belguith</b>


						<br/>

							<i>Segmentation de textes arabes en unités discursives minimales</i> <br/>

						<a href="actes/taln-2013-long-032.pdf">taln-2013-long-032</a> 
						<a href="bibtex/taln-2013-long-032.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-032-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-032-key');">mots clés</a> <br/>

							<p id="taln-2013-long-032-abs" class="resume">
							<b>Résumé : </b> La segmentation d’un texte en Unités Discursives Minimales (UDM) a pour but de découper le texte en segments qui ne se chevauchent pas. Ces segments sont ensuite reliés entre eux afin de construire la structure discursive d’un texte. La plupart des approches existantes utilisent une analyse syntaxique extensive. Malheureusement, certaines langues ne disposent pas d&#39;analyseur syntaxique robuste. Dans cet article, nous étudions la faisabilité de la segmentation discursive de textes arabes en nous basant sur une approche d&#39;apprentissage supervisée qui prédit les UDM et les UDM imbriqués. La performance de notre segmentation a été évaluée sur deux genres de corpus : des textes de livres de l’enseignement secondaire et des textes du corpus Arabic Treebank. Nous montrons que la combinaison de traits typographiques, morphologiques et lexicaux permet une bonne reconnaissance des bornes de segments. De plus, nous montrons que l&#39;ajout de traits syntaxiques n’améliore pas les performances de notre segmentation.
							</p>

							<p id="taln-2013-long-032-key" class="mots_cles">
							<b>Mots clés : </b> Segmentation discursive, unité discursive minimale, langue arabe
							</p>

					</div>
					

					<div class="article">

						<b>Thomas Lavergne, Alexandre Allauzen, François Yvon</b>


						<br/>

							<i>Un cadre d’apprentissage intégralement discriminant pour la traduction statistique</i> <br/>

						<a href="actes/taln-2013-long-033.pdf">taln-2013-long-033</a> 
						<a href="bibtex/taln-2013-long-033.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-033-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-033-key');">mots clés</a> <br/>

							<p id="taln-2013-long-033-abs" class="resume">
							<b>Résumé : </b> Une faiblesse des systèmes de traduction statistiques est le caractère ad hoc du processus d’apprentissage, qui repose sur un empilement d’heuristiques et conduit à apprendre des paramètres dont la valeur est sous-optimale. Dans ce travail, nous reformulons la traduction automatique sous la forme familière de l’apprentissage d’un modèle probabiliste structuré utilisant une paramétrisation log-linéaire. Cette entreprise est rendue possible par le développement d’une implantation efficace qui permet en particulier de prendre en compte la présence de variables latentes dans le modèle. Notre approche est comparée, avec succès, avec une approche de l’état de l’art sur la tâche de traduction de données du BTEC pour le couple Français-Anglais.
							</p>

							<p id="taln-2013-long-033-key" class="mots_cles">
							<b>Mots clés : </b> Traduction Automatique, Apprentissage Discriminant
							</p>

					</div>
					

					<div class="article">

						<b>Yue Ma, François Lévy, Adeline Nazarenko</b>


						<br/>

							<i>Annotation sémantique pour des domaines spécialisés et des ontologies riches</i> <br/>

						<a href="actes/taln-2013-long-034.pdf">taln-2013-long-034</a> 
						<a href="bibtex/taln-2013-long-034.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-034-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-034-key');">mots clés</a> <br/>

							<p id="taln-2013-long-034-abs" class="resume">
							<b>Résumé : </b> Explorer et maintenir une documentation technique est une tâche difficile pour laquelle on pourrait bénéficier d’un outillage efficace, à condition que les documents soient annotés sémantiquement. Les annotations doivent être riches, cohérentes, suffisamment spécialisées et s’appuyer sur un modèle sémantique explicite – habituellement une ontologie – qui modélise la sémantique du domaine cible. Il s’avère que les approches d’annotation traditionnelles donnent pour cette tâche des résultats limités. Nous proposons donc une nouvelle approche, l’annotation sémantique statistique basée sur les syntagmes, qui prédit les annotations sémantiques à partir d’un ensemble d’apprentissage réduit. Cette modélisation facilite l’annotation sémantique spécialisée au regard de modèles sémantiques de domaine arbitrairement riches. Nous l’évaluons à l’aide de plusieurs métriques et sur deux textes décrivant des réglementations métier. Notre approche obtient de bons résultats. En particulier, la F-mesure est de l’ordre de 91, 9% et 97, 6% pour la prédiction de l’étiquette et de la position avec différents paramètres. Cela suggère que les annotateurs humains peuvent être fortement aidés pour l’annotation sémantique dans des domaines spécifiques.
							</p>

							<p id="taln-2013-long-034-key" class="mots_cles">
							<b>Mots clés : </b> Annotation sémantique, Ontologie de domaine, Annotation automatique, Analyse sémantique des textes, Méthodes statistiques
							</p>

					</div>
					

					<div class="article">

						<b>Nicolas Foucault, Sophie Rosset, Gilles Adda</b>


						<br/>

							<i>Pré-segmentation de pages web et sélection de documents pertinents en Questions-Réponses</i> <br/>

						<a href="actes/taln-2013-long-035.pdf">taln-2013-long-035</a> 
						<a href="bibtex/taln-2013-long-035.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-035-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-035-key');">mots clés</a> <br/>

							<p id="taln-2013-long-035-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous présentons une méthode de segmentation de pages web en blocs de texte pour la sélection de documents pertinents en questions-réponses. La segmentation des documents se fait préalablement à leur indexation en plus du découpage des segments obtenus en passages au moment de l’extraction des réponses. L’extraction du contenu textuel des pages est faite à l’aide d’un extracteur maison. Nous avons testé deux méthodes de segmentation. L’une segmente les textes extraits des pages web uniformément en blocs de taille fixe, l’autre les segmente par TextTiling (Hearst, 1997) en blocs thématiques de taille variable. Les expériences menées sur un corpus de 500K pages web et un jeu de 309 questions factuelles en français, issus du projet Quaero (Quintard et al., 2010), montrent que la méthode employée tend à améliorer la précision globale (top-10) du système RITEL–QR (Rosset et al., 2008) dans sa tâche.
							</p>

							<p id="taln-2013-long-035-key" class="mots_cles">
							<b>Mots clés : </b> pages web, TextTiling, sélection de documents, questions-réponses, Quaero, Ritel, segmentation textuelle, segmentation thématique
							</p>

					</div>
					

					<div class="article">

						<b>Anne-Laure Ligozat, Cyril Grouin, Anne Garcia-Fernandez, Delphine Bernhard</b>


						<br/>

							<i>Approches à base de fréquences pour la simplification lexicale</i> <br/>

						<a href="actes/taln-2013-long-036.pdf">taln-2013-long-036</a> 
						<a href="bibtex/taln-2013-long-036.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-long-036-abs');">résumé</a>
							<a onclick="toggle('taln-2013-long-036-key');">mots clés</a> <br/>

							<p id="taln-2013-long-036-abs" class="resume">
							<b>Résumé : </b> La simplification lexicale consiste à remplacer des mots ou des phrases par leur équivalent plus simple. Dans cet article, nous présentons trois modèles de simplification lexicale, fondés sur différents critères qui font qu’un mot est plus simple à lire et à comprendre qu’un autre. Nous avons testé différentes tailles de contextes autour du mot étudié : absence de contexte avec un modèle fondé sur des fréquences de termes dans un corpus d’anglais simplifié ; quelques mots de contexte au moyen de probabilités à base de n-grammes issus de données du web ; et le contexte étendu avec un modèle fondé sur les fréquences de cooccurrences.
							</p>

							<p id="taln-2013-long-036-key" class="mots_cles">
							<b>Mots clés : </b> simplification lexicale, fréquence lexicale, modèle de langue
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="court">Articles courts</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Florian Boudin</b>


						<br/>

							<i>TALN Archives : une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue</i> <br/>

						<a href="actes/taln-2013-court-001.pdf">taln-2013-court-001</a> 
						<a href="bibtex/taln-2013-court-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-001-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-001-key');">mots clés</a> <br/>

							<p id="taln-2013-court-001-abs" class="resume">
							<b>Résumé : </b> La recherche scientifique est un processus incrémental. La première étape à effectuer avant de débuter des travaux consiste à réaliser un état de l’art des méthodes existantes. La communauté francophone du Traitement Automatique de la Langue (TAL) produit de nombreuses publications scientifiques qui sont malheureusement dispersées sur différents sites et pour lesquelles aucune méta-donnée n’est disponible. Cet article présente la construction de TALN Archives, une archive numérique francophone des articles de recherche en TAL dont le but est d’offrir un accès simplifié aux différents travaux effectués dans notre domaine. Nous présentons également une analyse du réseau de collaboration construit à partir des méta-données que nous avons extraites et dévoilons l’identité du Kevin Bacon de TALN Archives, i.e. l’auteur le plus central dans le réseau de collaboration.
							</p>

							<p id="taln-2013-court-001-key" class="mots_cles">
							<b>Mots clés : </b> TALN Archives, archive numérique, articles scientifiques
							</p>

					</div>
					

					<div class="article">

						<b>Pierre-Francois Marteau, Gildas Ménier</b>


						<br/>

							<i>Similarités induites par mesure de comparabilité : signification et utilité pour le clustering et l’alignement de textes comparables</i> <br/>

						<a href="actes/taln-2013-court-002.pdf">taln-2013-court-002</a> 
						<a href="bibtex/taln-2013-court-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-002-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-002-key');">mots clés</a> <br/>

							<p id="taln-2013-court-002-abs" class="resume">
							<b>Résumé : </b> En présence de corpus comparables bilingues, nous sommes confrontés à des données qu’il est naturel de plonger dans deux espaces de représentation linguistique distincts, chacun éventuellement muni d’une mesure quantifiable de similarité (ou d’une distance). Dès lors que ces données bilingues sont comparables au sens d’une mesure de comparabilité également calculable (Li et Gaussier, 2010), nous pouvons établir une connexion entre ces deux espaces de représentation linguistique en exploitant une carte d’association pondérée (&#34;mapping&#34;) appréhendée sous la forme d’un graphe bi-directionnel dit de comparabilité. Nous abordons dans cet article les conséquences conceptuelles et pratique d’une telle connexion similarité-comparabilité en développant un algorithme (Hit-ComSim) basé sur sur le principe de similarité induite par la topologie du graphe de comparabilité. Nous essayons de qualifier qualitativement l’intérêt de cet algorithme en considérant quelques expériences préliminaires de clustering de documents comparables bilingues (Français/Anglais) collectés sur des flux RSS.
							</p>

							<p id="taln-2013-court-002-key" class="mots_cles">
							<b>Mots clés : </b> Graphe de comparabilité, Similarités induites, Documents comparables, Clustering
							</p>

					</div>
					

					<div class="article">

						<b>Denis Maurel, Béatrice Bouchou Markhoff</b>


						<br/>

							<i>ProLMF version 1.2. Une ressource libre de noms propres avec des expansions contextuelles</i> <br/>

						<a href="actes/taln-2013-court-003.pdf">taln-2013-court-003</a> 
						<a href="bibtex/taln-2013-court-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-003-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-003-key');">mots clés</a> <br/>

							<p id="taln-2013-court-003-abs" class="resume">
							<b>Résumé : </b> ProLMF est la version LMF de la base lexicale multilingue de noms propres Prolexbase. Disponible librement sur le site du CNRTL, la version 1.2 a été largement améliorée et augmentée par de nouvelles entrées en français, complétées par des expansions contextuelles, et par de petits lexiques en une huitaine de langues.
							</p>

							<p id="taln-2013-court-003-key" class="mots_cles">
							<b>Mots clés : </b> ressource libre, base lexicale multilingue, noms propres, expansions contextuelles, schémas de contextualisation, relations sémantiques, alias, point de vue, Prolexbase
							</p>

					</div>
					

					<div class="article">

						<b>Benjamin Lecouteux, Laurent Besacier</b>


						<br/>

							<i>Vers un décodage guidé pour la traduction automatique</i> <br/>

						<a href="actes/taln-2013-court-004.pdf">taln-2013-court-004</a> 
						<a href="bibtex/taln-2013-court-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-004-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-004-key');">mots clés</a> <br/>

							<p id="taln-2013-court-004-abs" class="resume">
							<b>Résumé : </b> Récemment, le paradigme du décodage guidé a montré un fort potentiel dans le cadre de la reconnaissance automatique de la parole. Le principe est de guider le processus de décodage via l’utilisation de transcriptions auxiliaires. Ce paradigme appliqué à la traduction automatique permet d’envisager de nombreuses applications telles que la combinaison de systèmes, la traduction multi-sources etc. Cet article présente une approche préliminaire de l’application de ce paradigme à la traduction automatique (TA). Nous proposons d’enrichir le modèle log-linéaire d’un système primaire de TA avec des mesures de distance relatives à des systèmes de TA auxiliaires. Les premiers résultats obtenus sur la tâche de traduction Français/Anglais issue de la campagne d’évaluation WMT 2011 montrent le potentiel du décodage guidé.
							</p>

							<p id="taln-2013-court-004-key" class="mots_cles">
							<b>Mots clés : </b> Décodage guidé, traduction automatique, combinaison de systèmes
							</p>

					</div>
					

					<div class="article">

						<b>Johanna Gerlach, Victoria Porro, Pierrette Bouillon, Sabine Lehmann</b>


						<br/>

							<i>La La préédition avec des règles peu coûteuses, utile pour la TA statistique des forums ?</i> <br/>

						<a href="actes/taln-2013-court-005.pdf">taln-2013-court-005</a> 
						<a href="bibtex/taln-2013-court-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-005-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-005-key');">mots clés</a> <br/>

							<p id="taln-2013-court-005-abs" class="resume">
							<b>Résumé : </b> Cet article s’intéresse à la traduction automatique statistique des forums, dans le cadre du projet européen ACCEPT (« Automated Community Content Editing Portal »). Nous montrons qu’il est possible d’écrire des règles de préédition peu coûteuses sur le plan des ressources linguistiques et applicables sans trop d’effort avec un impact très significatif sur la traduction automatique (TA) statistique, sans avoir à modifier le système de TA. Nous décrivons la méthodologie proposée pour écrire les règles de préédition et les évaluer, ainsi que les résultats obtenus par type de règles.
							</p>

							<p id="taln-2013-court-005-key" class="mots_cles">
							<b>Mots clés : </b> préédition, langage contrôlé, traduction statistique, forums
							</p>

					</div>
					

					<div class="article">

						<b>Ludovic Hamon, Sylvie Gibet, Sabah Boustila</b>


						<br/>

							<i>Édition interactive d’énoncés en langue des signes française dédiée aux avatars signeurs</i> <br/>

						<a href="actes/taln-2013-court-006.pdf">taln-2013-court-006</a> 
						<a href="bibtex/taln-2013-court-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-006-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-006-key');">mots clés</a> <br/>

							<p id="taln-2013-court-006-abs" class="resume">
							<b>Résumé : </b> Les avatars signeurs en Langue des Signes Française (LSF) sont de plus en plus utilisés en tant qu’interface de communication à destination de la communauté sourde. L’un des critères d’acceptation de ces avatars est l’aspect naturel et réaliste des gestes produits. Par conséquent, des méthodes de synthèse de gestes ont été élaborées à l’aide de corpus de mouvements capturés et annotés provenant d’un signeur réel. Néanmoins, l’enrichissement d’un tel corpus, en faisant fi des séances de captures supplémentaires, demeure une problématique certaine. De plus, l’application automatique d’opérations sur ces mouvements (e.g. concaténation, mélange, etc.) ne garantit pas la consistance sémantique du geste résultant. Une alternative est d’insérer l’opérateur humain dans la boucle de construction des énoncés en LSF. Dans cette optique, cet article propose un premier système interactif d’édition de gestes en LSF, basé &#34;données capturées&#34; et dédié aux avatars signeurs.
							</p>

							<p id="taln-2013-court-006-key" class="mots_cles">
							<b>Mots clés : </b> Langue des Signes Française, édition, geste, base de données sémantiques, signeur virtuel, interaction
							</p>

					</div>
					

					<div class="article">

						<b>Judith Muzerelle, Anaïs Lefeuvre, Jean-Yves Antoine, Emmanuel Schang, Denis Maurel, Jeanne Villaneau, Iris Eshkol</b>


						<br/>

							<i>ANCOR, premier corpus de français parlé d’envergure annoté en coréférence et distribué librement</i> <br/>

						<a href="actes/taln-2013-court-007.pdf">taln-2013-court-007</a> 
						<a href="bibtex/taln-2013-court-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-007-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-007-key');">mots clés</a> <br/>

							<p id="taln-2013-court-007-abs" class="resume">
							<b>Résumé : </b> Cet article présente la réalisation d’ANCOR, qui constitue par son envergure (453 000 mots) le premier corpus francophone annoté en anaphores et coréférences permettant le développement d’approches centrées sur les données pour la résolution des anaphores et autres traitements de la coréférence. L’annotation a été réalisée sur trois corpus de parole conversationnelle (Accueil_UBS, OTG et ESLO) qui le destinent plus particulièrement au traitement du langage parlé. En l’absence d’équivalent pour le langage écrit, il est toutefois susceptible d’intéresser l’ensemble de la communauté TAL. Par ailleurs, le schéma d’annotation retenu est suffisamment riche pour permettre des études en linguistique de corpus. Le corpus sera diffusé librement à la mi-2013 sous licence Creative Commons BY-NC-SA. Cet article se concentre sur sa mise en oeuvre et décrit brièvement quelques résultats obtenus sur la partie déjà annotée de la ressource.
							</p>

							<p id="taln-2013-court-007-key" class="mots_cles">
							<b>Mots clés : </b> Corpus, annotation, coréférence, anaphore, parole conversationnelle
							</p>

					</div>
					

					<div class="article">

						<b>Elizaveta Loginova-Clouet, Béatrice Daille</b>


						<br/>

							<i>Segmentation Multilingue des Mots Composés</i> <br/>

						<a href="actes/taln-2013-court-008.pdf">taln-2013-court-008</a> 
						<a href="bibtex/taln-2013-court-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-008-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-008-key');">mots clés</a> <br/>

							<p id="taln-2013-court-008-abs" class="resume">
							<b>Résumé : </b> La composition est un phénomène fréquent dans plusieurs langues, surtout dans des langues ayant une morphologie riche. Le traitement des mots composés est un défi pour les systèmes de TAL car pour la plupart, ils ne sont pas présents dans les lexiques. Dans cet article, nous présentons une méthode de segmentation des composés qui combine des caractéristiques indépendantes de la langue (mesure de similarité, données du corpus) avec des règles de transformation sur les frontières des composants spécifiques à une langue. Nos expériences de segmentation de termes composés allemands et russes montrent une exactitude jusqu’à 95 % pour l’allemand et jusqu’à 91 % pour le russe. Nous constatons que l’utilisation de corpus spécialisés relevant du même domaine que les composés améliore la qualité de segmentation.
							</p>

							<p id="taln-2013-court-008-key" class="mots_cles">
							<b>Mots clés : </b> segmentation des mots composés, outil multilingue, mesure de similarité, règles de transformation des composants, corpus spécialisés
							</p>

					</div>
					

					<div class="article">

						<b>Ying Zhang, Mathieu Mangeot</b>


						<br/>

							<i>Gestion des terminologies riches : l&#39;exemple des acronymes</i> <br/>

						<a href="actes/taln-2013-court-009.pdf">taln-2013-court-009</a> 
						<a href="bibtex/taln-2013-court-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-009-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-009-key');">mots clés</a> <br/>

							<p id="taln-2013-court-009-abs" class="resume">
							<b>Résumé : </b> La gestion des terminologies pose encore des problèmes, en particulier pour des constructions complexes comme les acronymes. Dans cet article, nous proposons une solution en reliant plusieurs termes différents à un seul référent via les notions de pivot et de prolexème. Ces notions permettent par exemple de faire le lien entre plusieurs termes qui désignent un même et unique référent : Nations Unies, ONU, Organisation des Nations Unies et onusien. Il existe Jibiki, une plate-forme générique de gestion de bases lexicales permettant de gérer n&#39;importe quel type de structure (macro et microstructure). Nous avons implémenté une nouvelle macrostructure de ProAxie dans la plate-forme Jibiki pour réaliser la gestion des acronymes.
							</p>

							<p id="taln-2013-court-009-key" class="mots_cles">
							<b>Mots clés : </b> base lexicale multilingue, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolèxeme
							</p>

					</div>
					

					<div class="article">

						<b>Marcos Zampieri, Binyam Gebrekidan Gebre, Sascha Diwersy</b>


						<br/>

							<i>Ngrammes et Traits Morphosyntaxiques pour la Identification de Variétés de l’Espagnol</i> <br/>

						<a href="actes/taln-2013-court-010.pdf">taln-2013-court-010</a> 
						<a href="bibtex/taln-2013-court-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-010-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-010-key');">mots clés</a> <br/>

							<p id="taln-2013-court-010-abs" class="resume">
							<b>Résumé : </b> Notre article présente expérimentations portant sur la classification supervisée de variétés nationales de l’espagnol. Outre les approches classiques, basées sur l’utilisation de ngrammes de caractères ou de mots, nous avons testé des modèles calculés selon des traits morphosyntaxiques, l’objectif étant de vérifier dans quelle mesure il est possible de parvenir à une classification automatique des variétés d’une langue en s’appuyant uniquement sur des descripteurs grammaticaux. Les calculs ont été effectués sur la base d’un corpus de textes journalistiques de quatre pays hispanophones (Espagne, Argentine, Mexique et Pérou).
							</p>

							<p id="taln-2013-court-010-key" class="mots_cles">
							<b>Mots clés : </b> classification automatique, ngrammes, espagnol, variétés nationales
							</p>

					</div>
					

					<div class="article">

						<b>Amel Fraisse, Patrick Paroubek, Gil Francopoulo</b>


						<br/>

							<i>L’apport des Entités Nommées pour la classification des opinions minoritaires</i> <br/>

						<a href="actes/taln-2013-court-011.pdf">taln-2013-court-011</a> 
						<a href="bibtex/taln-2013-court-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-011-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-011-key');">mots clés</a> <br/>

							<p id="taln-2013-court-011-abs" class="resume">
							<b>Résumé : </b> La majeure partie des travaux en fouille d’opinion et en analyse de sentiment concerne le classement des opinions majoritaires. Les méthodes d’apprentissage supervisé à base de ngrammes sont souvent employées. Elles ont l’inconvénient d’avoir un biais en faveur des opinions majoritaires si on les utilise de manière classique. En fait la présence d’un terme particulier, fortement associé à la cible de l’opinion dans un document peut parfois suffire à faire basculer le classement de ce document dans la classe de ceux qui expriment une opinion majoritaire sur la cible. C’est un phénomène positif pour l’exactitude globale du classifieur, mais les documents exprimant des opinions minoritaires sont souvent mal classés. Ce point est un problème dans le cas où l’on s’intéresse à la détection des signaux faibles (détection de rumeur) ou pour l’anticipation de renversement de tendance. Nous proposons dans cet article d’améliorer la classification des opinions minoritaires en prenant en compte les Entités Nommées dans le calcul de pondération destiné à corriger le biais en faveur des opinions majoritaires.
							</p>

							<p id="taln-2013-court-011-key" class="mots_cles">
							<b>Mots clés : </b> Fouille d’opinions, Opinion minoritaires, Entités Nommées, Apprentissage, N-grammes, Pondération
							</p>

					</div>
					

					<div class="article">

						<b>Gemma Bel-Enguix, Michael Zock</b>


						<br/>

							<i>Trouver les mots dans un simple réseau de co-occurrences</i> <br/>

						<a href="actes/taln-2013-court-012.pdf">taln-2013-court-012</a> 
						<a href="bibtex/taln-2013-court-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-012-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-012-key');">mots clés</a> <br/>

							<p id="taln-2013-court-012-abs" class="resume">
							<b>Résumé : </b> Au cours des deux dernières décennies des psychologues et des linguistes informaticiens ont essayé de modéliser l&#39;accès lexical en construisant des simulations ou des ressources. Cependant, parmi ces chercheurs, pratiquement personne n&#39;a vraiment cherché à améliorer la navigation dans des &#39;dictionnaires électroniques destinés aux producteurs de langue&#39;. Pourtant, beaucoup de travaux ont été consacrés à l&#39;étude du phénomène du mot sur le bout de la langue et à la construction de réseaux lexicaux. Par ailleurs, vu les progrès réalisés en neurosciences et dans le domaine des réseaux complexes, on pourrait être tenté de construire un simulacre du dictionnaire mental, ou, à défaut une ressource destinée aux producteurs de langue (écrivains, conférenciers). Nous sommes restreints en construisant un réseau de co-occurrences à partir des résumés de Wikipedia, le but étant de vérifier jusqu&#39;où l&#39;on pouvait pousser une telle ressource pour trouver un mot, sachant que la ressource ne contient pas de liens sémantiques, car le réseau est construit de manière automatique et à partir de textes non-annotés.
							</p>

							<p id="taln-2013-court-012-key" class="mots_cles">
							<b>Mots clés : </b> accès lexical, anomie, mot sur le bout de la langue, réseaux lexicaux
							</p>

					</div>
					

					<div class="article">

						<b>Guy Perrier</b>


						<br/>

							<i>Analyse statique des interactions entre structures élémentaires d’une grammaire</i> <br/>

						<a href="actes/taln-2013-court-013.pdf">taln-2013-court-013</a> 
						<a href="bibtex/taln-2013-court-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-013-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-013-key');">mots clés</a> <br/>

							<p id="taln-2013-court-013-abs" class="resume">
							<b>Résumé : </b> Nous nous intéressons ici à la construction semi-automatique de grammaires computationnelles et à leur utilisation pour l’analyse syntaxique. Nous considérons des grammaires lexicalisées dont les structures élémentaires sont des arbres, sous-spécifiés ou pas. Nous présentons un algorithme qui vise à prévoir l’ensemble des arbres élémentaires attachés aux mots qui peuvent s’intercaler entre deux mots donnés d’une phrase, dont on sait que les arbres élémentaires associées sont des compagnons, c’est-à-dire qu’ils interagiront nécessairement dans la composition syntaxique de la phrase.
							</p>

							<p id="taln-2013-court-013-key" class="mots_cles">
							<b>Mots clés : </b> grammaire lexicalisée, grammaire d’interaction, construction de grammaires
							</p>

					</div>
					

					<div class="article">

						<b>Eric Charton, Michel Gagnon, Ludovic Jean-Louis</b>


						<br/>

							<i>Influence des annotations sémantiques sur un système de détection de coréférence à base de perceptron multi-couches</i> <br/>

						<a href="actes/taln-2013-court-014.pdf">taln-2013-court-014</a> 
						<a href="bibtex/taln-2013-court-014.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-014-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-014-key');">mots clés</a> <br/>

							<p id="taln-2013-court-014-abs" class="resume">
							<b>Résumé : </b> La série de campagnes d’évaluation CoNLL-2011/2012 a permis de comparer diverses propositions d’architectures de systèmes de détection de co-références. Cet article décrit le système de résolution de coréférence Poly-co développé dans le cadre de la campagne d’évaluation CoNLL-2011 et évalue son potentiel d’amélioration en introduisant des propriétés sémantiques dans son modèle de détection. Notre système s’appuie sur un classifieur perceptron multi-couches. Nous décrivons les heuristiques utilisées pour la sélection des paires de mentions candidates, ainsi que l’approche de sélection des traits caractéristiques que nous avons utilisée lors de la campagne CoNLL-2011. Nous introduisons ensuite un trait sémantique complémentaire et évaluons son influence sur les performances du système.
							</p>

							<p id="taln-2013-court-014-key" class="mots_cles">
							<b>Mots clés : </b> Coréférence, Perceptron multi-couches
							</p>

					</div>
					

					<div class="article">

						<b>Fatiha Sadat, Emad Mohamed</b>


						<br/>

							<i>Traduction automatique statistique pour l’arabe-français améliorée par le prétraitement et l’analyse de la langue</i> <br/>

						<a href="actes/taln-2013-court-015.pdf">taln-2013-court-015</a> 
						<a href="bibtex/taln-2013-court-015.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-015-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-015-key');">mots clés</a> <br/>

							<p id="taln-2013-court-015-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous nous intéressons au prétraitement de la langue arabe comme langue source à des fins de traduction automatique statistique. Nous présentons une étude sur la traduction automatique statistique basée sur les syntagmes, pour la paire de langues arabe-français utilisant le décodeur Moses ainsi que d’autres outils de base. Les propriétés morphologiques et syntaxiques de la langue arabe sont complexes, ce qui rend cette langue difficile à maîtriser dans le domaine du TALN. Aussi, les performances d’un système de traduction statistique dépendent considérablement de la quantité et de la qualité des corpus d’apprentissage. Dans cette étude, nous montrons qu’un prétraitement basé sur les mots de la langue source (arabe) et l’introduction de quelques règles linguistiques par rapport à la syntaxe de la langue cible (français), permet d’obtenir des améliorations du score BLEU. Cette amélioration est réalisée sans augmenter la quantité des corpus d’apprentissage.
							</p>

							<p id="taln-2013-court-015-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique statistique, traduction arabe-français, pré-traitement de corpus, morphologie de l’Arabe
							</p>

					</div>
					

					<div class="article">

						<b>Bruno Guillaume, Karën Fort</b>


						<br/>

							<i>Expériences de formalisation d’un guide d’annotation : vers l’annotation agile assistée</i> <br/>

						<a href="actes/taln-2013-court-016.pdf">taln-2013-court-016</a> 
						<a href="bibtex/taln-2013-court-016.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-016-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-016-key');">mots clés</a> <br/>

							<p id="taln-2013-court-016-abs" class="resume">
							<b>Résumé : </b> Nous proposons dans cet article une méthodologie, qui s’inspire du développement agile et qui permettrait d’assister la préparation d’une campagne d’annotation. Le principe consiste à formaliser au maximum les instructions contenues dans le guide d’annotation afin de vérifier automatiquement si le corpus en construction est cohérent avec le guide en cours d’écriture. Pour exprimer la partie formelle du guide, nous utilisons la réécriture de graphes, qui permet de décrire par des motifs les constructions définies. Cette formalisation permet de repérer les constructions prévues par le guide et, par contraste, celles qui ne sont pas cohérentes avec le guide. En cas d’incohérence, un expert peut soit corriger l’annotation, soit mettre à jour le guide et relancer le processus.
							</p>

							<p id="taln-2013-court-016-key" class="mots_cles">
							<b>Mots clés : </b> annotation, guide d’annotation, annotation agile, réécriture de graphes
							</p>

					</div>
					

					<div class="article">

						<b>Catherine Dominguès, Iris Eshkol-Taravella</b>


						<br/>

							<i>Repérer des toponymes dans des titres de cartes topographiques</i> <br/>

						<a href="actes/taln-2013-court-017.pdf">taln-2013-court-017</a> 
						<a href="bibtex/taln-2013-court-017.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-017-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-017-key');">mots clés</a> <br/>

							<p id="taln-2013-court-017-abs" class="resume">
							<b>Résumé : </b> Les titres de cartes topographiques personnalisées composent un corpus spécifique caractérisé par des variations orthographiques et un nombre élevé de désignations de lieux. L&#39;article présente le repérage des toponymes dans ces titres. Ce repérage est fondé sur l&#39;utilisation de BDNyme, la base de données de toponymes géoréférencés de l&#39;IGN, et sur une analyse de surface à l&#39;aide de patrons. La méthode proposée élargit la définition du toponyme pour tenir compte de la nature du corpus et des données qu&#39;il contient. Elle se décompose en trois étapes successives qui tirent parti du contexte extralinguistique de géoréférencement des toponymes et du contexte linguistique. Une quatrième étape qui ne retient pas le géoréférencement est aussi étudiée. Le balisage et le typage des toponymes permettent de mettre en avant d&#39;une part la diversité des désignations de lieux et d&#39;autre part leurs variations d&#39;écriture. La méthode est évaluée (rappel, précision, F-mesure) et les erreurs analysées.
							</p>

							<p id="taln-2013-court-017-key" class="mots_cles">
							<b>Mots clés : </b> toponyme, information spatiale, écriture des toponymes, BDNyme, ressource lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Pierre Zweigenbaum, Xavier TANNIER</b>


						<br/>

							<i>Extraction des relations temporelles entre événements médicaux dans des comptes rendus hospitaliers</i> <br/>

						<a href="actes/taln-2013-court-018.pdf">taln-2013-court-018</a> 
						<a href="bibtex/taln-2013-court-018.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-018-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-018-key');">mots clés</a> <br/>

							<p id="taln-2013-court-018-abs" class="resume">
							<b>Résumé : </b> Le défi i2b2/VA 2012 était dédié à la détection de relations temporelles entre événements et expressions temporelles dans des comptes rendus hospitaliers en anglais. Les situations considérées étaient beaucoup plus variées que dans les défis TempEval. Nous avons donc axé notre travail sur un examen systématique de 57 situations différentes et de leur importance dans le corpus d’apprentissage en utilisant un oracle, et avons déterminé empiriquement le classifieur qui se comportait le mieux dans chaque situation, atteignant ainsi une F-mesure globale de 0,623.
							</p>

							<p id="taln-2013-court-018-key" class="mots_cles">
							<b>Mots clés : </b> extraction d’information, événements médicaux, relations temporelles, médecine
							</p>

					</div>
					

					<div class="article">

						<b>Nikola Tulechki, Ludovic Tanguy</b>


						<br/>

							<i>Similarité de second ordre pour l’exploration de bases textuelles multilingues</i> <br/>

						<a href="actes/taln-2013-court-019.pdf">taln-2013-court-019</a> 
						<a href="bibtex/taln-2013-court-019.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-019-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-019-key');">mots clés</a> <br/>

							<p id="taln-2013-court-019-abs" class="resume">
							<b>Résumé : </b> Cet article décrit l’utilisation de la technique de similarité de second ordre pour l’identification de textes semblables au sein d’une base de rapports d’incidents aéronautiques mélangeant les langues française et anglaise. L’objectif du système est, pour un document donné, de retrouver des documents au contenu similaire quelle que soit leur langue. Nous utilisons un corpus bilingue aligné de rapports d’accidents aéronautiques pour construire des paires de pivots et indexons les documents avec des vecteurs de similarités, tels que chaque coordonnée correspond au score de similarité entre un document dans une langue donnée et la partie du pivot de la même langue. Nous évaluons les performances du système sur un volumineux corpus de rapports d’incidents aéronautiques pour lesquels nous disposons de traductions. Les résultats sont prometteurs et valident la technique.
							</p>

							<p id="taln-2013-court-019-key" class="mots_cles">
							<b>Mots clés : </b> similarité de second ordre, multilingue, ESA
							</p>

					</div>
					

					<div class="article">

						<b>François-Régis Chaumartin</b>


						<br/>

							<i>Apprentissage d’une classification thématique générique et cross-langue à partir des catégories de la Wikipédia</i> <br/>

						<a href="actes/taln-2013-court-020.pdf">taln-2013-court-020</a> 
						<a href="bibtex/taln-2013-court-020.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-020-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-020-key');">mots clés</a> <br/>

							<p id="taln-2013-court-020-abs" class="resume">
							<b>Résumé : </b> La catégorisation de textes nécessite généralement un investissement important en amont, avec une adaptation de domaine. L’approche que nous proposons ici permet d’associer finement à un texte tout-venant écrit dans une langue donnée, un graphe de catégories de la Wikipédia dans cette langue. L’utilisation de l’index inter-langues de l’encyclopédie en ligne permet de plus d’obtenir un sous-ensemble de ce graphe dans la plupart des autres langues.
							</p>

							<p id="taln-2013-court-020-key" class="mots_cles">
							<b>Mots clés : </b> catégorisation, apprentissage, recherche d’information, Wikipédia, graphes
							</p>

					</div>
					

					<div class="article">

						<b>Nadia Okinina, Damien Nouvel, Nathalie Friburger, Jean-Yves Antoine</b>


						<br/>

							<i>Apprentissage supervisé sur ressources encyclopédiques pour l’enrichissement d’un lexique de noms propres destiné à la reconnaissance des entités nommées</i> <br/>

						<a href="actes/taln-2013-court-021.pdf">taln-2013-court-021</a> 
						<a href="bibtex/taln-2013-court-021.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-021-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-021-key');">mots clés</a> <br/>

							<p id="taln-2013-court-021-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthode hybride d’enrichissement d’un lexique de noms propres à partir de la base encyclopédique en ligne Wikipedia. Une des particularités de cette recherche est de viser l’enrichissement d’une ressource existante (Prolexbase) très contrôlée décrivant finement les noms propres. A la différence d’autres travaux destinés à la reconnaissance des entités nommées, notre objectif est donc de réaliser un enrichissement automatique de qualité. Notre approche repose sur l’utilisation en pipe-line de règles déterministes basées sur certaines informations DBpedia et d’une catégorisation supervisée à base de classifieur SVM. Nos résultats montrent qu’il est ainsi possible d’enrichir un lexique de noms propres avec une très bonne précision.
							</p>

							<p id="taln-2013-court-021-key" class="mots_cles">
							<b>Mots clés : </b> reconnaissance des entités nommées, lexique de nom propre, enrichissement automatique de lexique, Wikipedia, règles, classification supervisée, machine à vecteurs de support, SVM
							</p>

					</div>
					

					<div class="article">

						<b>Patrick Paroubek, Munshi Asadullah, Anne Vilnat</b>


						<br/>

							<i>Convertir des analyses syntaxiques en dépendances vers les relations fonctionnelles PASSAGE</i> <br/>

						<a href="actes/taln-2013-court-022.pdf">taln-2013-court-022</a> 
						<a href="bibtex/taln-2013-court-022.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-022-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-022-key');">mots clés</a> <br/>

							<p id="taln-2013-court-022-abs" class="resume">
							<b>Résumé : </b> Nous présentons ici les premiers travaux concernant l’établissement d’une passerelle bidirectionnelle entre d’une, part les schémas d’annotation syntaxique en dépendances qui ont été définis pour convertir les annotations du French Treebank en arbres de dépendances de surface pour l’analyseur syntaxique Bonsai, et d’autre part le formalisme d’annotation PASSAGE développé initialement pour servir de support à des campagnes d’évaluation ouvertes en mode objectif quantitatif boîte-noire pour l’analyse syntaxique du français.
							</p>

							<p id="taln-2013-court-022-key" class="mots_cles">
							<b>Mots clés : </b> Analyse Syntaxique, Corpus arboré, Dependances, DepFTB, ConLL, PASSAGE
							</p>

					</div>
					

					<div class="article">

						<b>Sharid Loáiciga</b>


						<br/>

							<i>Résolution d’anaphores et traitement des pronoms en traduction automatique à base de règles</i> <br/>

						<a href="actes/taln-2013-court-023.pdf">taln-2013-court-023</a> 
						<a href="bibtex/taln-2013-court-023.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-023-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-023-key');">mots clés</a> <br/>

							<p id="taln-2013-court-023-abs" class="resume">
							<b>Résumé : </b> La traduction des pronoms est l’un des problèmes actuels majeurs en traduction automatique. Étant donné que les pronoms ne transmettent pas assez de contenu sémantique en euxmêmes, leur traitement automatique implique la résolution des anaphores. La recherche en résolution des anaphores s’intéresse à établir le lien entre les entités sans contenu lexical (potentiellement des syntagmes nominaux et pronoms) et leurs référents dans le texte. Dans cet article, nous mettons en oeuvre un premier prototype d’une méthode inspirée de la théorie du liage chomskyenne pour l’interprétation des pronoms dans le but d’améliorer la traduction des pronoms personnels entre l’espagnol et le français.
							</p>

							<p id="taln-2013-court-023-key" class="mots_cles">
							<b>Mots clés : </b> Résolution d’anaphores, traduction automatique à base de règles, sujets nuls
							</p>

					</div>
					

					<div class="article">

						<b>Frederik Cailliau, Ariane Cavet, Clément de Groc, Claude de Loupy</b>


						<br/>

							<i>Lexiques de corpus comparables et recherche d’information multilingue</i> <br/>

						<a href="actes/taln-2013-court-024.pdf">taln-2013-court-024</a> 
						<a href="bibtex/taln-2013-court-024.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-024-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-024-key');">mots clés</a> <br/>

							<p id="taln-2013-court-024-abs" class="resume">
							<b>Résumé : </b> Nous évaluons l’utilité de trois lexiques bilingues dans un cadre de recherche interlingue français vers anglais sur le corpus CLEF. Le premier correspond à un dictionnaire qui couvre le corpus, alors que les deux autres ont été construits automatiquement à partir des sous-ensembles français et anglais de CLEF, en les considérant comme des corpus comparables. L’un contient des mots simples, alors que le deuxième ne contient que des termes complexes. Les lexiques sont intégrés dans des interfaces différentes dont les performances de recherche interlingue sont évaluées par 5 utilisateurs sur 15 thèmes de recherche CLEF. Les meilleurs résultats sont obtenus en intégrant le lexique de mots simples généré à partir des corpus comparables dans une interface proposant les cinq « meilleures » traductions pour chaque mot de la requête.
							</p>

							<p id="taln-2013-court-024-key" class="mots_cles">
							<b>Mots clés : </b> recherche d’information multilingue, corpus comparables, lexiques multilingues
							</p>

					</div>
					

					<div class="article">

						<b>Philippe Suignard, Sofiane Kerroua</b>


						<br/>

							<i>Utilisation de contextes pour la correction automatique ou semi-automatique de réclamations clients</i> <br/>

						<a href="actes/taln-2013-court-025.pdf">taln-2013-court-025</a> 
						<a href="bibtex/taln-2013-court-025.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-025-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-025-key');">mots clés</a> <br/>

							<p id="taln-2013-court-025-abs" class="resume">
							<b>Résumé : </b> Cet article présente deux méthodes permettant de corriger des réclamations contenant des erreurs rédactionnelles, en s’appuyant sur le graphe des voisins orthographiques et contextuels. Ce graphe est constitué des formes ou mots trouvés dans un corpus d’apprentissage. Un lien entre deux formes traduit le fait que les deux formes se « ressemblent » et partagent des contextes similaires. La première méthode est semi-automatique et consiste à produire un dictionnaire de substitution à partir de ce graphe. La seconde méthode, plus ambitieuse, est entièrement automatisée. Elle s’appuie sur les contextes pour déterminer à quel mot correspond telle forme abrégée ou erronée. Les résultats ainsi obtenus permettent d’améliorer le processus déjà existant de constitution d’un dictionnaire de substitution mis en place au sein d’EDF.
							</p>

							<p id="taln-2013-court-025-key" class="mots_cles">
							<b>Mots clés : </b> Correction automatique, analyse distributionnelle, graphe, contexte
							</p>

					</div>
					

					<div class="article">

						<b>Luis Adrián Cabrera-Diego, Juan-Manuel Torres-Moreno, Marc El-Bèze</b>


						<br/>

							<i>SegCV : traitement efficace de CV avec analyse et correction d’erreurs</i> <br/>

						<a href="actes/taln-2013-court-026.pdf">taln-2013-court-026</a> 
						<a href="bibtex/taln-2013-court-026.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-026-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-026-key');">mots clés</a> <br/>

							<p id="taln-2013-court-026-abs" class="resume">
							<b>Résumé : </b> Le marché d’offres d’emploi et des candidatures sur Internet a connu, ces derniers temps, une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de textes libres) intraitables manuellement. Les CV sont dans des formats très divers : .pdf, .doc, .dvi, .ps, etc., ce qui peut provoquer des erreurs lors de la conversion en texte plein. Nous proposons SegCV, un système qui a pour but l’analyse automatique des CV des candidats. Dans cet article, nous présentons des algorithmes reposant sur une analyse de surface, afin de segmenter les CV de manière précise. Nous avons évalué la segmentation automatique selon des corpus de référence que nous avons constitués. Les expériences préliminaires réalisées sur une grande collection de CV en français avec correction du bruit montrent de bons résultats en précision, rappel et F-Score.
							</p>

							<p id="taln-2013-court-026-key" class="mots_cles">
							<b>Mots clés : </b> RI, Ressources humaines, traitement de CV, Modèle à base de règles
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Valère Cossu, Juan-Manuel Torres-Moreno, Marc El-Bèze</b>


						<br/>

							<i>Recherche et utilisation d&#39;entités nommées conceptuelles dans une tâche de catégorisation</i> <br/>

						<a href="actes/taln-2013-court-027.pdf">taln-2013-court-027</a> 
						<a href="bibtex/taln-2013-court-027.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-027-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-027-key');">mots clés</a> <br/>

							<p id="taln-2013-court-027-abs" class="resume">
							<b>Résumé : </b> Les recherches présentées sont directement liées aux travaux menés pour résoudre les problèmes de catégorisation automatique de texte. Les mots porteurs d’opinions jouent un rôle important pour déterminer l’orientation du message. Mais il est essentiel de pouvoir identifier les cibles auxquelles ils se rapportent pour en contextualiser la portée. L’analyse peut également être menée dans l’autre sens, on cherchant dans le contexte d’une cible détectée les termes polarisés. Une première étape d’apprentissage depuis des données permet d&#39;obtenir automatiquement les marqueurs de polarité les plus importants. A partir de cette base, nous cherchons les cibles qui apparaissent le plus fréquemment à proximité de ces marqueurs d&#39;opinions. Ensuite, nous construisons un ensemble de couples (marqueur de polarité, cible) pour montrer qu’en s’appuyant sur ces couples, on arrive à expliquer plus finement les prises de positions tout en maintenant (voire améliorant) le niveau de performance du classifieur.
							</p>

							<p id="taln-2013-court-027-key" class="mots_cles">
							<b>Mots clés : </b> Fouille d’opinion, Marqueurs de polarité, Reconnaissance d’entités nommées
							</p>

					</div>
					

					<div class="article">

						<b>Guillaume Wisniewski, Anil Kumar Singh, Natalia Segal, François Yvon</b>


						<br/>

							<i>Un corpus d’erreurs de traduction</i> <br/>

						<a href="actes/taln-2013-court-028.pdf">taln-2013-court-028</a> 
						<a href="bibtex/taln-2013-court-028.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-028-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-028-key');">mots clés</a> <br/>

							<p id="taln-2013-court-028-abs" class="resume">
							<b>Résumé : </b> Avec le développement de la post-édition, de plus en plus de corpus contenant des corrections de traductions sont disponibles. Ce travail présente un corpus de corrections d’erreurs de traduction collecté dans le cadre du projet ANR/TRACE et illustre les différents types d’analyses auxquels il peut servir. Nous nous intéresserons notamment à la détection des erreurs fréquentes et à l’analyse de la variabilité des post-éditions.
							</p>

							<p id="taln-2013-court-028-key" class="mots_cles">
							<b>Mots clés : </b> Traduction automatique, Analyse d’erreur, Post-édiition
							</p>

					</div>
					

					<div class="article">

						<b>Samira Walha Ellouze, Maher Jaoua, Lamia Hadrich Belguith</b>


						<br/>

							<i>Une méthode d’évaluation des résumés basée sur la combinaison de métriques automatiques et de complexité textuelle</i> <br/>

						<a href="actes/taln-2013-court-029.pdf">taln-2013-court-029</a> 
						<a href="bibtex/taln-2013-court-029.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-029-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-029-key');">mots clés</a> <br/>

							<p id="taln-2013-court-029-abs" class="resume">
							<b>Résumé : </b> Cet article présente une méthode automatique d’évaluation du contenu des résumés automatiques. La méthode proposée est basée sur une combinaison de caractéristiques englobant des scores de contenu et d’autres de complexité textuelle et ce en s’appuyant sur une technique d’apprentissage, à savoir la régression linéaire. L’objectif de cette combinaison consiste à prédire le score manuel PYRAMID à partir des caractéristiques utilisées. Afin d’évaluer la méthode présentée, nous nous sommes intéressés à deux niveaux de granularité d’évaluation : la première est qualifiée de Micro-évaluation et propose l’évaluation de chaque résumé, alors que la deuxième est une Macro-évaluation et s’applique au niveau de chaque système.
							</p>

							<p id="taln-2013-court-029-key" class="mots_cles">
							<b>Mots clés : </b> Evaluation intrinsèque, évaluation du contenu, résumé automatique, complexité textuelle, régression linéaire
							</p>

					</div>
					

					<div class="article">

						<b>Abdessalam Bouchekif, Géraldine Damnati, Delphine Charlet</b>


						<br/>

							<i>Segmentation thématique : processus itératif de pondération intra-contenu</i> <br/>

						<a href="actes/taln-2013-court-030.pdf">taln-2013-court-030</a> 
						<a href="bibtex/taln-2013-court-030.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-030-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-030-key');">mots clés</a> <br/>

							<p id="taln-2013-court-030-abs" class="resume">
							<b>Résumé : </b> Dans cet article, nous nous intéressons à la segmentation thématique d’émissions télévisées exploitant la cohésion lexicale. Le but est d’étudier une approche générique, reposant uniquement sur la transcription automatique sans aucune information externe ni aucune information structurelle sur le contenu traité. L’étude porte plus particulièrement sur le mécanisme de pondération des mots utilisés lors du calcul de la cohésion lexicale. Les poids TF-IDF sont estimés à partir du contenu lui-même, qui est considéré comme une collection de documents mono-thème. Nous proposons une approche itérative, intégrée à un algorithme de segmentation, visant à raffiner la partition du contenu en documents pour l’estimation de la pondération. La segmentation obtenue à une itération donnée fournit un ensemble de documents à partir desquels les poids TF-IDF sont ré-estimés pour la prochaine itération. Des expériences menées sur un corpus couvrant différents formats des journaux télévisés issus de 8 chaînes françaises montrent une amélioration du processus global de segmentation.
							</p>

							<p id="taln-2013-court-030-key" class="mots_cles">
							<b>Mots clés : </b> Segmentation thématique, pondération TF-IDF, cohésion lexicale, TextTiling
							</p>

					</div>
					

					<div class="article">

						<b>Alexander Panchenko, Hubert Naets, Laetitia Brouwers, Pavel Romanov, Cédrick Fairon</b>


						<br/>

							<i>Recherche et visualisation de mots sémantiquement liés</i> <br/>

						<a href="actes/taln-2013-court-031.pdf">taln-2013-court-031</a> 
						<a href="bibtex/taln-2013-court-031.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-031-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-031-key');">mots clés</a> <br/>

							<p id="taln-2013-court-031-abs" class="resume">
							<b>Résumé : </b> Nous présentons PatternSim, une nouvelle mesure de similarité sémantique qui repose d’une part sur des patrons lexico-syntaxiques appliqués à de très vastes corpus et d’autre part sur une formule de réordonnancement des candidats extraits. Le système, initialement développé pour l’anglais, a été adapté au français. Nous rendons compte de cette adaptation, nous en proposons une évaluation et décrivons l’usage de ce nouveau modèle dans la plateforme de consultation en ligne Serelex.
							</p>

							<p id="taln-2013-court-031-key" class="mots_cles">
							<b>Mots clés : </b> Mesure de similarité sémantique, relations sémantiques
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Philippe Guilbaud, Christian Boitet, Vincent Berment</b>


						<br/>

							<i>Un analyseur morphologique étendu de l&#39;allemand traitant les formes verbales à particule séparée</i> <br/>

						<a href="actes/taln-2013-court-032.pdf">taln-2013-court-032</a> 
						<a href="bibtex/taln-2013-court-032.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-032-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-032-key');">mots clés</a> <br/>

							<p id="taln-2013-court-032-abs" class="resume">
							<b>Résumé : </b> Nous décrivons l’organisation et l&#39;état courant de l’analyseur morphologique de l’allemand AMALD de grande taille couvrant (près de 103000 lemmes et 500000 formes fléchies simples, en croissance) développé dans le cadre du projet ANR-Émergence Traouiero. C’est le premier lemmatiseur de l’allemand capable de traiter non seulement les mots simples et les mots composés, mais aussi les verbes à particules séparables quand elles sont séparées, même par un grand nombre de mots (ex : Hier schlagen wir eine neue Methode für die morphologische Analyse vor).
							</p>

							<p id="taln-2013-court-032-key" class="mots_cles">
							<b>Mots clés : </b> analyse morphologique, lemmatisation, allemand, verbes à particule séparable
							</p>

					</div>
					

					<div class="article">

						<b>Marc Vincent, Grégoire Winterstein</b>


						<br/>

							<i>Construction et exploitation d’un corpus français pour l’analyse de sentiment</i> <br/>

						<a href="actes/taln-2013-court-033.pdf">taln-2013-court-033</a> 
						<a href="bibtex/taln-2013-court-033.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-033-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-033-key');">mots clés</a> <br/>

							<p id="taln-2013-court-033-abs" class="resume">
							<b>Résumé : </b> Ce travail présente un corpus en français dédié à l’analyse de sentiment. Nous y décrivons la construction et l’organisation du corpus. Nous présentons ensuite les résultats de l’application de techniques d’apprentissage automatique pour la tâche de classification d’opinion (positive ou négative) véhiculée par un texte. Deux techniques sont utilisées : la régression logistique et la classification basée sur des Support Vector Machines (SVM). Nous mentionnons également l’intérêt d’appliquer une sélection de variables avant la classification (par régularisation par elastic net).
							</p>

							<p id="taln-2013-court-033-key" class="mots_cles">
							<b>Mots clés : </b> Analyse de sentiments, Corpus, Classification, Apprentissage automatique, Sélection de variable
							</p>

					</div>
					

					<div class="article">

						<b>Luka Nerima, Eric Wehrli</b>


						<br/>

							<i>Résolution d&#39;anaphores appliquée aux collocations: une évaluation préliminaire</i> <br/>

						<a href="actes/taln-2013-court-034.pdf">taln-2013-court-034</a> 
						<a href="bibtex/taln-2013-court-034.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-034-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-034-key');">mots clés</a> <br/>

							<p id="taln-2013-court-034-abs" class="resume">
							<b>Résumé : </b> Le traitement des collocations en analyse et en traduction est depuis de nombreuses années au centre de nos intérêts de recherche. L’analyseur Fips a été récemment enrichi d’un module de résolution d’anaphores. Dans cet article nous décrivons comment la résolution d’anaphores a été appliquée à l’identification des collocations et comment cela permet à l’analyseur de repérer une collocation même si un de ses termes a été pronominalisé. Nous décrivons aussi la méthodologie de l’évaluation, notamment la préparation des données pour le calcul du rappel. Dans la tâche d’identification des collocations pronominalisées, Fips montre des résultats très encourageants : la précision mesurée est de 98% alors que le rappel est proche de 50%. Dans cette évaluation nous nous intéressons aux collocations de type verbe-objet direct en conjonction avec les pronoms anaphoriques à la 3e personne. Le corpus utilisé est un corpus anglais d’environ dix millions de mots.
							</p>

							<p id="taln-2013-court-034-key" class="mots_cles">
							<b>Mots clés : </b> Analyse, résolution d’anaphores, pronoms personnels, collocations, corpus
							</p>

					</div>
					

					<div class="article">

						<b>Thibault Mondary, Adeline Nazarenko, Haïfa Zargayouna, Sabine Barreaux</b>


						<br/>

							<i>Aide à l’enrichissement d’un référentiel terminologique : propositions et expérimentations</i> <br/>

						<a href="actes/taln-2013-court-035.pdf">taln-2013-court-035</a> 
						<a href="bibtex/taln-2013-court-035.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-court-035-abs');">résumé</a>
							<a onclick="toggle('taln-2013-court-035-key');">mots clés</a> <br/>

							<p id="taln-2013-court-035-abs" class="resume">
							<b>Résumé : </b> En s’appuyant sur une expérience d’enrichissement terminologique, cet article montre comment assister le travail d’acquisition terminologique et surmonter concrètement les deux difficultés qu’il présente : la masse de candidats-termes à considérer et la subjectivité des jugements terminologiques qui varient notamment en fonction du type de terminologie à produire. Nous proposons des stratégies simples pour filtrer a priori une partie du bruit des résultats des extracteurs et rendre ainsi la validation praticable pour des terminologues et nous démontrons leur efficacité sur un échantillon de candidats-termes proposés à la validation de deux spécialistes du domaine. Nous montrons également qu’en appliquant à une campagne de validation terminologique les mêmes principes méthodologiques que pour une campagne d’annotation, on peut contrôler la qualité des jugements de validation posés et de la terminologie qui en résulte.
							</p>

							<p id="taln-2013-court-035-key" class="mots_cles">
							<b>Mots clés : </b> Acquisition terminologique, validation de candidats-termes, filtrage de termes, distance terminologique, vote, accord inter-juges
							</p>

					</div>
					

					

					

					

					

					

					

					

					

					

					

					

					

					

				<h1 id="démonstration">Démonstrations</h1>
			

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					

					<div class="article">

						<b>Gaël Lejeune, Romain Brixtel, Charlotte Lecluze, Antoine Doucet, Nadine Lucas</b>


						<br/>

							<i>DAnIEL : Veille épidémiologique multilingue parcimonieuse</i> <br/>

						<a href="actes/taln-2013-demo-001.pdf">taln-2013-demo-001</a> 
						<a href="bibtex/taln-2013-demo-001.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-001-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-001-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-001-abs" class="resume">
							<b>Résumé : </b> DAnIEL est un système multilingue de veille épidémiologique. DAnIEL permet de traiter un grand nombre de langues à faible coût grâce à une approche parcimonieuse en ressources.
							</p>

							<p id="taln-2013-demo-001-key" class="mots_cles">
							<b>Mots clés : </b> extraction d’information, recherche d’information, veille, multilinguisme, genre journalistique, grain caractère
							</p>

					</div>
					

					<div class="article">

						<b>Elena Kozlova, Maria Gontcharova, Tatiana Popova</b>


						<br/>

							<i>Lexique multilingue dans le cadre du modèle Compreno développé ABBYY</i> <br/>

						<a href="actes/taln-2013-demo-002.pdf">taln-2013-demo-002</a> 
						<a href="bibtex/taln-2013-demo-002.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-002-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-002-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-002-abs" class="resume">
							<b>Résumé : </b> Le lexique multilingue basé sur une hiérarchie sémantique universelle fait partie du modèle linguistique Compreno destiné à plusieurs applications du TALN, y compris la traduction automatique et l’analyse sémantique et syntaxique. La ressource est propriétaire et n’est pas librement disponible.
							</p>

							<p id="taln-2013-demo-002-key" class="mots_cles">
							<b>Mots clés : </b> Lexique multilingue, hiérarchie sémantique universelle, traduction automatique
							</p>

					</div>
					

					<div class="article">

						<b>Manon Quintana</b>


						<br/>

							<i>Inbenta Semantic Search Engine : un moteur de recherche sémantique inspiré de la Théorie Sens-Texte</i> <br/>

						<a href="actes/taln-2013-demo-003.pdf">taln-2013-demo-003</a> 
						<a href="bibtex/taln-2013-demo-003.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-003-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-003-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-003-abs" class="resume">
							<b>Résumé : </b> Avec la digitalisation massive de documents apparaît la nécessité de disposer de systèmes de recherche capables de s’adapter aux habitudes de recherche des utilisateurs et de leur permettre d’accéder à l’information rapidement et efficacement. INBENTA a ainsi créé un moteur de recherche intelligent appellé Inbenta Semantic Search Engine (ISSE). Les deux tâches principales de l’ISSE sont d’analyser les questions des utilisateurs et de trouver la réponse appropriée à la requête en effectuant une recherche dans une base de connaissances. Pour cela, la solution logicielle d’INBENTA se base sur la Théorie Sens-Texte qui se concentre sur le lexique et la sémantique.
							</p>

							<p id="taln-2013-demo-003-key" class="mots_cles">
							<b>Mots clés : </b> Moteur de Recherche Sémantique, Théorie Sens-Texte, fonction lexicale
							</p>

					</div>
					

					<div class="article">

						<b>Jean-Leon Bouraoui, Marc Canitrot</b>


						<br/>

							<i>FMO : un outil d’analyse automatique de l’opinion</i> <br/>

						<a href="actes/taln-2013-demo-004.pdf">taln-2013-demo-004</a> 
						<a href="bibtex/taln-2013-demo-004.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-004-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-004-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-004-abs" class="resume">
							<b>Résumé : </b> Nous décrivons notre prototype d’analyse automatique d’opinion. Celui-ci est basé sur un moteur d’analyse linguistique. Il permet de détecter finement les segments de texte porteurs d’opinions, de les extraire, et de leur attribuer une note selon la polarité qu’ils expriment. Nous présentons enfin les différentes perspectives que nous envisageons pour ce prototype.
							</p>

							<p id="taln-2013-demo-004-key" class="mots_cles">
							<b>Mots clés : </b> Analyse d’opinion, e-reputation, extraction d’information
							</p>

					</div>
					

					<div class="article">

						<b>Patrick Séguéla, Dominique Laurent</b>


						<br/>

							<i>Corriger, analyser et représenter le texte Synapse Développement</i> <br/>

						<a href="actes/taln-2013-demo-005.pdf">taln-2013-demo-005</a> 
						<a href="bibtex/taln-2013-demo-005.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-005-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-005-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-005-abs" class="resume">
							<b>Résumé : </b> Synapse Développement souhaite échanger avec les conférenciers autour des technologies qu&#39;elle commercialise : correction de textes et analyse sémantique. Plusieurs produits et démonstrateurs seront présentés, notre but étant d&#39;instaurer un dialogue et de confronter notre approche du TAL, à base de méthodes symboliques et statistiques influencées par des contraintes de production, et celles utilisées par les chercheurs, industriels ou passionnés qui viendront à notre rencontre.
							</p>

							<p id="taln-2013-demo-005-key" class="mots_cles">
							<b>Mots clés : </b> Correction grammaticale, analyse syntaxique, analyse sémantique, analyse d&#39;opinions
							</p>

					</div>
					

					<div class="article">

						<b>Xavier Tannier, Véronique Moriceau, Erwan Le Flem</b>


						<br/>

							<i>Une interface pour la validation et l’évaluation de chronologies thématiques</i> <br/>

						<a href="actes/taln-2013-demo-006.pdf">taln-2013-demo-006</a> 
						<a href="bibtex/taln-2013-demo-006.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-006-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-006-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-006-abs" class="resume">
							<b>Résumé : </b> Cet article décrit une interface graphique de visualisation de chronologies événementielles construites automatiquement à partir de requêtes thématiques en utilisant un corpus de dépêches fourni par l’Agence France Pressse (AFP). Cette interface permet également la validation des chronologies par des journalistes qui peuvent ainsi les éditer et les modifier.
							</p>

							<p id="taln-2013-demo-006-key" class="mots_cles">
							<b>Mots clés : </b> chronologie événementielle, évaluation, validation
							</p>

					</div>
					

					<div class="article">

						<b>Denis Maurel, Nathalie Friburger</b>


						<br/>

							<i>CasSys Un système libre de cascades de transducteurs</i> <br/>

						<a href="actes/taln-2013-demo-007.pdf">taln-2013-demo-007</a> 
						<a href="bibtex/taln-2013-demo-007.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-007-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-007-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-007-abs" class="resume">
							<b>Résumé : </b> CasSys est un système de création et de mise en oeuvre de cascades de transducteurs intégré à la plateforme Unitex. Nous présentons dans cette démonstration la nouvelle version implantée fin 2012. En particulier ont été ajoutées une interface plus conviviale et la possibilité d’itérer un même transducteur jusqu’à ce qu’il n’ait plus d’influence sur le texte. Un premier exemple concernera le traitement de texte avec une gestion complexe de balises XML et un deuxième présentera la cascade CasEN de reconnaissance des entités nommées.
							</p>

							<p id="taln-2013-demo-007-key" class="mots_cles">
							<b>Mots clés : </b> cascade de transducteurs, graphes Unitex, texte avec balises XML, reconnaissance d&#39;entités nommées
							</p>

					</div>
					

					<div class="article">

						<b>Lingxiao Wang, Ying Zhang</b>


						<br/>

							<i>iMAG : post-édition, évaluation de qualité de TA et production d&#39;un corpus parallèle</i> <br/>

						<a href="actes/taln-2013-demo-008.pdf">taln-2013-demo-008</a> 
						<a href="bibtex/taln-2013-demo-008.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-008-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-008-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-008-abs" class="resume">
							<b>Résumé : </b> Une passerelle interactive d’accès multilingue (iMAG) dédiée à un site Web S (iMAG-­‐S) est un bon outil pour rendre S accessible dans beaucoup de langues, immédiatement et sans responsabilité éditoriale. Les visiteurs de S ainsi que des post-­‐éditeurs et des modérateurs payés ou non contribuent à l’amélioration continue et incrémentale des segments textuels les plus importants, et éventuellement de tous. Dans cette approche, les pré-­‐traductions sont produites par un ou plusieurs systèmes de Traduction Automatique (TA) gratuits. Il y a deux effets de bord intéressants, obtenables sans coût additionnel : les iMAGs peuvent être utilisées pour produire des corpus parallèles de haute qualité, et pour mettre en place une évaluation permanente et finalisée de multiples systèmes de TA.
							</p>

							<p id="taln-2013-demo-008-key" class="mots_cles">
							<b>Mots clés : </b> post-édition, évaluation de systèmes de TA, production d’un corpus parallèle
							</p>

					</div>
					

					<div class="article">

						<b>David Rouquet</b>


						<br/>

							<i>Technologies du Web Sémantique pour l&#39;exploitation de données lexicales en réseau (Lexical Linked Data)</i> <br/>

						<a href="actes/taln-2013-demo-009.pdf">taln-2013-demo-009</a> 
						<a href="bibtex/taln-2013-demo-009.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-009-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-009-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-009-abs" class="resume">
							<b>Résumé : </b> Nous présentons un système basé sur les technologies du Web Sémantique pour la gestion, le développement et l&#39;exploitation de données lexicales en réseau (Lexical Linked Data, LLD).
							</p>

							<p id="taln-2013-demo-009-key" class="mots_cles">
							<b>Mots clés : </b> Lexical Linked Data, Lexique Multilingue, Pivot, Axies, Sparql, Spin
							</p>

					</div>
					

					<div class="article">

						<b>Achille Falaise</b>


						<br/>

							<i>Adaptation de la plateforme corporale ScienQuest pour l&#39;aide à la rédaction en langue seconde</i> <br/>

						<a href="actes/taln-2013-demo-010.pdf">taln-2013-demo-010</a> 
						<a href="bibtex/taln-2013-demo-010.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-010-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-010-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-010-abs" class="resume">
							<b>Résumé : </b> La plateforme ScienQuest fut initialement créée pour l&#39;étude linguistique du positionnement et du raisonnement dans le corpus Scientext. Cette démonstration présente les modifications apportées à cette plateforme, pour en faire une base phraséologique adaptée à l&#39;aide à la rédaction en langue seconde. Cette adaptation est utilisée dans le cadre de deux expérimentations en cours : l&#39;aide à la rédaction en anglais pour les scientifiques, et l&#39;aide à la rédaction académique en français pour les apprenants.
							</p>

							<p id="taln-2013-demo-010-key" class="mots_cles">
							<b>Mots clés : </b> Aide à la rédaction, langue seconde, ScienQuest, Scientext
							</p>

					</div>
					

					<div class="article">

						<b>Sébastian Peña Saldarriaga, Damien Vintache, Béatrice Daille</b>


						<br/>

							<i>Démonstrateur Apopsis pour l’analyse des tweets</i> <br/>

						<a href="actes/taln-2013-demo-011.pdf">taln-2013-demo-011</a> 
						<a href="bibtex/taln-2013-demo-011.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-011-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-011-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-011-abs" class="resume">
							<b>Résumé : </b> Le démonstrateur Apopsis permet de délimiter et de catégoriser les opinions émises sur les tweets en temps réel pour un sujet choisi par l’utilisateur au travers d’une interface web.
							</p>

							<p id="taln-2013-demo-011-key" class="mots_cles">
							<b>Mots clés : </b> fouille d’opinion, polarité, twitter
							</p>

					</div>
					

					<div class="article">

						<b>Frederik Cailliau, Ariane Cavet</b>


						<br/>

							<i>L’analyse des sentiments au service des centres d’appels</i> <br/>

						<a href="actes/taln-2013-demo-012.pdf">taln-2013-demo-012</a> 
						<a href="bibtex/taln-2013-demo-012.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-012-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-012-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-012-abs" class="resume">
							<b>Résumé : </b> Les conversations téléphoniques qui contiennent du sentiment négatif sont particulièrement intéressantes pour les centres d’appels, aussi bien pour évaluer la perception d’un produit par les clients que pour améliorer la formation des télé-conseillers. Néanmoins, ces conversations sont peu nombreuses et difficiles à trouver dans la masse d’enregistrements. Nous présentons un module d’analyse des sentiments qui permet de visualiser le déroulement émotionnel des conversations. Il se greffe sur un moteur de recherche, ce qui permet de trouver rapidement les conversations problématiques grâce à l’ordonnancement par score de négativité.
							</p>

							<p id="taln-2013-demo-012-key" class="mots_cles">
							<b>Mots clés : </b> analyse des sentiments, conversations téléphoniques, recherche d’information, parole spontanée, parole conversationnelle
							</p>

					</div>
					

					<div class="article">

						<b>Béatrice Daille, Rima Harastani</b>


						<br/>

							<i>TTC TermSuite alignement terminologique à partir de corpus comparables</i> <br/>

						<a href="actes/taln-2013-demo-013.pdf">taln-2013-demo-013</a> 
						<a href="bibtex/taln-2013-demo-013.bib">bibtex</a> 
							<a onclick="toggle('taln-2013-demo-013-abs');">résumé</a>
							<a onclick="toggle('taln-2013-demo-013-key');">mots clés</a> <br/>

							<p id="taln-2013-demo-013-abs" class="resume">
							<b>Résumé : </b> TermSuite est outil libre multilingue réalisant une extraction terminologique monolingue et une extraction terminologique bilingue à partir de corpus comparables.
							</p>

							<p id="taln-2013-demo-013-key" class="mots_cles">
							<b>Mots clés : </b> corpus comparable, extraction terminologique, alignement, UIMA
							</p>

					</div>
					


			</section>

			<footer>
				&copy; <a href="http://www.florianboudin.org">Florian Boudin</a>
			</footer>
			
		</div>
	</body>
</html>